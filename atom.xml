<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>yangyichao-mango&#39;s blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://yangyichao-mango.github.io/"/>
  <updated>2021-04-04T08:55:33.143Z</updated>
  <id>https://yangyichao-mango.github.io/</id>
  
  <author>
    <name>yangyichao-mango</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>深入浅出 | 全局一致性快照</title>
    <link href="https://yangyichao-mango.github.io/2021/09/12/wechat-blog/apache-flink:state-1/"/>
    <id>https://yangyichao-mango.github.io/2021/09/12/wechat-blog/apache-flink:state-1/</id>
    <published>2021-09-12T06:21:53.000Z</published>
    <updated>2021-04-04T08:55:33.143Z</updated>
    
    <content type="html"><![CDATA[<h1 id="深入浅出-全局一致性快照"><a href="#深入浅出-全局一致性快照" class="headerlink" title="深入浅出 | 全局一致性快照"></a>深入浅出 | 全局一致性快照</h1><blockquote><p>本系列每篇文章都比较短小，不定期更新，从一些实际的 case 出发抛砖引玉，提高小伙伴的姿♂势水平。本文介绍 Flink sink schema 字段设计小技巧，阅读时长大概 2 分钟，话不多说，直接进入正文！</p></blockquote><h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><ol><li>什么是状态？</li></ol><p>发散思维的去思考状态。我们所理解的状态不仅仅只限于 flink 的状态。让大家了解到状态是一个无处不在的东西</p><ol start="2"><li>什么是全局一致性快照？和状态有什么关系？</li></ol><p>全局一致性快照的一些生活、工作中应用的例子</p><ol start="3"><li>为什么需要一致性快照？全局一致性快照和 flink 的关系？</li></ol><p>jvm GC，分布式应用做故障恢复（比如 flink），死锁检测等</p><ol start="4"><li>全局一致性快照的分布式应用举例</li></ol><p>通过一个简单分布式应用介绍一下全局一致性状态是每时每刻都存在的。时间轴上的每一个时刻都存在一个全局一致性快照（类似拍照片）。flink 做 cp，sp，类似于每隔固定的时间从时间轴上的一个点拿出来这个时间点对应的一个全局一致性状态</p><ol start="5"><li>全局一致性快照的标准定义</li></ol><p>假如说有两个事件，a和b，在绝对时间下，如果a发生在b之前，且b被包含在快照当中，那么则a事件或者其对快照产生的影响也被包含在快照当中</p><p>6.怎么实现全局一致性快照？</p><p>同步去做，包括时钟同步、Stop-the-world，但是这两种方法都不可接受；<br>既然同步无法做，那如果异步能做出相同的全局一致性状态也可以</p><ol start="7"><li><p>分布式应用的全局一致性快照其 Process 状态和 Channel状态到底需要记录什么？其之间需要满足什么关系的一些思考？<br>不是必须要在同一时刻嘛，为啥还能异步去做？只要异步做出来的状态和同步做出来的状态效果一致也可以。</p></li><li><p>Chandy-Lamport 算法流程、例子</p></li></ol><p>介绍 Chandy-Lamport 算法流程并以一个例子介绍其执行过程</p><p>9.flink 实现的全局一致性快照介绍</p><h1 id="什么是状态？（了解状态）"><a href="#什么是状态？（了解状态）" class="headerlink" title="什么是状态？（了解状态）"></a>什么是状态？（了解状态）</h1><p>目标：首先想让大家发散思维的去思考状态？我们所理解的状态不仅仅只限于 flink 的状态。让大家了解到状态是一个普遍存在的东西<br>定义：就是当前计算需要依赖到之前计算的结果，那么之前计算的结果就是状态<br>举例：</p><ol><li>比如生活中的例子：为什么我知道这个是电脑，因为眼睛接收到外界的图案，然后我的大脑接收到这个图案后，拿记忆中存储的图案进行对比，匹配得到这是电脑。那么记忆中存储的图案就是状态；日久生情等都存在状态</li><li>比如 web server 应用中的状态：打开青藤 flink 页面，列表展示了羊艺超的归属任务。其中就是 web client 发了查询羊艺超的归属任务请求，web server 接收到请求之后，然后去 mysql 中进行查询匹配返回。那么 mysql 中存储的内容就是状态</li></ol><ol start="3"><li>比如 flink 应用中的状态：要去重，就要存储所有的 key；要获取当前最大值，那么历史最大值就是状态</li></ol><p>2.什么是全局一致性快照？（了解全局一致性快照）<br>全局：代表是一个分布式的<br>一致性快照：代表绝对时间的同一时刻的状态<br>相当于打开上帝视角，去观察同一时刻的应用所有的状态<br>其实这里的快照 = 状态，文章之后我可能会把这两个词混用，大家明白他们的意思一致即可</p><p>● 比如生活中的例子：比如拍了一个照片，那么照片的内容就是当时的一个全局一致性快照；每一个首脑都是一个进程，所有的进程的状态在同一时刻的组合就是一个全局一致性快照</p><p>● 比如分布式应用的例子：首先是一个分布式应用，它有多个进程分布在多个服务器上；其次，它在应用内部有自己的处理逻辑和状态；第三，应用间是可以互相通信的；第四，在这种分布式的应用，有内部状态，硬件可以通信的情况下；某一时刻的全局状态，就叫做全局的快照。</p><p>分布式应用某一时刻的全局一致性快照 = 各个 process 的本地状态 + channel 中正在传递的消息</p><p>3.为什么需要一致性快照？全局一致性快照和 flink 的关系？</p><p>● 实时案例<br>    ○ 做检查点（全局一致性快照）用来故障恢复，重点就在于我们不必要从历史起点开始重跑所有的数据；（1.kafka 不可能存储历史所有数据 2.重跑历史数据的情况下，时效性是达不到要求的）</p><pre><code>○ 可以做任务的死锁检测</code></pre><p>4.全局一致性快照的分布式应用案例？<br>通过一个简单分布式应用介绍一下全局一致性状态是每时每刻都存在的。时间轴上的每一个时刻都存在一个全局一致性快照（拍照片）。</p><p>示例<br>下面分布式应用的一个示例：</p><p>每时每刻都存在全局一致性快照<br>上面这个只是四个时刻的四个快照，其实应用的每一个时刻都存在一个全局一致性快照。</p><p>5.全局一致性快照的标准定义<br>定义<br>假如说有两个事件，a和b，在绝对时间下，如果a发生在b之前，且b被包含在快照当中，那么则a也被包含在快照当中。满足这个条件的全局快照，就称为全局一致性快照。</p><p>本质<br>就是如果将做了绝对时刻 T 的一个快照，那么这个绝对时刻 T 之前发生的所有事件以及其影响都会被包含在这个快照中</p><p>6.怎么实现全局一致性快照？<br>同步实现方式<br>● 同步实现方式：时钟同步<br>NTP（<a href="https://baike.baidu.com/item/NTP%E6%9C%8D%E5%8A%A1%E5%99%A8/8633994?fr=aladdin）">https://baike.baidu.com/item/NTP%E6%9C%8D%E5%8A%A1%E5%99%A8/8633994?fr=aladdin）</a>: NTP服务器【Network Time Protocol（NTP）】是用来使计算机时间同步化的一种协议，它可以使计算机对其服务器或时钟源（如石英钟，GPS等等)做同步化，它可以提供高精准度的时间校正（LAN上与标准间差小于1毫秒，WAN上几十毫秒）<br>结论：无法实现<br>● 同步实现方式：Stop-The-World（<a href="https://www.jianshu.com/p/b210f9db19a3）">https://www.jianshu.com/p/b210f9db19a3）</a></p><p>结论：不满足需求，无法采用</p><p>异步实现方式<br>如果同步实现方式不满足需求，那么能使用异步做到同步相同的快照也是可以满足需求的<br>● 异步实现方式：chandy-lamport<br>论文：<a href="https://www.microsoft.com/en-us/research/uploads/prod/2016/12/Determining-Global-States-of-a-Distributed-System.pdf?ranMID=24542&amp;ranEAID=J84DHJLQkR4&amp;ranSiteID=J84DHJLQkR4-mVoVymFnAblBx3zwyf98Pw&amp;epi=J84DHJLQkR4-mVoVymFnAblBx3zwyf98Pw&amp;irgwc=1&amp;OCID=AID2000142_aff_7593_1243925&amp;tduid=%28ir__1hs2uuow6wkfq3oxkk0sohzzwm2xpc33lxd0o6g200%29%287593%29%281243925%29%28J84DHJLQkR4-mVoVymFnAblBx3zwyf98Pw%29%28%29&amp;irclickid=_1hs2uuow6wkfq3oxkk0sohzzwm2xpc33lxd0o6g200">https://www.microsoft.com/en-us/research/uploads/prod/2016/12/Determining-Global-States-of-a-Distributed-System.pdf?ranMID=24542&amp;ranEAID=J84DHJLQkR4&amp;ranSiteID=J84DHJLQkR4-mVoVymFnAblBx3zwyf98Pw&amp;epi=J84DHJLQkR4-mVoVymFnAblBx3zwyf98Pw&amp;irgwc=1&amp;OCID=AID2000142_aff_7593_1243925&amp;tduid=%28ir__1hs2uuow6wkfq3oxkk0sohzzwm2xpc33lxd0o6g200%29%287593%29%281243925%29%28J84DHJLQkR4-mVoVymFnAblBx3zwyf98Pw%29%28%29&amp;irclickid=_1hs2uuow6wkfq3oxkk0sohzzwm2xpc33lxd0o6g200</a></p><p>7.分布式应用的全局一致性快照其 Process 状态和 Channel状态记录了什么？怎么记录 Channel 的状态？<br>Channel 记录了什么<br>分布式应用要记录的状态<br>如下图案例 Single-Token conservation，这是一个分布式应用，分布式应用中有 p 和 q 两个进程，p 可以通过 Channel pq 向 q 发消息，q 可以通过 Channel qp 向 p 发消息，其中有一个叫 token 的消息，在这个系统中一直不停的流转</p><p>如之前所述，分布式应用的全局一致性快照包含 Process 状态和 Channel 状态<br>那么上图 Single-Token conservation 示例中的<br>全局一致性快照 S = S(p) + S(Cpq) + S(q) + S(Cqp)<br>其中：<br>● S：全局一致性快照<br>● S(p)：p 进程的状态<br>● S(Cpq)：p 进程到 q 进程的 Channel 状态<br>● S(q)：q 进程的状态<br>● S(Cqp)：q 进程到 p 进程的 Channel 状态</p><p>问题：<br>这里大家可能会提到一个问题：做全局一致性快照时，其中 S(p)，S(q) 好理解，但是 S(Cpq)，S(Cqp) 到底应该记录什么东西？接下来详细讲讲我的理解<br>Process 状态应该记录什么内容<br>记录和用户业务需求相关的状态内容，用到了关于状态的地方，进行记录就好了<br>举例：uid 去重就存储历史所有的 uid 就可以了</p><p>Channel 状态应该记录什么内容</p><p>做全局一致性状态 token 应该都在相同的地方<br>全局一致性快照<br>token 在 p 时，对应第一张图，这时的全局一致性快照为：<br>S(token-in-p) = S(p-token-in-p) + S(Cpq-token-in-p) + S(q-token-in-p) + S(Cqp-token-in-p) ；<br>其中：<br>● S(token-in-p)：token 在 p 时，做的全局一致性快照<br>● S(p-token-in-p)：token 在 p 时，p 进程的状态<br>● S(Cpq-token-in-p)：token 在 p 时，p 进程到 q 进程的 Channel 状态<br>● S(q-token-in-p)：token 在 p 时，q 进程的状态<br>● S(Cqp-token-in-p)：token 在 p 时，q 进程到 p 进程的 Channel 状态</p><p>注意，上述这个表达式其实是结论，这个结论是很好理解的，但是你有想过站在实际应用的角度去思考怎样才能做一个实际快照时x下面的问题吗？</p><p>● 问题1：为什么都是 Process 和 Channel 做的快照都有 token-in-p 呢？<br>根据之前的拍照片的类比，当前这个绝对时刻做快照时，token 在 p；那么所有的 process 和 channel 记录状态时，token 都应该在 p。<br>● 问题2：S(p-token-in-p) 好理解，在这个时刻 token 还没有从 p 发出去，p 做快照时肯定知道 token 还在 p；但是站在 Cpq 做状态的角度来说</p><ol><li>Cpq 做状态时，怎么保障 Cpq 知道 token in p？需要我们探索下有什么方法怎么让 Cpq 在做状态时知道 token in p？</li><li>站在实际在应用中实现的角度时，满足怎样的数学条件（我们要开始实现一个真实的全局一致性快照啦，肯定会涉及到一些数据知识，别急，往后看，用到的数学知识并不复杂）才能做出一个 S(Cpq-token-in-p) 的状态？</li><li>同样的 S(q-token-in-p)，S(Cqp-token-in-p) 又怎么去做才能做到呢？</li></ol><p>首先我们得知道 S(Cpq-token-in-p)  存储了什么东西，然后我们才能知道做到存储出 S(Cpq-token-in-p) 这些东西需要满足什么条件：我们从上面的结论 S(token-in-p) = S(p-token-in-p) + S(Cpq-token-in-p) + S(q-token-in-p) + S(Cqp-token-in-p)  出发，来推导 Cpq 和 Cqp 这两个 Channel 做的状态到底存储了什么东西。</p><p>Cpq 记录 S(Cpq) 前应该满足的条件<br>条件1</p><ol><li>定义两个变量：<br>● n：在 p 的状态记录前，p 记录的 p 发往 Cpq 的 msg 数；<br>● n′：在 Cpq 的状态记录前，Cpq 记录的 p 发往 Cpq 的 msg 数；</li><li>结论：如果要让 Cpq 做状态时，知道 token in p（即满足 S(Cpq-token-in-p) 的全局状态），那么必然会有 n = n’；</li><li>反证法：如果 n != n’，则会有两种情况：<br>● n &gt; n’ 时：<br> ○ 可能会出现 n = 10（p 记录状态前，p 记录 p 发往 Cpq msg 数为 10（msg 编号 1 - 10））；<br> ○ n’ = 7（Cpq 记录状态前，Cpq 记录 p 发往 Cpq 的 msg 数为 7（msg 编号 1 - 7））；<br> ○ 那么假设 token 的编号为 9，就会出现 p 记录的状态为 S(p-token-in-Cpq)，Cpq 记录的状态为 S(p-token-in-p)，实际是不可能出现的；<br>● n &lt; n’ 时：<br> ○ 可能会出现 n = 7（p 记录状态前，p 记录 p 发往 Cpq msg 数为 7（编号 1 - 7））；<br> ○ n’ = 10（Cpq 记录状态前，Cpq 记录 p 发往 Cpq 的 msg 数为 10（编号 1 - 10））；<br> ○ 那么假设 token 的编号为 9，就会出现 p 记录的状态为 S(p-token-in-p)，Cpq 记录的状态为 S(p-token-in-Cpq)，实际是不可能出现的；<br>●  n = n’ 时：保障了无论什么情况下，只要 p 做出 S(p-token-in-p) 的状态时，因为 n = n’，代表 p 没有把 token 发出去，Cpq 也没有接受到 token，就能让 Cpq 也做出 S(Cpq-token-in-p)；</li></ol><p>Token in Cpq<br>全局一致性快照<br>token 在 Cpq 时，对应第二张图，这时的全局一致性状态为：<br>S(token-in-Cpq) = S(p-token-in-Cpq) + S(Cpq-token-in-Cpq) + S(q-token-inCpq) + S(Cqp-token-in-Cqp) ；<br>其中：<br>● S(token-in-Cpq) ：token 在 p 时，做的全局一致性快照<br>● S(p-token-in-Cpq) ：token 在 p 时，p 进程的状态<br>● S(Cpq-token-in-Cpq) ：token 在 p 时，p 进程到 q 进程的 Channel 状态<br>● S(q-token-in-p)：token 在 p 时，q 进程的状态<br>● S(Cqp-token-in-p)：token 在 p 时，q 进程到 p 进程的 Channel 状态</p><p>条件2</p><ol><li>定义两个变量：<br>● m：在 q 的状态记录前，q 记录的 q 从 Cpq 中接收到的 msg 数；<br>● m′：在 Cpq 的状态记录前，Cpq 记录的 q 从 Cpq 中接收到的 msg 数；</li><li>结论：如果要让 q 做状态时，知道 token in p（即做出满足 S(q-token-in-p) 的全局状态），那么必然会有 n′ ≥ m′ and n ≥ m and m = m’；</li><li>证明：<br>●  n′≥m′ and n≥m：在任何一种情况下，做全局一致性快照时，都会有 Cpq 下游接收到的 msg 数不可能超过 p 发送给 Channel 的 msg 条数，即：n′≥m′以及 n≥m<br>● m = m’：也是反证法，同上<br> ○ m &gt; m’ 时：<pre><code>■ 可能会出现 n = n&apos; = m &gt; m&apos;，q 记录状态前，Cpq 记录 q 从 Cpq 接收到的 msg 数为 10（编号 1 - 10，因为 n = n&apos; = m 也即 Cpq 记录的 p 发往 Cpq 的那些 msg）；■ Cpq记录状态前，Cpq 记录的 q 从 Cpq 接收到的 msg 数为 7（编号 1 - 7）；■ 那么假设 token 的编号为 9，就会出现 Cpq 记录的状态为 S(Cpq-token-in-Cpq)，q 记录的状态为 S(q-token-in-p)，实际是不可能出现的；</code></pre></li></ol><p>结论<br>在任何一种状态下，都会有 Cpq 下游接收到的 msg 数不可能超过 p 发送给 Channel 的 msg 条数，即：n′≥m′以及 n≥m</p><p>Channel 状态记录了什么内容？<br>一个 Channel 要记录的状态是，它 sender 记录自己状态之前它所接收到的 msg 列表，再减去 receiver 记录自己状态之前它已经收到的 msg 列表，减去的之后的数据列表就是还在通道中的数据列表，这个列表是需要 Channel 作为状态记录下来的。<br>而如果 n′=m′，那么 Channel c 中要记录的 msg 列表就是 empty 列表。如果 n′&gt;m′，那么要记录的列表是 (m′+1),…n′对应的 msg 列表。<br>至此，Channel 状态记录的内容也就确定了下来，那么我们应该怎么去记录 Channel 中的内容？</p><p>Chandy-Lamport 记录 Channel 状态中内容的方法<br>当消息在 Channel 上乱飞时，我们是无法记录这些消息作为 Channel 的状态的，但是这些消息终究会到达目的地，我们可以在消息的目的地去记录这些消息作为 Channel 的状态。那么具体怎么做的？就是在发送数据中插入一条特殊的数据 —— marker 数据，这条数据不会对计算有任何影响，即不会对应用的状态有任何影响，只是一个标识；<br>q 在没有记录自己的状态时，接收到了 Cpq 传来的 marker，那么 q 就开始记录自己的状态，并且把 Cpq 记录为空；如果 q 已经记录了自己的状态，那么在收到 marker 之前从 Cpq 接收到 msg 列表就是 Cpq 的状态</p><p>那么可以得到一个算子应该记录的状态就是自己的状态和 input Channel 的状态；</p><p>S_all = null;</p><p>// 1.假设 q 进程初始化发起快照的进程<br>S_q_all = S_q; // 初始化快照进程 q 做完快照，快照为 S_q</p><p>// 由于当前 q 已经做了快照，所以直接开始记录  y 个 input channel 的消息数据<br>for (int i = 1; i &lt;= y; i++) { // 假设当前源进程有 y 个 input channel<br>  S_Ciq = Message[m_iq + 1] + … + Message[n_iq];<br>  // n_iq 为 i 做快照前，i 发往 Ciq 的消息个数<br>  // m_iq 为 q 做快照前，q 收到 Ciq 的消息个数，其中 m_iq 可以que’d<br>  // 但是这里的 n_iq 不确定，所以要等待 i 做快照，并且 i 发往 Ciq 的消息全部到达 q，就知道 n_iq 是多少了<br>  S_q_all += S_Ciq;<br>}</p><p>S_all += S_q_all;</p><p>// 2.假设 p 进程为其他进程（非初始化发起快照进程）<br>S_p_all = S_p;</p><p>for (int i = 1; i &lt;= x; i++) { // 假设有 x 个 input channel<br>  S_Cip = Message[m_ip + 1] + … + Message[n_ip]; // n_ip 为 i 做快照前，i 发往 Cip 的消息个数，m_ip 为 p 做快照前，p 收到 Cip 的消息个数；<br>  // m_ip 可以在 p 做快照时确定下来<br>  // n_ip 得等 i 做快照，并且 i 发往 Ciq 的消息全部到达 p，就知道了 n_ip 是多少了</p><p>  S_p_all += S_Cip;<br>}</p><p>S_all += S_p_all;<br>marker 的功能：就是为了帮我们找出 n_iq 和 n_ip 的</p><p>接下来我们介绍介绍 Chandy-Lamport 是怎么做的。</p><p>8.Chandy-Lamport 算法流程、示例<br>算法流程</p><p>发起快照</p><p>解读：本次快照的起始点，先把起始点的快照给做了，然后发出 marker（这个 Marker 消息是干啥用的呢？？？），开始记录 input channel</p><p>执行快照</p><p>解读：<br>● Pi 记录本地快照，标记 Cki 为空：因为从 Cki 接收到了 marker，这时的状态是 Pk 刚刚做完快照，Pk 做完快照发往 Cki 的消息个数 = Pi 做完快照从 Cki 接收到的消息个数。即 n = n’ = m’ = m；即 Cki = [Empty]；<br>● Pi 开始向所有 output Channle 发 marker，开始记录除 Cki 之外的 input channel 消息，因为本地快照已经做完了；然后上游还有部分进程没有做完快照，为了记录除 Cki 之外的 input Channel 消息，</p><p>解读：结合前一张图说的开始记录 input channel 消息，Pi 停止记录 Cki 的消息，同时将此前记录所有 Cki 收到的消息作为本次快照中的最终状态；n’ &gt; m’，在 Pi 这里记录了 Cki 的状态，即 Cki = [m‘ + 1, m’ + 2…n]</p><p>终止快照</p><p>示例</p><p>9.flink 实现的全局一致性快照介绍（flink 容错机制）<br>Chandy-Lamport与 Flink之间的关系</p><p>论文：<a href="https://arxiv.org/pdf/1506.08603.pdf">https://arxiv.org/pdf/1506.08603.pdf</a></p><p>Flink 是分布式系统，所以 Flink 会采用全局一致性快照的方式形成检查点，来支持故障恢复。Flink的异步全局一致性快照算法跟Chandy-Lamport算法的区别主要有以下几点：<br>● 第一，Chandy-Lamput支持强连通图，而 Flink支持弱连通图；<br>● 第二，Flink采用的是裁剪的（Tailored）Chandy-Lamput异步快照算法；<br>● 第三，Flink的异步快照算法在DAG场景下不需要存储 Channel state，从而极大减少快照的存储空间。<br>flink 的容错机制</p><p>端到端的Exactly once<br>Exactly once的意思是，作业结果总是正确的，但是很可能产出多次；所以它的要求是需要有可重放的source。<br>端到端的Exactly once，是指作业结果正确且只会被产出一次，它的要求除了有可重放的source外，还要求有事务型的sink和可以接收幂等的产出结果。</p><p>flink 的全局一致性快照</p><p>Barrier 对齐</p><p>状态后端<br>JVM Heap<br>、<br>第一种，JVM Heap，它里面的数据是以Java对象形式存在的，读写也是以对象形式去完成的，所以速度很快。但是也存在两个弊端：第一个弊端，以对象方式存储所需的空间是磁盘上序列化压缩后的数据大小的很多倍，所以占用的内存空间很大；第二个弊端，虽然读写不用做序列化，但是在形成snapshot时需要做序列化，所以它的异步snapshot过程会比较慢。</p><p>RocksDB</p><p>第二种，RocksDB，这个类型在读写时就需要做序列化，所以它读写的速度比较慢。但是它有一个好处，基于LSM的数据结构在快照之后会形成sst文件，它的异步checkpoint过程就是文件拷贝的过程，CPU消耗会比较低。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>实时数据时效监控体系建设</title>
    <link href="https://yangyichao-mango.github.io/2020/12/01/wechat-blog/apache-flink:realtime-time-monitor/"/>
    <id>https://yangyichao-mango.github.io/2020/12/01/wechat-blog/apache-flink:realtime-time-monitor/</id>
    <published>2020-12-01T06:21:53.000Z</published>
    <updated>2020-12-27T05:43:46.288Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列每篇文章都比较短小，不定期更新，从一些实际的经历出发抛砖引玉，希望给小伙伴一些启发。<br>本文介绍了博主从做数据到玩数据的整个思考过程的转变，阅读时长大概 2 分钟，话不多说，直接进入正文！</p></blockquote><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>数据延迟为是实时数据的一个最大问题。<br>其实不管是哪部分产生延迟，最终的结果是都会直接影响到上层指标（数据质量问题 + 数据时效问题）。<br>为了方便我们在开发阶段，运维阶段快速定位、解决延迟导致的问题；<br>以及为后续可能的报警能力提供基础能力，因此需要建设实时数据流时效监控体系。</p><p>以一张图描述整个传输链路与耗时相关的问题。</p><p><img src="/blog-img/apache-flink:realtime-time-monitor/time-cost.png" alt="架构"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>上述图总共分为以下三部分。</p><ul><li><strong>数据延迟监控</strong></li><li><strong>数据乱序监控</strong></li><li><strong>数据加工延迟监控</strong></li></ul>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>生产实践 | Flink + 直播（三）| 如何建设当前正在直播 xx 数？</title>
    <link href="https://yangyichao-mango.github.io/2020/11/11/wechat-blog/apache-flink:realtime-live-stream-3/"/>
    <id>https://yangyichao-mango.github.io/2020/11/11/wechat-blog/apache-flink:realtime-live-stream-3/</id>
    <published>2020-11-11T06:21:53.000Z</published>
    <updated>2020-11-15T09:05:00.777Z</updated>
    
    <content type="html"><![CDATA[<h1 id="生产实践-Flink-直播（三）-如何建设当前正在直播-xx-数？"><a href="#生产实践-Flink-直播（三）-如何建设当前正在直播-xx-数？" class="headerlink" title="生产实践 | Flink + 直播（三）| 如何建设当前正在直播 xx 数？"></a>生产实践 | Flink + 直播（三）| 如何建设当前正在直播 xx 数？</h1><blockquote><p>本系列每篇文章都是从一些实际的 case 出发，分析一些生产环境中经常会遇到的问题，抛砖引玉，以帮助小伙伴们解决一些实际问题。本篇文章主要介绍直播间生产侧指标的建设过程，如果对小伙伴有帮助的话，欢迎点赞 + 再看~</p></blockquote><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p>本文主要介绍<strong>生产侧指标的建设</strong>，比如当前正在直播直播间数，或者主播数等。在介绍生产侧指标的建设过程之前，我们先回顾下上一节的<strong>架构</strong>图。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-3/tec-arc.png" alt="架构"></p><p>而本篇要介绍的<strong>生产侧指标</strong>的数据链路主要对应以下几个模块。</p><ul><li>数据源：读取直播生产，比如开播，关播等 kafka 数据源日志；</li><li>数据处理：使用生产侧数据源 + 实时画像维表 + flink 建设生产侧实时指标；</li><li>数据汇：将处理完成的指标数据写入到 kafka 中。</li></ul><p>我用另一张图进行了标注，图中<strong>标红</strong>模块为生产侧指标的数据链路涉及到的模块。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-3/metric-prod-tec-arc.png" alt="生产侧架构"></p><p>其中直播间实时画像维表的介绍已经在上节进行了介绍，感兴趣的话可以点击以下链接，跳转到上节进行阅读~</p><p>本小节就不针对<strong>生产侧指标的建设</strong>中所有涉及指标的建设过程进行详细介绍了，我们主要以<strong>当前分钟正在开播直播间数</strong>作为<strong>生产侧指标建设</strong>的一个代表性案例，介绍这个指标的整个建设过程。<br>来为大家还原生产侧指标的业务过程以及技术方案。</p><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><p>仍然从几个问题入手，介绍<strong>当前分钟正在开播直播间数</strong>的建设过程。</p><ul><li><strong>当前分钟正在开播直播间数</strong>的定义什么？业务过程是怎么样的？举例？</li><li>怎样去建设这个指标？整体的指标计算流程？</li></ul><h2 id="1-聊聊定义？"><a href="#1-聊聊定义？" class="headerlink" title="1.聊聊定义？"></a>1.聊聊定义？</h2><p>当前分钟正在开播直播间数，其定义就是整个平台中，当前分钟正在开播的直播间数 + 单层维度下钻的当前分钟正在开播的直播间数。</p><p>举例：</p><p>现在的时间点是 2020-11-11 12:42，真实直播的直播间数为 3000 个（平台维度下钻：IOS 平台为 1500，安卓平台为 1500）</p><p>到了 12:43 时，有 200 个直播间进行了关播（其中 100 个为 IOS，100 个为安卓），有 100 个直播间开播（全部为 IOS），则当前正在直播的直播间数为 2900（平台维度下钻：IOS 平台为 1500，安卓平台为 1400）。</p><p>其中 2020-11-11 12:42 的 3000 以及 2020-11-11 12:43 的 2900 以及按照平台下钻的数值就为当前时间正在开播的直播间数。</p><p>因此根据上述定义和分析，我们可以直接将数据源和数据汇的 schema 定义下来，主体信息如下。</p><h3 id="数据源-schema"><a href="#数据源-schema" class="headerlink" title="数据源 schema"></a>数据源 schema</h3><table><thead><tr><th>字段</th><th>备注</th></tr></thead><tbody><tr><td>live_stream_id</td><td>直播间 id</td></tr><tr><td>author_id</td><td>主播 id</td></tr><tr><td>start_or_end</td><td>开播还是关播</td></tr><tr><td>timestamp</td><td>时间戳</td></tr><tr><td>…</td><td>…</td></tr></tbody></table><h3 id="数据汇-schema"><a href="#数据汇-schema" class="headerlink" title="数据汇 schema"></a>数据汇 schema</h3><table><thead><tr><th>字段</th><th>备注</th></tr></thead><tbody><tr><td>timestamp</td><td>时间戳，汇总到分钟粒度</td></tr><tr><td>metric_name</td><td>指标名，举例：开播直播间数</td></tr><tr><td>metric_value</td><td>指标值，举例：3000（开播直播间数）</td></tr><tr><td>dim_name</td><td>维度名，举例：平台，版本</td></tr><tr><td>dim_value</td><td>维度值，举例：IOS，8.1</td></tr><tr><td>…</td><td>…</td></tr></tbody></table><blockquote><p>Notes:</p><p><strong>metric_name 和 metric_value</strong>：</p><p>这两个字段是为了之后进行指标扩充时进行的设计。比如后续如果需要加入开播主播数，开播时长等指标，不用修改数据汇 schema，只需要加一种 metric_name，就可以使用原有 schema 进行数据产出。</p><p><strong>dim_name 和 dim_value</strong>：</p><p>目前我们建设的指标只提供了进行单维度下钻的能力，所以设计了 dim_name 和 dim_value 两个字段，可满足用户查看平台为 IOS 的当前开播直播间数或者使用开播软件版本为 8.1 的当前开播直播间数。<br>如果后续业务场景需要多维下钻能力，可以在字段上面进行扩充。或者也可以提供明细数据在 OLAP 中进行多维下钻。</p></blockquote><h2 id="2-怎样建设？"><a href="#2-怎样建设？" class="headerlink" title="2.怎样建设？"></a>2.怎样建设？</h2><p>对于当前分钟正在开播直播间数来说，其计算方式很简单，就是下面这个数学公式：</p><p><strong>当前分钟正在开播直播间数</strong> = <strong>上一分钟正在开播直播间数</strong> + <strong>当前分钟开播直播间数</strong> - <strong>当前分钟关播直播间数</strong></p><p>可以从上面的公式可以看出，对于当前分钟正在开播直播间数的计算来说，是依赖上下文信息的，即<strong>上一分钟正在开播直播间数</strong>，这也就是我们所说的<strong>状态</strong>。</p><h3 id="指标处理逻辑"><a href="#指标处理逻辑" class="headerlink" title="指标处理逻辑"></a>指标处理逻辑</h3><p>从获取到数据源，到产出指标的整体处理逻辑如下图所示。这里就不进行赘述了。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-3/metric-current-live-live-stream-number-life-cycle.png" alt="技术架构"></p><p>其中标为<strong>粉色</strong>的模块为任务中的<strong>状态</strong>，即任务中一直存储的当前分钟正在开播直播间数。</p><h3 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h3><p>上述指标涉及到了，状态，那么我这里讲一下我对<strong>状态</strong>的理解。如有错误，请在文末讨论中进行指出，我会和大家讨论。</p><p>状态其实就是一个记录上下文信息的东西，如果当前的计算过程依赖到上次计算的结果，那么上次计算的结果就是状态。举几个🌰；</p><ul><li><p><strong>流处理</strong>：如本节介绍的<strong>当前分钟正在开播直播间数</strong>的计算，就是依赖上一分钟的正在开播直播间数（状态）进行的计算。<br>可能有小伙伴会说，我不依赖上一分钟，我从头开始计算可以不？答案是可以的，但是从头开始计算，也需要将所有历史数据进行存储，这些历史数据其实也就是状态，只不过我们将其优化为了上一分钟开播直播间数。</p></li><li><p><strong>批处理</strong>：今天的全量表 = 昨天全量表（状态） + 今天的增量表。</p></li><li><p><strong>数据库存储</strong>：最常见的 mysql 主键自增，unique key 等。<br>为什么新插入一条数据主键会自增？因为 mysql 存储了主键的上一个值（状态）。<br>为什么插入相同数据时，由于 unique key 会导致报错，就是因为 mysql 存储了所有 unique key 的字段的数据（状态）。</p></li><li><p><strong>生活</strong>：当前的手机电量 = 上一分钟的手机电量（状态） + （充电/用电量）。<br>为什么你越来越喜欢你的另一半？因为你对她的感觉 = 前一秒你对她的感觉（状态） + 当前这一秒她亲了你一下。</p></li></ul><p>生活中随处可见状态，即使你不是程序员，我相信也都可以理解状态的概念。</p><h3 id="指标计算代码示例"><a href="#指标计算代码示例" class="headerlink" title="指标计算代码示例"></a>指标计算代码示例</h3><p>按照最简单的实现方式举例如下。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LiveStreamRealtimeMetricProdProcessorJob</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStream&lt;SourceModel&gt; source = SourceFactory.getSourceDataStream(...);</span><br><span class="line"></span><br><span class="line">        DataStream&lt;SinkModel&gt; result = source</span><br><span class="line">                .keyBy(<span class="keyword">new</span> KeySelector&lt;SourceModel, Long&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Long <span class="title">getKey</span><span class="params">(SourceModel commonModel)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> commonModel.getLiveStreamId() % <span class="number">1000</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .timeWindow(Time.seconds(<span class="number">60</span>))</span><br><span class="line">                .process(<span class="keyword">new</span> ProcessWindowFunction&lt;SourceModel, SinkModel, Long, TimeWindow&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">private</span> ValueState&lt;Long&gt; playingLiveStreamNumberValueState;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                        <span class="keyword">this</span>.playingLiveStreamNumberValueState = getRuntimeContext().getState(...);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Long bucket, Context context, Iterable&lt;SourceModel&gt; iterable,</span></span></span><br><span class="line"><span class="function"><span class="params">                            Collector&lt;SinkModel&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        Long playingLiveStreamNumber = <span class="keyword">this</span>.playingLiveStreamNumberValueState.value();</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">if</span> (<span class="keyword">null</span> == playingLiveStreamNumber) &#123;</span><br><span class="line">                            playingLiveStreamNumber = <span class="number">0L</span>;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        List&lt;SourceModel&gt; sourceModels = (List&lt;SourceModel&gt;) iterable;</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">for</span> (SourceModel sourceModel : sourceModels) &#123;</span><br><span class="line">                            <span class="keyword">if</span> (BizType.I == sourceModel.getBizType()) &#123;</span><br><span class="line">                                playingLiveStreamNumber++;</span><br><span class="line">                            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                                playingLiveStreamNumber--;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">this</span>.playingLiveStreamNumberValueState.update(playingLiveStreamNumber);</span><br><span class="line"></span><br><span class="line">                        collector.collect(</span><br><span class="line">                                SinkModel.builder().build()</span><br><span class="line">                        );</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        SinkFactory.setSinkDataStream(...);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Data</span></span><br><span class="line">    <span class="meta">@Builder</span></span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SourceModel</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 直播间id</span></span><br><span class="line">        <span class="keyword">private</span> Long liveStreamId;</span><br><span class="line">        <span class="comment">// 开播时间，关播时间</span></span><br><span class="line">        <span class="keyword">private</span> Long time;</span><br><span class="line">        <span class="comment">// 主播id</span></span><br><span class="line">        <span class="keyword">private</span> Long authorId;</span><br><span class="line">        <span class="comment">// binlog 时间戳</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">long</span> binlogTimestamp;</span><br><span class="line">        <span class="comment">// 开播，关播</span></span><br><span class="line">        <span class="keyword">private</span> BizType bizType;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">enum</span> <span class="title">BizType</span> </span>&#123;</span><br><span class="line">        I, <span class="comment">// 开播</span></span><br><span class="line">        D, <span class="comment">// 关播</span></span><br><span class="line">        ;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Data</span></span><br><span class="line">    <span class="meta">@Builder</span></span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SinkModel</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 时间戳，汇总到分钟粒度</span></span><br><span class="line">        <span class="keyword">private</span> Long timestamp;</span><br><span class="line">        <span class="comment">// 指标名</span></span><br><span class="line">        <span class="keyword">private</span> String metricName;</span><br><span class="line">        <span class="comment">// 指标值</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">double</span> metricValue;</span><br><span class="line">        <span class="comment">// 维度名</span></span><br><span class="line">        <span class="keyword">private</span> String dimName;</span><br><span class="line">        <span class="comment">// 维度值</span></span><br><span class="line">        <span class="keyword">private</span> String dimValue;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文衔接上文，主要介绍直播间<strong>生产侧指标的建设</strong>，以<strong>当前分钟正在开播直播间数</strong>为代表举例。提出定义以及建设过程相关的问题，以这两个个问题出发，引出了以下两小节。</p><p>第一节简单介绍了当前分钟正在开播直播间数的定义。</p><p>第二节主要介绍了当前分钟正在开播直播间数的建设逻辑以及过程，并对<strong>状态</strong>这个概念进行了一个拓展介绍。</p><p>最后一节对本文进行了总结。</p><p>如果你也有相同的指标建设需求，或者存在一些指标建设过程中的问题，欢迎关注博主公众号，或者添加博主微信，互相交流~</p><p>记得点赞 + 再看喔~</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>生产实践 | 实时压测方案沉淀</title>
    <link href="https://yangyichao-mango.github.io/2020/11/11/wechat-blog/apache-flink:realtime-pressure/"/>
    <id>https://yangyichao-mango.github.io/2020/11/11/wechat-blog/apache-flink:realtime-pressure/</id>
    <published>2020-11-11T06:21:53.000Z</published>
    <updated>2020-11-17T13:28:30.671Z</updated>
    
    <content type="html"><![CDATA[<h1 id="生产实践-Flink-直播（三）-如何建设当前正在直播-xx-数？"><a href="#生产实践-Flink-直播（三）-如何建设当前正在直播-xx-数？" class="headerlink" title="生产实践 | Flink + 直播（三）| 如何建设当前正在直播 xx 数？"></a>生产实践 | Flink + 直播（三）| 如何建设当前正在直播 xx 数？</h1><blockquote><p>本系列每篇文章都是从一些实际的 case 出发，分析一些生产环境中经常会遇到的问题，抛砖引玉，以帮助小伙伴们解决一些实际问题。本篇文章主要介绍直播间生产侧指标的建设过程，如果对小伙伴有帮助的话，欢迎点赞 + 再看~</p></blockquote><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p>本文主要介绍<strong>生产侧指标的建设</strong>，比如当前正在直播直播间数，或者主播数等。在介绍生产侧指标的建设过程之前，我们先回顾下上一节的<strong>架构</strong>图。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-3/tec-arc.png" alt="架构"></p><p>而本篇要介绍的<strong>生产侧指标</strong>的数据链路主要对应以下几个模块。</p><ul><li>数据源：读取直播生产，比如开播，关播等 kafka 数据源日志；</li><li>数据处理：使用生产侧数据源 + 实时画像维表 + flink 建设生产侧实时指标；</li><li>数据汇：将处理完成的指标数据写入到 kafka 中。</li></ul><p>我用另一张图进行了标注，图中<strong>标红</strong>模块为生产侧指标的数据链路涉及到的模块。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-3/metric-prod-tec-arc.png" alt="生产侧架构"></p><p>其中直播间实时画像维表的介绍已经在上节进行了介绍，感兴趣的话可以点击以下链接，跳转到上节进行阅读~</p><p>本小节就不针对<strong>生产侧指标的建设</strong>中所有涉及指标的建设过程进行详细介绍了，我们主要以<strong>当前分钟正在开播直播间数</strong>作为<strong>生产侧指标建设</strong>的一个代表性案例，介绍这个指标的整个建设过程。<br>来为大家还原生产侧指标的业务过程以及技术方案。</p><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><p>仍然从几个问题入手，介绍<strong>当前分钟正在开播直播间数</strong>的建设过程。</p><ul><li><strong>当前分钟正在开播直播间数</strong>的定义什么？业务过程是怎么样的？举例？</li><li>怎样去建设这个指标？整体的指标计算流程？</li></ul><h2 id="1-聊聊定义？"><a href="#1-聊聊定义？" class="headerlink" title="1.聊聊定义？"></a>1.聊聊定义？</h2><p>当前分钟正在开播直播间数，其定义就是整个平台中，当前分钟正在开播的直播间数 + 单层维度下钻的当前分钟正在开播的直播间数。</p><p>举例：</p><p>现在的时间点是 2020-11-11 12:42，真实直播的直播间数为 3000 个（平台维度下钻：IOS 平台为 1500，安卓平台为 1500）</p><p>到了 12:43 时，有 200 个直播间进行了关播（其中 100 个为 IOS，100 个为安卓），有 100 个直播间开播（全部为 IOS），则当前正在直播的直播间数为 2900（平台维度下钻：IOS 平台为 1500，安卓平台为 1400）。</p><p>其中 2020-11-11 12:42 的 3000 以及 2020-11-11 12:43 的 2900 以及按照平台下钻的数值就为当前时间正在开播的直播间数。</p><p>因此根据上述定义和分析，我们可以直接将数据源和数据汇的 schema 定义下来，主体信息如下。</p><h3 id="数据源-schema"><a href="#数据源-schema" class="headerlink" title="数据源 schema"></a>数据源 schema</h3><table><thead><tr><th>字段</th><th>备注</th></tr></thead><tbody><tr><td>live_stream_id</td><td>直播间 id</td></tr><tr><td>author_id</td><td>主播 id</td></tr><tr><td>start_or_end</td><td>开播还是关播</td></tr><tr><td>timestamp</td><td>时间戳</td></tr><tr><td>…</td><td>…</td></tr></tbody></table><h3 id="数据汇-schema"><a href="#数据汇-schema" class="headerlink" title="数据汇 schema"></a>数据汇 schema</h3><table><thead><tr><th>字段</th><th>备注</th></tr></thead><tbody><tr><td>timestamp</td><td>时间戳，汇总到分钟粒度</td></tr><tr><td>metric_name</td><td>指标名，举例：开播直播间数</td></tr><tr><td>metric_value</td><td>指标值，举例：3000（开播直播间数）</td></tr><tr><td>dim_name</td><td>维度名，举例：平台，版本</td></tr><tr><td>dim_value</td><td>维度值，举例：IOS，8.1</td></tr><tr><td>…</td><td>…</td></tr></tbody></table><blockquote><p>Notes:</p><p><strong>metric_name 和 metric_value</strong>：</p><p>这两个字段是为了之后进行指标扩充时进行的设计。比如后续如果需要加入开播主播数，开播时长等指标，不用修改数据汇 schema，只需要加一种 metric_name，就可以使用原有 schema 进行数据产出。</p><p><strong>dim_name 和 dim_value</strong>：</p><p>目前我们建设的指标只提供了进行单维度下钻的能力，所以设计了 dim_name 和 dim_value 两个字段，可满足用户查看平台为 IOS 的当前开播直播间数或者使用开播软件版本为 8.1 的当前开播直播间数。<br>如果后续业务场景需要多维下钻能力，可以在字段上面进行扩充。或者也可以提供明细数据在 OLAP 中进行多维下钻。</p></blockquote><h2 id="2-怎样建设？"><a href="#2-怎样建设？" class="headerlink" title="2.怎样建设？"></a>2.怎样建设？</h2><p>对于当前分钟正在开播直播间数来说，其计算方式很简单，就是下面这个数学公式：</p><p><strong>当前分钟正在开播直播间数</strong> = <strong>上一分钟正在开播直播间数</strong> + <strong>当前分钟开播直播间数</strong> - <strong>当前分钟关播直播间数</strong></p><p>可以从上面的公式可以看出，对于当前分钟正在开播直播间数的计算来说，是依赖上下文信息的，即<strong>上一分钟正在开播直播间数</strong>，这也就是我们所说的<strong>状态</strong>。</p><h3 id="指标处理逻辑"><a href="#指标处理逻辑" class="headerlink" title="指标处理逻辑"></a>指标处理逻辑</h3><p>从获取到数据源，到产出指标的整体处理逻辑如下图所示。这里就不进行赘述了。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-3/metric-current-live-live-stream-number-life-cycle.png" alt="技术架构"></p><p>其中标为<strong>粉色</strong>的模块为任务中的<strong>状态</strong>，即任务中一直存储的当前分钟正在开播直播间数。</p><h3 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h3><p>上述指标涉及到了，状态，那么我这里讲一下我对<strong>状态</strong>的理解。如有错误，请在文末讨论中进行指出，我会和大家讨论。</p><p>状态其实就是一个记录上下文信息的东西，如果当前的计算过程依赖到上次计算的结果，那么上次计算的结果就是状态。举几个🌰；</p><ul><li><p><strong>流处理</strong>：如本节介绍的<strong>当前分钟正在开播直播间数</strong>的计算，就是依赖上一分钟的正在开播直播间数（状态）进行的计算。<br>可能有小伙伴会说，我不依赖上一分钟，我从头开始计算可以不？答案是可以的，但是从头开始计算，也需要将所有历史数据进行存储，这些历史数据其实也就是状态，只不过我们将其优化为了上一分钟开播直播间数。</p></li><li><p><strong>批处理</strong>：今天的全量表 = 昨天全量表（状态） + 今天的增量表。</p></li><li><p><strong>数据库存储</strong>：最常见的 mysql 主键自增，unique key 等。<br>为什么新插入一条数据主键会自增？因为 mysql 存储了主键的上一个值（状态）。<br>为什么插入相同数据时，由于 unique key 会导致报错，就是因为 mysql 存储了所有 unique key 的字段的数据（状态）。</p></li><li><p><strong>生活</strong>：当前的手机电量 = 上一分钟的手机电量（状态） + （充电/用电量）。<br>为什么你越来越喜欢你的另一半？因为你对她的感觉 = 前一秒你对她的感觉（状态） + 当前这一秒她亲了你一下。</p></li></ul><p>生活中随处可见状态，即使你不是程序员，我相信也都可以理解状态的概念。</p><h3 id="指标计算代码示例"><a href="#指标计算代码示例" class="headerlink" title="指标计算代码示例"></a>指标计算代码示例</h3><p>按照最简单的实现方式举例如下。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LiveStreamRealtimeMetricProdProcessorJob</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStream&lt;SourceModel&gt; source = SourceFactory.getSourceDataStream(...);</span><br><span class="line"></span><br><span class="line">        DataStream&lt;SinkModel&gt; result = source</span><br><span class="line">                .keyBy(<span class="keyword">new</span> KeySelector&lt;SourceModel, Long&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Long <span class="title">getKey</span><span class="params">(SourceModel commonModel)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> commonModel.getLiveStreamId() % <span class="number">1000</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .timeWindow(Time.seconds(<span class="number">60</span>))</span><br><span class="line">                .process(<span class="keyword">new</span> ProcessWindowFunction&lt;SourceModel, SinkModel, Long, TimeWindow&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">private</span> ValueState&lt;Long&gt; playingLiveStreamNumberValueState;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                        <span class="keyword">this</span>.playingLiveStreamNumberValueState = getRuntimeContext().getState(...);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Long bucket, Context context, Iterable&lt;SourceModel&gt; iterable,</span></span></span><br><span class="line"><span class="function"><span class="params">                            Collector&lt;SinkModel&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        Long playingLiveStreamNumber = <span class="keyword">this</span>.playingLiveStreamNumberValueState.value();</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">if</span> (<span class="keyword">null</span> == playingLiveStreamNumber) &#123;</span><br><span class="line">                            playingLiveStreamNumber = <span class="number">0L</span>;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        List&lt;SourceModel&gt; sourceModels = (List&lt;SourceModel&gt;) iterable;</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">for</span> (SourceModel sourceModel : sourceModels) &#123;</span><br><span class="line">                            <span class="keyword">if</span> (BizType.I == sourceModel.getBizType()) &#123;</span><br><span class="line">                                playingLiveStreamNumber++;</span><br><span class="line">                            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                                playingLiveStreamNumber--;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">this</span>.playingLiveStreamNumberValueState.update(playingLiveStreamNumber);</span><br><span class="line"></span><br><span class="line">                        collector.collect(</span><br><span class="line">                                SinkModel.builder().build()</span><br><span class="line">                        );</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        SinkFactory.setSinkDataStream(...);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Data</span></span><br><span class="line">    <span class="meta">@Builder</span></span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SourceModel</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 直播间id</span></span><br><span class="line">        <span class="keyword">private</span> Long liveStreamId;</span><br><span class="line">        <span class="comment">// 开播时间，关播时间</span></span><br><span class="line">        <span class="keyword">private</span> Long time;</span><br><span class="line">        <span class="comment">// 主播id</span></span><br><span class="line">        <span class="keyword">private</span> Long authorId;</span><br><span class="line">        <span class="comment">// binlog 时间戳</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">long</span> binlogTimestamp;</span><br><span class="line">        <span class="comment">// 开播，关播</span></span><br><span class="line">        <span class="keyword">private</span> BizType bizType;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">enum</span> <span class="title">BizType</span> </span>&#123;</span><br><span class="line">        I, <span class="comment">// 开播</span></span><br><span class="line">        D, <span class="comment">// 关播</span></span><br><span class="line">        ;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Data</span></span><br><span class="line">    <span class="meta">@Builder</span></span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SinkModel</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 时间戳，汇总到分钟粒度</span></span><br><span class="line">        <span class="keyword">private</span> Long timestamp;</span><br><span class="line">        <span class="comment">// 指标名</span></span><br><span class="line">        <span class="keyword">private</span> String metricName;</span><br><span class="line">        <span class="comment">// 指标值</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">double</span> metricValue;</span><br><span class="line">        <span class="comment">// 维度名</span></span><br><span class="line">        <span class="keyword">private</span> String dimName;</span><br><span class="line">        <span class="comment">// 维度值</span></span><br><span class="line">        <span class="keyword">private</span> String dimValue;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文衔接上文，主要介绍直播间<strong>生产侧指标的建设</strong>，以<strong>当前分钟正在开播直播间数</strong>为代表举例。提出定义以及建设过程相关的问题，以这两个个问题出发，引出了以下两小节。</p><p>第一节简单介绍了当前分钟正在开播直播间数的定义。</p><p>第二节主要介绍了当前分钟正在开播直播间数的建设逻辑以及过程，并对<strong>状态</strong>这个概念进行了一个拓展介绍。</p><p>最后一节对本文进行了总结。</p><p>如果你也有相同的指标建设需求，或者存在一些指标建设过程中的问题，欢迎关注博主公众号，或者添加博主微信，互相交流~</p><p>记得点赞 + 再看喔~</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>生产实践 | Flink + 直播（二）| 如何建设实时公共画像维表？</title>
    <link href="https://yangyichao-mango.github.io/2020/11/02/wechat-blog/apache-flink:realtime-live-stream-2/"/>
    <id>https://yangyichao-mango.github.io/2020/11/02/wechat-blog/apache-flink:realtime-live-stream-2/</id>
    <published>2020-11-02T06:21:53.000Z</published>
    <updated>2020-11-07T10:37:44.932Z</updated>
    
    <content type="html"><![CDATA[<h1 id="生产实践-Flink-直播（二）-如何建设实时公共画像维表？"><a href="#生产实践-Flink-直播（二）-如何建设实时公共画像维表？" class="headerlink" title="生产实践 | Flink + 直播（二）| 如何建设实时公共画像维表？"></a>生产实践 | Flink + 直播（二）| 如何建设实时公共画像维表？</h1><blockquote><p>本系列每篇文章都是从一些实际生产实践需求出发，解决一些生产实践中的问题，抛砖引玉，以帮助小伙伴们解决一些实际生产问题。本篇文章主要介绍直播间画像实时维表建设的整个过程，如果对小伙伴有帮助的话，欢迎点赞 + 再看~</p></blockquote><h2 id="技术架构"><a href="#技术架构" class="headerlink" title="技术架构"></a>技术架构</h2><p>回顾上一节的<strong>技术架构</strong>图。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-2/tec-arc.png" alt="技术架构"></p><p>整个架构相对来说是比较好理解的。从数据源到数据处理以及最后到数据汇部分。</p><p>但是大家的疑惑点可能就集中在三个维表的建设上，包含<strong>主播用户画像维表，观众用户画像维表，直播间画像维表</strong>。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-2/dim-tec-arc.png" alt="技术架构"></p><p>我们依然从以下几个角度的问题出发，通过分析场景，解答这几个问题来给大家介绍以上三个维表的建设过程。</p><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><ul><li><strong>WHAT：直播实时公共画像维表是指什么？离线公共画像维表又指什么？区别？</strong></li><li><strong>WHY：为什么架构图中的三类公共画像维表要按照实时和离线进行划分？为什么需要建设实时公共画像维表，离线公共画像维表不能满足需求？</strong></li><li><strong>HOW：怎样才能建设满足直播实时数据的实时公共画像维表？</strong></li><li><strong>WHO：需要使用什么样的组件建设直播实时公共画像维表？为什么选用这些组件进行建设？</strong></li></ul><h2 id="WHAT：实时-amp-离线公共画像维表？"><a href="#WHAT：实时-amp-离线公共画像维表？" class="headerlink" title="WHAT：实时 &amp; 离线公共画像维表？"></a>WHAT：实时 &amp; 离线公共画像维表？</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>首先简单介绍下，<strong>实时 &amp; 离线公共画像维表</strong>中存储的内容就是实体的固有属性（比如用户的年龄等），我理解这两个词本身是高层抽象的概念，本文中介绍的<strong>主播用户画像维表，观众用户画像维表，直播间画像维表</strong>是其具体实现。</p><p>其他大佬的文章解释中会对<strong>实时公共画像维表</strong> &amp; <strong>离线公共画像维表</strong>有更加深度的理解，这里我只说明我在直播实时数据建设过程中的理解~</p><h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><p>其实这两个词的区别从名字上就可以区分出来，实时公共画像维表和离线公共画像维表的最大区别就是数据建设和应用场景要求的时效性不同。</p><h3 id="离线公共画像维表"><a href="#离线公共画像维表" class="headerlink" title="离线公共画像维表"></a>离线公共画像维表</h3><p>特点：</p><ul><li><strong>场景</strong>：适合离线场景，<strong>时效性要求比较弱</strong>的场景，为指标提供画像维度填充或者打标服务</li><li><strong>建设</strong>：一般都是以离线 t + 1 的方式进行建设</li><li><strong>应用</strong>：使用的数据为离线 t + 1 的数据</li><li><strong>举例</strong>：数据仓库中的用户画像维表，为应用层数据提供画像服务；比如不但需要统计总 uv，还需要统计分年龄段的 uv。</li></ul><h3 id="实时公共画像维表"><a href="#实时公共画像维表" class="headerlink" title="实时公共画像维表"></a>实时公共画像维表</h3><p>特点：</p><ul><li><strong>场景</strong>：适合实时场景，<strong>时效性要求比较强</strong>的场景，为指标提供画像维度填充或者打标服务</li><li><strong>建设</strong>：实时的进行建设，延迟一般在秒级别</li><li><strong>应用</strong>：使用的数据都是实时建设好的，必须可以实时获取（秒级别延迟后获取到）并使用</li></ul><h2 id="WHY：为什么建设实时公共画像维表？"><a href="#WHY：为什么建设实时公共画像维表？" class="headerlink" title="WHY：为什么建设实时公共画像维表？"></a>WHY：为什么建设实时公共画像维表？</h2><p>为什么架构图中的三类公共画像维表要按照实时和离线进行划分？为什么需要建设实时公共画像维表，离线公共画像维表不能满足需求？</p><p>这几个问题其实围绕着我们的直播实时数据建设以及应用的场景就可以展开解答。</p><p>接上篇技术架构图，其中直播实时数据需要建设的公共维表分为以下三类：</p><ul><li><strong>直播间画像维表</strong>：包含直播对应的直播类别、开播客户端、标题、开播地址等信息</li><li><strong>主播画像维表</strong>：主播对应的主播名、主播类别、性别、年龄段等</li><li><strong>观众画像维表</strong>：观众对应的观众性别、年龄段等</li></ul><h3 id="直播间画像维表"><a href="#直播间画像维表" class="headerlink" title="直播间画像维表"></a>直播间画像维表</h3><p>首先抛出结论：<strong>直播间画像都是直播间的固有属性画像，直播间画像维表的建设过程是实时的</strong>。</p><p>由于大多数直播的时长都在几小时不等，随着直播的开始，主播域观众的互动也随即产生，从而直播生产和消费的指标也开始产出，随着直播的结束，主播和观众的互动也就结束了，对应的直播生产和消费指标也就不存在了，因此直播间画像的所能提供给其他指标作为维表的价值也就快速消失了，所以直播间画像（标题，开播地址）的应用场景特点就是<strong>时效性很强</strong>。<br>因此直播间画像维表对于直播生产消费指标的建设和应用来说，需要满足可实时建设、可实时查询获取的要求。</p><h3 id="主播-amp-观众用户画像维表"><a href="#主播-amp-观众用户画像维表" class="headerlink" title="主播 &amp; 观众用户画像维表"></a>主播 &amp; 观众用户画像维表</h3><p>结论：<strong>这类画像都是用户的固有属性画像，而非直播间固有属性，和直播间是非强相关的。主播 &amp; 观众用户画像维表的建设过程可以是离线的</strong>。</p><p>无论直播间的开播关播，直播过程中的生产消费，主播画像和观众画像基本上不会产生变动。<br>（举例：大多数情况下，当已经判定一个用户的年龄段画像为 18 - 23 时，即使这个用户开了 10 场直播，或者这个用户观看了 10 场直播，其年龄段判定也基本不会有变化）。<br>因此主播用户画像维表 &amp; 观众用户画像维表对于直播生产消费指标的建设和应用来说，可以满足离线 t + 1 建设，提供数据服务进行实时获取的要求。</p><blockquote><p>Notes：</p><p>主播 &amp; 观众用户画像需要根据用户生产消费行为以及其他信息，使用到机器学习进行性别和年龄段等的用户画像信息判定产出。<br>也有非常多的场景将这类画像进行实时建设，用于实时个性化推荐等。只不过本文的直播实时数据建设对于这两类画像的时效性要求较弱，所以采用了离线的方式进行建设。</p></blockquote><h2 id="HOW-WHO：怎样建设？用什么建设？"><a href="#HOW-WHO：怎样建设？用什么建设？" class="headerlink" title="HOW + WHO：怎样建设？用什么建设？"></a>HOW + WHO：怎样建设？用什么建设？</h2><h3 id="直播间生命周期-amp-数据流转"><a href="#直播间生命周期-amp-数据流转" class="headerlink" title="直播间生命周期 &amp; 数据流转"></a>直播间生命周期 &amp; 数据流转</h3><p>直播间整个生命周期如图所示。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-2/live-stream-life-cycle.png" alt="生命周期"></p><ul><li>1.主播创建直播间，直播间进入开播的状态；</li><li>2.观众进入直播间后，在直播间内与主播进行互动；</li><li>3.最后就是主播对直播间进行关播，标识着直播间生命周期的结束状态。</li></ul><h3 id="直播间画像维表-实时"><a href="#直播间画像维表-实时" class="headerlink" title="直播间画像维表-实时"></a>直播间画像维表-实时</h3><p>实时画像维表的建设。上图中<strong>红色</strong>的字体为实时画像维表的建设和应用过程。</p><h4 id="直播间画像实时数据流转"><a href="#直播间画像实时数据流转" class="headerlink" title="直播间画像实时数据流转"></a>直播间画像实时数据流转</h4><ul><li>1.当主播开播，直播间进行直播后，直播间产生了直播间画像信息，这时可以将画像信息实时的建设到直播间画像实时维表中。<br>并且可以同时建设生产侧的实时指标，利用建设好的<strong>直播间画像实时维表 + 主播 &amp; 观众画像离线维表</strong>进行生产侧指标的维度填充；</li><li>2.当观众进入直播间后，与主播进行互动，产生一系列的消费行为，随即可以建设消费侧的实时指标，利用建设的<strong>直播间画像实时维表 + 主播 &amp; 观众画像离线维表</strong>进行消费侧指标的维度填充；</li><li>3.当主播对直播间进行关播的时候，从直播间画像实时维表中就可以对该直播间的画像进行删除。</li></ul><h4 id="组件选型"><a href="#组件选型" class="headerlink" title="组件选型"></a>组件选型</h4><p>通过上文的分析，可以了解到直播间画像实时维表建设的要求如下：</p><ul><li>实时画像：首先需要支持实时建设，实时访问；</li><li>实时画像：建设的数据都为实时指标，即要求低延迟的请求响应时间；</li><li>公共画像：需要支撑多个大流量生产消费实时任务的访问请求，即提供高 QPS 画像数据服务；</li><li>公共画像：高稳定性。</li></ul><p>因此组件选型就自然落在了高速缓存的范畴中，我们最后经过方案对比之后，选择了 redis 作为我们的实时维表的存储引擎。</p><p>使用了 redis 中的 hash 作为维表存储结构，其中直播间画像维度存储设计如下图。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-2/live-stream-dim-redis-hash.png" alt="维度存储"></p><h4 id="flink-实时维表建设代码示例"><a href="#flink-实时维表建设代码示例" class="headerlink" title="flink 实时维表建设代码示例"></a>flink 实时维表建设代码示例</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LiveStreamRealtimeDimBuilderJob</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStream&lt;<span class="keyword">byte</span>[]&gt; source = SourceFactory.getSourceDataStream();</span><br><span class="line">        source.process(<span class="keyword">new</span> ProcessFunction&lt;<span class="keyword">byte</span>[], String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(<span class="keyword">byte</span>[] bytes, Context context, Collector&lt;String&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                CommonModel c = CommonModel.parseFrom(bytes);</span><br><span class="line">                <span class="comment">// 开播</span></span><br><span class="line">                <span class="keyword">if</span> (c.isStartLiveStream()) &#123;</span><br><span class="line">                    RedisConfig</span><br><span class="line">                            .get()</span><br><span class="line">                            .hmset(c.getLiveStreamId()</span><br><span class="line">                                    , ImmutableMap.&lt;String, String&gt;builder()</span><br><span class="line">                                            .put(<span class="string">&quot;type&quot;</span>, c.getType())</span><br><span class="line">                                            .put(<span class="string">&quot;client&quot;</span>, c.getClient())</span><br><span class="line">                                            .put(<span class="string">&quot;title&quot;</span>, c.getTitle())</span><br><span class="line">                                            .put(<span class="string">&quot;address&quot;</span>, c.getAddress())</span><br><span class="line">                                            .build()</span><br><span class="line">                            );</span><br><span class="line">                    RedisConfig</span><br><span class="line">                            .get()</span><br><span class="line">                            .expire(c.getLiveStreamId(), <span class="number">30</span> * <span class="number">24</span> * <span class="number">60</span> * <span class="number">60</span>);</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (c.isEndLiveStream()) &#123;</span><br><span class="line">                <span class="comment">// 关播</span></span><br><span class="line">                    RedisConfig</span><br><span class="line">                            .get()</span><br><span class="line">                            .expire(c.getLiveStreamId(), <span class="number">2</span> * <span class="number">24</span> * <span class="number">60</span> * <span class="number">60</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Data</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CommonModel</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> String liveStreamId; <span class="comment">// 直播间 id</span></span><br><span class="line">        <span class="keyword">private</span> String type; <span class="comment">// 直播间类型</span></span><br><span class="line">        <span class="keyword">private</span> String client; <span class="comment">// 开播客户端</span></span><br><span class="line">        <span class="keyword">private</span> String title; <span class="comment">// 直播间标题</span></span><br><span class="line">        <span class="keyword">private</span> String address; <span class="comment">// 直播间开播地址</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> CommonModel <span class="title">parseFrom</span><span class="params">(<span class="keyword">byte</span>[] bytes)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 逻辑根据业务逻辑判定</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isStartLiveStream</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 逻辑根据业务逻辑判定</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isEndLiveStream</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 逻辑根据业务逻辑判定</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="主播-amp-观众用户画像维表-离线"><a href="#主播-amp-观众用户画像维表-离线" class="headerlink" title="主播 &amp; 观众用户画像维表-离线"></a>主播 &amp; 观众用户画像维表-离线</h3><p>离线画像维表的建设。主要包含主播和观众的用户画像，性别，年龄等信息。如下图<strong>蓝色</strong>的字体为离线画像维表的应用过程。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-2/live-stream-life-cycle.png" alt="生命周期"></p><h4 id="主播-amp-观众画像数据流转"><a href="#主播-amp-观众画像数据流转" class="headerlink" title="主播 &amp; 观众画像数据流转"></a>主播 &amp; 观众画像数据流转</h4><p>在产出直播间生产侧、消费侧实时数据时，使用主播 &amp; 观众画像进行了画像维度填充。</p><h4 id="存储组件"><a href="#存储组件" class="headerlink" title="存储组件"></a>存储组件</h4><p>其中离线画像维表的存储组件选型与实时相同，同为 redis，画像信息存储方式也是使用 redis hash 结构进行存储。</p><p>以 t + 1 的方式进行画像数据建设并进行数据同步，将建设好的全量主播和观众用户画像同步到 redis 高速缓存当中。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文衔接上文，主要介绍直播间实时维表的建设过程。提出几个建设的问题，以这几个问题出发，引出了一下三小节。</p><p>第一节简单介绍了实时 &amp; 离线公共画像维表的概念。</p><p>第二节从数据应用场景的角度出发，介绍了为什么需要建设实时的公共画像维表。</p><p>第三节主要介绍了实时画像维表的建设过程以及详细的技术方案。</p><p>最后一节对本文进行了总结。</p><p>如果你也建设过实时画像维表，或者有相同的需求，欢迎留言或者留下你的文章链接，相互交流~</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yangyichao-mango.github.io/2020/11/01/wechat-blog/apache-flink:realtime-live-stream-1/"/>
    <id>https://yangyichao-mango.github.io/2020/11/01/wechat-blog/apache-flink:realtime-live-stream-1/</id>
    <published>2020-11-01T08:15:28.673Z</published>
    <updated>2020-11-01T08:15:28.674Z</updated>
    
    <content type="html"><![CDATA[<h1 id="生产实践-基于-Flink-的直播实时数据建设-（一）-需求和架构篇"><a href="#生产实践-基于-Flink-的直播实时数据建设-（一）-需求和架构篇" class="headerlink" title="生产实践 | 基于 Flink 的直播实时数据建设 （一）| 需求和架构篇"></a>生产实践 | 基于 Flink 的直播实时数据建设 （一）| 需求和架构篇</h1><blockquote><p>本系列每篇文章都是从一些实际生产实践需求出发，解决一些生产实践中的问题，抛砖引玉，以帮助小伙伴们解决一些实际生产问题。相信大家或多或少都观看过直播，那大家有没有想过，如果自己负责建设公司内整体直播实时数据，会怎样去建设呢？本系列文章主要介绍直播实时数据建设的整个过程，如果对小伙伴有帮助的话，欢迎点赞 + 再看~</p></blockquote><h2 id="首先思考几个问题"><a href="#首先思考几个问题" class="headerlink" title="首先思考几个问题"></a>首先思考几个问题</h2><ul><li><strong>WHAT：相信大家或多或少都观看过直播，甚至自己就是一名主播或负责的业务就是直播相关的，那大家有没有思考过，在直播业务场景中，你最关心什么指标以及需要关注、建设什么数据？</strong></li><li><strong>WHY：为什么需要建设直播实时数据？离线建设不能满足吗？</strong></li><li><strong>HOW：直播实时数据怎样赋能业务的？怎样根据公司直播场景的需求去划分直播实时数据？怎样去建设直播实时数据体？</strong></li><li><strong>WHO：在建设直播实时数据的过程中，需要使用什么样的组件进行建设？每个组件都负责哪一部分？</strong></li></ul><p>让我们带着以上几个问题出发~</p><h2 id="直播-短视频，内容运营的下一个战场"><a href="#直播-短视频，内容运营的下一个战场" class="headerlink" title="直播 + 短视频，内容运营的下一个战场"></a>直播 + 短视频，内容运营的下一个战场</h2><p>随着互联网络技术的发展，网络直播受到越来越多人的关注，直播在经过几年前的喷涌式大爆发之后，近段时间热度有所降低。内容的同质化和变现困难是直播现在面临的主要问题，随着移动终端普及和网络的提速，短视频以短平快的大流量传播方式快速获得各大平台、粉丝和资本的青睐，所以众多直播软件开始接入短视频的功能。<br>同时，一些以短视频为主发展起来的 app 也在软件中加入了直播功能，直播和短视频两者互相弥补不足，相辅相成，给用户带来了更好的使用体验，也给各大平台带来更多的流量，”直播 + 短视频”的模式已经也成为新的发展趋势。</p><p>本系列文章主要围绕着直播实时数据建设而展开。本文是本系列文章的的第一篇，需求和架构篇，主要分为三个部分，按顺序为<strong>WHY - WHAT - HOW</strong>，以这三个角度出发，解答开头提出的三个问题，其中 <strong>WHO</strong>部分在本系列文章的后续建设细节章节进行介绍！</p><h2 id="WHY：为什么建设直播实时数据？"><a href="#WHY：为什么建设直播实时数据？" class="headerlink" title="WHY：为什么建设直播实时数据？"></a>WHY：为什么建设直播实时数据？</h2><p>相比短视频的生产消费来说，直播的主播和观看直播的观众的纽带都是在直播间建立的，相互之间的互动行为也都只在直播间内产生，并且通常情况下，一场直播的时长也就在几个小时之内，因此直播的生产消费时效性相比短视频会更强，因而直播数据对于实时性的诉求也就更高。</p><h2 id="WHAT：需要关注、建设什么直播实时数据？"><a href="#WHAT：需要关注、建设什么直播实时数据？" class="headerlink" title="WHAT：需要关注、建设什么直播实时数据？"></a>WHAT：需要关注、建设什么直播实时数据？</h2><p>需要关注、建设什么直播实时数据？换一句话来说就是根据<strong>数据分析业务的需求</strong>出发，决定建设什么样的直播实时数据？</p><p>直播就是一个主播和观众联络互动的纽带，其中一切操作都是围绕着主播和观众而展开的，数据分析的同学都会以这个最基础的角度出发进行分析，因此首先我们就可以将整个直播的数据按照<strong>直播生产</strong>和<strong>直播消费</strong>进最基本的划分。</p><p>除此角度之外，数据分析的同学也还会从<strong>全局直播业务洞察</strong>和<strong>单个直播间洞察</strong>不同粒度上进行分析洞察，因此还可以按照<strong>大盘数据</strong>、<strong>单直播间数据</strong>进行划分。</p><p>从这两个角度出发，基本可以涵盖对于直播业务分析场景的诉求，因此直播实时数据也自然可以从这两个角度进行划分和建设。</p><p>综上则整体<strong>直播实时数据业务划分和赋能应用架构</strong>如下图所示。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-1/biz-arc.png" alt="业务划分和应用架构"></p><p>其中</p><p><strong>直播大盘实时数据</strong>在宏观上监控直播业务，提供预测大盘的能力；其中分钟粒度时间序列可快速定位直播各行为的高峰时刻，可以基于该时刻进行详细归因。除此之外，当直播在做运营活动时，也能快速基于实时数据来看运营活动的活动效果，赋能活动策略实时优化。</p><p><strong>单直播间直播实时数据</strong>可以以细粒度监控单直播间的直播业务，用来在直播过程中对外输出直播数据战报、以及可基于数据战报效果实时对单直播间进资源投放进行实时效果评估和合理调配。</p><p>详细的直播实时数据需求和样例如下文。</p><h3 id="大盘"><a href="#大盘" class="headerlink" title="大盘"></a>大盘</h3><p><strong>生产侧</strong></p><ul><li><strong>指标</strong>：总体开播直播间数…</li><li><strong>维度</strong>：直播间画像、主播用户画像</li><li><strong>举例</strong>：[开播直播间为游戏类直播]的[总开播主播数]</li></ul><p><strong>消费侧</strong></p><ul><li><strong>指标</strong>：总体观众观看、点赞、评论数…</li><li><strong>维度</strong>：观众用户画像、日志上报其他维度</li><li><strong>举例</strong>：[目前在河北观看直播]的[总观众数]</li></ul><h3 id="单直播间"><a href="#单直播间" class="headerlink" title="单直播间"></a>单直播间</h3><p><strong>生产侧</strong><br>单直播间一般都是一些画像信息，所以此类指标较少，暂时不做讨论。</p><p><strong>消费侧</strong></p><ul><li><strong>指标</strong>：单直播间观众观看、点赞、评论数…</li><li><strong>维度</strong>：观众用户画像、日志上报其他维度</li><li><strong>举例</strong>：某直播间[18-23岁年龄段]的[总观众数]</li></ul><p>目前已经了解了要建设直播实时数据都包含了什么内容，接下来就是大干一场的时候了。</p><h2 id="HOW：怎样去建设？"><a href="#HOW：怎样去建设？" class="headerlink" title="HOW：怎样去建设？"></a>HOW：怎样去建设？</h2><p>怎样去建设？换一句话来说就是从技术的角度出发，怎样将<strong>直播实时数据的业务需求</strong>转化为<strong>直播实时数据的技术方案</strong>进行落地？</p><p>从技术角度出发，上述直播实时数据需要建设的需求内容总结下来就是一个词：<strong>直播实时多维指标</strong>。</p><h3 id="多维"><a href="#多维" class="headerlink" title="多维"></a>多维</h3><p>即产出指标是多维度的，包含公共维度和非公共维度。</p><p>第一类是<strong>公共维度</strong>。包含三部分，直播间画像，主播用户画像，观众用户画像，公共两字代表这类维度是可以被多个指标进行共享使用的。举例：某直播间开播之后，该直播间画像只需要一次建设，就可以被多个指标多次重复使用，不但可以作为大盘侧生产、消费指标的维度，也可以作为单直播间生产、消费指标的维度。</p><p>第二类是<strong>非公共维度</strong>。非公共维度是和特定消费行为绑定的，也就是和某个指标绑定的，随着日志上报一同上报的维度。举例：某观众观看直播时的客户端类型（安卓？IOS？），观看直播时的省份等维度，这类维度只和当前的消费行为相关，不能被其他指标所共享。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-1/dim.png" alt="多维"></p><h3 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h3><p>其实都是 pv，uv 类指标。简单理解就是各个维度下对应的 xx 量。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-1/metric.png" alt="指标"></p><h3 id="实时数据建设技术架构"><a href="#实时数据建设技术架构" class="headerlink" title="实时数据建设技术架构"></a>实时数据建设技术架构</h3><p>对应到直播实时数据建设的过程主要包含两部分：公共部分和非公共部分。</p><p>公共部分就是实时公共维表的建设。</p><p>非公共部分就是指标非公共维度以及对应生产、消费指标建设。</p><p>直接给出总体<strong>技术架构</strong>图，本系列后续的文章进行介绍这样进行整体架构设计的详细原因。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-1/tec-arc.png" alt="技术架构"></p><p>简单说明下。</p><p>其中数据源包含生产侧，消费侧数据源；</p><p>数据处理部分包含公共实时维表建设，和指标建设，其中一部分公共维表的建设也使用了离线的方式提供了支持；</p><p>最后就是数据汇部分，产出了生产侧，消费侧的多维指标供数据分析师使用。</p><h2 id="下节预告"><a href="#下节预告" class="headerlink" title="下节预告"></a>下节预告</h2><p>下节主要介绍<strong>直播实时公共画像的建设</strong>，其中是技术架构图中的<strong>主播用户、关注用户画像、以及直播间画像</strong>的建设方案。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文首先提出了几个关于直播实时数据建设的问题。以这几个问题触发，引出了一下三小节。</p><p>第一节简单介绍了直播时效性强的原因，因此直播对于实时数据的需求更加强烈。</p><p>第二节从数据分析的角度出发，引出了我们需要建设的直播实时数据都包含哪些内容，并且从大盘/单直播间，生产/消费角度进行了模块划分。</p><p>第三节对数据需求进行了技术方案的整体架构设计。</p><p>最后一节对本文进行了总结。</p><p>如果你也有相同的建设需求或者你以及建设了直播实时数据，欢迎留言或者留下你的文章链接，相互交流~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;生产实践-基于-Flink-的直播实时数据建设-（一）-需求和架构篇&quot;&gt;&lt;a href=&quot;#生产实践-基于-Flink-的直播实时数据建设-（一）-需求和架构篇&quot; class=&quot;headerlink&quot; title=&quot;生产实践 | 基于 Flink 的直播实时数据建
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Tips | Flink 使用 union 代替 join、cogroup</title>
    <link href="https://yangyichao-mango.github.io/2020/10/03/wechat-blog/apache-flink:realtime-tips-2-union-join/"/>
    <id>https://yangyichao-mango.github.io/2020/10/03/wechat-blog/apache-flink:realtime-tips-2-union-join/</id>
    <published>2020-10-03T06:21:53.000Z</published>
    <updated>2020-10-04T13:21:52.892Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Tips-Flink-使用-union-代替-join、cogroup"><a href="#Tips-Flink-使用-union-代替-join、cogroup" class="headerlink" title="Tips | Flink 使用 union 代替 join、cogroup"></a>Tips | Flink 使用 union 代替 join、cogroup</h1><blockquote><p>本系列每篇文章都比较短小，不定期更新，从一些实际的 case 出发抛砖引玉，提高小伙伴的姿♂势水平。本文介绍在满足原有需求、实现原有逻辑的场景下，在 Flink 中使用 union 代替 cogroup(或者join) ，简化任务逻辑，提升任务性能的方法，阅读时长大概一分钟，话不多说，直接进入正文！</p></blockquote><h2 id="需求场景分析"><a href="#需求场景分析" class="headerlink" title="需求场景分析"></a>需求场景分析</h2><h3 id="需求场景"><a href="#需求场景" class="headerlink" title="需求场景"></a>需求场景</h3><p>需求诱诱诱来了。。。数据产品妹妹想要统计单个短视频粒度的<strong>点赞，播放，评论，分享，举报</strong>五类实时指标，并且汇总成 photo_id、1 分钟时间粒度的实时视频消费宽表（即宽表字段至少为：<strong>photo_id + play_cnt + like_cnt + comment_cnt + share_cnt + negative_cnt + minute_timestamp</strong>）产出至实时大屏。</p><p>问题在于对同一个视频，五类视频消费行为的触发机制以及上报时间是不同，也就决定了对实时处理来说五类行为日志对应着五个不同的数据源。sql boy 们自然就想到了 join 操作将五类消费行为日志合并，可是实时 join(cogroup) 真的那么完美咩~，下文细谈。</p><h3 id="source-输入以及特点"><a href="#source-输入以及特点" class="headerlink" title="source 输入以及特点"></a>source 输入以及特点</h3><p>首先我们分析下需求中的 source 特点：</p><ul><li>photo_id 粒度 play（播放）、like（点赞）、comment（评论）、share（分享）、negative（举报）明细数据，<strong>用户播放（点赞、评论…）n 次，客户端\服务端就会上传 n 条播放（点赞、评论…）日志至数据源</strong></li><li>五类视频消费行为日志的 source schema 都为：<strong>photo_id + timestamp + 其他维度</strong></li></ul><h3 id="sink-输出以及特点"><a href="#sink-输出以及特点" class="headerlink" title="sink 输出以及特点"></a>sink 输出以及特点</h3><p>sink 特点如下：</p><ul><li>photo_id 粒度 play（播放）、like（点赞）、comment（评论）、share（分享）、negative（举报）<strong>1 分钟级别窗口聚合数据</strong></li><li>实时视频消费宽表 sink schema 为：<strong>photo_id + play_cnt + like_cnt + comment_cnt + share_cnt + negative_cnt +  minute_timestamp</strong></li></ul><h3 id="source、sink-样例数据"><a href="#source、sink-样例数据" class="headerlink" title="source、sink 样例数据"></a>source、sink 样例数据</h3><p>source 数据：<br>| photo_id       | timestamp |         user_id | 说明 |<br>| ——— | – | ———– | ———– |<br>| 1     |  2020/10/3 11:30:33  |     3 | 播放 |<br>| 1   |  2020/10/3 11:30:33  |   4 | 播放 |<br>| 1   |  2020/10/3 11:30:33  |   5 | 播放 |<br>| 1   |  2020/10/3 11:30:33  |   4 | 点赞 |<br>| 2 |  2020/10/3 11:30:33  | 5 | 点赞 |<br>| 1 |  2020/10/3 11:30:33  | 5 | 评论 |</p><p>sink 数据：<br>| photo_id       | timestamp | play_cnt | like_cnt | comment_cnt<br>| ——— | – | ———– | ———– | ———– |<br>| 1     |  2020/10/3 11:30:00  |     3 | 1 | 1 |<br>| 2   |  2020/10/3 11:30:00  |   0 | 1 | 0 |</p><p>我们已经对数据源输入和输出有了完整的分析，那就瞧瞧有什么方案可以实现上述需求吧。</p><h2 id="实现方案"><a href="#实现方案" class="headerlink" title="实现方案"></a>实现方案</h2><ul><li>方案1：<strong>本小节 cogroup 方案</strong>直接消费原始日志数据，对五类不同的视频消费行为日志使用 cogroup 或者 join 进行窗口聚合计算</li><li>方案2：对五类不同的视频消费行为日志分别单独聚合计算出分钟粒度指标数据，下游再对聚合好的指标数据按照 photo_id 进行合并</li><li>方案3：<strong>本小节 union 方案</strong>既然数据源 schema 相同，直接对五类不同的视频消费行为日志做 union 操作，在后续的窗口函数中对五类指标进行聚合计算。后文介绍 union 方案的设计过程</li></ul><p>我们先上 cogroup 方案的示例代码。</p><h3 id="cogroup"><a href="#cogroup" class="headerlink" title="cogroup"></a>cogroup</h3><p>cogroup 实现示例如下，示例代码直接使用了处理时间（也可替换为事件时间~），因此对数据源的时间戳做了简化（直接干掉）：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Cogroup</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Long -&gt; photo_id 播放一次</span></span><br><span class="line">        DataStream&lt;Long&gt; play = SourceFactory.getDataStream(xxx);</span><br><span class="line">        <span class="comment">// Long -&gt; photo_id 点赞一次</span></span><br><span class="line">        DataStream&lt;Long&gt; like = SourceFactory.getDataStream(xxx);</span><br><span class="line">        <span class="comment">// Long -&gt; photo_id 评论一次</span></span><br><span class="line">        DataStream&lt;Long&gt; comment = SourceFactory.getDataStream(xxx);</span><br><span class="line">        <span class="comment">// Long -&gt; photo_id 分享一次</span></span><br><span class="line">        DataStream&lt;Long&gt; share = SourceFactory.getDataStream(xxx);</span><br><span class="line">        <span class="comment">// Long -&gt; photo_id 举报一次</span></span><br><span class="line">        DataStream&lt;Long&gt; negative = SourceFactory.getDataStream(xxx);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Tuple3&lt;Long, Long, Long&gt; -&gt; photo_id + play_cnt + like_cnt 播放和点赞的数据合并</span></span><br><span class="line">        DataStream&lt;Tuple3&lt;Long, Long, Long&gt;&gt; playAndLikeCnt = play</span><br><span class="line">            .coGroup(like)</span><br><span class="line">            .where(KeySelectorFactory.get(Function.identity()))</span><br><span class="line">            .equalTo(KeySelectorFactory.get(Function.identity()))</span><br><span class="line">            .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">60</span>)))</span><br><span class="line">            .apply(xxx1);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Tuple4&lt;Long, Long, Long, Long&gt; -&gt; photo_id + play_cnt + like_cnt + comment_cnt 播放、点赞、评论的数据合并</span></span><br><span class="line">        DataStream&lt;Tuple4&lt;Long, Long, Long, Long, Long&gt;&gt; playAndLikeAndComment = playAndLikeCnt</span><br><span class="line">            .coGroup(comment)</span><br><span class="line">            .where(KeySelectorFactory.get(playAndLikeModel -&gt; playAndLikeModel.f0))</span><br><span class="line">            .equalTo(KeySelectorFactory.get(Function.identity()))</span><br><span class="line">            .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">60</span>)))</span><br><span class="line">            .apply(xxx2);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Tuple5&lt;Long, Long, Long, Long, Long&gt; -&gt; photo_id + play_cnt + like_cnt + comment_cnt + share_cnt 播放、点赞、评论、分享的数据合并</span></span><br><span class="line">        DataStream&lt;Tuple5&lt;Long, Long, Long, Long, Long, Long&gt;&gt; playAndLikeAndCommentAndShare = playAndLikeAndComment</span><br><span class="line">            .coGroup(share)</span><br><span class="line">            .where(KeySelectorFactory.get(playAndLikeAndCommentModel -&gt; playAndLikeAndCommentModel.f0))</span><br><span class="line">            .equalTo(KeySelectorFactory.get(Function.identity()))</span><br><span class="line">            .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">60</span>)))</span><br><span class="line">            .apply(xxx2);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Tuple7&lt;Long, Long, Long, Long, Long, Long, Long&gt; -&gt; photo_id + play_cnt + like_cnt + comment_cnt + share_cnt + negative_cnt + minute_timestamp 播放、点赞、评论、分享、举报的数据合并</span></span><br><span class="line">        <span class="comment">// 同上~</span></span><br><span class="line">        DataStream&lt;Tuple7&lt;Long, Long, Long, Long, Long, Long, Long&gt;&gt; playAndLikeAndCommentAndShare = ***;</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>粗暴一想，上面这样一搞不就结束了么，事情没那么简单，我们来做一个详细点的分析。</p><h3 id="上述实现可能会存在的问题点"><a href="#上述实现可能会存在的问题点" class="headerlink" title="上述实现可能会存在的问题点"></a>上述实现可能会存在的问题点</h3><ul><li><strong>从 flink 消费到 play 数据源的一条数据到最终产出这条数据被聚合后的数据，整个过程的数据延迟 &gt; 3 分钟…</strong></li><li><strong>如果数据源持续增加（比如添加其他视频消费操作数据源），则整个任务算子变多，数据链路更长，任务稳定性会变差，产出数据延迟也会随着窗口计算变多，延迟更久</strong></li></ul><blockquote><p><strong>数据产品妹妹</strong>：🤩，小哥哥好棒，既然问题点都分析出来了，技术小哥哥就帮人家解决一下嘛~</p><p><strong>头文字 ∩ 技术小哥哥</strong>：搞。</p><p><strong>头文字 ∩ 技术小哥哥</strong>：既然可能由于过多的窗口导致数据产出延迟，job 不稳定，那有没有什么方法减少窗口数量呢，思路转换一下。我们直接以整个 job 中只包含一个窗口算子操作为基点，逆推一下，则有以下数据链路。</p></blockquote><h3 id="逆推链路"><a href="#逆推链路" class="headerlink" title="逆推链路"></a>逆推链路</h3><p>1 - 5 为逆推的整条链路。</p><ul><li><strong>1.五类指标的数据都在单个窗口中计算</strong></li><li><strong>2.五类指标的窗口 model 相同</strong></li><li><strong>3.keyby 中的 key 一致（photo_id）</strong></li><li><strong>4.五类指标的数据源都为 photo_id 粒度，并且五类数据源的 model 都必须相同，并且可以做合并</strong></li><li><strong>5.union 算子可以对五类数据源做合并！！！</strong></li></ul><p>话不多说直接上 union 方案代码。</p><h2 id="union"><a href="#union" class="headerlink" title="union"></a>union</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Union</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Tuple2&lt;Long, String&gt; -&gt; photo_id + &quot;PLAY&quot;标签</span></span><br><span class="line">        DataStream&lt;Tuple2&lt;Long, String&gt;&gt; play = SourceFactory.getDataStream(xxx);</span><br><span class="line">        <span class="comment">// Tuple2&lt;Long, String&gt; -&gt; photo_id + &quot;LIKE&quot;标签</span></span><br><span class="line">        DataStream&lt;Tuple2&lt;Long, String&gt;&gt; like = SourceFactory.getDataStream(xxx);</span><br><span class="line">        <span class="comment">// Tuple2&lt;Long, String&gt; -&gt; photo_id + &quot;COMMENT&quot;标签</span></span><br><span class="line">        DataStream&lt;Tuple2&lt;Long, String&gt;&gt; comment = SourceFactory.getDataStream(xxx);</span><br><span class="line">        <span class="comment">// Tuple2&lt;Long, String&gt; -&gt; photo_id + &quot;SHARE&quot;标签</span></span><br><span class="line">        DataStream&lt;Tuple2&lt;Long, String&gt;&gt; share = SourceFactory.getDataStream(xxx);</span><br><span class="line">        <span class="comment">// Tuple2&lt;Long, String&gt; -&gt; photo_id + &quot;NEGATIVE&quot;标签</span></span><br><span class="line">        DataStream&lt;Tuple2&lt;Long, String&gt;&gt; negative = SourceFactory.getDataStream(xxx);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Tuple5&lt;Long, Long, Long, Long&gt; -&gt; photo_id + play_cnt + like_cnt + comment_cnt + window_start_timestamp</span></span><br><span class="line">        DataStream&lt;Tuple3&lt;Long, Long, Long&gt;&gt; playAndLikeCnt = play</span><br><span class="line">            .union(like)</span><br><span class="line">            .union(comment)</span><br><span class="line">            .union(share)</span><br><span class="line">            .union(negative)</span><br><span class="line">            .keyBy(KeySelectorFactory.get(i -&gt; i.f0))</span><br><span class="line">            .timeWindow(Time.seconds(<span class="number">60</span>))</span><br><span class="line">            .process(xxx);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以发现，无论上游数据源怎样进行变化，上述 union 方案中始终可以保持只有一个窗口算子处理和计算数据，则可以解决之前列举的数据延迟以及 flink 任务算子过多的问题。</p><p>在数据源的 schema 相同（或者不同但经过处理之后可以 format 成相同格式）的情况下，或者处理逻辑相同的话，可以使用 union 进行逻辑简化。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文首先介绍了我们的需求场景，第二部分分析了使用 cogroup（案例代码）是如何解决此需求场景，再分析了此实现方案可能会存在一些问题，并引出了 union 解决方案的逆推和设计思路。<br>在第三部分针对此场景使用 union 代替 cogroup 进行了一定程度上的优化。如果针对此场景，大佬们有更好的优化方案的话，期待留言喔。</p><p><img src="/blog-img/gzh/wechat.png" alt="公众号"></p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>踩坑记 | Flink 天级别窗口中存在的时区问题</title>
    <link href="https://yangyichao-mango.github.io/2020/10/03/wechat-blog/apache-flink:realtime-tips-3-utc/"/>
    <id>https://yangyichao-mango.github.io/2020/10/03/wechat-blog/apache-flink:realtime-tips-3-utc/</id>
    <published>2020-10-03T06:21:53.000Z</published>
    <updated>2020-11-21T09:15:30.822Z</updated>
    
    <content type="html"><![CDATA[<h1 id="踩坑记-Flink-天级别窗口中存在的时区问题"><a href="#踩坑记-Flink-天级别窗口中存在的时区问题" class="headerlink" title="踩坑记 | Flink 天级别窗口中存在的时区问题"></a>踩坑记 | Flink 天级别窗口中存在的时区问题</h1><blockquote><p>本系列每篇文章都是从一些实际的 case 出发，分析一些生产环境中经常会遇到的问题，抛砖引玉，以帮助小伙伴们解决一些实际问题。本文介绍 Flink 时间以及时区问题，分析了在天级别的窗口时会遇到的时区问题，如果对小伙伴有帮助的话，欢迎点赞 + 再看~</p></blockquote><p>本文主要分为两部分：</p><p>第一部分（第 1 - 3 节）的分析主要针对 flink，分析了 flink 天级别窗口的中存在的时区问题以及解决方案。</p><p>第二部分（第 4 节）的分析可以作为所有时区问题的分析思路，主要以解决方案中的时区偏移量为什么是加 8 小时为案例做了通用的深度解析。</p><p>为了让读者能对本文探讨的问题有一个大致了解，本文先给出问题 sql，以及解决方案。后文给出详细的分析~</p><h2 id="1-问题以及解决方案"><a href="#1-问题以及解决方案" class="headerlink" title="1.问题以及解决方案"></a>1.问题以及解决方案</h2><h3 id="问题-sql"><a href="#问题-sql" class="headerlink" title="问题 sql"></a>问题 sql</h3><p>sql 很简单，用来统计当天累计 uv。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--------------- 伪代码 ---------------</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span></span><br><span class="line">  kafka_sink_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="comment">-- 窗口开始时间</span></span><br><span class="line">  <span class="built_in">CAST</span>(</span><br><span class="line">    TUMBLE_START(proctime, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>) <span class="keyword">AS</span> <span class="type">bigint</span></span><br><span class="line">  ) <span class="keyword">AS</span> window_start,</span><br><span class="line">  <span class="comment">-- 当前记录处理的时间</span></span><br><span class="line">  <span class="built_in">cast</span>(<span class="built_in">max</span>(proctime) <span class="keyword">AS</span> <span class="type">BIGINT</span>) <span class="keyword">AS</span> current_ts,</span><br><span class="line">  <span class="comment">-- 每个桶内的 uv</span></span><br><span class="line">  <span class="built_in">count</span>(<span class="keyword">DISTINCT</span> id) <span class="keyword">AS</span> part_daily_full_uv</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  kafka_source_table</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  <span class="built_in">mod</span>(id, bucket_number),</span><br><span class="line">  <span class="comment">-- bucket_number 为常数，根据具体场景指定具体数值</span></span><br><span class="line">  TUMBLE(proctime, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>)</span><br><span class="line"><span class="comment">--------------- 伪代码 ---------------</span></span><br></pre></td></tr></table></figure><p>你是否能一眼看出这个 sql 所存在的问题？（PS：数据源以及数据汇时区都为东八区）</p><p><strong>没错，天级别窗口所存在的时区问题，即这段代码统计的不是楼主所在东八区一整天数据的 uv，这段代码统计的一整天的范围在东八区是第一天早 8 点至第二天早 8 点。</strong></p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>楼主目前所处时区为东八区，解决方案如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--------------- 伪代码 ---------------</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> view_table <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">   id,</span><br><span class="line">   <span class="comment">-- 通过注入时间解决</span></span><br><span class="line">   <span class="comment">-- 加上东八区的时间偏移量，设置注入时间为时间戳列</span></span><br><span class="line">   <span class="built_in">CAST</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">AS</span> <span class="type">BIGINT</span>) <span class="operator">*</span> <span class="number">1000</span> <span class="operator">+</span> <span class="number">8</span> <span class="operator">*</span> <span class="number">60</span> <span class="operator">*</span> <span class="number">60</span> <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">as</span> ingest_time</span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">   source_table;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span></span><br><span class="line">  target_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="built_in">CAST</span>(</span><br><span class="line">    TUMBLE_START(ingest_time, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>) <span class="keyword">AS</span> <span class="type">bigint</span></span><br><span class="line">  ) <span class="keyword">AS</span> window_start,</span><br><span class="line">  <span class="built_in">cast</span>(<span class="built_in">max</span>(ingest_time) <span class="keyword">AS</span> <span class="type">BIGINT</span>) <span class="operator">-</span> <span class="number">8</span> <span class="operator">*</span> <span class="number">3600</span> <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">AS</span> current_ts,</span><br><span class="line">  <span class="built_in">count</span>(<span class="keyword">DISTINCT</span> id) <span class="keyword">AS</span> part_daily_full_uv</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  view_table</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  <span class="built_in">mod</span>(id, <span class="number">1024</span>),</span><br><span class="line">   <span class="comment">-- 根据注入时间划分天级别窗口</span></span><br><span class="line">  TUMBLE(ingest_time, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>)</span><br><span class="line"><span class="comment">--------------- 伪代码 ---------------</span></span><br></pre></td></tr></table></figure><p>通过上述方案，就可以将统计的数据时间范围调整为东八区的今日 0 点至明日 0 点。下文详细说明整个需求场景以及解决方案的实现和分析过程。</p><h2 id="2-需求场景以及实现方案"><a href="#2-需求场景以及实现方案" class="headerlink" title="2.需求场景以及实现方案"></a>2.需求场景以及实现方案</h2><h3 id="需求场景"><a href="#需求场景" class="headerlink" title="需求场景"></a>需求场景</h3><p>coming，需求场景比较简单，就是消费上游的一个埋点日志数据源，根据埋点中的 id 统计当天 0 点至当前时刻的累计 uv，按照分钟级别产出到下游 OLAP 引擎中进行简单的聚合，最后在 BI 看板进行展示，没有任何维度字段（感动到哭😭）。</p><h3 id="数据链路以及组件选型"><a href="#数据链路以及组件选型" class="headerlink" title="数据链路以及组件选型"></a>数据链路以及组件选型</h3><p>客户端用户行为埋点日志 -&gt; logServer -&gt; kafka -&gt; flink（sql） -&gt; kafka -&gt; druid -&gt; BI 看板。</p><p>实现方案以及具体的实现方式很多，这次使用的是 sql API。</p><h3 id="flink-sql-schema"><a href="#flink-sql-schema" class="headerlink" title="flink sql schema"></a>flink sql schema</h3><p>source 和 sink 表 schema 如下（只保留关键字段）：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--------------- 伪代码 ---------------</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> kafka_sink_table (</span><br><span class="line">  <span class="comment">-- 天级别窗口开始时间</span></span><br><span class="line">  window_start <span class="type">BIGINT</span>,</span><br><span class="line">  <span class="comment">-- 当前记录处理的时间</span></span><br><span class="line">  current_ts <span class="type">BIGINT</span>,</span><br><span class="line">  <span class="comment">-- 每个桶内的 uv（处理过程对 id 进行了分桶）</span></span><br><span class="line">  part_daily_full_uv <span class="type">BIGINT</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"> <span class="comment">-- ... </span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> kafka_source_table (</span><br><span class="line">  <span class="comment">-- ... </span></span><br><span class="line">  <span class="comment">-- 需要进行 uv 计算的 id</span></span><br><span class="line">  id <span class="type">BIGINT</span>,</span><br><span class="line">  <span class="comment">-- 处理时间</span></span><br><span class="line">  proctime <span class="keyword">AS</span> PROCTIME()</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="comment">-- ... </span></span><br><span class="line">);</span><br><span class="line"><span class="comment">--------------- 伪代码 ---------------</span></span><br></pre></td></tr></table></figure><h3 id="flink-sql-transform"><a href="#flink-sql-transform" class="headerlink" title="flink sql transform"></a>flink sql transform</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--------------- 伪代码 ---------------</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span></span><br><span class="line">  kafka_sink_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="comment">-- 窗口开始时间</span></span><br><span class="line">  <span class="built_in">CAST</span>(</span><br><span class="line">    TUMBLE_START(proctime, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>) <span class="keyword">AS</span> <span class="type">bigint</span></span><br><span class="line">  ) <span class="keyword">AS</span> window_start,</span><br><span class="line">  <span class="comment">-- 当前记录处理的时间</span></span><br><span class="line">  <span class="built_in">cast</span>(<span class="built_in">max</span>(proctime) <span class="keyword">AS</span> <span class="type">BIGINT</span>) <span class="keyword">AS</span> current_ts,</span><br><span class="line">  <span class="comment">-- 每个桶内的 uv</span></span><br><span class="line">  <span class="built_in">count</span>(<span class="keyword">DISTINCT</span> id) <span class="keyword">AS</span> part_daily_full_uv</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  kafka_source_table</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  <span class="built_in">mod</span>(id, bucket_number),</span><br><span class="line">  <span class="comment">-- bucket_number 为常数，根据具体场景指定具体数值</span></span><br><span class="line">  TUMBLE(proctime, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>)</span><br><span class="line"><span class="comment">--------------- 伪代码 ---------------</span></span><br></pre></td></tr></table></figure><p>使用 early-fire 机制（同 DataStream API 中的 ContinuousProcessingTimeTrigger），并设定触发间隔为 60 s。</p><p>在上述实现 sql 中，我们对 id 进行了分桶，那么每分钟输出的数据条数即为 bucket_number 条，最终在 druid 中按照分钟粒度将所有桶的数据进行 sum 聚合，即可得到从当天 0 点累计到当前分钟的全量 uv。</p><h3 id="时区问题"><a href="#时区问题" class="headerlink" title="时区问题"></a>时区问题</h3><blockquote><p>激情场景还原：</p><p><strong>头文字 ∩ 技术小哥哥</strong>：使用 sql，easy game，闲坐摸鱼…</p><p><strong>头文字 ∩ 技术小哥哥</strong>：等到 <strong>00:00</strong> 时，发现指标还在不停地往上涨，难道是 sql 逻辑错了，不应该啊，试过分钟，小时级别窗口都木有这个问题</p><p><strong>头文字 ∩ 技术小哥哥</strong>：抠头ing，算了，稍后再分析这个问题吧，现在还有正事要干😏</p><p><strong>头文字 ∩ 技术小哥哥</strong>：到了早上，瞅了一眼配置的时间序列报表，发现在 <strong>08:00</strong> 点的时候指标归零，重新开始累计。想法一闪而过，东八区？（当时为啥没 format 下 sink 数据中的 window_start…）</p></blockquote><h2 id="3-问题定位"><a href="#3-问题定位" class="headerlink" title="3.问题定位"></a>3.问题定位</h2><h3 id="问题说明"><a href="#问题说明" class="headerlink" title="问题说明"></a>问题说明</h3><p>flink 在使用时间的这个概念的时候是基于 java 时间纪元（即格林威治 1970/01/01 00:00:00，也即 Unix 时间戳为 0）概念的，窗口对齐以及触发也是基于 <a href="https://cloud.tencent.com/developer/article/1447368" title="java 时间纪元">java 时间纪元</a>。</p><h3 id="问题场景复现"><a href="#问题场景复现" class="headerlink" title="问题场景复现"></a>问题场景复现</h3><p>可以通过直接查看 sink 数据的 window_start 得出上述结论。</p><p>但为了还原整个过程，我们按照如下 source 和 sink 数据进行整个问题的复现：</p><p>source 数据如下：<br>| id       | proctime | proctime UTC + 0（格林威治） 格式化时间 | proctime UTC + 8（北京） 格式化时间<br>| ——— | – | – | – |<br>| 1     | 1599091140000     | 2020/09/02 23:59:00     |  2020/09/03 07:59:00  |<br>| 2     | 1599091140000     | 2020/09/02 23:59:00     |  2020/09/03 07:59:00  |<br>| 3     | 1599091140000     | 2020/09/02 23:59:00     |  2020/09/03 07:59:00  |<br>| 1     | 1599091200000     | 2020/09/03 00:00:00     |  2020/09/03 08:00:00  |<br>| 2     | 1599091200000     | 2020/09/03 00:00:00     |  2020/09/03 08:00:00  |<br>| 3     | 1599091260000     | 2020/09/03 00:01:00     |  2020/09/03 08:01:00  |</p><p>sink 数据（<strong>为了方便理解，直接按照 druid 聚合之后的数据展示</strong>）：<br>| window_start       | current_ts | part_daily_full_uv | window_start  UTC + 8（北京） 格式化时间 | current_ts  UTC + 8（北京） 格式化时间<br>| ——— | – | ———– | ———– | ———– |<br>| 1599004800000     |  1599091140000  | 3 | 2020/09/02 08:00:00 | 2020/09/03 07:59:00 |<br>| 1599091200000     |  1599091200000  | 2 | 2020/09/03 08:00:00 | 2020/09/03 08:00:00 |<br>| 1599091200000     |  1599091260000  | 3 | 2020/09/03 08:00:00 | 2020/09/03 08:01:00 |</p><p>从上述数据可以发现，天级别窗口<strong>开始时间</strong>在 UTC + 8（北京）的时区是每天早上 8 点，即 UTC + 0（格林威治）的凌晨 0 点。</p><p><strong>下文先给出解决方案，然后详细解析各个时间以及时区概念~</strong></p><h3 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h3><ul><li><strong>框架层面解决</strong>：<a href="https://www.alibabacloud.com/help/zh/doc-detail/96910.htm?spm=a2c63.p38356.b99.48.28613830DXb6FQ" title="Blink Planner 时区设置">Blink Planner 支持时区设置</a></li><li><strong>sql层面解决</strong>：从 sql 实现层面给出解决方案</li></ul><h3 id="sql-层面解决方案"><a href="#sql-层面解决方案" class="headerlink" title="sql 层面解决方案"></a>sql 层面解决方案</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--------------- 伪代码 ---------------</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> view_table <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">   id,</span><br><span class="line">   <span class="comment">-- 通过注入时间解决</span></span><br><span class="line">   <span class="comment">-- 加上东八区的时间偏移量，设置注入时间为时间戳列</span></span><br><span class="line">   <span class="built_in">CAST</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">AS</span> <span class="type">BIGINT</span>) <span class="operator">*</span> <span class="number">1000</span> <span class="operator">+</span> <span class="number">8</span> <span class="operator">*</span> <span class="number">60</span> <span class="operator">*</span> <span class="number">60</span> <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">as</span> ingest_time</span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">   source_table;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span></span><br><span class="line">  target_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="built_in">CAST</span>(</span><br><span class="line">    TUMBLE_START(ingest_time, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>) <span class="keyword">AS</span> <span class="type">bigint</span></span><br><span class="line">  ) <span class="keyword">AS</span> window_start,</span><br><span class="line">  <span class="built_in">cast</span>(<span class="built_in">max</span>(ingest_time) <span class="keyword">AS</span> <span class="type">BIGINT</span>) <span class="operator">-</span> <span class="number">8</span> <span class="operator">*</span> <span class="number">3600</span> <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">AS</span> current_ts,</span><br><span class="line">  <span class="built_in">count</span>(<span class="keyword">DISTINCT</span> id) <span class="keyword">AS</span> part_daily_full_uv</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  view_table</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  <span class="built_in">mod</span>(id, <span class="number">1024</span>),</span><br><span class="line">   <span class="comment">-- 根据注入时间划分天级别窗口</span></span><br><span class="line">  TUMBLE(ingest_time, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>)</span><br><span class="line"><span class="comment">--------------- 伪代码 ---------------</span></span><br></pre></td></tr></table></figure><p>我目前所属的时区是东八区（北京时间），通过上述 sql，设置注入时间，并对注入时间加上 8 小时的偏移量进行天级别窗口的划分，就可以对此问题进行解决（也可以在 create table 时，在 schema 中根据计算列添加对应的注入时间戳进行解决）。如果你在 sql 层面有更好的解决方案，欢迎讨论~</p><blockquote><p>Notes：</p><ul><li><strong>东 n 区的解决方案就是时间戳 +n * 3600 秒的偏移量，西 n 区的解决方案就是时间戳 -n * 3600 秒的偏移量</strong></li><li><strong>DataStream API 存在相同的天级别窗口时区问题</strong></li></ul></blockquote><p>这里提出一个问题，为什么东八区是需要在时间戳上加 8 小时偏移量进行天级别窗口计算，而不是减 8 小时或是加上 32（24 + 8） 小时，小伙伴们有详细分析过嘛~</p><p>根据上述问题，引出本文的第二大部分，即深度解析时区偏移量问题，这部分可以作为所有时区问题的分析思路。</p><h2 id="4-为什么东八区是加-8-小时？"><a href="#4-为什么东八区是加-8-小时？" class="headerlink" title="4.为什么东八区是加 8 小时？"></a>4.为什么东八区是加 8 小时？</h2><h3 id="时间和时区基本概念"><a href="#时间和时区基本概念" class="headerlink" title="时间和时区基本概念"></a>时间和时区基本概念</h3><p><strong><a href="https://baike.baidu.com/item/%E6%97%B6%E5%8C%BA/491122?fr=aladdin" title="时区">时区</a></strong>：由于世界各国家与地区经度不同，地方时也有所不同，因此会划分为不同的时区。</p><p><strong><a href="https://baike.baidu.com/item/unix%E6%97%B6%E9%97%B4%E6%88%B3/2078227?fr=aladdin" title="Unix 时间戳(Unix timestamp)">Unix 时间戳(Unix timestamp)</a></strong>： Unix 时间戳(Unix timestamp)，或称 Unix 时间(Unix time)、POSIX 时间(POSIX time)，是一种时间表示方式，定义为从格林威治时间 1970 年 01 月 01 日 00 时 00 分 00 秒（UTC/GMT的午夜）起至现在的总秒数。<br>Unix 时间戳不仅被使用在 Unix 系统、类 Unix 系统中，也在许多其他操作系统中被广泛采用。</p><p><strong>GMT</strong>：Greenwich Mean Time 格林威治标准时间。这是以英国格林威治天文台观测结果得出的时间，这是英国格林威治当地时间，这个地方的当地时间过去被当成世界标准的时间。</p><p><strong>UT</strong>：Universal Time 世界时。根据原子钟计算出来的时间。</p><p><strong>UTC</strong>：Coordinated Universal Time 协调世界时。因为地球自转越来越慢，每年都会比前一年多出零点几秒，每隔几年协调世界时组织都会给世界时 +1 秒，让基于原子钟的世界时和基于天文学（人类感知）的格林威治标准时间相差不至于太大。并将得到的时间称为 UTC，这是现在使用的世界标准时间。<br>协调世界时不与任何地区位置相关，也不代表此刻某地的时间，所以在说明某地时间时要加上时区也就是说 GMT 并不等于 UTC，而是等于 UTC + 0，只是格林威治刚好在 0 时区上。</p><h3 id="白话时间和时区"><a href="#白话时间和时区" class="headerlink" title="白话时间和时区"></a>白话时间和时区</h3><p>当时看完这一系列的时间以及时区说明之后我大脑其实是一片空白。…ojbk…，我用自己现在的一些理解，尝试将上述所有涉及到时间的概念解释一下。</p><ul><li><strong>GMT</strong>：格林威治标准时间。</li><li><strong>UTC</strong>：基于原子钟协调之后的世界标准时间。可以认为 UTC 时间和格林威治标准时间一致。即 GMT = UTC + 0，其中 0 代表格林威治为 0 时区。</li><li><strong>时区</strong>：逆向思维来解释下（只从技术层面解释，不从其他复杂层面解释），没有时区划分代表着全世界都是同一时区，那么同一时刻看到的外显时间是一样的。举个🌰：假如全世界都按照格林威治时间作为统一时间，在格林威治时间 0 点时，对于北京和加拿大的两个同学来说，这两个同学感知到的是北京是太阳刚刚升起（清晨），加拿大是太阳刚刚落下（傍晚）。<br>但是由于没有时区划分，这两个同学看到的时间都是 0 点，因此这是不符合人类对<strong>感知到的时间</strong>和自己<strong>看到的时间</strong>的理解的。所以划分时区之后，可以满足北京（东八区 UTC + 8）同学看到的时间是上午 8 点，加拿大（西四区 UTC - 4）同学看到的时间是下午 8 点。注意时区的划分是和 UTC 绑定的。东八区即 UTC + 8。</li><li><strong>flink 时间</strong>：flink 使用的时间基于 java 时间纪元（GMT 1970/01/01 00:00:00，UTC + 0 1970/01/01 00:00:00）。</li><li><strong>Unix 时间戳</strong>：世界上任何一个地方，同时接收到的数据的对应的 Unix 时间戳都是相同的，类似时区中我们举的不分时区的🌰，全世界同一时刻的 Unix 时间戳一致。</li><li><strong>Unix 时间戳为 0</strong>：对应的格林威治时间：1970-01-01 00:00:00，对应的北京时间（东八区）：1970-01-01 08:00:00**</li></ul><p>概念关系如图所示：</p><h3 id="为什么东八区是加-8-小时？"><a href="#为什么东八区是加-8-小时？" class="headerlink" title="为什么东八区是加 8 小时？"></a>为什么东八区是加 8 小时？</h3><p>下述表格只对一些重要的时间进行了标注：</p><table><thead><tr><th>Unix 时间戳</th><th>格林威治时间（外显）</th><th>北京时间（外显）</th></tr></thead><tbody><tr><td>-8 * 3600</td><td>-</td><td>1970/01/01 00:00:00</td></tr><tr><td>0</td><td>1970/01/01 00:00:00</td><td>1970/01/01 08:00:00</td></tr><tr><td>16 * 3600</td><td>-</td><td>1970/01/02 00:00:00</td></tr><tr><td>24 * 3600</td><td>1970/01/02 00:00:00</td><td>-</td></tr></tbody></table><p>拿第一条数据解释下，其代表在北京时间 1970/01/01 00:00:00 时，生成的一条数据所携带的 Unix 时间戳为 -8 * 3600。</p><p>根据需求和上图和上述表格内容，我们可以得到如下推导过程：</p><ul><li><p>需求场景是统计一个整天的 uv，即天级别窗口，比如统计北京时间 1970/01/01 00:00:00 - 1970/01/02 00:00:00 范围的数据时，这个日期范围内的数据所携带的 Unix 时间戳范围为 -8 * 3600 到 16 * 3600</p></li><li><p>对于 flink 来说，默认情况下它所能统计的一个整天的 Unix 时间戳的范围是 0 到 24 * 3600</p></li><li><p>所以当我们想通过 flink 实现正确统计北京时间（1970/01/01 00:00:00 - 1970/01/02 00:00:00）范围内的数据时，即统计 Unix 时间戳为 -8 * 3600 到 16 * 3600 的数据时，就需要对时间戳做个映射。</p></li><li><p>映射方法如下，就是将整体范围内的时间戳做在时间轴上做平移映射，就是把 -8 * 3600 映射到 0，16 * 3600 映射到 24 * 3600。相当于是对北京时间的 Unix 时间戳整体加 8 * 3600。</p></li><li><p>最后在产出的时间戳上把加上的 8 小时再减掉（因为外显时间会自动按照时区对 Unix 时间戳进行格式化）。</p></li></ul><blockquote><p>Notes：</p><ul><li><strong>可以加 32 小时吗？答案是可以。在东八区，对于天级别窗口的划分，加 8 小时和加 8 + n * 24（其中 n 为整数）小时后进行的天级别窗口划分和计算的效果是一样的，flink 都会将东八区的整一天内的数据划分到一个天级别窗口内。所以加 32（8 + 24），56（8 + 48），-16（8 - 24）小时效果都相同，上述例子只是选择了时间轴平移最小的距离，即 8 小时。注意某些系统的 Unix 时间戳为负值时会出现异常。</strong></li><li><strong>此推理过程适用于所有遇到时区问题的场景，如果你也有其他应用场景有这个问题，也可以按照上述方式解决</strong></li></ul></blockquote><h3 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h3><p>求输入 Unix 时间戳对应的东八区每天 0 点的 Unix 时间戳。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> ONE_DAY_MILLS = <span class="number">24</span> * <span class="number">60</span> * <span class="number">60</span> * <span class="number">1000L</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">long</span> <span class="title">transform</span><span class="params">(<span class="keyword">long</span> timestamp)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> timestamp - (timestamp + <span class="number">8</span> * <span class="number">60</span> * <span class="number">60</span> * <span class="number">1000</span>) % ONE_DAY_MILLS;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h2><p>本文首先介绍了直接给出了我们的问题 sql 和解决方案。</p><p>第二节从需求场景以及整个数据链路的实现方案出发，解释了我们怎样使用 flink sql 进行了需求实现，并进而引出了 sql 中天级别窗口存在的时区问题。</p><p>第三节确认了天级别窗口时区问题原因，引出了 flink 使用了 java 时间纪元，并针对此问题给出了引擎层面和 sql 层面的解决方案。也进而提出了一个问题：为什么我们的解决方案是加 8 小时偏移量？</p><p>第四节针对加 8 小时偏移量的原因进行了分析，并详细阐述了时区，UTC，GMT，Unix 时间戳之间的关系。</p><p>最后一节对本文进行了总结。</p><p>如果你有更方便的时区偏移量理解方式，欢迎留言~</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Tips | Flink sink schema 字段设计小技巧</title>
    <link href="https://yangyichao-mango.github.io/2020/09/12/wechat-blog/apache-flink:realtime-tips-1/"/>
    <id>https://yangyichao-mango.github.io/2020/09/12/wechat-blog/apache-flink:realtime-tips-1/</id>
    <published>2020-09-12T06:21:53.000Z</published>
    <updated>2020-09-24T13:38:30.956Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Tips-Flink-sink-schema-字段设计小技巧"><a href="#Tips-Flink-sink-schema-字段设计小技巧" class="headerlink" title="Tips | Flink sink schema 字段设计小技巧"></a>Tips | Flink sink schema 字段设计小技巧</h1><blockquote><p>本系列每篇文章都比较短小，不定期更新，从一些实际的 case 出发抛砖引玉，提高小伙伴的姿♂势水平。本文介绍 Flink sink schema 字段设计小技巧，阅读时长大概 2 分钟，话不多说，直接进入正文！</p></blockquote><h2 id="sink-schema-中添加-version-版本字段"><a href="#sink-schema-中添加-version-版本字段" class="headerlink" title="sink schema 中添加 version 版本字段"></a>sink schema 中添加 version 版本字段</h2><p>如 title，直接上实践案例和使用方式。</p><h3 id="实践案例及使用方式"><a href="#实践案例及使用方式" class="headerlink" title="实践案例及使用方式"></a>实践案例及使用方式</h3><ul><li><strong>非故障场景下产出的每条记录的 version 字段值为 1</strong></li><li><strong>故障场景下，可以在同一 sink 中产出 version &gt; 1（非 1）的数据，代表故障修复数据提供给下游消费</strong></li></ul><h3 id="可应对的故障场景"><a href="#可应对的故障场景" class="headerlink" title="可应对的故障场景"></a>可应对的故障场景</h3><p>上游 flink 任务 A 发生故障导致产出脏数据至 kafka X，并且下游消费方可以按照下面两类进行划分：</p><ul><li><strong>下游为 flink 任务</strong>：flink 任务 B 消费 kafka X 中的脏数据，结果计算并产出错误数据</li><li><strong>下游为 OLAP 引擎以及 BI 看板</strong>：结果导致看板展示数据异常</li></ul><p>首先介绍下避免以及处理上述问题的整体思路：</p><ul><li><strong>1.优化逻辑，保障上游任务稳定性</strong>：首先通过一些优化手段，尽可能保证上游 flink 任务 A 不出现故障</li><li><strong>2.配置作业监控报警</strong>：针对整条链路配置对应的监控报警等，以及时发现和定位问题</li><li><strong>3.制定故障处理、修复预案</strong>：需要制定对应的故障处理、修复预案，一旦出现故障，需要有可处理故障的能力</li><li><strong>4.下游针对数据源特性改进消费和处理方式</strong>：保障即使消费了脏数据也不会对业务逻辑产生影响</li></ul><p>下文主要介绍<strong>第 2 点</strong>，出现上述故障时修复的方案，针对以上场景，目前有如下 3 种可选方案修复数据：</p><ul><li><strong>方案 1 - 离线方式修复</strong>：通过离线方式产出修复数据，对脏数据进行覆盖操作。缺点是故障修复延迟较高，需要切换离线、实时数据源，人工操作成本较高</li><li><strong>方案 2 - 实时方式修复</strong>：重跑修数逻辑，产出修复数据至 kafka X-fix，下游 flink 任务 B 重新从 kafka X-fix 中的指定 offset 开始消费，计算并产出正确的数据。此方案对下游 flink 任务 B 来说，需要改动代码逻辑，存在修数 topic 和原 topic 切换逻辑，修复逻辑较为复杂</li><li><strong>方案 3 - 实时方式修复（本小节 version 字段方案）</strong>：为避免下游产生数据源切换操作带来的高成本操作，可在原有 kafka topic 中产出修复数据，通过 version 字段区分正常产出数据以及修复数据，相对方案 1 和 2 的优点在于，不存在数据源切换逻辑，下游通过控制 version 字段值就可消费到对应的修复数据，明显降低人工操作成本，且修复逻辑相对简单</li></ul><blockquote><p>Note: 方案 3 需要对 Kafka X 预留一定的 buffer，否则在产出修复数据时，由于写入或读出 Kafka X 的 QPS 过高，会影响正常产出数据的任务。</p></blockquote><h2 id="sink-schema-中添加时间戳字段"><a href="#sink-schema-中添加时间戳字段" class="headerlink" title="sink schema 中添加时间戳字段"></a>sink schema 中添加时间戳字段</h2><h3 id="实践案例及使用方式-1"><a href="#实践案例及使用方式-1" class="headerlink" title="实践案例及使用方式"></a>实践案例及使用方式</h3><p>有窗口场景中，sink schema 中可添加以下字段：</p><ul><li><strong>flink_process_start_time(long)：代表 flink 窗口开始逻辑处理的时间戳</strong></li><li><strong>flink_process_end_time(long)：代表 flink 窗口结束逻辑处理的时间戳</strong></li><li><strong>window_start(long)：代表 flink 窗口开始时间戳</strong></li><li><strong>window_end(long)：代表 flink 窗口结束时间戳</strong></li></ul><h3 id="生产实践案例"><a href="#生产实践案例" class="headerlink" title="生产实践案例"></a>生产实践案例</h3><ul><li><strong>flink_process_start_time，flink_process_end_time 在开发、测试、验数阶段可帮助用户定位数据偏差原因</strong></li><li><strong>window_start，window_end 可以帮助用户定位每个窗口处理是否有丢数，及每个窗口处理的具体数据</strong></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要介绍了在 sink schema 中添加 version（版本），时间戳扩展字段的小技巧，以帮助用户在生产环境中提升实时数据故障修复效率以及可用性。</p><p><img src="/blog-img/gzh/wechat.png" alt="公众号"></p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Tips | flink watermark 一定只能用时间戳衡量？？？</title>
    <link href="https://yangyichao-mango.github.io/2020/09/12/wechat-blog/apache-flink:realtime-tips-4-watermark/"/>
    <id>https://yangyichao-mango.github.io/2020/09/12/wechat-blog/apache-flink:realtime-tips-4-watermark/</id>
    <published>2020-09-12T06:21:53.000Z</published>
    <updated>2020-11-22T08:27:11.053Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列每篇文章都比较短小，不定期更新，从一些实际的 case 出发抛砖引玉，提高小伙伴的姿♂势水平。本文从另一种角度介绍 flink 的 watermark，阅读时长大概 2 分钟，话不多说，直接进入正文！</p></blockquote><h1 id="关于-watermark"><a href="#关于-watermark" class="headerlink" title="关于 watermark"></a>关于 watermark</h1><p>Nicki<br>是某一线互联网大厂的数据开发，</p><p>最近<br>由于公司业务的发展，<br>以及业务对数据实时性要求变高，<br>Nicki 开始使用 flink 进行实时数据开发，</p><p>今天<br>Nicki 在使用 flink datastream api 进行开发，<br>当她写万 watermark 分配器之后，<br>突然有了一个疑问，<br>watermark 的分配只能使用时间戳吗？</p><p>带着这个疑问<br>Nicki 找到了 lsp 数据羊。</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>时间戳作为 watermark 的原因<br>时间戳是我们最常用的标识以及分析方式</p><p>但是可以作为 watermark 的<br>不止时间戳</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>踩坑记 | Flink 事件时间语义下数据乱序丢数踩坑</title>
    <link href="https://yangyichao-mango.github.io/2020/09/11/wechat-blog/apache-flink:realtime-out-of-order/"/>
    <id>https://yangyichao-mango.github.io/2020/09/11/wechat-blog/apache-flink:realtime-out-of-order/</id>
    <published>2020-09-11T06:21:53.000Z</published>
    <updated>2020-09-20T12:37:21.791Z</updated>
    
    <content type="html"><![CDATA[<h1 id="踩坑记-Flink-事件时间语义下数据乱序丢数踩坑"><a href="#踩坑记-Flink-事件时间语义下数据乱序丢数踩坑" class="headerlink" title="踩坑记 | Flink 事件时间语义下数据乱序丢数踩坑"></a>踩坑记 | Flink 事件时间语义下数据乱序丢数踩坑</h1><blockquote><p>本文详细介绍了在上游使用处理时间语义的 flink 任务出现故障后，重启消费大量积压在上游的数据并产出至下游数据乱序特别严重时，下游 flink 任务使用事件时间语义时遇到的大量丢数问题以及相关的解决方案。</p></blockquote><p>本文分为以下几个部分：</p><ul><li><strong>1.本次踩坑的应用场景</strong></li><li><strong>2.应用场景中发生的丢数故障分析</strong></li><li><strong>3.待修复的故障点</strong></li><li><strong>4.丢数故障解决方案及原理</strong></li><li><strong>5.总结</strong></li></ul><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>应用场景如下：</p><ul><li><strong>flink 任务 A</strong> 以<strong>处理时间</strong>语义做过滤产出新增 xx 明细数据至 <strong>Kafka Y</strong></li><li><strong>flink 任务 B</strong> 以<strong>事件时间</strong>语义消费 <strong>Kafka Y</strong> 做窗口聚合操作产出分钟级别聚合指标至 <strong>Kafka Z</strong></li><li><strong>Kafka Z</strong> 实时导入至 <strong>Druid</strong> 以做即时 OLAP 分析，并且展示在 BI 应用看板</li></ul><p><img src="/blog-img/apache-flink:realtime-out-of-order/stream.png" alt="场景"></p><h2 id="丢数故障分析"><a href="#丢数故障分析" class="headerlink" title="丢数故障分析"></a>丢数故障分析</h2><p>简要介绍下这次生产中故障场景。整条故障追踪链路如下：</p><p>故障一：</p><ul><li>收到报警反馈 <strong>flink 任务 A</strong> 入口流量为 0</li><li>定位 <strong>flink 任务 A</strong> 中某个算子的故障导致整个 job 卡住</li><li>导致此 <strong>flink 任务 A</strong> 上游 <strong>kafka X</strong> 积压了大量数据</li><li>重启 <strong>flink 任务 A</strong>后，消费大量积压在上游 <strong>kafka X</strong> 数据完成，任务恢复正常</li></ul><p>故障一从而引发下游的故障二：</p><ul><li>由于 <strong>flink 任务 A</strong> 使用了<strong>处理时间</strong>语义处理数据，并且有过滤和 keyBy 分桶窗口逻辑，在重启后消费大量积压在上游的数据时，导致 sink rebalance 后产出到下游 <strong>kafka Y</strong> 各个分区数据中的 server_timestamp 是乱序的</li><li>下游 <strong>flink 任务 B</strong> 在消费 <strong>Kafka Y</strong> 时使用了<strong>事件时间</strong>语义处理数据，并且使用了数据中的 server_timestamp 作为<strong>事件时间</strong>时间戳</li><li><strong>flink 任务 B</strong> 消费了乱序很严重的数据之后，导致在窗口聚合计算时丢失了大量数据</li><li>最终展示在 BI 应用中的报表有丢失数据的情况</li></ul><p><img src="/blog-img/apache-flink:realtime-out-of-order/bug_stream.png" alt="故障场景"></p><h2 id="待修复的故障点"><a href="#待修复的故障点" class="headerlink" title="待修复的故障点"></a>待修复的故障点</h2><ul><li>1.<strong>flink 任务 A</strong> 的稳定性故障，这部分解决方案暂不在本文中介绍</li><li>2.<strong>flink 任务 B</strong> 消费上游乱序丢数故障，解决方案在下文介绍</li></ul><h2 id="解决方案以及原理"><a href="#解决方案以及原理" class="headerlink" title="解决方案以及原理"></a>解决方案以及原理</h2><h3 id="丢数故障解决方案"><a href="#丢数故障解决方案" class="headerlink" title="丢数故障解决方案"></a>丢数故障解决方案</h3><p>解决方案是以下游 <strong>flink 任务 B</strong> 作为切入点，直接给出 <strong>flink 任务 B</strong> 的 sql 代码解决方案，java code 也可以按照这个方案实现，其本质原理相同。下文进行原理解释。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  to_unix_timestamp(server_timestamp <span class="operator">/</span> bucket) <span class="keyword">AS</span> <span class="type">timestamp</span>, <span class="comment">-- format 成原有的事件时间戳</span></span><br><span class="line">  <span class="built_in">count</span>(id) <span class="keyword">as</span> id_cnt,</span><br><span class="line">  <span class="built_in">sum</span>(duration) <span class="keyword">as</span> duration_sum</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  source_table</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  TUMBLE(proctime, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">MINUTE</span>),</span><br><span class="line">  server_timestamp <span class="operator">/</span> bucket <span class="comment">-- 根据事件时间分桶计算，将相同范围（比如 1 分钟）事件时间的数据分到一个桶内</span></span><br></pre></td></tr></table></figure><h3 id="解决方案原理"><a href="#解决方案原理" class="headerlink" title="解决方案原理"></a>解决方案原理</h3><p>首先明确一个无法避免的问题，在不考虑 watermark 允许延迟设置特别大的情况下，只要上游使用到了处理时间语义，下游使用事件时间语义，一旦上游发生故障重启并在短时间内消费大量数据，就不可避免的会出现上述错误以及故障。</p><p>在下游消费方仍然需要将对应事件时间戳的数据展示在 BI 平台报表中、并且全链路时间语义都为处理时间保障不丢数的前提下。解决方案就是在聚合并最终产出对应事件时间戳的数据。</p><p>最后的方案如下：<br>整条链路全部为处理时间语义，窗口计算也使用处理时间，但是产出数据中的时间戳全部为事件时间戳。<br>在出现故障的场景下，一分钟的窗口内的数据的事件时间戳可能相差几个小时，但在最终窗口聚合时可以根据事件时间戳划分到对应的事件时间窗口内，下游 BI 应用展示时使用此事件时间戳即可。</p><blockquote><p>注意：sql 中的 bucket 需要根据具体使用场景进行设置，如果设置过于小，比如非故障场景下按照处理时间开 1 分钟的窗口，bucket<br>设为 60000（1 分钟），那么极有可能，这个时间窗口中所有数据的 server_timestamp 都集中在某两分钟内，那么这些数据就会被分到两个桶（bucket）内，则会导致严重的数据倾斜。</p></blockquote><h3 id="输入数据样例"><a href="#输入数据样例" class="headerlink" title="输入数据样例"></a>输入数据样例</h3><p>模拟上述故障，<strong>flink B</strong> 的任务某一个窗口内的数据输入如下。</p><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">server_timestamp</th>            <th style="width: 20%; text-align: center;">id</th>            <th style="width: 20%; text-align: center;">duration</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">2020/9/01 21:14:38</td>            <td>1</td>            <td>300</td>        </tr>        <tr>            <td style="text-align: center;">2020/9/01 21:14:50</td>            <td>1</td>            <td>500</td>        </tr>        <tr>            <td style="text-align: center;">2020/9/01 21:25:38</td>            <td>2</td>            <td>600</td>        </tr>        <tr>            <td style="text-align: center;">2020/9/01 21:25:38</td>            <td>3</td>            <td>900</td>        </tr>        <tr>            <td style="text-align: center;">2020/9/01 21:25:38</td>            <td>2</td>            <td>800</td>        </tr>    </tbody></table><h3 id="输出数据样例"><a href="#输出数据样例" class="headerlink" title="输出数据样例"></a>输出数据样例</h3><p>按照上述解决方案中的 sql 处理过后，输出数据如下，则可以解决此类型丢数故障。</p><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">timestamp</th>            <th style="width: 20%; text-align: center;">id_cnt</th>            <th style="width: 20%; text-align: center;">duration_sum</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">2020/9/01 21:14:00</td>            <td>2</td>            <td>900</td>        </tr>        <tr>            <td style="text-align: center;">2020/9/01 21:25:00</td>            <td>3</td>            <td>2300</td>        </tr>    </tbody></table><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文分析了在 flink 应用中：</p><ul><li><strong>上游使用处理时间语义的 flink 任务出现故障、重启消费大量积压数据并产出至下游数据乱序特别严重时，下游使用事件时间语义时遇到的大量丢数问题</strong></li><li><strong>以整条链路为处理时间语义的前提下，产出的数据时间戳为事件时间戳解决上述问题</strong></li><li><strong>以 sql 代码给出了丢数故障解决方案样例</strong></li></ul><h2 id="学习资料"><a href="#学习资料" class="headerlink" title="学习资料"></a>学习资料</h2><h3 id="flink"><a href="#flink" class="headerlink" title="flink"></a>flink</h3><ul><li><a href="https://github.com/flink-china/flink-training-course/blob/master/README.md">https://github.com/flink-china/flink-training-course/blob/master/README.md</a></li><li><a href="https://ververica.cn/developers-resources/">https://ververica.cn/developers-resources/</a></li><li><a href="https://space.bilibili.com/33807709">https://space.bilibili.com/33807709</a></li></ul><p><img src="/blog-img/gzh/wechat.png" alt="公众号"></p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>生产实践 | 基于 Flink 的视频直播核心指标监控</title>
    <link href="https://yangyichao-mango.github.io/2020/09/01/wechat-blog/apache-flink:learning-files/"/>
    <id>https://yangyichao-mango.github.io/2020/09/01/wechat-blog/apache-flink:learning-files/</id>
    <published>2020-09-01T06:21:53.000Z</published>
    <updated>2020-09-26T05:43:26.187Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Flink-学习资料"><a href="#Flink-学习资料" class="headerlink" title="Flink 学习资料"></a>Flink 学习资料</h1><blockquote><p>Flink 系列官网文档还是最准确的。直接上<strong>视频</strong>和<strong>资料</strong>，懒癌患者直接拿走即可~</p></blockquote><h2 id="视频"><a href="#视频" class="headerlink" title="视频"></a>视频</h2><p><a href="https://space.bilibili.com/33807709。" title="b站链接">b站链接</a>，很多大厂的实战，讲解很详细，很适合初学者。</p><h3 id="资料"><a href="#资料" class="headerlink" title="资料"></a><a href="https://mp.weixin.qq.com/s/0Wa5rcO6DU8t8Gzy7zIv2Q。" title="Flink菜鸟">资料</a></h3><ul><li>链接：<a href="https://pan.baidu.com/s/1GzVJYpUxkucLS9ZikZWuRw、提取码：64n4">https://pan.baidu.com/s/1GzVJYpUxkucLS9ZikZWuRw、提取码：64n4</a></li><li>其他视频链接可点击公众号 [call me] 联系博主私聊获取</li></ul><h2 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h2><p>1.<a href="https://flink.apache.org/。" title="Flink 官网文档">Flink 官网文档</a>、<a href="https://flink.apache.org/zh/。" title="Flink 官网中文文档">Flink 官网中文文档</a></p><p>2.《Stream Processing with Apache Flink》 这本由 Flink PMC 写的 Flink 书籍也是很适合初学者，并且中文版已经出版了 ：《基于 Apache Flink 的流处理》，由 Flink committer 崔星灿大佬翻译</p><p>3.<a href="https://ververica.cn。" title="ververica 中文网">ververica 中文网</a>和 <a href="https://ververica.cn/developers/flink-training-course-basics。" title="Flink 钉钉群直播教程">Flink 钉钉群直播教程</a></p><p>4.微信公众号(由Flink 中文社区维护)：<strong>Flink 中文社区</strong> 以及 <strong>Flink</strong></p><p>5.<a href="https://ververica.cn/developers/special-issue。" title="Flink 知识图谱以及社区专刊">Flink 知识图谱以及社区专刊</a></p><p>6.加入 Apache Flink China社区 钉钉群（群号：23138101），很多大佬活跃</p><p>7.订阅 Flink user、user-zh 邮件列表，可在官网查看订阅方式，邮件列表中有很多解决方案，也有大佬们活跃解答</p><p>8.<a href="https://github.com/flink-china/flink-training-course/blob/master/README.md。" title="Flink 视频系列 github地址">Flink 视频系列 github地址</a></p><h3 id="oreilly-流模型介绍"><a href="#oreilly-流模型介绍" class="headerlink" title="oreilly 流模型介绍"></a>oreilly 流模型介绍</h3><ul><li><p><strong><a href="https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101。" title="Streaming 101: The world beyond batch">Streaming 101: The world beyond batch</a></strong></p></li><li><p><strong><a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102。" title="Streaming 102: The world beyond batch">Streaming 102: The world beyond batch</a></strong></p></li></ul><h2 id="Flink-源码"><a href="#Flink-源码" class="headerlink" title="Flink 源码"></a>Flink 源码</h2><ul><li><strong><a href="https://github.com/apache/flink。" title="github">github</a></strong></li><li><strong><a href="https://gitee.com/mirrors/apache-flink?_from=gitee_search。" title="gitee">gitee</a></strong></li></ul><h2 id="书籍社刊"><a href="#书籍社刊" class="headerlink" title="书籍社刊"></a>书籍社刊</h2><h3 id="书籍"><a href="#书籍" class="headerlink" title="书籍"></a>书籍</h3><p>《Stream Processing with Apache Flink》</p><p>《基于 Apache Flink 的流处理》</p><h3 id="社刊"><a href="#社刊" class="headerlink" title="社刊"></a>社刊</h3><p>源自 ververica，<a href="https://ververica.cn/developers/special-issue/。" title="社刊链接">社刊链接</a>，总共分为 4 期，可以直接在 ververica 官网下载。</p><h3 id="系列博客"><a href="#系列博客" class="headerlink" title="系列博客"></a>系列博客</h3><p>1.<a href="http://wuchong.me/tags/Flink%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0/。" title="云邪原理系列">云邪原理系列</a></p><p>2.<a href="https://yq.aliyun.com/users/ohyfzrwxmb3me?spm=a2c4e.11153940.0.0.1b994ae7llGnsY。" title="金竹漫谈系列">金竹漫谈系列</a></p><p>3.<a href="https://github.com/bethunebtj/flink_tutorial/blob/master/%E8%BF%BD%E6%BA%90%E7%B4%A2%E9%AA%A5%EF%BC%9A%E9%80%8F%E8%BF%87%E6%BA%90%E7%A0%81%E7%9C%8B%E6%87%82Flink%E6%A0%B8%E5%BF%83%E6%A1%86%E6%9E%B6%E7%9A%84%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B.pdf。" title="杨华的源码">杨华的源码</a></p><p>4.<a href="https://www.infoq.cn/article/fRt1RF1pxu_ZtmeObOoJ。" title="Apache Flink 零基础入门系列">Apache Flink 零基础入门系列</a></p><p>5.<a href="http://www.54tianzhisheng.cn/tags/Flink/。" title="zhisheng">zhisheng</a></p><p>6.<a href="https://www.aboutyun.com/forum.php?mod=forumdisplay&fid=443&filter=typeid&typeid=1393。" title="彻底明白Flink系统学习系列">彻底明白Flink系统学习系列</a></p><p>7.<a href="https://www.jianshu.com/p/0242f456f02a。" title="kangqi">kangqi</a></p><p>8.<a href="https://www.cnblogs.com/Springmoon-venn/category/1380180.html。" title="flink菜鸟">flink菜鸟</a></p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>实时开发需求确认模板</title>
    <link href="https://yangyichao-mango.github.io/2020/09/01/wechat-blog/apache-flink:realtime-demand-template/"/>
    <id>https://yangyichao-mango.github.io/2020/09/01/wechat-blog/apache-flink:realtime-demand-template/</id>
    <published>2020-09-01T06:21:53.000Z</published>
    <updated>2020-09-06T02:00:13.392Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实时开发需求确认模板"><a href="#实时开发需求确认模板" class="headerlink" title="实时开发需求确认模板"></a>实时开发需求确认模板</h1><blockquote><p>实时指标整个链路开发过程中的一些经验。</p></blockquote><h2 id="需求评估"><a href="#需求评估" class="headerlink" title="需求评估"></a>需求评估</h2><p>分析实时指标是否符合该需求场景，以及在该场景中实时指标能够发挥的价值。</p><h3 id="1-实时指标所能提供的分析能力"><a href="#1-实时指标所能提供的分析能力" class="headerlink" title="1.实时指标所能提供的分析能力"></a>1.实时指标所能提供的分析能力</h3><p>实时计算的输出内容，以及提供的分析能力：OLAP 分析，key-value 实时数据服务，维度填充，数据打标等。</p><h3 id="2-产出维度，指标的合理性"><a href="#2-产出维度，指标的合理性" class="headerlink" title="2.产出维度，指标的合理性"></a>2.产出维度，指标的合理性</h3><p>从需求出发，评估实时指标产出的维度和指标的合理性。</p><h3 id="3-需求可配置变量"><a href="#3-需求可配置变量" class="headerlink" title="3.需求可配置变量"></a>3.需求可配置变量</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">评估维度</th>            <th style="width: 20%; text-align: center;">评估指标</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">监控数据范围</td>            <td>监控数据范围动态配置改变？</td>        </tr>        <tr>            <td style="text-align: center;">监控数据起止时间</td>            <td>监控数据的起止时间动态配置改变？</td>        </tr>        <tr>            <td style="text-align: center;">监控数据的某些配置变量</td>            <td>当变量发生变动时，可能会对产出的实时数据有什么影响，对计算链路有什么影响，会决定实时计算链路的实现方式。</td>        </tr>    </tbody></table><h3 id="4-面向用户范围"><a href="#4-面向用户范围" class="headerlink" title="4.面向用户范围"></a>4.面向用户范围</h3><p>评估 SLA 等服务质量等级保障，以及提供的实时数据服务的可用性等级，并且提前和业务方确认可用性和出现故障时的恢复时间等问题。</p><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">评估维度</th>            <th style="width: 20%; text-align: center;">评估指标</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">时间语义</td>            <td>事件时间、处理时间？事件时间：可以通过获取数据的时间戳，使用处理时间来真实反映和还原事件，但是可能会出现数据条数过小窗口不能触发，或者在一些有截止日期的活动结束的最后一个窗口不能触发的问题；处理时间：处理时间一般和事件时间差距很小，经验值一般 diff 小于 1%，只能反映流式框架处理数据时的时间戳，但是不会出现上述事件时间的两个问题。因此需要评估需求的逻辑精确度是否要求很高？</td>        </tr>        <tr>            <td style="text-align: center;">数据一致性</td>            <td>至少一次、精确一次？至少一次：受限于目前的上下游以及依赖中间件的能力，比如 010 版本及以下的 Kafka 不支持两阶段提交，所以只能达到至少一次的语义；精确一次：整条实时计算链路中的所有组件都需要支持精确一次的语义（从技术层面或者业务逻辑层面达到精确一次）。评估需求逻辑是否可接受任务发生失败时有重复数据产生？</td>        </tr>        <tr>            <td style="text-align: center;">SLA 要求</td>            <td>评估需求的 SLA 要求，整条实时计算链路的 SLA 要求，产出数据最多延迟多长时间？数据准确率几个9？提供的接口服务是否需要考虑跨集群、机房备份、双写；是否需要建立多条计算链路以供故障切换？一旦发生故障，下游消费方能容忍的最大故障时长？下游消费方在发生故障时的兜底策略？</td>        </tr>    </tbody></table><h2 id="可行性评估"><a href="#可行性评估" class="headerlink" title="可行性评估"></a>可行性评估</h2><h3 id="1-技术可行性"><a href="#1-技术可行性" class="headerlink" title="1.技术可行性"></a>1.技术可行性</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">评估维度</th>            <th style="width: 20%; text-align: center;">评估指标</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">QPS</td>            <td>确定 QPS 以评估实时上下游以及依赖组件的选型以及能力。</td>        </tr>        <tr>            <td style="text-align: center;">数据输入</td>            <td>首先确定数据输入是否能够计算实时指标，然后评估上游提供的数据在计算实时指标时整个实时计算链路的逻辑以及复杂度。比如：是否需要用到双流 join，需要评估双流 join 所存在的误差是否在可接受范围内，一般可通过离线误差对比或经验值给出结论。常见输入中间件：消息队列，接口等，常用中间件：Kafka，rpc，http。</td>        </tr>        <tr>            <td style="text-align: center;">数据依赖</td>            <td>调研目前可用的哪些中间件可以提供能力来支持当前指标计算？举例：key-value等，常用依赖中间件：Redis。</td>        </tr>        <tr>            <td style="text-align: center;">数据输出</td>            <td>确定输出下游消费方的消费需求以及能力，以评估实时产出的数据以及存储组件是否能够满足其需求？常见输出中间件：消息队列，OLAP，key-value，接口等，常用中间件：Kafka，Druid，Redis，rpc。产出维度，一般场景下，维度值不建议是大数量级别的数据，比如说使用 user_id，device_id。</td>        </tr>    </tbody></table><h3 id="2-成本可行性"><a href="#2-成本可行性" class="headerlink" title="2.成本可行性"></a>2.成本可行性</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">评估维度</th>            <th style="width: 20%; text-align: center;">评估指标</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">QPS</td>            <td>确定 QPS 以评估实时上下游以及依赖中间件的资源消耗。确定资源消耗是否在可接受范围之内？</td>        </tr>        <tr>            <td style="text-align: center;">数据输入</td>            <td>确定整个实时计算链路的逻辑以及复杂度，来评估可能的资源消耗。</td>        </tr>        <tr>            <td style="text-align: center;">数据依赖</td>            <td>确定整个实时计算链路的逻辑以及复杂度，来评估可能的资源消耗。</td>        </tr>        <tr>            <td style="text-align: center;">数据输出</td>            <td>由输出内容以及存储组件来评估下游存储中间件的资源消耗。举例：维度值不建议是大数量级别的数据，比如说使用 user_id，device_id 作为维度或者产出明细数据，虽然使用 OLAP 在维度聚合场景下很灵活，但是这些场景下使用 OLAP 可能会造成很大的资源消耗。</td>        </tr>    </tbody></table><p>综合以上技术和成本可行性以及需求收益等指标，以评估实时指标的 ROI。</p><h3 id="3-数据输入"><a href="#3-数据输入" class="headerlink" title="3.数据输入"></a>3.数据输入</h3><h4 id="消息队列日志"><a href="#消息队列日志" class="headerlink" title="消息队列日志"></a>消息队列日志</h4><p>最常见的实时数据源就是消息队列日志，首先我们需要确定日志类型，不同的日志类型决定了指标或者维度字段是否可以产出以及其准确性，一般情况下有以下三种类型日志：</p><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">日志类型</th>            <th style="width: 20%; text-align: center;">备注</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">埋点日志</td>            <td>维度最全，数据准确</td>        </tr>        <tr>            <td style="text-align: center;">web server log</td>            <td>维度次全，数据准确度一般</td>        </tr>        <tr>            <td style="text-align: center;">binlog</td>            <td>数据库真实数据，反映的是真实数据，数据最准确，维度信息一般很少</td>        </tr>    </tbody></table><h4 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h4><p>这里的接口一般是用来根据需求圈定一批需要进行监控的数据内容。接口的提供方式可以是http，配置中心配置，rpc接口等。</p><h3 id="4-数据依赖"><a href="#4-数据依赖" class="headerlink" title="4.数据依赖"></a>4.数据依赖</h3><p>实时一般情况下都会或多或少依赖到外部组件，最常见的就是 key-value 存储。<br>场景：很常见的一类需求就是对数据源中的数据进行打标然后产出，这里的标签数据就会存储在 key-value 中间件中。<br>需要评估访问外存的 QPS，以及外存提供的能力。</p><h3 id="5-数据输出"><a href="#5-数据输出" class="headerlink" title="5.数据输出"></a>5.数据输出</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">输出组件</th>            <th style="width: 20%; text-align: center;">备注</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">消息队列</td>            <td>常见中间件：kafka等</td>        </tr>        <tr>            <td style="text-align: center;">key-value存储</td>            <td>常见中间件：Redis，HBase，服务化接口等</td>        </tr>        <tr>            <td style="text-align: center;">OLAP</td>            <td>常见中间件：Druid，ClickHouse等</td>        </tr>    </tbody></table><h2 id="技术方案评估"><a href="#技术方案评估" class="headerlink" title="技术方案评估"></a>技术方案评估</h2>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>生产实践 | 基于 Flink 的短视频生产消费监控</title>
    <link href="https://yangyichao-mango.github.io/2020/09/01/wechat-blog/apache-flink:realtime-monitor-video/"/>
    <id>https://yangyichao-mango.github.io/2020/09/01/wechat-blog/apache-flink:realtime-monitor-video/</id>
    <published>2020-09-01T06:21:53.000Z</published>
    <updated>2020-09-06T12:46:06.628Z</updated>
    
    <content type="html"><![CDATA[<h1 id="生产实践-基于-Flink-的短视频生产消费监控"><a href="#生产实践-基于-Flink-的短视频生产消费监控" class="headerlink" title="生产实践 | 基于 Flink 的短视频生产消费监控"></a>生产实践 | 基于 Flink 的短视频生产消费监控</h1><blockquote><p>本文详细介绍了实时监控类指标的数据流转链路以及技术方案，大多数的实时监控类指标都可按照本文中的几种方案实现。</p></blockquote><h2 id="短视频生产消费监控"><a href="#短视频生产消费监控" class="headerlink" title="短视频生产消费监控"></a>短视频生产消费监控</h2><p>短视频带来了全新的传播场域和节目形态，小屏幕、快节奏成为行业潮流的同时，也催生了新的用户消费习惯，为创作者和商户带来收益。而多元化的短视频也可以为品牌方提供营销机遇。</p><p>其中对于垂类生态短视频的生产消费热点的监控分析目前成为了实时数据处理很常见的一个应用场景，比如对某个圈定的垂类生态下的视频生产或者视频消费进行监控，对热点视频生成对应的优化推荐策略，促进热点视频的生产或者消费，构建整个生产消费数据链路的闭环，从而提高创作者收益以及消费者留存。</p><p>本文将完整分析垂类生态短视频生产消费数据的整条链路流转方式，并基于 Flink 提供几种对于垂类视频生产消费监控的方案设计。通过本文，你可以了解到：</p><ul><li><p><strong>垂类生态短视频生产消费数据链路闭环</strong></p></li><li><p><strong>实时监控短视频生产消费的方案设计</strong></p></li><li><p><strong>不同监控量级场景下的代码实现</strong></p></li><li><p><strong>flink 学习资料</strong></p></li></ul><h2 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h2><p>垂类生态短视频生产消费数据链路流转架构图如下，此数据流转图也适用于其他场景：</p><p><img src="/blog-img/monitor/data_transform.png" alt="链路"></p><p>在上述场景中，用户生产和消费短视频，从而客户端、服务端以及数据库会产生相应的行为操作日志，这些日志会通过日志抽取中间件抽取到消息队列中，我们目前的场景中是使用 Kafka 作为消息队列；然后使用 flink 对垂类生态中的视频进行生产或消费监控（内容生产通常是圈定垂类作者 id 池，内容消费通常是圈定垂类视频 id 池），最后将实时聚合数据产出到下游；下游可以以数据服务，实时看板的方式展现，运营同学或者自动化工具最终会帮助我们分析当前垂类下的生产或者消费热点，从而生成推荐策略。</p><h2 id="方案设计"><a href="#方案设计" class="headerlink" title="方案设计"></a>方案设计</h2><p><img src="/blog-img/monitor/monitor_flink.png" alt="架构"></p><p>其中数据源如下：</p><ul><li><strong>Kafka</strong> 为全量内容生产和内容消费的日志。</li><li><strong>Rpc/Http/Mysql/配置中心/Redis/HBase</strong> 为需要监控的垂类生态内容 id 池（内容生产则为作者 id 池，内容消费则为视频 id 池），其主要是提供给运营同学动态配置需要监控的 id 范围，其可以在 flink 中进行实时查询，解析运营同学想要的监控指标范围，以及监控的指标和计算方式，然后加工数据产出，可以支持随时配置，实时数据随时计算产出。</li></ul><p>其中数据汇为聚类好的内容生产或者消费热点话题或者事件指标：</p><ul><li><strong>Redis/HBase</strong> 主要是以低延迟（Redis 5ms p99，HBase 100ms p99，不同公司的服务能力不同）并且高 QPS 提供数据服务，给 Server 端或者线上用户提供低延迟的数据查询。</li><li><strong>Druid/Mysql</strong> 可以做为 OLAP 引擎为 BI 分析提供灵活的上卷下钻聚合分析能力，供运营同学配置可视化图表使用。</li><li><strong>Kafka</strong> 可以以流式数据产出，从而提供给下游继续消费或者进行特征提取。</li></ul><p>废话不多说，我们直接上方案和代码，下述几种方案按照监控 id 范围量级区分，不同的量级对应着不同的方案，其中的代码示例为 ProcessWindowFunction，也可以使用 AggregateFunction 代替，其中主要监控逻辑都相同。</p><h3 id="方案-1"><a href="#方案-1" class="headerlink" title="方案 1"></a>方案 1</h3><p>适合监控 id 数据量小的场景（<strong>几千 id</strong>），其实现方式是在 flink 任务初始化时将需要监控的 id 池或动态配置中心的 id 池加载到内存当中，之后只需要在内存中判断内容生产或者消费数据是否在这个监控池当中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">ProcessWindowFunction p = <span class="keyword">new</span> ProcessWindowFunction&lt;CommonModel, CommonModel, Long, TimeWindow&gt;() &#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 配置中心动态 id 池</span></span><br><span class="line">    <span class="keyword">private</span> Config&lt;Set&lt;Long&gt;&gt; needMonitoredIdsConfig;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.needMonitoredIdsConfig = ConfigBuilder</span><br><span class="line">                .buildSet(<span class="string">&quot;needMonitoredIds&quot;</span>, Long.class);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Long bucket, Context context, Iterable&lt;CommonModel&gt; iterable, Collector&lt;CommonModel&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Set&lt;Long&gt; needMonitoredIds = needMonitoredIdsConfig.get();</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 判断 commonModel 中的 id 是否在 needMonitoredIds 池中</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>监控的 id 池可以按照固定或者可配置从而分出两种获取方式：第一种是在 flink 任务开始时就全部加载进内存中，这种方式适合监控 id 池不变的情况；第二种是使用动态配置中心，每次都从配置中心访问到最新的监控 id 池，其可以满足动态配置或者更改 id 池的需求，并且这种实现方式通常可以实时感知到配置更改，几乎无延迟。</p></blockquote><h3 id="方案-2"><a href="#方案-2" class="headerlink" title="方案 2"></a>方案 2</h3><p>适合监控 id 数据量适中（<strong>几十万 id</strong>），监控数据范围会不定时发生变动的场景。其实现方式是在 flink 算子中定时访问接口获取最新的监控 id 池，以获取最新监控数据范围。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">ProcessWindowFunction p = <span class="keyword">new</span> ProcessWindowFunction&lt;CommonModel, CommonModel, Long, TimeWindow&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> lastRefreshTimestamp;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Set&lt;Long&gt; needMonitoredIds;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        <span class="keyword">this</span>.refreshNeedMonitoredIds(System.currentTimeMillis());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Long bucket, Context context, Iterable&lt;CommonModel&gt; iterable, Collector&lt;CommonModel&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> windowStart = context.window().getStart();</span><br><span class="line">        <span class="keyword">this</span>.refreshNeedMonitoredIds(windowStart);</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 判断 commonModel 中的 id 是否在 needMonitoredIds 池中</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">refreshNeedMonitoredIds</span><span class="params">(<span class="keyword">long</span> windowStart)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 每隔 10 秒访问一次</span></span><br><span class="line">        <span class="keyword">if</span> (windowStart - <span class="keyword">this</span>.lastRefreshTimestamp &gt;= <span class="number">10000L</span>) &#123;</span><br><span class="line">            <span class="keyword">this</span>.lastRefreshTimestamp = windowStart;</span><br><span class="line">            <span class="keyword">this</span>.needMonitoredIds = Rpc.get(...)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>根据上述代码实现方式，按照时间间隔的方式刷新 id 池，其缺点在于不能实时感知监控 id 池的变化，所以刷新时间可能会和需求场景强耦合（如果 id 池会频繁更新，那么就需要缩小刷新时间间隔）。也可根据需求场景在每个窗口开始前刷新 id 池，这样可保证每个窗口中的 id 池中的数据一直保持更新。</p></blockquote><h3 id="方案-3"><a href="#方案-3" class="headerlink" title="方案 3"></a>方案 3</h3><p>方案 3 对方案 2 的一个优化（<strong>几十万 id，我们生产环境中最常用的</strong>）。其实现方式是在 flink 中使用 broadcast 算子定时访问监控 id 池，并将 id 池以广播的形式下发给下游参与计算的各个算子。其优化点在于：比如任务的并行度为 500，每 1s 访问一次，采用方案 2 则访问监控 id 池接口的 QPS 为 500，在使用 broadcast 算子之后，其访问 QPS 可以减少到 1，可以大大减少对接口的访问量，减轻接口压力。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Example</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Slf4j</span></span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">NeedMonitorIdsSource</span> <span class="keyword">implements</span> <span class="title">SourceFunction</span>&lt;<span class="title">Map</span>&lt;<span class="title">Long</span>, <span class="title">Set</span>&lt;<span class="title">Long</span>&gt;&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isCancel;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Map&lt;Long, Set&lt;Long&gt;&gt;&gt; sourceContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">while</span> (!<span class="keyword">this</span>.isCancel) &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    TimeUnit.SECONDS.sleep(<span class="number">1</span>);</span><br><span class="line">                    Set&lt;Long&gt; needMonitorIds = Rpc.get(...);</span><br><span class="line">                    <span class="comment">// 可以和上一次访问的数据做比较查看是否有变化，如果有变化，才发送出去</span></span><br><span class="line">                    <span class="keyword">if</span> (CollectionUtils.isNotEmpty(needMonitorIds)) &#123;</span><br><span class="line">                        sourceContext.collect(<span class="keyword">new</span> HashMap&lt;Long, Set&lt;Long&gt;&gt;() &#123;&#123;</span><br><span class="line">                            put(<span class="number">0L</span>, needMonitorIds);</span><br><span class="line">                        &#125;&#125;);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">                    <span class="comment">// 防止接口访问失败导致的错误导致 flink job 挂掉</span></span><br><span class="line">                    log.error(<span class="string">&quot;need monitor ids error&quot;</span>, e);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.isCancel = <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        ParameterTool parameterTool = ParameterTool.fromArgs(args);</span><br><span class="line">        InputParams inputParams = <span class="keyword">new</span> InputParams(parameterTool);</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> MapStateDescriptor&lt;Long, Set&lt;Long&gt;&gt; broadcastMapStateDescriptor = <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line">                <span class="string">&quot;config-keywords&quot;</span>,</span><br><span class="line">                BasicTypeInfo.LONG_TYPE_INFO,</span><br><span class="line">                TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Set&lt;Long&gt;&gt;() &#123;</span><br><span class="line">                &#125;));</span><br><span class="line"></span><br><span class="line">        <span class="comment">/********************* kafka source *********************/</span></span><br><span class="line">        BroadcastStream&lt;Map&lt;Long, Set&lt;Long&gt;&gt;&gt; broadcastStream = env</span><br><span class="line">                .addSource(<span class="keyword">new</span> NeedMonitorIdsSource()) <span class="comment">// redis photoId 数据广播</span></span><br><span class="line">                .setParallelism(<span class="number">1</span>)</span><br><span class="line">                .broadcast(broadcastMapStateDescriptor);</span><br><span class="line"></span><br><span class="line">        DataStream&lt;CommonModel&gt; logSourceDataStream = SourceFactory.getSourceDataStream(...);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/********************* dag *********************/</span></span><br><span class="line">        DataStream&lt;CommonModel&gt; resultDataStream = logSourceDataStream</span><br><span class="line">                .keyBy(KeySelectorFactory.getStringKeySelector(CommonModel::getKeyField))</span><br><span class="line">                .connect(broadcastStream)</span><br><span class="line">                .process(<span class="keyword">new</span> KeyedBroadcastProcessFunction&lt;String, CommonModel, Map&lt;Long, Set&lt;Long&gt;&gt;, CommonModel&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">private</span> Set&lt;Long&gt; needMonitoredIds;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                        <span class="keyword">this</span>.needMonitoredIds = Rpc.get(...)</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(CommonModel commonModel, ReadOnlyContext readOnlyContext, Collector&lt;CommonModel&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="comment">// 判断 commonModel 中的 id 是否在 needMonitoredIds 池中</span></span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processBroadcastElement</span><span class="params">(Map&lt;Long, Set&lt;Long&gt;&gt; longSetMap, Context context, Collector&lt;CommonModel&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="comment">// 需要监控的字段</span></span><br><span class="line">                        Set&lt;Long&gt; needMonitorIds = longSetMap.get(<span class="number">0L</span>);</span><br><span class="line">                        <span class="keyword">if</span> (CollectionUtils.isNotEmpty(needMonitorIds)) &#123;</span><br><span class="line">                            <span class="keyword">this</span>.needMonitoredIds = needMonitorIds;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/********************* kafka sink *********************/</span></span><br><span class="line">        SinkFactory.setSinkDataStream(...);</span><br><span class="line">        </span><br><span class="line">        env.execute(inputParams.jobName);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="方案-4"><a href="#方案-4" class="headerlink" title="方案 4"></a>方案 4</h3><p>适合于超大监控范围的数据（<strong>几百万，我们自己的生产实践中使用扩量到 500 万</strong>）。其原理是将监控范围接口按照 id 按照一定规则分桶。flink 消费到日志数据后将 id 按照 监控范围接口 id 相同的分桶方法进行分桶 keyBy，这样在下游算子中每个算子中就可以按照桶名称，从接口中拿到对应桶的监控 id 数据，这样 flink 中并行的每个算子只需要获取到自己对应的桶的数据，可以大大减少请求的压力。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Example</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        ParameterTool parameterTool = ParameterTool.fromArgs(args);</span><br><span class="line">        InputParams inputParams = <span class="keyword">new</span> InputParams(parameterTool);</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> MapStateDescriptor&lt;Long, Set&lt;Long&gt;&gt; broadcastMapStateDescriptor = <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line">                <span class="string">&quot;config-keywords&quot;</span>,</span><br><span class="line">                BasicTypeInfo.LONG_TYPE_INFO,</span><br><span class="line">                TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Set&lt;Long&gt;&gt;() &#123;</span><br><span class="line">                &#125;));</span><br><span class="line"></span><br><span class="line">        <span class="comment">/********************* kafka source *********************/</span></span><br><span class="line"></span><br><span class="line">        DataStream&lt;CommonModel&gt; logSourceDataStream = SourceFactory.getSourceDataStream(...);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/********************* dag *********************/</span></span><br><span class="line">        DataStream&lt;CommonModel&gt; resultDataStream = logSourceDataStream</span><br><span class="line">                .keyBy(KeySelectorFactory.getLongKeySelector(CommonModel::getKeyField))</span><br><span class="line">                .timeWindow(Time.seconds(inputParams.accTimeWindowSeconds))</span><br><span class="line">                .process(<span class="keyword">new</span> ProcessWindowFunction&lt;CommonModel, CommonModel, Long, TimeWindow&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">private</span> <span class="keyword">long</span> lastRefreshTimestamp;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">private</span> Set&lt;Long&gt; oneBucketNeedMonitoredIds;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Long bucket, Context context, Iterable&lt;CommonModel&gt; iterable, Collector&lt;CommonModel&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">long</span> windowStart = context.window().getStart();</span><br><span class="line">                        <span class="keyword">this</span>.refreshNeedMonitoredIds(windowStart, bucket);</span><br><span class="line">                        <span class="comment">/**</span></span><br><span class="line"><span class="comment">                         * 判断 commonModel 中的 id 是否在 needMonitoredIds 池中</span></span><br><span class="line"><span class="comment">                         */</span></span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">refreshNeedMonitoredIds</span><span class="params">(<span class="keyword">long</span> windowStart, <span class="keyword">long</span> bucket)</span> </span>&#123;</span><br><span class="line">                        <span class="comment">// 每隔 10 秒访问一次</span></span><br><span class="line">                        <span class="keyword">if</span> (windowStart - <span class="keyword">this</span>.lastRefreshTimestamp &gt;= <span class="number">10000L</span>) &#123;</span><br><span class="line">                            <span class="keyword">this</span>.lastRefreshTimestamp = windowStart;</span><br><span class="line">                            <span class="keyword">this</span>.oneBucketNeedMonitoredIds = Rpc.get(bucket, ...)</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/********************* kafka sink *********************/</span></span><br><span class="line">        SinkFactory.setSinkDataStream(...);</span><br><span class="line"></span><br><span class="line">        env.execute(inputParams.jobName);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文首先介绍了，在短视频领域中，短视频生产消费数据链路的整个闭环，并且其数据链路闭环一般情况下也适用于其他场景；以及对应的实时监控方案的设计和不同场景下的代码实现，包括：</p><ul><li><p><strong>垂类生态短视频生产消费数据链路闭环：用户操作行为日志的流转，日志上传，实时计算，以及流转到 BI，数据服务，最后数据赋能的整个流程</strong></p></li><li><p><strong>实时监控方案设计：监控类实时计算流程中各类数据源，数据汇的选型</strong></p></li><li><p><strong>监控 id 池在不同量级场景下具体代码实现</strong></p></li></ul><h2 id="学习资料"><a href="#学习资料" class="headerlink" title="学习资料"></a>学习资料</h2><h3 id="flink"><a href="#flink" class="headerlink" title="flink"></a>flink</h3><ul><li><a href="https://github.com/flink-china/flink-training-course/blob/master/README.md">https://github.com/flink-china/flink-training-course/blob/master/README.md</a></li><li><a href="https://ververica.cn/developers-resources/">https://ververica.cn/developers-resources/</a></li><li><a href="https://space.bilibili.com/33807709">https://space.bilibili.com/33807709</a></li></ul>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>实时新增类指标标准化处理方案</title>
    <link href="https://yangyichao-mango.github.io/2020/09/01/wechat-blog/apache-flink:realtime-new-id/"/>
    <id>https://yangyichao-mango.github.io/2020/09/01/wechat-blog/apache-flink:realtime-new-id/</id>
    <published>2020-09-01T06:21:53.000Z</published>
    <updated>2020-09-06T02:04:08.078Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实时新增类指标标准化处理方案"><a href="#实时新增类指标标准化处理方案" class="headerlink" title="实时新增类指标标准化处理方案"></a>实时新增类指标标准化处理方案</h1><blockquote><p>实时指标整个链路开发过程中的一些经验。</p></blockquote><h2 id="实时新增类指标"><a href="#实时新增类指标" class="headerlink" title="实时新增类指标"></a>实时新增类指标</h2><p>大体上可以将实时新增类指标以以下两种维度进行分类。</p><h3 id="identity-id-类型维度"><a href="#identity-id-类型维度" class="headerlink" title="identity id 类型维度"></a>identity id 类型维度</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">identity id 类型</th>            <th style="width: 20%; text-align: center;">备注</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">number(long) 类型 identity id</td>            <td>数值类型 identity id 的好处在于可以使用 Bitmap 类组件做到精确去重。</td>        </tr>        <tr>            <td style="text-align: center;">字符类型 identity id</td>            <td>字符类型 identity id 去重相对复杂，有两种方式，在误差允许范围之内使用 BloomFilter 进行去重，或者使用 key-value 组件进行精确去重。</td>        </tr>    </tbody></table><h3 id="产出数据类型维度"><a href="#产出数据类型维度" class="headerlink" title="产出数据类型维度"></a>产出数据类型维度</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">产出数据类型</th>            <th style="width: 20%; text-align: center;">备注</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">明细类数据</td>            <td>此类数据一般是要求将新增的数据明细产出，uv 的含义是做过滤，产出的明细数据中的 identity id 不会有重复。输出明细数据的好处在于，我们可以在下游使用 OLAP 引擎对明细数据进行各种维度的聚合计算，从而很方便的产出不同维度下的 uv 数据。</td>        </tr>        <tr>            <td style="text-align: center;">聚合类数据</td>            <td>将一个时间窗口内的 uv 进行聚合，并且可以计算出分维度的 uv，其产出数据一般都是[维度 + uv_count]，但是这里的维度一般情况下是都是固定维度。如果需要拓展则需要改动源码。</td>        </tr>    </tbody></table><h2 id="计算链路"><a href="#计算链路" class="headerlink" title="计算链路"></a>计算链路</h2><p>因此新增产出的链路多数就是以上两种维度因子的相互组合。</p><h3 id="number-long-类型-identity-id"><a href="#number-long-类型-identity-id" class="headerlink" title="number(long) 类型 identity id"></a>number(long) 类型 identity id</h3><p><img src="/blog-img/new/new_uid_roaringbitmap.png" alt="使用 RoaringBitmap 的 uv 计算链路"></p><p>代码示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">RoaringBitmapDuplicateable</span>&lt;<span class="title">Model</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> DEFAULT_DUPLICATE_MILLS = <span class="number">24</span> * <span class="number">3600</span> * <span class="number">1000L</span>;</span><br><span class="line"></span><br><span class="line">    BiPredicate&lt;Long, Long&gt; ROARING_BIT_MAP_CLEAR_BI_PREDICATE =</span><br><span class="line">            (start, end) -&gt; end - start &gt;= DEFAULT_DUPLICATE_MILLS;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化</span></span><br><span class="line">    <span class="keyword">default</span> ValueState&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt; getBitMapValueState(String name) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.getRuntimeContext().getState(</span><br><span class="line">                <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(name, TypeInformation.of(</span><br><span class="line">                        <span class="keyword">new</span> TypeHint&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt;() &#123; &#125;))</span><br><span class="line">        );</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">RuntimeContext <span class="title">getRuntimeContext</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">getLongId</span><span class="params">(Model model)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">Optional&lt;Logger&gt; <span class="title">getLogger</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> BiPredicate&lt;Long, Long&gt; <span class="title">roaringBitMapClearBiPredicate</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> ROARING_BIT_MAP_CLEAR_BI_PREDICATE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> List&lt;Model&gt; <span class="title">duplicateAndGet</span><span class="params">(List&lt;Model&gt; models, <span class="keyword">long</span> windowStartTimestamp</span></span></span><br><span class="line"><span class="function"><span class="params">            , ValueState&lt;Tuple2&lt;Date, Roaring64NavigableMap&gt;&gt; bitMapValueState)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Tuple2&lt;Date, Roaring64NavigableMap&gt; bitMap = checkAndGetState(windowStartTimestamp, bitMapValueState);</span><br><span class="line"></span><br><span class="line">        Map&lt;Long, Model&gt; idModelsMap = models</span><br><span class="line">                .stream()</span><br><span class="line">                .collect(Collectors.toMap(<span class="keyword">this</span>::getLongId, Function.identity(), (oldOne, newOne) -&gt; oldOne));</span><br><span class="line"></span><br><span class="line">        Set&lt;Long&gt; ids = idModelsMap.keySet();</span><br><span class="line"></span><br><span class="line">        List&lt;Model&gt; newModels = Lists.newArrayList();</span><br><span class="line">        <span class="keyword">for</span> (Long id : ids) &#123;</span><br><span class="line">            <span class="keyword">if</span> (!bitMap.f1.contains(id)) &#123;</span><br><span class="line">                <span class="keyword">if</span> (idModelsMap.containsKey(id)) &#123;</span><br><span class="line">                    newModels.add(idModelsMap.get(id));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        newModels.stream()</span><br><span class="line">                .map(<span class="keyword">this</span>::getLongId)</span><br><span class="line">                .forEach(bitMap.f1::add);</span><br><span class="line">        bitMapValueState.update(bitMap);</span><br><span class="line">        <span class="keyword">return</span> newModels;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> <span class="keyword">long</span> <span class="title">duplicateAndCount</span><span class="params">(List&lt;Model&gt; models, <span class="keyword">long</span> windowStartTimestamp</span></span></span><br><span class="line"><span class="function"><span class="params">            , ValueState&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt; bitMapValueState)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Tuple2&lt;Long, Roaring64NavigableMap&gt; bitMap = checkAndGetState(windowStartTimestamp, bitMapValueState);</span><br><span class="line"></span><br><span class="line">        Set&lt;Long&gt; ids = models</span><br><span class="line">                .stream()</span><br><span class="line">                .map(<span class="keyword">this</span>::getLongId)</span><br><span class="line">                .collect(Collectors.toSet());</span><br><span class="line"></span><br><span class="line">        List&lt;Long&gt; newIds = Lists.newArrayList();</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (Long id : ids) &#123;</span><br><span class="line">            <span class="keyword">if</span> (!bitMap.f1.contains(id)) &#123;</span><br><span class="line">                newIds.add(id);</span><br><span class="line">                count++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        newIds.forEach(bitMap.f1::add);</span><br><span class="line">        bitMapValueState.update(bitMap);</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> Tuple2&lt;Long, Roaring64NavigableMap&gt; <span class="title">checkAndGetState</span><span class="params">(<span class="keyword">long</span> windowStartTimestamp</span></span></span><br><span class="line"><span class="function"><span class="params">            , ValueState&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt; bitMapValueState)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Tuple2&lt;Long, Roaring64NavigableMap&gt; bitmap = bitMapValueState.value();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> == bitmap) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">this</span>.getLogger().ifPresent(logger -&gt;</span><br><span class="line">                    logger.info(<span class="string">&quot;New RoaringBitMapValueState Timestamp=&#123;&#125;&quot;</span>, windowStartTimestamp));</span><br><span class="line">            Tuple2&lt;Long, Roaring64NavigableMap&gt; newBitMap = Tuple2.of(windowStartTimestamp, <span class="keyword">new</span> Roaring64NavigableMap());</span><br><span class="line">            bitMapValueState.update(newBitMap);</span><br><span class="line">            <span class="keyword">return</span> newBitMap;</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="keyword">this</span>.roaringBitMapClearBiPredicate().test(bitmap.f0, windowStartTimestamp)) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">this</span>.getLogger().ifPresent(logger -&gt;</span><br><span class="line">                    logger.info(<span class="string">&quot;Clear RoaringBitMapValueState, from start=&#123;&#125; to end=&#123;&#125;&quot;</span>, bitmap.f0, windowStartTimestamp));</span><br><span class="line"></span><br><span class="line">            bitMapValueState.clear();</span><br><span class="line">            bitmap.f1.clear();</span><br><span class="line">            Tuple2&lt;Long, Roaring64NavigableMap&gt; newBitMap = Tuple2.of(windowStartTimestamp, <span class="keyword">new</span> Roaring64NavigableMap());</span><br><span class="line">            bitMapValueState.update(newBitMap);</span><br><span class="line">            <span class="keyword">return</span> newBitMap;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> bitmap;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="字符类型-identity-id"><a href="#字符类型-identity-id" class="headerlink" title="字符类型 identity id"></a>字符类型 identity id</h3><h4 id="使用-Flink-state"><a href="#使用-Flink-state" class="headerlink" title="使用 Flink state"></a>使用 Flink state</h4><p><img src="/blog-img/new/new_did_flink_state.png" alt="使用 flink state 的 uv 计算链路"></p><h4 id="使用-key-value-外存"><a href="#使用-key-value-外存" class="headerlink" title="使用 key-value 外存"></a>使用 key-value 外存</h4><p><img src="/blog-img/new/new_did_key_value.png" alt="使用 key-value 的 uv 计算链路"></p><p>如果选用的是 Redis 作为 key-value 过滤，那么这里会有一个巧用 Redis bit 特性的优化。举一个一般场景下的方案与使用 Redis bit 特性的方案做对比：</p><p>场景：假如需要同一天有几十场活动，并且都希望计算出这几十场活动的 uv，那么我们就可以按照下图设计 Redis bit 结构。</p><p>通常方案：</p><p><img src="/blog-img/new/new_did_redis.png" alt="使用 Redis 的 多 uv 指标计算链路"></p><p>这种场景下，如果有 1 亿用户，需要同时计算 50 个活动或者 50 个不同维度下的 uv。那么理论上最大 key 数量为 1 亿 * 50 = 50 亿个 key。</p><p>Redis bit 方案：</p><p><img src="/blog-img/new/new_did_redis_bit.png" alt="使用 Redis bit 特性的 多 uv 指标计算链路"></p><p>这样做的一个优点，就是这几十场活动的 uv 计算都使用了相同的 Redis key 来计算，可以大幅度减少 Redis 的容量占用。使用此方案的话，以上述相同的用户和活动场数，理论上最大<br>key 数量仅仅为 1 亿，只是 value 数量会多占几十个 bit。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>项目经验 | 一定要多思考，过脑子</title>
    <link href="https://yangyichao-mango.github.io/2020/09/01/wechat-blog/work:life-cycle/"/>
    <id>https://yangyichao-mango.github.io/2020/09/01/wechat-blog/work:life-cycle/</id>
    <published>2020-09-01T06:21:53.000Z</published>
    <updated>2020-11-29T13:06:55.630Z</updated>
    
    <content type="html"><![CDATA[<p>项目经验 | 一定要多思考，过脑子</p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>老妹是一个数据开发同学，最近在参与一个中台项目的实时数据建设，这也是她第一次完全的投入到一个项目体系当中（之前都是在某一个项目中负责一小块）。<br>就在做这个项目的过程中，遇到了一些问题。</p><p>比如可行性调研和技术方案的细节也七七八八的写了不少，但是实际上需求整体的推进进度很慢，迟迟没有进入到开发阶段。<br>依赖的上游接口不确定，整个项目的推进就暂停了。<br>和上游沟通了半天，但是聊完之后发现好像没有什么可行的结论。</p><p>总的来说就是，需求推进慢，推进方向不正确，导致浪费了很多时间。</p><p>于是老妹抱着这些问题找到了羊老狗。</p><h1 id="需求提出"><a href="#需求提出" class="headerlink" title="需求提出"></a>需求提出</h1><p>需求评审阶段之前好像还有一个阶段，提出需求阶段？？？<br>开会时候说的【这个可以提成需求】？？？不太理解，这里是否有一个阶段？？？</p><h1 id="需求评审阶段"><a href="#需求评审阶段" class="headerlink" title="需求评审阶段"></a>需求评审阶段</h1><p>需求评审阶段非常重要，需要根据目前的能力合理评估需求</p><ul><li>A：哪部分需求，以目前的能力经验能评估可行性或者收益很低的，可以直接同步出去；</li><li>B：哪部分需求能做，但是从长远来看，目前做完的结果只是一种过渡方案，可能 1-2 个月之后就会有代替方案的项目，这部分可以支持<br>，但是没必要为了做成而去支持复杂模块，不必要投入全部人力做这类临时的方案；</li><li>C：哪部分可以做，并且从长远来看，收益很高的，这部分我们可以深度调研方案。</li></ul><p>分别举例如下</p><h2 id="A"><a href="#A" class="headerlink" title="A"></a>A</h2><ul><li>目前从技术上是不可行的，或者从资源层面是不可行的，比如我们目前的能力是能抗住 100w qps 的压力，但是需求可能是 1000w qps；</li><li>分析场景上不合理，我是数据人，有的数据需求可能从分析场景上不合理，比如这个场景其实是算 pv 就可以得出结论的，但是需求是 uv，这类就没有必要，而且 uv 相对 pv 肯定更加耗时耗力。</li></ul><h2 id="B"><a href="#B" class="headerlink" title="B"></a>B</h2><ul><li>我目前是做实时数据的，用数据举例。目前的项目有两种方案：第一种方案是实时数据产出 A 维度数据到 OLAP 中，后端在查询 OLAP 时填充 B、C 维度，最后在看板展示数据；但是目前由于人力问题，后端没法支持完成。<br>因此就有了第二中方案，第二种方案是实时数据产出 A、B、C 维度数据到 OLAP 中，但是数据源没有 B、C 维度，B、C 维度的数据只有后端才存储，所以实时这边需要去存储了 B、C 维度的维表当中做关联操作，并且关联存在一定的难度。<br>这类情况下，实时关联 B、C 只是一个过渡方案，从长期来看，其实这部分没必要因为一个临时方案，而过度投入人力在收益很小的这个方案里面，可以做一些舍弃。</li></ul><h2 id="C"><a href="#C" class="headerlink" title="C"></a>C</h2><ul><li>从长远来看可以做，之后会有越来越多的业务需要使用我们产出的数据，那么我们就可以针对这些业务做通用化模型建设，收益高。即使多加一些人力在这类需求上也可以。</li></ul><p>以上这些情况在需求评审阶段，都需要对产品有一定的观点输出，可以表达我们的观点，技术可以做什么和技术应该做什么。</p><p>需求评审完成之后，就可以进入需求技术方案调研阶段。</p><h1 id="需求技术方案调研阶段"><a href="#需求技术方案调研阶段" class="headerlink" title="需求技术方案调研阶段"></a>需求技术方案调研阶段</h1><h2 id="技术方案调研-设计（5W-1H）"><a href="#技术方案调研-设计（5W-1H）" class="headerlink" title="技术方案调研 + 设计（5W 1H）"></a>技术方案调研 + 设计（5W 1H）</h2><p><strong>我们和上游能够沟通的内容是哪些？比如数据就是加字段，能不能同步一些表？？还能做什么沟通？？<br>排期能不能沟通，应不应该沟通，应不应该自己去沟通，或者最好什么时候去沟通？比如等整体的技术方案确定下来，确定了工作量，然后去定排期</strong></p><p><strong>和产品能够沟通的东西是哪些？</strong><br>需求合理性？<br>需求目前遇到的问题？<br>需求的技术方案可行性？<br>得等技术方案完全确认才能够评估工作量？</p><p><strong>和上下游能够沟通的东西是哪些？</strong><br>能否做支持？</p><p>整体就是 5W 1H 的方式去调研整个技术方案。</p><p>首先第一点，最最最重要的就是<strong>一定要有目的的盘东西</strong>。我最终要产出什么（WHAT），我的上游依赖是什么（WHAT）。<br>举例：需要产出的东西 - 目前手上有的东西 = 上游依赖。只要明确了目的，其实这部分东西很快就能够盘清楚。</p><p>第二点，我们盘清楚上游依赖之后，就是需要去想一下这些上游依赖可能存在的提供方式。<br>举例：这些上游依赖是必须都要其他上游提供吗？能不能通过一些其他的方式自己进行实现？</p><p>第三点，如果目前的技术方案满足不了需求，那么还有没有其他的方式进行实现，<strong>一定要多想可行性方案</strong>，可以多提供方案，但是方案的优劣可以让产品进行取舍抉择。<br>举例：比如数据上面需要一些维度数据，我之前可能就评估各类方法去关联维表去填充这些维度信息。其实还可以推动上游数据去添加这些维度数据。<br>让可行性方案变多，我们只需要去评估可行性方案的优劣。<br>推动上游去做改动时，一定要通过业务角度解释这些数据的通用性，不可能每来一次需求添加一次，那样会被喷，上下游压力都很大。</p><p>第四点，数据人遇到问题时能使用数据说话就使用数据说话，包括方案的可行性等。<br>举例：调研后发现，这个实现成本很高，反馈的时候一定要用数据说话，比如成本达到 xxx，使用 xxx 台机器，这个成本是不是可控的。尽量避免技术方案评审时只能反馈实现成本很高。</p><p>第五点，遇到问题时一定需要尽早的抛出问题，推动各方解决问题，而不能 block 在自己这里。<br>举例：比如在技术方案上，自己可以先和 tech leader 详细讨论，一定注意详细讨论的前提是自己对整个项目目前的需求理解一定要到位，理解有问题的地方及时和产品沟通，明确理解需求。<br>我就犯过比较明显的错误，经常会被 tech leader 问你调研的这个东西合不合理，有什么价值，这个东西能不能用其他的东西进行替代，或者你的方案为什么是这样的，这只是一个过渡的方案，我们有没有必要去完全按照需求要求的产出。</p><p>第六点，技术调研过程中，如果遇到上下游依赖的话，可以先和上下游方进行沟通，<strong>与上下游沟通一定要把握一个度</strong>，这部分可以慢慢学习理解。<br>需要站在自己做的事情的全局上和未来的通用性上去思考问题，不但要思考自己的东西，还要站在上下游的角度上思考问题。</p><p>举例：沟通时可以和上游确认这部分上游能不能做支持，哪部分能帮忙支持，哪部分不能支持。站在对方的角度上想问题，比如自己需要的某些字段或者数据，其实站在上下游的角度上是不需要的，这个就需要多考量。<br>上下游支持如果还存在难度的话，那就需要和产品同步目前的整体问题，让产品去推动各方或者是做一些取舍。</p><p>第七点，方案的每个细节都需要确认。做数据，那么数据每个字段都得清清楚楚的列出计算逻辑才算需求调研完成！！！不然很容易出现口径不一致和可行性问题。</p><p>通过上述几个步骤之后，都要最终和产品确认我们产出最终交付物。</p><p>这里结束之后，其实我们自己大致的工作量也就大致可以评估出来了，工作量是一个很迷的东西，如果工作经验不长的话，很难估计一个准确的工作量。</p><p><strong>不一定就是利用现在手上有的东西去做需求，还可以去推动其他上下游去支持需求。</strong></p><p>做项目时，没必要一口吃一个胖子，也没有必要钻一些牛角尖，有些东西比较难做，我们可以先实现简单的，后续再优化方案，进行迭代，没有什么项目是一次性能做完美的。</p><h1 id="需求排期"><a href="#需求排期" class="headerlink" title="需求排期"></a>需求排期</h1><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><p>1.什么时候可以进入排期阶段？</p><p>技术方案敲定后排人力，按照项目上下游情况排期？</p><p>2.排期的时候如果依赖上游，上游的排期应该怎么沟通？或者我们和上游能够沟通的内容是哪些？</p><p>上游的排期不应该自己去沟通。但是产品又会问，你拍的这个期有和上游沟通吗？</p><p>3.有哪些方式去做排期？</p><p>倒排：比如产品希望这个需求或者功能在某个时间点上线，那么我们就需要按照这个时间点，往前排。如果有上游依赖，还需要定下来上游依赖最晚给到的时间点。<br>举例：比如产品希望 11.30 号整体上线，如果开发、自测需要 5 人天，联调预估需要 2 人天的话，我们就就可以排 11.22 开始开发，并且还需要根据上游依赖的强弱指定上游依赖给到的时间，比如 11.23 号给到。<br>最后还需要留一定的 buffer 给自己，避免中途出现问题。</p><p>正排：这种情况下，一般都没有给定的截止日期。</p><p>3.为什么要这样划分排期？</p><p>排期一般划分为一下几个：</p><ul><li>开发</li><li>自测</li><li>联调</li><li>回测</li><li>上线</li></ul><p>很多情况下，尤其是多个项目组合作的项目，一般都只能排到联调</p><p>4.排期过程中容易忽略的关键点？</p><ul><li><strong>上下游依赖</strong>：上下游依赖很重要，上下游如果一旦发生 delay，咱的排期可能就会受到很大的影响。资源、上下游接口等等等等。</li><li><strong>风险控制</strong>：一定要说明风险点。比如资源，上游依赖，上线前的前置依赖等。最好可以有一个 checklist，根据经验列举自己在开发过程中可能会遇到的各类问题。</li></ul><p>5.所有的东西不能都以 mock 的形式进行，比如为了赶排期，复杂项目的联调全部使用 mock 数据就会存在问题</p><h1 id="开发"><a href="#开发" class="headerlink" title="开发"></a>开发</h1><p>1.开发过程中最需要注意的就是单测的编写，一定要记得编写单测，保证每个单元的代码都是正确的</p><p>2.数据开发完成后需要验数，验数过程是很重要的</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yangyichao-mango.github.io/2020/09/01/wechat-blog/broadcast/"/>
    <id>https://yangyichao-mango.github.io/2020/09/01/wechat-blog/broadcast/</id>
    <published>2020-09-01T06:21:53.000Z</published>
    <updated>2020-09-06T02:00:13.372Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实时新增类指标标准化处理方案"><a href="#实时新增类指标标准化处理方案" class="headerlink" title="实时新增类指标标准化处理方案"></a>实时新增类指标标准化处理方案</h1><blockquote><p>实时指标整个链路开发过程中的一些经验。</p></blockquote><h2 id="实时新增类指标"><a href="#实时新增类指标" class="headerlink" title="实时新增类指标"></a>实时新增类指标</h2><p>大体上可以将实时新增类指标以以下两种维度进行分类。</p><h3 id="identity-id-类型维度"><a href="#identity-id-类型维度" class="headerlink" title="identity id 类型维度"></a>identity id 类型维度</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">identity id 类型</th>            <th style="width: 20%; text-align: center;">备注</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">number(long) 类型 identity id</td>            <td>数值类型 identity id 的好处在于可以使用 Bitmap 类组件做到精确去重。</td>        </tr>        <tr>            <td style="text-align: center;">字符类型 identity id</td>            <td>字符类型 identity id 去重相对复杂，有两种方式，在误差允许范围之内使用 BloomFilter 进行去重，或者使用 key-value 组件进行精确去重。</td>        </tr>    </tbody></table><h3 id="产出数据类型维度"><a href="#产出数据类型维度" class="headerlink" title="产出数据类型维度"></a>产出数据类型维度</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">产出数据类型</th>            <th style="width: 20%; text-align: center;">备注</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">明细类数据</td>            <td>此类数据一般是要求将新增的数据明细产出，uv 的含义是做过滤，产出的明细数据中的 identity id 不会有重复。输出明细数据的好处在于，我们可以在下游使用 OLAP 引擎对明细数据进行各种维度的聚合计算，从而很方便的产出不同维度下的 uv 数据。</td>        </tr>        <tr>            <td style="text-align: center;">聚合类数据</td>            <td>将一个时间窗口内的 uv 进行聚合，并且可以计算出分维度的 uv，其产出数据一般都是[维度 + uv_count]，但是这里的维度一般情况下是都是固定维度。如果需要拓展则需要改动源码。</td>        </tr>    </tbody></table><h2 id="计算链路"><a href="#计算链路" class="headerlink" title="计算链路"></a>计算链路</h2><p>因此新增产出的链路多数就是以上两种维度因子的相互组合。</p><h3 id="number-long-类型-identity-id"><a href="#number-long-类型-identity-id" class="headerlink" title="number(long) 类型 identity id"></a>number(long) 类型 identity id</h3><p><img src="new_uid_roaringbitmap.png" alt="使用 RoaringBitmap 的 uv 计算链路"></p><p>代码示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">RoaringBitmapDuplicateable</span>&lt;<span class="title">Model</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> DEFAULT_DUPLICATE_MILLS = <span class="number">24</span> * <span class="number">3600</span> * <span class="number">1000L</span>;</span><br><span class="line"></span><br><span class="line">    BiPredicate&lt;Long, Long&gt; ROARING_BIT_MAP_CLEAR_BI_PREDICATE =</span><br><span class="line">            (start, end) -&gt; end - start &gt;= DEFAULT_DUPLICATE_MILLS;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化</span></span><br><span class="line">    <span class="keyword">default</span> ValueState&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt; getBitMapValueState(String name) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.getRuntimeContext().getState(</span><br><span class="line">                <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(name, TypeInformation.of(</span><br><span class="line">                        <span class="keyword">new</span> TypeHint&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt;() &#123; &#125;))</span><br><span class="line">        );</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">RuntimeContext <span class="title">getRuntimeContext</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">getLongId</span><span class="params">(Model model)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">Optional&lt;Logger&gt; <span class="title">getLogger</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> BiPredicate&lt;Long, Long&gt; <span class="title">roaringBitMapClearBiPredicate</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> ROARING_BIT_MAP_CLEAR_BI_PREDICATE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> List&lt;Model&gt; <span class="title">duplicateAndGet</span><span class="params">(List&lt;Model&gt; models, <span class="keyword">long</span> windowStartTimestamp</span></span></span><br><span class="line"><span class="function"><span class="params">            , ValueState&lt;Tuple2&lt;Date, Roaring64NavigableMap&gt;&gt; bitMapValueState)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Tuple2&lt;Date, Roaring64NavigableMap&gt; bitMap = checkAndGetState(windowStartTimestamp, bitMapValueState);</span><br><span class="line"></span><br><span class="line">        Map&lt;Long, Model&gt; idModelsMap = models</span><br><span class="line">                .stream()</span><br><span class="line">                .collect(Collectors.toMap(<span class="keyword">this</span>::getLongId, Function.identity(), (oldOne, newOne) -&gt; oldOne));</span><br><span class="line"></span><br><span class="line">        Set&lt;Long&gt; ids = idModelsMap.keySet();</span><br><span class="line"></span><br><span class="line">        List&lt;Model&gt; newModels = Lists.newArrayList();</span><br><span class="line">        <span class="keyword">for</span> (Long id : ids) &#123;</span><br><span class="line">            <span class="keyword">if</span> (!bitMap.f1.contains(id)) &#123;</span><br><span class="line">                <span class="keyword">if</span> (idModelsMap.containsKey(id)) &#123;</span><br><span class="line">                    newModels.add(idModelsMap.get(id));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        newModels.stream()</span><br><span class="line">                .map(<span class="keyword">this</span>::getLongId)</span><br><span class="line">                .forEach(bitMap.f1::add);</span><br><span class="line">        bitMapValueState.update(bitMap);</span><br><span class="line">        <span class="keyword">return</span> newModels;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> <span class="keyword">long</span> <span class="title">duplicateAndCount</span><span class="params">(List&lt;Model&gt; models, <span class="keyword">long</span> windowStartTimestamp</span></span></span><br><span class="line"><span class="function"><span class="params">            , ValueState&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt; bitMapValueState)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Tuple2&lt;Long, Roaring64NavigableMap&gt; bitMap = checkAndGetState(windowStartTimestamp, bitMapValueState);</span><br><span class="line"></span><br><span class="line">        Set&lt;Long&gt; ids = models</span><br><span class="line">                .stream()</span><br><span class="line">                .map(<span class="keyword">this</span>::getLongId)</span><br><span class="line">                .collect(Collectors.toSet());</span><br><span class="line"></span><br><span class="line">        List&lt;Long&gt; newIds = Lists.newArrayList();</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (Long id : ids) &#123;</span><br><span class="line">            <span class="keyword">if</span> (!bitMap.f1.contains(id)) &#123;</span><br><span class="line">                newIds.add(id);</span><br><span class="line">                count++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        newIds.forEach(bitMap.f1::add);</span><br><span class="line">        bitMapValueState.update(bitMap);</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> Tuple2&lt;Long, Roaring64NavigableMap&gt; <span class="title">checkAndGetState</span><span class="params">(<span class="keyword">long</span> windowStartTimestamp</span></span></span><br><span class="line"><span class="function"><span class="params">            , ValueState&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt; bitMapValueState)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Tuple2&lt;Long, Roaring64NavigableMap&gt; bitmap = bitMapValueState.value();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> == bitmap) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">this</span>.getLogger().ifPresent(logger -&gt;</span><br><span class="line">                    logger.info(<span class="string">&quot;New RoaringBitMapValueState Timestamp=&#123;&#125;&quot;</span>, windowStartTimestamp));</span><br><span class="line">            Tuple2&lt;Long, Roaring64NavigableMap&gt; newBitMap = Tuple2.of(windowStartTimestamp, <span class="keyword">new</span> Roaring64NavigableMap());</span><br><span class="line">            bitMapValueState.update(newBitMap);</span><br><span class="line">            <span class="keyword">return</span> newBitMap;</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="keyword">this</span>.roaringBitMapClearBiPredicate().test(bitmap.f0, windowStartTimestamp)) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">this</span>.getLogger().ifPresent(logger -&gt;</span><br><span class="line">                    logger.info(<span class="string">&quot;Clear RoaringBitMapValueState, from start=&#123;&#125; to end=&#123;&#125;&quot;</span>, bitmap.f0, windowStartTimestamp));</span><br><span class="line"></span><br><span class="line">            bitMapValueState.clear();</span><br><span class="line">            bitmap.f1.clear();</span><br><span class="line">            Tuple2&lt;Long, Roaring64NavigableMap&gt; newBitMap = Tuple2.of(windowStartTimestamp, <span class="keyword">new</span> Roaring64NavigableMap());</span><br><span class="line">            bitMapValueState.update(newBitMap);</span><br><span class="line">            <span class="keyword">return</span> newBitMap;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> bitmap;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="字符类型-identity-id"><a href="#字符类型-identity-id" class="headerlink" title="字符类型 identity id"></a>字符类型 identity id</h3><h4 id="使用-Flink-state"><a href="#使用-Flink-state" class="headerlink" title="使用 Flink state"></a>使用 Flink state</h4><p><img src="new_did_flink_state.png" alt="使用 flink state 的 uv 计算链路"></p><h4 id="使用-key-value-外存"><a href="#使用-key-value-外存" class="headerlink" title="使用 key-value 外存"></a>使用 key-value 外存</h4><p><img src="new_did_key_value.png" alt="使用 key-value 的 uv 计算链路"></p><p>如果选用的是 Redis 作为 key-value 过滤，那么这里会有一个巧用 Redis bit 特性的优化。举一个一般场景下的方案与使用 Redis bit 特性的方案做对比：</p><p>场景：假如需要同一天有几十场活动，并且都希望计算出这几十场活动的 uv，那么我们就可以按照下图设计 Redis bit 结构。</p><p>通常方案：</p><p><img src="new_did_redis.png" alt="使用 Redis 的 多 uv 指标计算链路"></p><p>这种场景下，如果有 1 亿用户，需要同时计算 50 个活动或者 50 个不同维度下的 uv。那么理论上最大 key 数量为 1 亿 * 50 = 50 亿个 key。</p><p>Redis bit 方案：</p><p><img src="new_did_redis_bit.png" alt="使用 Redis bit 特性的 多 uv 指标计算链路"></p><p>这样做的一个优点，就是这几十场活动的 uv 计算都使用了相同的 Redis key 来计算，可以大幅度减少 Redis 的容量占用。使用此方案的话，以上述相同的用户和活动场数，理论上最大<br>key 数量仅仅为 1 亿，只是 value 数量会多占几十个 bit。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/categories/Apache-Druid/"/>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/tags/Apache-Druid/"/>
    
      <category term="Apache Zookeeper" scheme="https://yangyichao-mango.github.io/tags/Apache-Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>实时开发标准化处理方案</title>
    <link href="https://yangyichao-mango.github.io/2020/09/01/wechat-blog/realtime/"/>
    <id>https://yangyichao-mango.github.io/2020/09/01/wechat-blog/realtime/</id>
    <published>2020-09-01T06:21:53.000Z</published>
    <updated>2020-09-06T02:00:13.380Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实时开发标准化处理方案"><a href="#实时开发标准化处理方案" class="headerlink" title="实时开发标准化处理方案"></a>实时开发标准化处理方案</h1><blockquote><p>实时指标整个链路开发过程中的一些经验。</p></blockquote><h2 id="指标类型"><a href="#指标类型" class="headerlink" title="指标类型"></a>指标类型</h2><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">指标类型</th>            <th style="width: 20%; text-align: center;">备注</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">pv</td>            <td>简单 pv 类型指标，来一条日志信息加一，计算 count</td>        </tr>        <tr>            <td style="text-align: center;">uv</td>            <td>uv 类型指标，需要在一段时间范围内（一小时、一天、一场活动）正对 user_id 等 identity_id 去重计数。</td>        </tr>        <tr>            <td style="text-align: center;">监控圈定集合内的 identity_id 数据表现</td>            <td>有一组鉴定的 identity_id 集合，实时的监控或者计算这组 identity_id 相关的数据</td>        </tr>        <tr>            <td style="text-align: center;">排名</td>            <td>实时监控某些数据并根据某些指标进行排序</td>        </tr>    </tbody></table>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>项目经验 | 一定要多思考，过脑子</title>
    <link href="https://yangyichao-mango.github.io/2020/09/01/wechat-blog/work:thinker/"/>
    <id>https://yangyichao-mango.github.io/2020/09/01/wechat-blog/work:thinker/</id>
    <published>2020-09-01T06:21:53.000Z</published>
    <updated>2020-12-06T04:25:19.341Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列每篇文章都比较短小，不定期更新，从一些实际的经历出发抛砖引玉，希望给小伙伴一些启发。<br>本文介绍了博主从做数据到玩数据的整个思考过程的转变，阅读时长大概 2 分钟，话不多说，直接进入正文！</p></blockquote><p>项目经验 | 怎么从做数据转变到玩数据？</p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>是不是还一直是在<br>执行工作<br>而不是<br>真正的去思考工作？</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>为什么hashmap的数组初始化大小都是2的次方大小时，hashmap的效率最高</title>
    <link href="https://yangyichao-mango.github.io/2020/01/06/java:study-hashmap/"/>
    <id>https://yangyichao-mango.github.io/2020/01/06/java:study-hashmap/</id>
    <published>2020-01-06T12:39:35.000Z</published>
    <updated>2020-01-07T06:32:24.844Z</updated>
    
    <content type="html"><![CDATA[<p>为什么hashmap的数组初始化大小都是2的次方大小时，hashmap的效率最高</p><span id="more"></span><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">indexFor</span><span class="params">(<span class="keyword">int</span> hashcode, <span class="keyword">int</span> length)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> hashcode &amp; (length-<span class="number">1</span>);  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="怎样提高get-key-效率？"><a href="#怎样提高get-key-效率？" class="headerlink" title="怎样提高get(key)效率？"></a>怎样提高get(key)效率？</h1><p>怎样提高get(key)效率 = 怎样提高确定key的所在hashmap中数组index的效率</p><p>hashmap的数据结构是数组和链表的结合，所以我们当然希望这个hashmap里面的元素位置尽量的分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，而不用再去遍历链表。</p><p>看下图，左边两组是数组长度为16（2的4次方），右边两组是数组长度为15。两组的hashcode均为8和9，但是很明显，当它们和1110“与”的时候，产生了相同的结果，也就是说它们会定位到数组中的同一个位置上去，这就产生了碰撞，8和9会被放到同一个链表上，那么查询的时候就需要遍历这个链表，得到8或者9，这样就降低了查询的效率。同时，我们也可以发现，当数组长度为15的时候，hashcode的值会与14（1110）进行“与”，那么最后一位永远是0，而0001，0011，0101，1001，1011，0111，1101这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组长度小了很多，这意味着进一步增加了碰撞的几率，减慢了查询的效率！<br><img src="/blog-img/java:study-hashmap/hashmap-index-for.jpg" alt="hashmap-index"></p><p>1.假设key的hashcode为h，数组长度为length，为了将数据打散，使hashmap中的数组下标对应的Entry链表都有数据，首先想到的就是对length取模，计算方法如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">indexFor</span><span class="params">(<span class="keyword">int</span> h, <span class="keyword">int</span> length)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> h % length;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>因此确定了hashmap的indexFor函数的计算方式。</p><h1 id="怎样确定hashmap数组的length"><a href="#怎样确定hashmap数组的length" class="headerlink" title="怎样确定hashmap数组的length"></a>怎样确定hashmap数组的length</h1><p>“模”运算的消耗还是比较大的，能不能找一种更快速，消耗更小的方式？我们发现做位运算的消耗是很小的，所以尝试将取模运算转换成位预算，由此发现length为2的n次方时会有</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">indexFor</span><span class="params">(<span class="keyword">int</span> h, <span class="keyword">int</span> length)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> h % length; <span class="comment">// 等价于 h &amp; (length - 1);</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于 &amp; 运算符计算效率大大高于 % 运算符，所以上述计算转换为：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">indexFor</span><span class="params">(<span class="keyword">int</span> h, <span class="keyword">int</span> length)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> h &amp; (length - <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>也规定了 length 必须是2的n次方（n&gt;0）<br>除此之外，我们也可以发现，当数组长度为15的时候，hashcode的值会与14（1110）进行“与”，那么最后一位永远是0，而0001，0011，0101，1001，1011，0111，1101这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组长度小了很多，这意味着进一步增加了碰撞的几率，减慢了查询的效率！ </p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="JAVA" scheme="https://yangyichao-mango.github.io/categories/JAVA/"/>
    
    
      <category term="JAVA" scheme="https://yangyichao-mango.github.io/tags/JAVA/"/>
    
  </entry>
  
  <entry>
    <title>apache-flink:study-flink</title>
    <link href="https://yangyichao-mango.github.io/2019/11/22/apache-flink:study-flink/"/>
    <id>https://yangyichao-mango.github.io/2019/11/22/apache-flink:study-flink/</id>
    <published>2019-11-22T03:30:41.000Z</published>
    <updated>2020-05-10T10:38:05.892Z</updated>
    
    <content type="html"><![CDATA[<p>所有operator中的初始化如果写在构造函数当中就会出错，问题是序列化时的问题<br>flink从jobmanager序列化到各个 taskmanager时可能会出问题</p><p>split组件功能可以减少多个flatmap的性能损失<br>多个flatmap数据每个都是使用全部的流进行filter<br>split一个就可以满足</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;所有operator中的初始化如果写在构造函数当中就会出错，问题是序列化时的问题&lt;br&gt;flink从jobmanager序列化到各个 taskmanager时可能会出问题&lt;/p&gt;
&lt;p&gt;split组件功能可以减少多个flatmap的性能损失&lt;br&gt;多个flatmap数据每个
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：Jobs 和 Scheduling</title>
    <link href="https://yangyichao-mango.github.io/2019/11/20/apache-flink:study-flink-jobs-and-scheduling/"/>
    <id>https://yangyichao-mango.github.io/2019/11/20/apache-flink:study-flink-jobs-and-scheduling/</id>
    <published>2019-11-20T03:27:03.000Z</published>
    <updated>2019-11-20T03:35:25.990Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：Jobs 和 Scheduling</p><span id="more"></span><h1 id="Scheduling"><a href="#Scheduling" class="headerlink" title="Scheduling"></a><strong>Scheduling</strong></h1><p>Flink中的执行资源是通过 Task Slots 定义的。每个 TaskManager 都有一个或多个 Task Slots，每个 Slot 可以运行一个并行任务流。<br>并行任务流由多个连续的任务组成，例如 MapFunction 的第n个并行实例和 ReduceFunction 的第n个并行实例。请注意，Flink 经常并发地执行连续的任务：对于流式程序，基本上都会使用并行任务，对于批处理程序，也会经常使用并行任务。</p><p>下图说明了这一点。一个具有数据源、MapFunction 和 ReduceFunction 的程序。源函数和 MapFunction 的并行度为4，而 ReduceFunction 的并行度为3。流由 Source - Map - Reduce 组成。<br>在这个集群中，有两个 TaskManager，每个 TaskManager 有三个 slot，则程序将按如下所述执行。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：DataStream Api 中 State 和容错——State 的使用</title>
    <link href="https://yangyichao-mango.github.io/2019/11/19/apache-flink:study-flink-datastream-state-and-fault-tolerance-working-with-state/"/>
    <id>https://yangyichao-mango.github.io/2019/11/19/apache-flink:study-flink-datastream-state-and-fault-tolerance-working-with-state/</id>
    <published>2019-11-19T07:48:16.000Z</published>
    <updated>2019-11-20T03:29:23.261Z</updated>
    
    <content type="html"><![CDATA[<p>flink有两种基本的state，分别是Keyed State以及Operator State(non-keyed state)；其中Keyed State只能在KeyedStream上的functions及operators上使用；每个operator state会跟parallel operator中的一个实例绑定；Operator State支持parallelism变更时进行redistributing<br>Keyed State及Operator State都分别有managed及raw两种形式，managed由flink runtime来管理，由runtime负责encode及写入checkpoint；raw形式的state由operators自己管理，flink runtime无法了解该state的数据结构，将其视为raw bytes；所有的datastream function都可以使用managed state，而raw state一般仅限于自己实现operators来使用<br>stateful function可以通过CheckpointedFunction接口或者ListCheckpointed接口来使用managed operator state；CheckpointedFunction定义了snapshotState、initializeState两个方法；每当checkpoint执行的时候，snapshotState会被调用；而initializeState方法在每次用户定义的function初始化的时候(第一次初始化或者从前一次checkpoint recover的时候)被调用，该方法不仅可以用来初始化state，还可以用于处理state recovery的逻辑<br>对于manageed operator state，目前仅仅支持list-style的形式，即要求state是serializable objects的List结构，方便在rescale的时候进行redistributed；关于redistribution schemes的模式目前有两种，分别是Even-split redistribution(在restore/redistribution的时候每个operator仅仅得到整个state的sublist)及Union redistribution(在restore/redistribution的时候每个operator得到整个state的完整list)<br>FunctionSnapshotContext继承了ManagedSnapshotContext接口，它定义了getCheckpointId、getCheckpointTimestamp方法；FunctionInitializationContext继承了ManagedInitializationContext接口，它定义了isRestored、getOperatorStateStore、getKeyedStateStore方法，可以用来判断是否是在前一次execution的snapshot中restored，以及获取OperatorStateStore、KeyedStateStore对象</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Hadoop 学习：hdfs架构</title>
    <link href="https://yangyichao-mango.github.io/2019/11/13/apache-hadoop:study-hadoop-hdfs-design/"/>
    <id>https://yangyichao-mango.github.io/2019/11/13/apache-hadoop:study-hadoop-hdfs-design/</id>
    <published>2019-11-13T06:04:52.000Z</published>
    <updated>2019-11-13T07:24:26.271Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Hadoop 学习：hdfs架构</p><span id="more"></span><h1 id="文件系统-Namespace"><a href="#文件系统-Namespace" class="headerlink" title="文件系统 Namespace"></a><strong>文件系统 Namespace</strong></h1><p><img src="/blog-img/apache-hadoop:study-hadoop-hdfs-design/hdfs%E6%9E%B6%E6%9E%84.png" alt="hdfs架构"></p><p>HDFS支持传统的分层文件组织。用户或应用程序可以在这些目录中创建目录并存储文件。文件系统命名空间层次结构与大多数其他现有文件系统相似；可以创建和删除文件，将文件从一个目录移动到另一个目录，或者重命名文件。HDFS支持用户配额和访问权限。HDFS不支持硬链接或软链接。然而，HDFS体系结构并不排除实现这些特性。</p><p>虽然HDFS遵循文件系统的命名约定，但某些路径和名称（例如/.reserved和.snapshot）是保留的。透明加密和快照等功能使用保留路径。</p><p>NameNode维护文件系统名称空间。对文件系统命名空间或其属性的任何更改都由NameNode记录。应用程序可以指定HDFS应该维护的文件副本的数量。文件的副本数称为该文件的复制因子。此信息由NameNode存储。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/categories/Apache-Hadoop/"/>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/tags/Apache-Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：Table Api &amp; SQL</title>
    <link href="https://yangyichao-mango.github.io/2019/11/12/apache-flink:study-flink-table-api-and-sql/"/>
    <id>https://yangyichao-mango.github.io/2019/11/12/apache-flink:study-flink-table-api-and-sql/</id>
    <published>2019-11-12T01:51:01.000Z</published>
    <updated>2019-11-13T01:53:02.813Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：Table Api &amp; SQL</p><span id="more"></span>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：DataStream Api 中 Operators（算子）——Joining</title>
    <link href="https://yangyichao-mango.github.io/2019/11/11/apache-flink:study-flink-datastream-operators-joining/"/>
    <id>https://yangyichao-mango.github.io/2019/11/11/apache-flink:study-flink-datastream-operators-joining/</id>
    <published>2019-11-11T10:30:13.000Z</published>
    <updated>2019-11-13T02:14:11.303Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：DataStream Api 中 Operators（算子）——Joining</p><span id="more"></span><h1 id="Window-Join"><a href="#Window-Join" class="headerlink" title="Window Join"></a><strong>Window Join</strong></h1><p>Window Join 可以将两个流中相同key并且在同一个窗口中的元素进行链接。窗口可以使用 Window Assigner 进行定义，并且对来自不同的流的元素进行计算。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">stream.join(otherStream)</span><br><span class="line">    .where(&lt;KeySelector&gt;)</span><br><span class="line">    .equalTo(&lt;KeySelector&gt;)</span><br><span class="line">    .window(&lt;WindowAssigner&gt;)</span><br><span class="line">    .apply(&lt;JoinFunction&gt;)</span><br></pre></td></tr></table></figure><p>关于一些语义的解释：<br>1.两个流的成对组合的过程类似于 Inner Join，意味着如果一个流中的元素没有另一个流的元素要与之连接，则不会发出这些元素。</p><p>2.那些被连接的元素的时间戳是位于相应窗口中的最大时间戳。例如，以[5，10)为边界的窗口，则进行连接的元素的时间戳为9。</p><h2 id="Tumbling-Window-Join"><a href="#Tumbling-Window-Join" class="headerlink" title="Tumbling Window Join"></a>Tumbling Window Join</h2><p>执行 Tumbling Window Join，在相同 key，相同时间窗口内的元素会进行笛卡尔积组合，这种组合类似于 inner join，如果一个流的对应流的同一窗口中没有元素，则这个流的当前窗口数据不会发出去。</p><p><img src="/blog-img/apache-flink:study-flink-datastream-operators-joining/tumbling-window-join.svg" alt="tumbling-window-join"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"> </span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">DataStream&lt;Integer&gt; orangeStream = ...</span><br><span class="line">DataStream&lt;Integer&gt; greenStream = ...</span><br><span class="line"></span><br><span class="line">orangeStream.join(greenStream)</span><br><span class="line">    .where(&lt;KeySelector&gt;)</span><br><span class="line">    .equalTo(&lt;KeySelector&gt;)</span><br><span class="line">    .window(TumblingEventTimeWindows.of(Time.milliseconds(<span class="number">2</span>)))</span><br><span class="line">    .apply (<span class="keyword">new</span> JoinFunction&lt;Integer, Integer, String&gt; ()&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">join</span><span class="params">(Integer first, Integer second)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> first + <span class="string">&quot;,&quot;</span> + second;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><h2 id="Sliding-Window-Join"><a href="#Sliding-Window-Join" class="headerlink" title="Sliding Window Join"></a>Sliding Window Join</h2><p>执行 Sliding Window Join，具有公共 key 和公共滑动窗口的所有元素都作为成对组合进行连接，并传递给 JoinFunction 或 FlatJoinFunction。也是 inner join，当前流窗口匹配不到对应流窗口的元素则不会发送数据到下游！请注意，某些元素可能在一个滑动窗口中连接，在另一个滑动窗口中不会进行连接！</p><p><img src="/blog-img/apache-flink:study-flink-datastream-operators-joining/sliding-window-join.svg" alt="sliding-window-join"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">DataStream&lt;Integer&gt; orangeStream = ...</span><br><span class="line">DataStream&lt;Integer&gt; greenStream = ...</span><br><span class="line"></span><br><span class="line">orangeStream.join(greenStream)</span><br><span class="line">    .where(&lt;KeySelector&gt;)</span><br><span class="line">    .equalTo(&lt;KeySelector&gt;)</span><br><span class="line">    .window(SlidingEventTimeWindows.of(Time.milliseconds(<span class="number">2</span>) <span class="comment">/* size */</span>, Time.milliseconds(<span class="number">1</span>) <span class="comment">/* slide */</span>))</span><br><span class="line">    .apply (<span class="keyword">new</span> JoinFunction&lt;Integer, Integer, String&gt; ()&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">join</span><span class="params">(Integer first, Integer second)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> first + <span class="string">&quot;,&quot;</span> + second;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><h2 id="Session-Window-Join"><a href="#Session-Window-Join" class="headerlink" title="Session Window Join"></a>Session Window Join</h2><p>执行 Session Window Join，具有相同 key 的所有元素（当“组合”满足会话条件时）将以成对组合联接，并传递给JoinFunction或FlatJoinFunction。同样，也是 inner join，当前流窗口匹配不到对应流窗口的元素则不会发送数据到下游！</p><p><img src="/blog-img/apache-flink:study-flink-datastream-operators-joining/session-window-join.svg" alt="session-window-join"></p><h1 id="Interval-Join"><a href="#Interval-Join" class="headerlink" title="Interval Join"></a><strong>Interval Join</strong></h1><p>interval join 用一个公共 key 连接两个流的元素（流A和流B），其中流B的元素具有与流A中元素的时间戳相对时间间隔内的时间戳，那么这个时间间隔内两个流的元素就会 join。</p><p>即：<strong>b.timestamp ∈ [a.timestamp + lowerBound; a.timestamp + upperBound] or a.timestamp + lowerBound &lt;= b.timestamp &lt;= a.timestamp + upperBound</strong></p><p>其中 lowerBound 和 upperBound 可正可负，只要 lowerBound &lt;= upperBound。</p><p><img src="/blog-img/apache-flink:study-flink-datastream-operators-joining/interval-join.svg" alt="interval-join"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.co.ProcessJoinFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">DataStream&lt;Integer&gt; orangeStream = ...</span><br><span class="line">DataStream&lt;Integer&gt; greenStream = ...</span><br><span class="line"></span><br><span class="line">orangeStream</span><br><span class="line">    .keyBy(&lt;KeySelector&gt;)</span><br><span class="line">    .intervalJoin(greenStream.keyBy(&lt;KeySelector&gt;))</span><br><span class="line">    .between(Time.milliseconds(-<span class="number">2</span>), Time.milliseconds(<span class="number">1</span>))</span><br><span class="line">    .process (<span class="keyword">new</span> ProcessJoinFunction&lt;Integer, Integer, String()&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Integer left, Integer right, Context ctx, Collector&lt;String&gt; out)</span> </span>&#123;</span><br><span class="line">            out.collect(first + <span class="string">&quot;,&quot;</span> + second);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>apache-flink:study-allowedLateness-and-maxOutOfOrderness</title>
    <link href="https://yangyichao-mango.github.io/2019/11/11/apache-flink:study-allowedLateness-and-maxOutOfOrderness/"/>
    <id>https://yangyichao-mango.github.io/2019/11/11/apache-flink:study-allowedLateness-and-maxOutOfOrderness/</id>
    <published>2019-11-11T02:32:17.000Z</published>
    <updated>2019-11-11T02:51:52.244Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Apache Kafka 学习：kafka在数据处理中的应用</title>
    <link href="https://yangyichao-mango.github.io/2019/11/10/apache-kafka:study-kafka-in-data-process/"/>
    <id>https://yangyichao-mango.github.io/2019/11/10/apache-kafka:study-kafka-in-data-process/</id>
    <published>2019-11-10T15:12:14.000Z</published>
    <updated>2019-11-11T01:42:14.697Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Kafka 学习：kafka在数据处理中的应用的一些个人理解。</p><span id="more"></span><h1 id="离线数据处理"><a href="#离线数据处理" class="headerlink" title="离线数据处理"></a><strong>离线数据处理</strong></h1><h2 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h2><p>在离线数据处理的过程中。<br>如果不使用消息队列，在并发量很小的情况下，所有的客户端数据日志直接向hdfs写数据暂时不会产生什么问题。<br>如果不使用消息队列，在并发量很大的情况下，向 hdfs 写数据就会出现问题，首先，向hdfs写数据会有锁竞争的情况，可能会导致大部分写请求很长时间得不到锁，导致大量请求延迟或者超时，这是不能接受的；并且 hdfs 作为文件系统不能承受太大的并发量，在并发很高的情况下，集群可能会崩溃。</p><h2 id="问题原因总结"><a href="#问题原因总结" class="headerlink" title="问题原因总结"></a>问题原因总结</h2><p>总的来说，在这种情况下，问题的根本在于高并发情况下，hdfs 作为文件系统不能承受高并发请求的问题。</p><h2 id="实施方案"><a href="#实施方案" class="headerlink" title="实施方案"></a>实施方案</h2><p>所以需要一种工具可以将客户端日志这种高并发请求转换为低并发的请求，这时候就可以使用kafka这样的消息队列，客户端的高并发请求直接写入 kafka，然后 hdfs 以低并发消费 kafka 中的数据。这样就解决了文件系统不能支持高并发的情况。并且由于 kafka 的HA特性，可以保证数据的正确性。</p><h2 id="kafka作用"><a href="#kafka作用" class="headerlink" title="kafka作用"></a>kafka作用</h2><p>将高并发请求以低并发方式处理。这种方式中解耦效果不明显，下面的实时数据处理使用到的解耦效果比较明显。</p><h1 id="实时数据处理"><a href="#实时数据处理" class="headerlink" title="实时数据处理"></a><strong>实时数据处理</strong></h1><h2 id="场景-1"><a href="#场景-1" class="headerlink" title="场景"></a>场景</h2><p>在实时数据处理的过程中。<br>如果不使用消息队列，上游数据写入到类似 flink 这样的实时处理引擎当中，flink处理完成后向下游 olap 引擎（druid，clickhouse等）或者hdfs，hive，es等的文件系统写数据时，就需要为每一种 olap 引擎开发一种 connector，这样的情况下，每出现一种 olap 引擎或者每当下游的 olap 引擎升级版本引入新特性时，就需要 flink 开发工程师开发一种新的 connector 或者跟随 olap 引擎的升级而升级自己的 connector，这样 flink 开发工程师的维护成本之后就会特别高。<br>这里有同学可能会说可以在数据处理的过程中使用下游 olap 等的引擎提供的 sdk，这种方法是可以的，但是实时处理打不风情况下并发量很高，olap 引擎提供的 sdk 应对这种高并发的场景可能会有很多问题。</p><h2 id="问题原因总结-1"><a href="#问题原因总结-1" class="headerlink" title="问题原因总结"></a>问题原因总结</h2><p>问题的根本在于实时处理引擎和下游之间的耦合问题，这就需要一种HA的中间件来将各个模块进行解耦，kafka这样的消息队列可以很好的解决这中模块之间高度耦合的情况。</p><h2 id="实施方案-1"><a href="#实施方案-1" class="headerlink" title="实施方案"></a>实施方案</h2><p>在 flink 和 druid中间使用 kafka 进行解耦，让 flink 向 kafka 生成数据，druid 消费 kafka 的数据。<br>这样就使得 flink 可以只开发和维护一套针对于 kafka 的 connector，druid也只用开发和维护一套针对 kafka 的 connector，这样无论是实时处理引擎的升级或替换，或者是实时处理引擎下游的模块的升级或替换，都不会互相影响，并且这些模块的工程师只需要对消息队列的 connector 进行维护即可，并且可以根据其特性进行更好的优化。</p><h2 id="kafka作用-1"><a href="#kafka作用-1" class="headerlink" title="kafka作用"></a>kafka作用</h2><p>模块之间的解耦。让各个模块各司其职。<br>拓展：<br>1.Java 虚拟机在 Java 语言和各个系统之间的作用。<br>2.Sql 进行三范式优化，不需要将所有数据都放在一张表当中，将n对n的表拆分出一张维表进行解耦。将维度拆分为维表可以减少数据量，更好划分和使用维度数据。<br>3.经典网络五层模型，划分为五层，则在每一层中更新或者新建协议栈只需要对上下层进行兼容即可，不需要对整个网络架构模型做调整。<br>4.Maven multiModule 划分。<br>5.springMvc。<br>等等。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Kafka" scheme="https://yangyichao-mango.github.io/categories/Apache-Kafka/"/>
    
    
      <category term="Apache Kafka" scheme="https://yangyichao-mango.github.io/tags/Apache-Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：DataStream Api 中 Operators（算子）——窗口</title>
    <link href="https://yangyichao-mango.github.io/2019/11/09/apache-flink:study-flink-datastream-operators-windows/"/>
    <id>https://yangyichao-mango.github.io/2019/11/09/apache-flink:study-flink-datastream-operators-windows/</id>
    <published>2019-11-09T14:48:53.000Z</published>
    <updated>2019-11-11T10:45:12.300Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：DataStream Api 中 Operators（算子）——窗口</p><span id="more"></span><p>Keyed Windows</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">       .keyBy(...)               &lt;-  keyed versus non-keyed windows</span><br><span class="line">       .window(...)              &lt;-  required: <span class="string">&quot;assigner&quot;</span></span><br><span class="line">      [.trigger(...)]            &lt;-  optional: <span class="string">&quot;trigger&quot;</span> (<span class="keyword">else</span> <span class="keyword">default</span> trigger)</span><br><span class="line">      [.evictor(...)]            &lt;-  optional: <span class="string">&quot;evictor&quot;</span> (<span class="keyword">else</span> no evictor)</span><br><span class="line">      [.allowedLateness(...)]    &lt;-  optional: <span class="string">&quot;lateness&quot;</span> (<span class="keyword">else</span> zero)</span><br><span class="line">      [.sideOutputLateData(...)] &lt;-  optional: <span class="string">&quot;output tag&quot;</span> (<span class="keyword">else</span> no side output <span class="keyword">for</span> late data)</span><br><span class="line">       .reduce/aggregate/fold/apply()      &lt;-  required: <span class="string">&quot;function&quot;</span></span><br><span class="line">      [.getSideOutput(...)]      &lt;-  optional: <span class="string">&quot;output tag&quot;</span></span><br></pre></td></tr></table></figure><p>Non-Keyed Windows</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">       .windowAll(...)           &lt;-  required: <span class="string">&quot;assigner&quot;</span></span><br><span class="line">      [.trigger(...)]            &lt;-  optional: <span class="string">&quot;trigger&quot;</span> (<span class="keyword">else</span> <span class="keyword">default</span> trigger)</span><br><span class="line">      [.evictor(...)]            &lt;-  optional: <span class="string">&quot;evictor&quot;</span> (<span class="keyword">else</span> no evictor)</span><br><span class="line">      [.allowedLateness(...)]    &lt;-  optional: <span class="string">&quot;lateness&quot;</span> (<span class="keyword">else</span> zero)</span><br><span class="line">      [.sideOutputLateData(...)] &lt;-  optional: <span class="string">&quot;output tag&quot;</span> (<span class="keyword">else</span> no side output <span class="keyword">for</span> late data)</span><br><span class="line">       .reduce/aggregate/fold/apply()      &lt;-  required: <span class="string">&quot;function&quot;</span></span><br><span class="line">      [.getSideOutput(...)]      &lt;-  optional: <span class="string">&quot;output tag&quot;</span></span><br></pre></td></tr></table></figure><h1 id="Window-生命周期"><a href="#Window-生命周期" class="headerlink" title="Window 生命周期"></a><strong>Window 生命周期</strong></h1><p>简而言之，当属于该窗口的第一个元素到达时，将会创建一个窗口，并且当时间（Event Time 或者 Processing Time）超过其结束时间戳加上用户指定的允许延迟时间时，将完全删除该窗口，<strong>注意窗口都是左开右闭，比如：[0, 5)</strong>。<br>Flink 保证只删除基于时间的窗口，而不删除其他类型的窗口，例如 Global Window。例如，使用基于事件时间的窗口，并且创建一个窗口大小为5分钟的滚动（Tumbing）窗口，并且允许延迟1分钟。当时间戳属于12:00到12:05之间的第一个元素到达时，Flink 将创建一个新窗口，当 Watermark 通过12:06时间戳时，就会把这个窗口删除。</p><p>此外，每个窗口都包含一个 Trigger 和一个函数（ProcessWindowFunction, ReduceFunction, AggregateFunction or FoldFunction）。函数包含了要应用于窗口内容的计算，而 Trigger 制定了什么情况下才触发执行这些函数。比如，触发策略可能类似于“当窗口中的元素数超过4时”或“当 WaterMark 通过窗口结束时”进行触发。Trigger 还可以决定什么时候删除窗口中的元素。</p><p>除上述内容外，您还可以指定一个 Evictor，该 Evictor 将能够在 Trigger 触发后、应用函数之前和/或之后从窗口中移除元素。</p><p>下面例子中的窗口都是按照 Event Time 或者 Processing Time进行指定。</p><h1 id="Keyed-vs-Non-Keyed-Windows"><a href="#Keyed-vs-Non-Keyed-Windows" class="headerlink" title="Keyed vs Non-Keyed Windows"></a><strong>Keyed vs Non-Keyed Windows</strong></h1><p>首先要指定的是是否应该为流设置 key。使用keyBy（…）可以将无限流拆分为逻辑 keyed 流。</p><p>在 Keyed Stream 的情况下，传入 event 的任何属性都可以用作键。拥有一个 Keyed Stream 将允许您的窗口计算由多个任务并行执行，因为每个逻辑 Keyed Stream 都可以独立于其他任务进行处理。所有引用同一个键的 event 都将被发送到同一个并行任务（通过 partitioner 完成）。</p><p>如果不是 Keyed Stream，则不会将原始流拆分为多个逻辑流，所有窗口逻辑将由单个任务执行，即并行度为1。</p><h1 id="Tumbling-Windows"><a href="#Tumbling-Windows" class="headerlink" title="Tumbling Windows"></a><strong>Tumbling Windows</strong></h1><p><img src="/blog-img/apache-flink:study-flink-datastream-operators-windows/%E6%BB%9A%E5%8A%A8%E7%AA%97%E5%8F%A3.svg" alt="滚动窗口"></p><p>滚动窗口，如果你指定的滚动窗口大小为一天计算一次，并且你需要更具你本地的时间的 00:00:00 开始，则必须按照时区来指定窗口。<br>可以看到上图中，无论是多少个 key，每个 key 的窗口的起始和截止时间都相同。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Creates a new &#123;<span class="doctag">@code</span> TumblingEventTimeWindows&#125; &#123;<span class="doctag">@link</span> WindowAssigner&#125; that assigns</span></span><br><span class="line"><span class="comment"> * elements to time windows based on the element timestamp and offset.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 可以根据 时间戳 以及 偏移量 来指定 窗口的范围</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * &lt;p&gt;For example, if you want window a stream by hour,but window begins at the 15th minutes</span></span><br><span class="line"><span class="comment"> * of each hour, you can use &#123;<span class="doctag">@code</span> of(Time.hours(1),Time.minutes(15))&#125;,then you will get</span></span><br><span class="line"><span class="comment"> * time windows start at 0:15:00,1:15:00,2:15:00,etc.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 比如，如果需要一个窗口大小为一个小时，从每个小时的第15分钟开始计数的窗口，则可以使用下面的代码实现</span></span><br><span class="line"><span class="comment"> * of(Time.hours(1),Time.minutes(15))</span></span><br><span class="line"><span class="comment"> * 这样获得的窗口就是 0:15:00，1:15:00，2:15:00 ...</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;Rather than that,if you are living in somewhere which is not using UTC±00:00 time,</span></span><br><span class="line"><span class="comment"> * such as China which is using UTC+08:00,and you want a time window with size of one day,</span></span><br><span class="line"><span class="comment"> * and window begins at every 00:00:00 of local time,you may use &#123;<span class="doctag">@code</span> of(Time.days(1),Time.hours(-8))&#125;.</span></span><br><span class="line"><span class="comment"> * The parameter of offset is &#123;<span class="doctag">@code</span> Time.hours(-8))&#125; since UTC+08:00 is 8 hours earlier than UTC time.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 除此之外，如果您的时区不是 UTC±00:00 时间，比如在 中国（时区是 UTC+08:00），并且你需要一个一天大小的窗口，</span></span><br><span class="line"><span class="comment"> * 并且窗口时间是本地 00:00:00开始，则可以使用下面的代码实现</span></span><br><span class="line"><span class="comment"> * of(Time.days(1), Time.hours(-8))</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> size The size of the generated windows.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> offset The offset which window start would be shifted by.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> The time policy.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> TumblingEventTimeWindows <span class="title">of</span><span class="params">(Time size, Time offset)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> TumblingEventTimeWindows(size.toMilliseconds(), offset.toMilliseconds());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// daily tumbling event-time windows offset by -8 hours.</span></span><br><span class="line">    input</span><br><span class="line">        .keyBy(&lt;key selector&gt;)</span><br><span class="line">        .window(TumblingEventTimeWindows.of(Time.days(<span class="number">1</span>), Time.hours(-<span class="number">8</span>)))</span><br><span class="line">        .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Sliding-Windows"><a href="#Sliding-Windows" class="headerlink" title="Sliding Windows"></a><strong>Sliding Windows</strong></h1><p><img src="/blog-img/apache-flink:study-flink-datastream-operators-windows/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3.svg" alt="滑动窗口"></p><p>滑动窗口，如果你指定窗口大小和滑动步长一样，那么和滚动窗口的作用一样，滑动窗口的一个明显的特征就是：<strong>窗口可能会重叠，即同一个元素可能会属于不同的窗口</strong>。</p><p>和滚动窗口相同，如果你指定的滚动窗口大小为一天计算一次，你需要更具你本地的时间的 00:00:00 开始，则必须按照时区来指定窗口。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// sliding processing-time windows offset by -8 hours</span></span><br><span class="line">    input</span><br><span class="line">        .keyBy(&lt;key selector&gt;)</span><br><span class="line">        .window(SlidingProcessingTimeWindows.of(Time.hours(<span class="number">12</span>), Time.hours(<span class="number">1</span>), Time.hours(-<span class="number">8</span>)))</span><br><span class="line">        .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Session-Windows"><a href="#Session-Windows" class="headerlink" title="Session Windows"></a><strong>Session Windows</strong></h1><p><img src="/blog-img/apache-flink:study-flink-datastream-operators-windows/%E4%BC%9A%E8%AF%9D%E7%AA%97%E5%8F%A3.svg" alt="会话窗口"></p><p>会话窗口，根据会话来指定窗口，与滚动和滑动窗口相比，会话窗口不重叠，并且没有固定的开始和结束时间。会话窗口可以指定静态指定会话间隔，或者可以让用户动态指定会话间隔。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;T&gt; input = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">// event-time session windows with static gap</span></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(EventTimeSessionWindows.withGap(Time.minutes(<span class="number">10</span>)))</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br><span class="line">    </span><br><span class="line"><span class="comment">// event-time session windows with dynamic gap</span></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(EventTimeSessionWindows.withDynamicGap((element) -&gt; &#123;</span><br><span class="line">        <span class="comment">// determine and return session gap</span></span><br><span class="line">    &#125;))</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br></pre></td></tr></table></figure><h1 id="Global-Windows"><a href="#Global-Windows" class="headerlink" title="Global Windows"></a><strong>Global Windows</strong></h1><p><img src="/blog-img/apache-flink:study-flink-datastream-operators-windows/%E5%85%A8%E5%B1%80%E7%AA%97%E5%8F%A3.svg" alt="全局窗口"></p><p>全局窗口（即无窗口），代表所有元素斗数以一个全局窗口，如果你不指定 Trigger，那么永远也不会生产出数据，因为全局窗口没有窗口开始和窗口结束的概念。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;T&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">    .keyBy(<span class="xml"><span class="tag">&lt;<span class="name">key</span> <span class="attr">selector</span>&gt;</span>)</span></span><br><span class="line"><span class="xml">    .window(GlobalWindows.create())</span></span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br></pre></td></tr></table></figure><h1 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a><strong>窗口函数</strong></h1><p>在定义 window assigner 之后，我们需要指定要在每个窗口上执行的计算。当系统确定窗口准备好处理时（Trigger决定），这些窗口函数就可以用于处理每个（Keyed / Non-Keyed）窗口的元素。</p><p>窗口函数可以是ReduceFunction、AggregateFunction、FoldFunction或ProcessWindowFunction之一。前两个函数执行起来会更高效，因为 Flink 可以在每个窗口中的元素到达时递增地聚合元素。ProcessWindowFunction获取包含在窗口中的所有元素的Iterable，以及有关元素所属窗口的其他元信息。</p><p>使用ProcessWindowFunction的窗口函数不能像其他函数那样高效地执行，因为Flink在调用函数之前必须在内部缓冲窗口的所有元素。这可以通过将ProcessWindowFunction与ReduceFunction、AggregateFunction或FoldFunction组合使用来提高效率，从而使得其他窗口元数据或者窗口元素的进行增量聚合。</p><h2 id="ReduceFunction"><a href="#ReduceFunction" class="headerlink" title="ReduceFunction"></a>ReduceFunction</h2><p>变量：两个输入生成一个输出，三个变量的类型必须相同。<br>触发时间：在每个窗口的元素到来的时候进行增量聚合。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(&lt;window assigner&gt;)</span><br><span class="line">    .reduce(<span class="keyword">new</span> ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt; &#123;</span><br><span class="line">      <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Long&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Long&gt; v1, Tuple2&lt;String, Long&gt; v2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(v1.f0, v1.f1 + v2.f1);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><h2 id="AggregateFunction"><a href="#AggregateFunction" class="headerlink" title="AggregateFunction"></a>AggregateFunction</h2><p>AggregateFunction 是 ReduceFunction 的一个扩展版本。</p><p>变量：三个变量，一个是输入，一个是 accumulator 累加器，还有一个是输出，三个变量的类型可以不同。<br>触发时间：在每个窗口的元素到来的时候进行增量聚合。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The accumulator is used to keep a running sum and a count. The &#123;<span class="doctag">@code</span> getResult&#125; method</span></span><br><span class="line"><span class="comment"> * computes the average.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">AverageAggregate</span></span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;, <span class="title">Tuple2</span>&lt;<span class="title">Long</span>, <span class="title">Long</span>&gt;, <span class="title">Double</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">0L</span>, <span class="number">0L</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">add</span><span class="params">(Tuple2&lt;String, Long&gt; value, Tuple2&lt;Long, Long&gt; accumulator)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(accumulator.f0 + value.f1, accumulator.f1 + <span class="number">1L</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Double <span class="title">getResult</span><span class="params">(Tuple2&lt;Long, Long&gt; accumulator)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> ((<span class="keyword">double</span>) accumulator.f0) / accumulator.f1;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">merge</span><span class="params">(Tuple2&lt;Long, Long&gt; a, Tuple2&lt;Long, Long&gt; b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(a.f0 + b.f0, a.f1 + b.f1);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(&lt;window assigner&gt;)</span><br><span class="line">    .aggregate(<span class="keyword">new</span> AverageAggregate());</span><br></pre></td></tr></table></figure><h2 id="FoldFunction"><a href="#FoldFunction" class="headerlink" title="FoldFunction"></a>FoldFunction</h2><p>FoldFunction 是 AggregateFunction 的一个简易版本。</p><p>变量：三个个变量，一个是输出值的初始化值，一个是输入，还有一个是输出，三个变量中输入和输出的类型可以不同，但是输出和输出初始化值必须相同。<br>触发时间：在每个窗口的元素到来的时候进行增量聚合。</p><p>FoldFunction 指定如何将窗口的输入元素与输出类型的元素组合。对添加到窗口的每个元素和当前输出值增量调用 FoldFunction。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(&lt;window assigner&gt;)</span><br><span class="line">    .fold(<span class="string">&quot;&quot;</span>, <span class="keyword">new</span> FoldFunction&lt;Tuple2&lt;String, Long&gt;, String&gt;&gt; &#123;</span><br><span class="line">       <span class="function"><span class="keyword">public</span> String <span class="title">fold</span><span class="params">(String acc, Tuple2&lt;String, Long&gt; value)</span> </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> acc + value.f1;</span><br><span class="line">       &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><h2 id="ProcessWindowFunction"><a href="#ProcessWindowFunction" class="headerlink" title="ProcessWindowFunction"></a>ProcessWindowFunction</h2><p>ProcessWindowFunction 可以得到一个包含窗口的所有元素的迭代器，以及一个访问时间和状态信息的上下文对象，使得它能够提供比其他窗口函数更好的灵活性。<br>但是这是以性能和资源消耗为代价的，因为元素不能增量聚合，而是需要在内部缓冲，直到窗口可以处理为止。</p><p>变量：四个变量，一个是窗口的key，一个是包含了窗口信息的上下文，一个是窗口内所有元素的迭代器，一个是输出数据的收集器。<br>触发时间：窗口内有数据并且 Watermark 到达了窗口结束时间时触发。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">IN</span>, <span class="title">OUT</span>, <span class="title">KEY</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">implements</span> <span class="title">Function</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Evaluates the window and outputs none or several elements.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key The key for which this window is evaluated.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context The context in which the window is being evaluated.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> elements The elements in the window being evaluated.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> out A collector for emitting elements.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception The function may throw exceptions to fail the program and trigger recovery.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            KEY key,</span></span></span><br><span class="line"><span class="function"><span class="params">            Context context,</span></span></span><br><span class="line"><span class="function"><span class="params">            Iterable&lt;IN&gt; elements,</span></span></span><br><span class="line"><span class="function"><span class="params">            Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * The context holding window metadata.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Context</span> <span class="keyword">implements</span> <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span> </span>&#123;</span><br><span class="line">       <span class="comment">/**</span></span><br><span class="line"><span class="comment">        * Returns the window that is being evaluated.</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">       <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> W <span class="title">window</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">       <span class="comment">/** Returns the current processing time. */</span></span><br><span class="line">       <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">long</span> <span class="title">currentProcessingTime</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">       <span class="comment">/** Returns the current event-time watermark. */</span></span><br><span class="line">       <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">long</span> <span class="title">currentWatermark</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">       <span class="comment">/**</span></span><br><span class="line"><span class="comment">        * State accessor for per-key and per-window state.</span></span><br><span class="line"><span class="comment">        *</span></span><br><span class="line"><span class="comment">        * &lt;p&gt;&lt;b&gt;<span class="doctag">NOTE:</span>&lt;/b&gt;If you use per-window state you have to ensure that you clean it up</span></span><br><span class="line"><span class="comment">        * by implementing &#123;<span class="doctag">@link</span> ProcessWindowFunction#clear(Context)&#125;.</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">       <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> KeyedStateStore <span class="title">windowState</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">       <span class="comment">/**</span></span><br><span class="line"><span class="comment">        * State accessor for per-key global state.</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">       <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> KeyedStateStore <span class="title">globalState</span><span class="params">()</span></span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">  .keyBy(t -&gt; t.f0)</span><br><span class="line">  .timeWindow(Time.minutes(<span class="number">5</span>))</span><br><span class="line">  .process(<span class="keyword">new</span> MyProcessWindowFunction());</span><br><span class="line"></span><br><span class="line"><span class="comment">/* ... */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProcessWindowFunction</span> </span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;, <span class="title">String</span>, <span class="title">String</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String key, Context context, Iterable&lt;Tuple2&lt;String, Long&gt;&gt; input, Collector&lt;String&gt; out)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (Tuple2&lt;String, Long&gt; in: input) &#123;</span><br><span class="line">      count++;</span><br><span class="line">    &#125;</span><br><span class="line">    out.collect(<span class="string">&quot;Window: &quot;</span> + context.window() + <span class="string">&quot;count: &quot;</span> + count);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用 ProcessWindowFunction 进行元素 count 是非常低效的，下面会讲到怎样将 ReduceFunction 或者 AggregateFunction 与 ProcessWindowFunction 结合使用将增量的数据的与 ProcessWindowFunction 配合进行使用。</p><p>为什么需要用到 ProcessWindowFunction：如果必要的话，一般的业务逻辑是没必要使用到 ProcessWindowFunction 的，但是有的需求需要获取到当前元素时间戳，窗口开始结束等等的信息，这时就需要使用 ProcessWindowFunction 来获取这些信息了。</p><h3 id="ProcessWindowFunction-与-ReduceFunction"><a href="#ProcessWindowFunction-与-ReduceFunction" class="headerlink" title="ProcessWindowFunction 与 ReduceFunction"></a>ProcessWindowFunction 与 ReduceFunction</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;SensorReading&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">  .keyBy(&lt;key selector&gt;)</span><br><span class="line">  .timeWindow(&lt;duration&gt;)</span><br><span class="line">  .reduce(<span class="keyword">new</span> MyReduceFunction(), <span class="keyword">new</span> MyProcessWindowFunction());</span><br><span class="line"></span><br><span class="line"><span class="comment">// Function definitions</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReduceFunction</span> <span class="keyword">implements</span> <span class="title">ReduceFunction</span>&lt;<span class="title">SensorReading</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> SensorReading <span class="title">reduce</span><span class="params">(SensorReading r1, SensorReading r2)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> r1.value() &gt; r2.value() ? r2 : r1;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProcessWindowFunction</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">SensorReading</span>, <span class="title">Tuple2</span>&lt;<span class="title">Long</span>, <span class="title">SensorReading</span>&gt;, <span class="title">String</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String key,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Context context,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Iterable&lt;SensorReading&gt; minReadings,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Collector&lt;Tuple2&lt;Long, SensorReading&gt;&gt; out)</span> </span>&#123;</span><br><span class="line">      SensorReading min = minReadings.iterator().next();</span><br><span class="line">      out.collect(<span class="keyword">new</span> Tuple2&lt;Long, SensorReading&gt;(context.window().getStart(), min));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="ProcessWindowFunction-与-AggregateFunction"><a href="#ProcessWindowFunction-与-AggregateFunction" class="headerlink" title="ProcessWindowFunction 与 AggregateFunction"></a>ProcessWindowFunction 与 AggregateFunction</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">  .keyBy(&lt;key selector&gt;)</span><br><span class="line">  .timeWindow(&lt;duration&gt;)</span><br><span class="line">  .aggregate(<span class="keyword">new</span> AverageAggregate(), <span class="keyword">new</span> MyProcessWindowFunction());</span><br><span class="line"></span><br><span class="line"><span class="comment">// Function definitions</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The accumulator is used to keep a running sum and a count. The &#123;<span class="doctag">@code</span> getResult&#125; method</span></span><br><span class="line"><span class="comment"> * computes the average.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">AverageAggregate</span></span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;, <span class="title">Tuple2</span>&lt;<span class="title">Long</span>, <span class="title">Long</span>&gt;, <span class="title">Double</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">0L</span>, <span class="number">0L</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">add</span><span class="params">(Tuple2&lt;String, Long&gt; value, Tuple2&lt;Long, Long&gt; accumulator)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(accumulator.f0 + value.f1, accumulator.f1 + <span class="number">1L</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Double <span class="title">getResult</span><span class="params">(Tuple2&lt;Long, Long&gt; accumulator)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> ((<span class="keyword">double</span>) accumulator.f0) / accumulator.f1;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">merge</span><span class="params">(Tuple2&lt;Long, Long&gt; a, Tuple2&lt;Long, Long&gt; b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(a.f0 + b.f0, a.f1 + b.f1);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProcessWindowFunction</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">Double</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Double</span>&gt;, <span class="title">String</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String key,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Context context,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Iterable&lt;Double&gt; averages,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Collector&lt;Tuple2&lt;String, Double&gt;&gt; out)</span> </span>&#123;</span><br><span class="line">      Double average = averages.iterator().next();</span><br><span class="line">      out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(key, average));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="ProcessWindowFunction-与-FoldFunction"><a href="#ProcessWindowFunction-与-FoldFunction" class="headerlink" title="ProcessWindowFunction 与 FoldFunction"></a>ProcessWindowFunction 与 FoldFunction</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;SensorReading&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">  .keyBy(&lt;key selector&gt;)</span><br><span class="line">  .timeWindow(&lt;duration&gt;)</span><br><span class="line">  .fold(<span class="keyword">new</span> Tuple3&lt;String, Long, Integer&gt;(<span class="string">&quot;&quot;</span>,<span class="number">0L</span>, <span class="number">0</span>), <span class="keyword">new</span> MyFoldFunction(), <span class="keyword">new</span> MyProcessWindowFunction())</span><br><span class="line"></span><br><span class="line"><span class="comment">// Function definitions</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyFoldFunction</span></span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">FoldFunction</span>&lt;<span class="title">SensorReading</span>, <span class="title">Tuple3</span>&lt;<span class="title">String</span>, <span class="title">Long</span>, <span class="title">Integer</span>&gt; &gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Tuple3&lt;String, Long, Integer&gt; <span class="title">fold</span><span class="params">(Tuple3&lt;String, Long, Integer&gt; acc, SensorReading s)</span> </span>&#123;</span><br><span class="line">      Integer cur = acc.getField(<span class="number">2</span>);</span><br><span class="line">      acc.setField(cur + <span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line">      <span class="keyword">return</span> acc;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProcessWindowFunction</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">Tuple3</span>&lt;<span class="title">String</span>, <span class="title">Long</span>, <span class="title">Integer</span>&gt;, <span class="title">Tuple3</span>&lt;<span class="title">String</span>, <span class="title">Long</span>, <span class="title">Integer</span>&gt;, <span class="title">String</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String key,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Context context,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Iterable&lt;Tuple3&lt;String, Long, Integer&gt;&gt; counts,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Collector&lt;Tuple3&lt;String, Long, Integer&gt;&gt; out)</span> </span>&#123;</span><br><span class="line">    Integer count = counts.iterator().next().getField(<span class="number">2</span>);</span><br><span class="line">    out.collect(<span class="keyword">new</span> Tuple3&lt;String, Long, Integer&gt;(key, context.window().getEnd(),count));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="ProcessWindowFunction-中使用-state"><a href="#ProcessWindowFunction-中使用-state" class="headerlink" title="ProcessWindowFunction 中使用 state"></a>ProcessWindowFunction 中使用 state</h3><h2 id="WindowFunction（遗留）"><a href="#WindowFunction（遗留）" class="headerlink" title="WindowFunction（遗留）"></a>WindowFunction（遗留）</h2><p>在一些可以使用 ProcessWindowFunction 的地方，你也可以使用 WindowFunction，这是较旧版本的 ProcessWindowFunction，它提供的上下文信息较少，并且没有一些高级功能，例如 per-window keyed state。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">WindowFunction</span>&lt;<span class="title">IN</span>, <span class="title">OUT</span>, <span class="title">KEY</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">extends</span> <span class="title">Function</span>, <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Evaluates the window and outputs none or several elements.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> key The key for which this window is evaluated.</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> window The window that is being evaluated.</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> input The elements in the window being evaluated.</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> out A collector for emitting elements.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@throws</span> Exception The function may throw exceptions to fail the program and trigger recovery.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">apply</span><span class="params">(KEY key, W window, Iterable&lt;IN&gt; input, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(&lt;window assigner&gt;)</span><br><span class="line">    .apply(<span class="keyword">new</span> MyWindowFunction());</span><br></pre></td></tr></table></figure><h1 id="Triggers"><a href="#Triggers" class="headerlink" title="Triggers"></a><strong>Triggers</strong></h1><p>触发器决定了窗口函数什么时候处理窗口中的数据。每一个 WindowAssigner 都会带有一个默认的 Trigger，如果默认的 Trigger 不符合需求，你可以使用 trigger(…) 指定你需要的触发器。</p><p>一个 Trigger 接口有五个方法，可以通过编写函数指定如何对不同的 event 做出相应。</p><table>    <thead>        <tr>            <th style="width: 10%">Function</th>            <th style="width: 20%">作用</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;"><strong>TriggerResult onElement(T element, long timestamp, W window, TriggerContext ctx)</strong>            </td>            <td>每向窗口添加一个元素时触发一次。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>TriggerResult onEventTime(long time, W window, TriggerContext ctx)</strong>            </td>            <td>Event Time timer 触发时调用。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>TriggerResult onProcessingTime(long time, W window, TriggerContext ctx)</strong>            </td>            <td>Processing Time timer 调用时触发。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>void onMerge(W window, OnMergeContext ctx)</strong>            </td>            <td>方法与有状态 Trigger 相关，并在两个触发器的相应窗口合并时合并它们的状态，例如在使用会话窗口时（会话窗口每添加一个元素就会产生一个窗口）。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>void clear(W window, TriggerContext ctx)</strong>            </td>            <td>将当前窗口的 state 清除。            </td>        </tr>    </tbody></table><p>前三个函数通过 TriggerResult 决定如何处理它们的调用事件。</p><table>    <thead>        <tr>            <th style="width: 10%">TriggerResult</th>            <th style="width: 20%">作用</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;"><strong>TriggerResult.CONTINUE</strong>            </td>            <td>什么都不做。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>TriggerResult.FIRE</strong>            </td>            <td>触发计算。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>TriggerResult.PURGE</strong>            </td>            <td>清除窗口内的元素。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>TriggerResult.FIRE_AND_PURGE</strong>            </td>            <td>触发计算，并且在此之后清除窗口内的元素。            </td>        </tr>    </tbody></table><h2 id="触发运算并且清除元素"><a href="#触发运算并且清除元素" class="headerlink" title="触发运算并且清除元素"></a>触发运算并且清除元素</h2><h2 id="WindowAssigners-的默认-Triggers"><a href="#WindowAssigners-的默认-Triggers" class="headerlink" title="WindowAssigners 的默认 Triggers"></a>WindowAssigners 的默认 Triggers</h2><p>很多 WindowAssigners 的默认 Triggers是适用于很多场景的。例如，所有的 event-time window assigners 都将 EventTimeTrigger 作为默认的 Trigger。这个 Trigger 的作用就是当 Watermark 到达了窗口结束时间时就触发。<br><strong>提示1：GlobalWindow 的默认 Trigger 是永远不会触发的 NeverTrigger。</strong><br><strong>提示2：通过使用 trigger() 指定触发器，您将覆盖 WindowAssigner 的默认触发器。例如，如果为 TumblingEventTimeWindows 指定 CountTrigger，则不会再根据时间进度而仅按 count 触发窗口。</strong></p><h2 id="通用的-Triggers"><a href="#通用的-Triggers" class="headerlink" title="通用的 Triggers"></a>通用的 Triggers</h2><table>    <thead>        <tr>            <th style="width: 10%">Trigger</th>            <th style="width: 20%">作用</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;"><strong>EventTimeTrigger</strong>            </td>            <td>根据由 Watermark 计算的 Event Time 进度触发。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>ProcessingTimeTrigger</strong>            </td>            <td>根据 Processing Time 触发。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>CountTrigger</strong>            </td>            <td>在窗口中的元素数超过给定限额时触发。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>PurgingTrigger</strong>            </td>            <td>将另一个 Trigger 作为参数，并将其转换为清除触发器。            </td>        </tr>    </tbody></table><h1 id="Evictors"><a href="#Evictors" class="headerlink" title="Evictors"></a><strong>Evictors</strong></h1><p>Flink 的窗口模型中允许指定除 WindowAssigner 和 Trigger 之外的可选逐出器（Evictor）。可以使用exictor(…)方法指定。Exictor 能够在触发器触发之后，并在使用窗口函数之前或者之后移除元素。为此，逐出器接口有两个方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Optionally evicts elements. Called before windowing function.</span></span><br><span class="line"><span class="comment"> * 在执行窗口函数之前执行。</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> elements The elements currently in the pane.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> size The current number of elements in the pane.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> window The &#123;<span class="doctag">@link</span> Window&#125;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> evictorContext The context for the Evictor</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">evictBefore</span><span class="params">(Iterable&lt;TimestampedValue&lt;T&gt;&gt; elements, <span class="keyword">int</span> size, W window, EvictorContext evictorContext)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Optionally evicts elements. Called after windowing function.</span></span><br><span class="line"><span class="comment"> * 在执行窗口函数之后执行。</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> elements The elements currently in the pane.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> size The current number of elements in the pane.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> window The &#123;<span class="doctag">@link</span> Window&#125;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> evictorContext The context for the Evictor</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">evictAfter</span><span class="params">(Iterable&lt;TimestampedValue&lt;T&gt;&gt; elements, <span class="keyword">int</span> size, W window, EvictorContext evictorContext)</span></span>;</span><br></pre></td></tr></table></figure><table>    <thead>        <tr>            <th style="width: 10%">Evictor</th>            <th style="width: 20%">作用</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;"><strong>CountEvictor</strong>            </td>            <td>保持窗口内元素数量符合用户指定数量，如果多于用户指定的数量，从窗口缓冲区的开头丢弃剩余的元素。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>DeltaEvictor</strong>            </td>            <td>使用 DeltaFunction 和一个阈值，计算窗口缓冲区中的最后一个元素与其余每个元素之间的 delta 值，并删除 delta 值大于或等于阈值的元素。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>TimeEvictor</strong>            </td>            <td>以毫秒为单位的时间间隔作为参数，对于给定的窗口，找到元素中的最大的时间戳max_ts，并删除时间戳小于max_ts - interval的所有元素。            </td>        </tr>    </tbody></table><p>默认情况下，都只会在执行 WindowFunction 之前执行 Evictor。</p><p><strong>提示：Flink 不能保证窗口中元素的顺序。这意味着尽管逐出器可能会从窗口的开头移除元素，但这些元素不一定是最先到达或最后到达的元素。</strong></p><h1 id="Allowed-Lateness"><a href="#Allowed-Lateness" class="headerlink" title="Allowed Lateness"></a><strong>Allowed Lateness</strong></h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>使用 Event Time 窗口时，可能会发生元素到达晚的情况，即 Flink 用于跟踪 Event Time 进度的 Watermark 已超过元素所属窗口的结束时间戳。</p><p>默认情况下，当发现 Watermark 已经超过到达的元素所属的窗口结束时间时，将删除这个延迟元素。但是 Flink 可以给窗口算子指定一个最大允许延迟时间。Allowed lateness 指定元素在被删除之前可以延迟多少时间，其默认值为0。</p><p>在迟到元素到达时，如果 Watermark 大于其所属窗口的结束时间时，并且 Watermark 小于窗口结束时间加上 allowed lateness，这个迟到的元素仍然可以被加到这个窗口内进行运算。有的触发器会出现在延迟但是没有丢弃的元素到达时，使得窗口再次计算，比如 EventTimeTrigger。</p><p><strong>提示1：在 assignTimestampsAndWatermarks 时有一个 maxOutOfOrderness 的概念，maxOutOfOrderness 是生成 Watermark 所需要的，是指元素最大无序时间。而 Allowed Lateness 是指在 Watermark 到达窗口结束时间之后允许延迟多长时间，两个概念不一样。</strong><br><strong>提示2：<br>a.通过 watermark 机制来处理 out-of-order 的问题，属于第一层防护，属于全局性的防护，通常说的乱序问题的解决办法，就是指这类；<br>b.通过窗口上的 allowedLateness 机制来处理 out-of-order 的问题，属于第二层防护，属于特定 window operator 的防护，late element 的问题就是指这类</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;T&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(&lt;window assigner&gt;)</span><br><span class="line">    .allowedLateness(&lt;time&gt;)</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br></pre></td></tr></table></figure><p><strong>提示：当使用 GlobalWindows 时，没有元素会被认为是迟到的，因为这个窗口哦的结束时间时 Long.MAX_VALUE。</strong></p><h2 id="迟到的数据做旁路输出（side-output）"><a href="#迟到的数据做旁路输出（side-output）" class="headerlink" title="迟到的数据做旁路输出（side output）"></a>迟到的数据做旁路输出（side output）</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> OutputTag&lt;T&gt; lateOutputTag = <span class="keyword">new</span> OutputTag&lt;T&gt;(<span class="string">&quot;late-data&quot;</span>)&#123;&#125;;</span><br><span class="line"></span><br><span class="line">DataStream&lt;T&gt; input = ...;</span><br><span class="line"></span><br><span class="line">SingleOutputStreamOperator&lt;T&gt; result = input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(&lt;window assigner&gt;)</span><br><span class="line">    .allowedLateness(&lt;time&gt;)</span><br><span class="line">    .sideOutputLateData(lateOutputTag)</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br><span class="line"></span><br><span class="line">DataStream&lt;T&gt; lateStream = result.getSideOutput(lateOutputTag);</span><br></pre></td></tr></table></figure><h2 id="拓展思考"><a href="#拓展思考" class="headerlink" title="拓展思考"></a>拓展思考</h2><p>当指定 Allowed Lateness &gt; 0 时，在 Watermark 通过窗口结束时间后，<strong>将保留窗口及其内容</strong>。在这种情况下，当一个延迟但未被丢弃的元素到达时，它可能会再次触发窗口运算。这些被触发运算的被称为 late firing。在使用会话窗口时，它们可能会将两个预先存在的未合并窗口进行合并，下面是一个例子。</p><p>比如：有一个会话窗口且 Gap 为3分钟，现在有两个窗口，第一个窗口起始和结束时间为（01:00:00，01:00:05），第二个窗口起始和结束时间为（01:00:09，01:00:15），如果我们在此时不设置 Allowed Lateness 时，那么如果不保存第一个窗口的数据，运算第二个窗口的数据时，不会有什么问题，但是如果我们设置了 Allowed Lateness = 5 min，那么这时就会有问题了，比如有迟到元素01:00:15才到达，元素自己的时间戳为01:00:07，这样这个元素就可以将两个窗口的数据结合为一个窗口。</p><p><strong>提示：延迟数据触发的运算应该将之前的计算结果更新，所以如果下游 sink 使用了 kafka，则这种情况不是很适用（除非消费 kafka 的是一些 updateable dfs），否则，你将会得到很多的对相同组数据计算的结果。</strong></p><h1 id="窗口结果的使用"><a href="#窗口结果的使用" class="headerlink" title="窗口结果的使用"></a><strong>窗口结果的使用</strong></h1><p>窗口计算的结果也会转化为一个数据流，这份结果中不会包含窗口操作的任何信息，所以如果后续计算中需要这些信息，你必须使用 ProcessWindowFunction 将这些信息通过编码传输进去。</p><h2 id="窗口和-Watermark-的联系"><a href="#窗口和-Watermark-的联系" class="headerlink" title="窗口和 Watermark 的联系"></a>窗口和 Watermark 的联系</h2><p>当 Watermark 到达窗口算子处时，会触发两个事件：<br>1.Watermark 会触发所有的窗口中的最大时间戳（窗口结束时间戳 - 1）&lt; 到达的最新 Watermark的窗口运算。<br>2.将 Watermark 发送到下游算子。<br>Intuitively, a watermark “flushes” out any windows that would be considered late in downstream operations once they receive that watermark.</p><h2 id="连续的窗口算子"><a href="#连续的窗口算子" class="headerlink" title="连续的窗口算子"></a>连续的窗口算子</h2><h2 id="设置窗口时产生的状态大小的注意事项"><a href="#设置窗口时产生的状态大小的注意事项" class="headerlink" title="设置窗口时产生的状态大小的注意事项"></a>设置窗口时产生的状态大小的注意事项</h2><p>窗口大小可以定义的很大（如天、周或月），因此可能会累积非常大的状态（state）。所以在估计窗口计算的存储需求时，需要记住以下几个规则：</p><p>1.Flink 会为每个元素所属的窗口创建一个副本。因此，滚动的窗口保留每个元素的一个副本（一个元素只属于一个窗口）。相反，滑动窗口可能会创建每个元素中的几个副本。因此，1天大小的窗口，1秒滑动步长的滑动窗口可能不是一个好主意。</p><p>2.ReduceFunction、AggregateFunction 和 FoldFunction 可以显著减少存储需求，因为它们在元素到达时就聚合元素，并且每个窗口只存储一个值。相反，仅仅使用 ProcessWindowFunction 就需要累积所有元素。</p><p>3.使用 Evictor 可防止任何预聚合，因为在应用计算之前，窗口的所有元素都必须通过 Evictor 传递。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：DataStream Api 中 Operators（算子）——概览</title>
    <link href="https://yangyichao-mango.github.io/2019/11/09/apache-flink:study-flink-datastream-operators-overview/"/>
    <id>https://yangyichao-mango.github.io/2019/11/09/apache-flink:study-flink-datastream-operators-overview/</id>
    <published>2019-11-09T08:22:30.000Z</published>
    <updated>2019-11-09T14:47:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>描述了基本 Operators 的 Transformations，应用这些转换后如何进行 physical partitioning（物理分区），以及对Flink算子链的深入了解。</p><span id="more"></span><h1 id="DataStream-Transformations"><a href="#DataStream-Transformations" class="headerlink" title="DataStream Transformations"></a><strong>DataStream Transformations</strong></h1><h1 id="Physical-partitioning"><a href="#Physical-partitioning" class="headerlink" title="Physical partitioning"></a><strong>Physical partitioning</strong></h1><table>    <thead>        <tr>            <th style="width: 10%">Transformation</th>            <th style="width: 20%">Description</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">Custom partitioning<br>                DataStream → DataStream<br>                <strong>CustomPartitionerWrapper<<T>k, T></strong>            </td>            <td>使用自定义的 partitioner 为每一条 record 选择下一个 task<br>                <strong>dataStream.partitionCustom(partitioner, "someKey");</strong><br>                <strong>dataStream.partitionCustom(partitioner, 0);</strong>            </td>        </tr>        <tr>            <td style="text-align: center;">Random partitioning<br>                DataStream → DataStream<br>                <strong>ShufflePartitioner<<T>T></strong>            </td>            <td>按随机均匀划分元素<br>                <strong>dataStream.shuffle();</strong>            </td>        </tr>        <tr>            <td style="text-align: center;">Rebalancing (Round-robin partitioning)<br>                DataStream → DataStream<br>                <strong>RebalancePartitioner<<T>T></strong>            </td>            <td>分区循环划分元素，为每个下游创建相等的负载。对于存在数据倾斜的性能优化非常有用。<br>                <strong>dataStream.rebalance();</strong>            </td>        </tr>        <tr>            <td style="text-align: center;">Rescaling<br>                DataStream → DataStream<br>                <strong>RescalePartitioner<<T>T></strong>            </td>            <td>将元素循环（round robin）分配到下游 operator 的子集。如果你的 pipeline 是一下的情况，那么这种方式会非常有用。<br>                例如，将并行数据源的每个实例的数据传输到的下游多个算子（operators）一个子集以分配负载。但不希望 full rebalance，则这非常有用。<br>                如果合理配置 TaskManager 的 slot数量，则数据传输只需要本地传输，而不需要通过网络传输数据。<br><br>                上游 operators 向的下游 operators 发送 record 取决于上游 operators 和下游 operators 的并行度。<br>                例如，如果上游 operator 的并行度为2，而下游 operator 的并行度为6，则一个上游 operator 将 record 分配给三个下游 operator，而另一个上游 operator 将 record 分配给其他三个下游 operator。相反，如果下游 operator 的并行度为2，而上游 operator 的并行度为6，则三个上游 operator 将分配给一个下游 operator，而其他三个上游 operator 将分配给另一个下游 operator。<br><br>                如果上下游算子的并行度不是彼此的倍数，则一个或多个下游 operator 将具有来自上游 operator 的不同数量的输入。                如下图：                <figure>                    <div class="img-lightbox">                        <div class="overlay"></div>                        <img src="/blog-img/apache-flink:study-flink-datastream-operators-overview/rescale_partition.svg" alt="rescale_partition" title="">                    </div>                </figure>                <strong>dataStream.rescale();</strong>            </td>        </tr>        <tr>            <td style="text-align: center;">Broadcasting<br>                DataStream → DataStream<br>                <strong>BroadcastPartitioner<<T>T></strong>            </td>            <td>广播数据到下游的每个partition。<br>                <strong>dataStream.broadcast();</strong>            </td>        </tr>        <tr>            <td style="text-align: center;">Local Forward<br>                DataStream → DataStream<br>                <strong>ForwardPartitioner<<T>T></strong>            </td>            <td>数据传输到本地的下游算子。<br>                <strong>dataStream.forward();</strong>            </td>        </tr>        <tr>            <td style="text-align: center;">GlobalPartitioner<br>                DataStream → DataStream<br>                <strong>GlobalPartitioner<<T>T></strong>            </td>            <td>数据传输到下游子任务id为0的task中。<br>                <strong>dataStream.forward();</strong>            </td>        </tr>        <tr>            <td style="text-align: center;">Key Groups<br>                DataStream → DataStream<br>                <strong>KeyGroupStreamPartitioner<<T>T, K></strong>            </td>            <td>相同key的值会传输到同一个下游。类似于Rescaling，但是不用再api中指定，再使用keyBy时会自动指定此方法。<br>                <strong>dataStream.keyBy();</strong>            </td>        </tr>    </tbody></table><h1 id="Task-chaining-和-资源组"><a href="#Task-chaining-和-资源组" class="headerlink" title="Task chaining 和 资源组"></a><strong>Task chaining 和 资源组</strong></h1><p>链接两个 Transformations 意味着可以将它们共同放在在同一个线程中执行以获得更好的性能。<br>默认情况下，Flink会尽可能链接两个算子（例如，两个 Map Transformations）。如果需要，可以使用API对 Task chaining 进行细粒度控制：</p><p>在 Flink 中，一个 slot 就是一个资源组。如果需要的话，你可以通过使用api把上下游算子隔离在不同的 slot 中运行。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：DataStream Api 中 EventTime</title>
    <link href="https://yangyichao-mango.github.io/2019/11/09/apache-flink:study-flink-datastream-eventtime/"/>
    <id>https://yangyichao-mango.github.io/2019/11/09/apache-flink:study-flink-datastream-eventtime/</id>
    <published>2019-11-09T04:09:56.000Z</published>
    <updated>2019-11-09T07:41:48.694Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：DataStream Api 中 EventTime</p><span id="more"></span><p><img src="/blog-img/apache-flink:study-flink-datastream-eventtime/%E6%97%B6%E9%97%B4.svg" alt="时间"></p><h1 id="Processing-time"><a href="#Processing-time" class="headerlink" title="Processing time"></a><strong>Processing time</strong></h1><p>处理时间是指执行相应操作的机器的系统时间</p><h1 id="Event-time"><a href="#Event-time" class="headerlink" title="Event time"></a><strong>Event time</strong></h1><p>事件时间是每个事件在其生产设备（生产event的设备，手机等的源头设备）上发生的时间</p><h2 id="Event-Time-和-Watermark"><a href="#Event-Time-和-Watermark" class="headerlink" title="Event Time 和 Watermark"></a>Event Time 和 Watermark</h2><p>Flink中测量事件时间进度的机制是Watermark。Watermark作为数据流的一部分流动，并带有时间戳t。Watermark（t）声明该流中的 Event Time 已达到时间t，这意味着流中不应再有时间戳t’&lt;=t的元素（即 Event Time 早于或等于 Watermark 的事件）</p><p>下图显示了具有时间戳的事件流，以及内联流动的水印。在这个例子中，事件是有序的，这意味着 Watermark 只是流中的周期性标记。</p><p><img src="/blog-img/apache-flink:study-flink-datastream-eventtime/%E9%A1%BA%E5%BA%8F%E6%B5%81%E7%9A%84Watermark.svg" alt="顺序流的Watermark"></p><p><strong>Watermark对于无序流是至关重要的</strong>，如下所示，其中事件到达顺序不是按时间戳顺序。Watermark代表通过流中的该点，这个时间戳之前的事件都应该到达了。一旦Watermark到达算子，算子就可以将其内部事件时钟更新到Watermark的值。</p><p><img src="/blog-img/apache-flink:study-flink-datastream-eventtime/%E6%97%A0%E5%BA%8F%E6%B5%81%E7%9A%84Watermark.svg" alt="无序流的Watermark"></p><h2 id="并行流的-Watermarks"><a href="#并行流的-Watermarks" class="headerlink" title="并行流的 Watermarks"></a>并行流的 Watermarks</h2><p>Watermark 在源数据 Function 处生成，或直接在源数据 Function 之后生成。源数据 Function 的每个并行子任务通常独立生成其 Watermark。这些 Watermark 定义并行源数据的 Event Time。</p><p>当 Watermark 流过程序时，会更新到达算子的 Event Time，当一个 Watermark 更新了算子的 Event Time 时，它会为其下游的后续算子生成一个新的 Watermark。</p><p>某些算子消费多个输入流，例如：union 算子或者跟在 keyBy，partition 算子的之后的算子。这样的算子的 Event Time 是所有输入 stream 中最小的 Watermark，即：<br><strong>Operator Event Time = min(Input Stream 1 Watermark, Input Stream 2 Watermark…)</strong><br>算子会跟着输入流 Watermark 的更新来更新算子自己的 Event Time。</p><p>下图显示了流经并行流的事件和 Watermark 以及算子获取 Event Time 的示例。</p><p><img src="/blog-img/apache-flink:study-flink-datastream-eventtime/%E5%B9%B6%E8%A1%8C%E6%B5%81%E7%9A%84Watermarks.svg" alt="并行流的Watermarks"></p><p>注意，Kafka支持分区 Watermark</p><h1 id="Ingestion-time"><a href="#Ingestion-time" class="headerlink" title="Ingestion time"></a><strong>Ingestion time</strong></h1><p>注入时间是事件进入Flink Job的时间。在源Operator处，每条Operator获取源数据的时间作为Ingestion time时间戳</p><h1 id="生成-Timestamps-和-Watermarks"><a href="#生成-Timestamps-和-Watermarks" class="headerlink" title="生成 Timestamps 和 Watermarks"></a><strong>生成 Timestamps 和 Watermarks</strong></h1><h2 id="分配-Timestamps"><a href="#分配-Timestamps" class="headerlink" title="分配 Timestamps"></a>分配 Timestamps</h2><h3 id="数据流源生成-Timestamps-和-Watermarks"><a href="#数据流源生成-Timestamps-和-Watermarks" class="headerlink" title="数据流源生成 Timestamps 和 Watermarks"></a>数据流源生成 Timestamps 和 Watermarks</h3><p>数据源可以直接为它们产生的数据分配 Timestamp，并且他们也能发送 Watermark。这样做的话，在后面的处理中就没必要再去定义 Timestamp 分配器了，需要注意的是：如果在后面的处理中使用了一个 timestamp 分配器，由数据源提供的任何 timestamp 和 watermark 都会被重写。</p><h3 id="Timestamp-分配器-Watermark生成器"><a href="#Timestamp-分配器-Watermark生成器" class="headerlink" title="Timestamp 分配器 / Watermark生成器"></a>Timestamp 分配器 / Watermark生成器</h3><p>Timestamp 分配器获取一个流并生成一个新的带有 Timestamp 元素和 Watermark 的流。如果上游的原始数据流已经有 Timestamp 或 Watermark，则 Timestamp 分配器将覆盖上游的 Timestamp 或 Watermark</p><p>Timestamp 分配器通常在数据源之后立即指定，但这并不是严格要求的。通常是在 Timestamp 分配器之前先解析（MapFunction）和过滤（FilterFunction）数据源。在任何情况下，都需要在基于 Event Time 算子（例如 window 操作）运行之前指定 Timestamp 分配程序。<br>有一个特殊情况，当使用 Kafka 作为流作业的数据源时，Flink 允许在数据源内部指定 Timestamp 分配器和 Watermark 生成器。更多关于如何进行的信息请参考Kafka Connector的文档。</p><p>直接在FlinkKafkaConsumer010上面使用assignTimestampsAndWatermarks可以根据kafka source的partitions的特性进行设置Timestamps和Watermarks，让用户做一些特殊的处理</p><p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka<br>partition, allows users to let them exploit the per-partition characteristics.</p><h1 id="Kafka-分区的-Timestamp"><a href="#Kafka-分区的-Timestamp" class="headerlink" title="Kafka 分区的 Timestamp"></a><strong>Kafka 分区的 Timestamp</strong></h1><p>当使用 Apache Kafka 作为数据源时，每个 Kafka 分区可能有一个简单的 Event Time 模式（递增的时间戳或有界无序）。然而，当 Flink Job 使用来自Kafka的流时，多个分区常常并行消费，每一个 operator 算子并行消费时就会破坏各个分区的时间模式（这是 Kafka 客户端消费 Kafka 数据必然发生的）。</p><p>在这种情况下，可以使用 Flink’s Kafka-partition-aware watermark generation，使用该功能，每个 Kafka 分区在 Kafka consumer 内部生成 Watermark，每个分区合并 Watermark 的方式与流 shuffles 时合并 Watermark 的方式相同。</p><p>例如，如果事件时间戳严格按照 Kafka 分区递增，则使用递增时间戳 Watermark 生成器生成每个分区的 Watermark 将是完美的全局 Watermark。</p><p>下图显示了如何使用 Flink’s Kafka-partition-aware watermark generation，以及在这种情况下 Watermark 如何通过流数据流传播。</p><p><img src="/blog-img/apache-flink:study-flink-datastream-eventtime/KafkaSource%E5%A4%9A%E5%88%86%E5%8C%BAWatermark.svg" alt="KafkaSource多分区Watermark"></p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：slot和parallelism设置的关系</title>
    <link href="https://yangyichao-mango.github.io/2019/11/08/apache-flink:study-flink-slot-parallelism/"/>
    <id>https://yangyichao-mango.github.io/2019/11/08/apache-flink:study-flink-slot-parallelism/</id>
    <published>2019-11-08T07:31:08.000Z</published>
    <updated>2019-11-08T15:55:53.544Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：slot和parallelism设置的关系</p><span id="more"></span><h1 id="如何设置-parallelism"><a href="#如何设置-parallelism" class="headerlink" title="如何设置 parallelism"></a><strong>如何设置 parallelism</strong></h1><h2 id="flink-conf-yaml"><a href="#flink-conf-yaml" class="headerlink" title="flink-conf.yaml"></a>flink-conf.yaml</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cat flink-conf.yaml | grep parallelism</span><br><span class="line"></span><br><span class="line"><span class="comment"># The parallelism used for programs that did not specify and other parallelism.</span></span><br><span class="line">parallelism.default: 1</span><br></pre></td></tr></table></figure><h2 id="命令行启动"><a href="#命令行启动" class="headerlink" title="命令行启动"></a>命令行启动</h2><p>如果你是用命令行启动你的 Flink job，那么你也可以这样设置并行度(使用 -p 并行度)：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/flink run -p 10 ../word-count.jar</span><br></pre></td></tr></table></figure><h2 id="代码设置整个程序的并行度"><a href="#代码设置整个程序的并行度" class="headerlink" title="代码设置整个程序的并行度"></a>代码设置整个程序的并行度</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setParallelism(<span class="number">10</span>);</span><br></pre></td></tr></table></figure><p>注意：这样设置的并行度是你整个程序的并行度，那么后面如果你的每个算子不单独设置并行度覆盖的话，那么后面每个算子的并行度就都是这里设置的并行度的值了。</p><h2 id="每个算子单独设置并行度"><a href="#每个算子单独设置并行度" class="headerlink" title="每个算子单独设置并行度"></a>每个算子单独设置并行度</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data.keyBy(<span class="keyword">new</span> xxxKey())</span><br><span class="line">    .flatMap(<span class="keyword">new</span> XxxFlatMapFunction()).setParallelism(<span class="number">5</span>)</span><br><span class="line">    .map(<span class="keyword">new</span> XxxMapFunction).setParallelism(<span class="number">5</span>)</span><br><span class="line">    .addSink(<span class="keyword">new</span> XxxSink()).setParallelism(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>如上，就是在每个算子后面单独的设置并行度，这样的话，就算你前面设置了 env.setParallelism(10) 也是会被覆盖的。</p><p>这也说明优先级是：算子设置并行度 &gt; env 设置并行度 &gt; 配置文件默认并行度</p><h1 id="slot"><a href="#slot" class="headerlink" title="slot"></a><strong>slot</strong></h1><p><img src="/blog-img/apache-flink:study-flink-slot-parallelism/FlinkJob%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84.svg" alt="FlinkJob运行架构"></p><p>图中 Task Manager 是从 Job Manager 处接收需要部署的 Task，任务的并行性由每个 Task Manager 上可用的 slot 决定。每个任务代表分配给任务槽的一组资源，slot 在 Flink 里面可以认为是资源组，Flink 将每个任务分成子任务并且将这些子任务分配到 slot 来并行执行程序。</p><p>例如，如果 Task Manager 有四个 slot，那么它将为每个 slot 分配 25％ 的内存。 可以在一个 slot 中运行一个或多个线程。 同一 slot 中的线程共享相同的 JVM。 同一 JVM 中的任务共享 TCP 连接和心跳消息。Task Manager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离。默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 task 的 subtask，只要它们来自相同的 job。这种共享可以有更好的资源利用率。</p><p><img src="/blog-img/apache-flink:study-flink-slot-parallelism/TaskSlots%E6%89%A7%E8%A1%8C.svg" alt="TaskSlots执行"></p><p>默认情况下，Flink 允许 subtasks 共享 slots，即使它们是不同 tasks 的 subtasks，只要它们来自同一个 job。因此，一个 slot 可能会负责这个 job 的整个管道（pipeline）。允许 slot sharing 有两个好处：</p><p>1.Flink 集群需要与 job 中使用的最高并行度一样多的 slots。这样不需要计算作业总共包含多少个 tasks（具有不同并行度）。</p><p>2.更好的资源利用率。在没有 slot sharing 的情况下，简单的 subtasks（source/map()）将会占用和复杂的 subtasks （window）一样多的资源。通过 slot sharing，将示例中的并行度从 2 增加到 6 可以充分利用 slot 的资源，同时确保繁重的 subtask 在 TaskManagers 之间公平地获取资源。</p><p>下图即为Flink subtasks 共享 slots的模式：</p><p><img src="/blog-img/apache-flink:study-flink-slot-parallelism/TaskSlotsSharing%E6%89%A7%E8%A1%8C.svg" alt="TaskSlotsSharing执行"></p><p>上面图片中有两个 Task Manager，每个 Task Manager 有三个 slot，这样我们的算子最大并行度那么就可以达到 6 个，在同一个 slot 里面可以执行 1 至多个子任务。</p><p>那么再看上面的图片，source/map/keyby/window/apply 最大可以有 6 个并行度，sink 只用了 1 个并行。</p><p>每个 Flink TaskManager 在集群中提供 slot。 slot 的数量通常与每个 TaskManager 的可用 CPU 内核数成比例。一般情况下你的 slot 数是你每个 TaskManager 的 cpu 的核数</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：Flink Job ExecutionGraph生成过程</title>
    <link href="https://yangyichao-mango.github.io/2019/11/08/apache-flink:study-flink-job-ExecutionGraph/"/>
    <id>https://yangyichao-mango.github.io/2019/11/08/apache-flink:study-flink-job-ExecutionGraph/</id>
    <published>2019-11-08T01:49:18.000Z</published>
    <updated>2019-11-08T02:37:23.427Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：Flink Job 执行计划生成过程</p><span id="more"></span><h1 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a><strong>Transformations</strong></h1><p><img src="/blog-img/apache-flink:study-flink-job-ExecutionGraph/TransformationClasses.png" alt="TransformationClasses"></p><p>并不是每一个 Transformation 都会转换成runtime层中的物理操作。有一些只是逻辑概念，比如union、split/select、partition等。如下图所示的转换树，在运行时会优化成下方的操作图。</p><p><img src="/blog-img/apache-flink:study-flink-job-ExecutionGraph/Transformations.png" alt="Transformations"></p><h1 id="执行计划转换过程"><a href="#执行计划转换过程" class="headerlink" title="执行计划转换过程"></a><strong>执行计划转换过程</strong></h1><p><img src="/blog-img/apache-flink:study-flink-job-ExecutionGraph/4%E5%B1%82%E8%BD%AC%E5%8C%96.png" alt="4层转换"></p><p>1.转换过程 StreamExecutionEnvironment 存放的 transformation -&gt; StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; 物理执行图<br>2.StreamExecutionEnvironment 存放的 transformation -&gt; StreamGraph -&gt; JobGraph 在<strong>客户端</strong>完成，然后提交 JobGraph 到 JobManager<br>3.JobManager 的主节点 JobMaster，将 JobGraph 转化为 ExecutionGraph，然后发送到不同的 taskManager，得到实际的物理执行图</p><h2 id="LocalStreamEnvironment-中-parallelism"><a href="#LocalStreamEnvironment-中-parallelism" class="headerlink" title="LocalStreamEnvironment 中 parallelism"></a><strong>LocalStreamEnvironment 中 parallelism</strong></h2><p>其中 LocalStreamEnvironment Task 中的 parallelism 数量是根据以下代码生成的</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Public</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamExecutionEnvironment</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> defaultLocalParallelism = Runtime.getRuntime().availableProcessors();</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Creates a &#123;<span class="doctag">@link</span> LocalStreamEnvironment&#125;. The local execution environment</span></span><br><span class="line"><span class="comment">     * will run the program in a multi-threaded fashion in the same JVM as the</span></span><br><span class="line"><span class="comment">     * environment was created in. The default parallelism of the local</span></span><br><span class="line"><span class="comment">     * environment is the number of hardware contexts (CPU cores / threads),</span></span><br><span class="line"><span class="comment">     * unless it was specified differently by &#123;<span class="doctag">@link</span> #setParallelism(int)&#125;.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> A local execution environment.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> LocalStreamEnvironment <span class="title">createLocalEnvironment</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> createLocalEnvironment(defaultLocalParallelism);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Creates a &#123;<span class="doctag">@link</span> LocalStreamEnvironment&#125;. The local execution environment</span></span><br><span class="line"><span class="comment">     * will run the program in a multi-threaded fashion in the same JVM as the</span></span><br><span class="line"><span class="comment">     * environment was created in. It will use the parallelism specified in the</span></span><br><span class="line"><span class="comment">     * parameter.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parallelism</span></span><br><span class="line"><span class="comment">     * The parallelism for the local environment.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> A local execution environment with the specified parallelism.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> LocalStreamEnvironment <span class="title">createLocalEnvironment</span><span class="params">(<span class="keyword">int</span> parallelism)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> createLocalEnvironment(parallelism, <span class="keyword">new</span> Configuration());</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：异步IO之RichAsyncFunction</title>
    <link href="https://yangyichao-mango.github.io/2019/11/06/apache-flink:study-async-io-RichAsyncFunction/"/>
    <id>https://yangyichao-mango.github.io/2019/11/06/apache-flink:study-async-io-RichAsyncFunction/</id>
    <published>2019-11-06T10:45:59.000Z</published>
    <updated>2019-11-08T02:42:18.399Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：异步IO之RichAsyncFunction</p><span id="more"></span><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a><strong>问题</strong></h2><h3 id="设置kafka-consumer并行度的语义"><a href="#设置kafka-consumer并行度的语义" class="headerlink" title="设置kafka consumer并行度的语义"></a>设置kafka consumer并行度的语义</h3><p>1.如果设置kafka consumer的并发度为100，并且申请到集群中资源的Task Manager的slot个数也为100个，则每个slot中运行的任务都生成这么多数量的kafka consumer，还是每个slot一个kafka consumer?</p><p>2.场景：一个keyBy过后设置了一分钟的窗口dataStream中，如果保证每次触发这个窗口时，窗口的数据永远只有一条的话，并且在保证窗口为1分钟大小的情况下，接口返回速度保证在10秒，使用Async IO是否就没有意义了，因为当前请求队列里面只有一条数据</p><p>3.flink 默认执行一个Job的slot中线程数为什么是8，在哪里设置的</p><h3 id="使用AsyncIO需要考虑的指标"><a href="#使用AsyncIO需要考虑的指标" class="headerlink" title="使用AsyncIO需要考虑的指标"></a>使用AsyncIO需要考虑的指标</h3><p>1.每个slot中Flink Job的线程数<br>2.如果需要使用时间窗口：时间窗口的大小，几分钟的窗口<br>2.如果需要keyBy：每个slot中Flink Job的大概key的个数（什么情况使用，什么情况不使用）</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a><strong>简介</strong></h2><p>我们知道flink对于外部数据源的操作可以通过自带的连接器，或者自定义sink和source实现数据的交互，那么为啥还需要异步IO呢？<br>那时因为对于实时处理，当我们需要使用外部存储数据参与计算时，与外部系统之间的交互延迟对流处理的整个工作进度起决定性的影响。<br>如果我们是使用传统方式mapfunction等算子里访问外部存储，实际上该交互过程是同步的，比如下图中：请求a发送到数据库，那么function会一直等待响应。在很多案例中，这个等待过程是非常浪费函数时间的。</p><p><img src="/blog-img/apache-flink:study-async-io-RichAsyncFunction/%E5%BC%82%E6%AD%A5IO.jpeg" alt="异步IO"></p><p>图中棕色的长条表示等待时间，可以发现网络等待时间极大地阻碍了吞吐和延迟。为了解决同步访问的问题，异步模式可以并发地处理多个请求和回复。<br>也就是说，你可以连续地向数据库发送用户a、b、c等的请求，与此同时，哪个请求的回复先返回了就处理哪个回复，从而连续的请求之间不需要阻塞等待，如上图右边所示。<br>这也正是 Async I/O 的实现原理。</p><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a><strong>目的</strong></h2><p>将MapFunction或者FlatMapFunction中的同步访问外部存储设备的方法通过AsyncFunction替换以实现异步访问<br>在执行过程中，如果使用了keyBy，则相同的key整个执行周期都使用同一个线程，但是不同的key也可以使用同一个线程</p><h2 id="如何使用Async-I-O"><a href="#如何使用Async-I-O" class="headerlink" title="如何使用Async I/O"></a><strong>如何使用Async I/O</strong></h2><p>我们需要自定义一个类实现RichAsyncFunction这个抽象类，实现其中的抽象方法，这点和自定义source很像。<br>主要是的抽象方法如下，然后在asyncInvoke()使用CompletableFuture执行异步操作（CompletableFuture会提供一个ForkJoinPool作为请求线程池）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">asyncInvoke</span><span class="params">(IN var1, ResultFuture&lt;OUT&gt; var2)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">default</span> <span class="keyword">void</span> <span class="title">timeout</span><span class="params">(IN input, ResultFuture&lt;OUT&gt; resultFuture)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    resultFuture.completeExceptionally(<span class="keyword">new</span> TimeoutException(<span class="string">&quot;Async function call has timed out.&quot;</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后在AsyncDataStream中使用我们定义好的类，去实现主流异步的访问外部数据源</p><h2 id="原理实现"><a href="#原理实现" class="headerlink" title="原理实现"></a><strong>原理实现</strong></h2><p>AsyncDataStream.(un)orderedWait 的主要工作就是创建了一个 AsyncWaitOperator。<br>AsyncWaitOperator 是支持异步 IO 访问的算子实现，该算子会运行 AsyncFunction 并处理异步返回的结果，其内部原理如下图所示</p><p><img src="/blog-img/apache-flink:study-async-io-RichAsyncFunction/%E5%BC%82%E6%AD%A5%E5%8E%9F%E7%90%86.jpeg" alt="异步原理"></p><p>如图所示，AsyncWaitOperator 主要由两部分组成：StreamElementQueue 和 Emitter。<br>StreamElementQueue 是一个 Promise 队列，所谓 Promise 是一种异步抽象表示将来会有一个值（参考 Scala Promise 了解更多），这个队列是未完成的 Promise 队列，也就是进行中的请求队列。<br>Emitter 是一个单独的线程，负责发送消息（收到的异步回复）给下游。</p><p>图中E5表示进入该算子的第五个元素（”Element-5”），在执行过程中首先会将其包装成一个 “Promise” P5，然后将P5放入队列。<br>最后调用 AsyncFunction 的 ayncInvoke 方法，该方法会向外部服务发起一个异步的请求，并注册回调。<br>该回调会在异步请求成功返回时调用 AsyncCollector.collect 方法将返回的结果交给框架处理。<br>实际上 AsyncCollector 也一个 Promise，也就是 P5，在调用 collect 的时候会标记 Promise 为完成状态，并通知 Emitter 线程有完成的消息可以发送了。<br>Emitter 就会从队列中拉取完成的 Promise ，并从 Promise 中取出消息发送给下游。</p><h2 id="消息的顺序性"><a href="#消息的顺序性" class="headerlink" title="消息的顺序性"></a><strong>消息的顺序性</strong></h2><p>上文提到 Async I/O 提供了两种输出模式。<br>其实细分有三种模式: 有序，ProcessingTime 无序，EventTime 无序。<br>Flink 使用队列来实现不同的输出模式，并抽象出一个队列的接口（StreamElementQueue），这种分层设计使得AsyncWaitOperator和Emitter不用关心消息的顺序问题。<br>StreamElementQueue有两种具体实现，分别是 OrderedStreamElementQueue 和 UnorderedStreamElementQueue。<br>UnorderedStreamElementQueue 比较有意思，它使用了一套逻辑巧妙地实现完全无序和 EventTime 无序</p><h3 id="有序"><a href="#有序" class="headerlink" title="有序"></a>有序</h3><p>有序比较简单，使用一个队列就能实现。<br>所有新进入该算子的元素（包括 watermark），都会包装成 Promise 并按到达顺序放入该队列。<br>如下图所示，尽管P4的结果先返回，但并不会发送，只有 P1 （队首）的结果返回了才会触发 Emitter 拉取队首元素进行发送</p><p><img src="/blog-img/apache-flink:study-async-io-RichAsyncFunction/%E6%9C%89%E5%BA%8F.jpeg" alt="有序"></p><h3 id="ProcessingTime-无序"><a href="#ProcessingTime-无序" class="headerlink" title="ProcessingTime 无序"></a>ProcessingTime 无序</h3><p>ProcessingTime 无序也比较简单，因为没有 watermark，不需要协调 watermark 与消息的顺序性，所以使用两个队列就能实现，一个 uncompletedQueue 一个 completedQueue。<br>所有新进入该算子的元素，同样的包装成 Promise 并放入 uncompletedQueue 队列，当uncompletedQueue队列中任意的Promise返回了数据，则将该 Promise 移到 completedQueue 队列中，并通知 Emitter 消费。<br>如下图所示：</p><p><img src="/blog-img/apache-flink:study-async-io-RichAsyncFunction/ProcessingTime%E6%97%A0%E5%BA%8F.jpeg" alt="ProcessingTime无序"></p><h3 id="EventTime-无序"><a href="#EventTime-无序" class="headerlink" title="EventTime 无序"></a>EventTime 无序</h3><p>EventTime 无序类似于有序与 ProcessingTime 无序的结合体。<br>因为有 watermark，需要协调 watermark 与消息之间的顺序性，所以uncompletedQueue中存放的元素从原先的 Promise 变成了 Promise 集合。<br>如果进入算子的是消息元素，则会包装成 Promise 放入队尾的集合中。<br>如果进入算子的是 watermark，也会包装成 Promise 并放到一个独立的集合中，再将该集合加入到 uncompletedQueue 队尾，最后再创建一个空集合加到 uncompletedQueue 队尾。<br>这样，watermark 就成了消息顺序的边界。<br>只有处在队首的集合中的 Promise 返回了数据，才能将该 Promise 移到 completedQueue 队列中，由 Emitter 消费发往下游。<br>只有队首集合空了，才能处理第二个集合。这样就保证了当且仅当某个 watermark 之前所有的消息都已经被发送了，该 watermark 才能被发送。过程如下图所示：</p><p><img src="/blog-img/apache-flink:study-async-io-RichAsyncFunction/EventTime%E6%97%A0%E5%BA%8F.jpeg" alt="EventTime无序"></p><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a><strong>说明</strong></h2><p>1、AsyncDataStream有2个方法，unorderedWait表示数据不需要关注顺序，处理完立即发送，orderedWait表示数据需要关注顺序，为了实现该目标，操作算子会在该结果记录之前的记录为发送之前缓存该记录。这往往会引入额外的延迟和一些Checkpoint负载，因为相比于无序模式结果记录会保存在Checkpoint状态内部较长的时间。<br>2、Timeout配置，主要是为了处理死掉或者失败的任务，防止资源被长期阻塞占用。<br>3、最后一个参数Capacity表示同时最多有多少个异步请求在处理，异步IO的方式会导致更高的吞吐量，但是对于实时应用来说该操作也是一个瓶颈。限制并发请求数，算子不会积压过多的未处理请求，但是一旦超过容量的显示会触发背压。<br>该参数可以不配置，但是默认是100</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Common 包学习：常用集合类Collections4学习</title>
    <link href="https://yangyichao-mango.github.io/2019/11/06/apache-common:study-apache-common-collections4/"/>
    <id>https://yangyichao-mango.github.io/2019/11/06/apache-common:study-apache-common-collections4/</id>
    <published>2019-11-06T08:04:06.000Z</published>
    <updated>2019-11-06T08:57:35.325Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Common 包学习：常用集合类Collections4学习</p><span id="more"></span><h2 id="Maven依赖"><a href="#Maven依赖" class="headerlink" title="Maven依赖"></a><strong>Maven依赖</strong></h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.commons<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-collections4<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="CollectionUtils"><a href="#CollectionUtils" class="headerlink" title="CollectionUtils"></a><strong>CollectionUtils</strong></h2><h3 id="lt-O-gt-Collection-lt-O-gt-subtract-final-Iterable-lt-extends-O-gt-a-final-Iterable-lt-extends-O-gt-b"><a href="#lt-O-gt-Collection-lt-O-gt-subtract-final-Iterable-lt-extends-O-gt-a-final-Iterable-lt-extends-O-gt-b" class="headerlink" title="&lt;O&gt; Collection&lt;O&gt; subtract(final Iterable&lt;? extends O&gt; a, final Iterable&lt;? extends O&gt; b)"></a>&lt;O<O>&gt; Collection&lt;O<O>&gt; subtract(final Iterable&lt;? extends O&gt; a, final Iterable&lt;? extends O&gt; b)</h3><p>a是做差集运算的左集，b是做差集运算的右集，下面是一个例子</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = LoggerFactory.getLogger(Demo.class);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Set&lt;Pair&lt;String, String&gt;&gt; allProductDevices = Sets.newHashSet(Pair.of(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>), Pair.of(<span class="string">&quot;c&quot;</span>, <span class="string">&quot;d&quot;</span>));</span><br><span class="line"></span><br><span class="line">        Set&lt;Pair&lt;String, String&gt;&gt; oldProductDevices = Sets.newHashSet(Pair.of(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>), Pair.of(<span class="string">&quot;e&quot;</span>, <span class="string">&quot;f&quot;</span>));</span><br><span class="line"></span><br><span class="line">        List&lt;Pair&lt;String, String&gt;&gt; newProductDevices =</span><br><span class="line">                (ArrayList&lt;Pair&lt;String, String&gt;&gt;) CollectionUtils.subtract(allProductDevices, oldProductDevices);</span><br><span class="line"></span><br><span class="line">        List&lt;String&gt; list1 = Lists.newArrayList(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>);</span><br><span class="line"></span><br><span class="line">        List&lt;String&gt; list2 = Lists.newArrayList(<span class="string">&quot;c&quot;</span>, <span class="string">&quot;b&quot;</span>);</span><br><span class="line"></span><br><span class="line">        List&lt;String&gt; list3 = (ArrayList&lt;String&gt;) CollectionUtils.subtract(list1, list2);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Common包" scheme="https://yangyichao-mango.github.io/categories/Apache-Common%E5%8C%85/"/>
    
    
      <category term="Apache Common包" scheme="https://yangyichao-mango.github.io/tags/Apache-Common%E5%8C%85/"/>
    
  </entry>
  
  <entry>
    <title>Google Guava 学习：guava cache缓存学习</title>
    <link href="https://yangyichao-mango.github.io/2019/11/06/google-guava:study-guava-cache/"/>
    <id>https://yangyichao-mango.github.io/2019/11/06/google-guava:study-guava-cache/</id>
    <published>2019-11-06T08:03:38.000Z</published>
    <updated>2019-11-06T10:38:23.662Z</updated>
    
    <content type="html"><![CDATA[<p>Google Guava 学习：guava cache缓存学习</p><span id="more"></span><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a><strong>背景</strong></h2><p>缓存的主要作用是暂时在内存中保存业务系统的数据处理结果，并且等待下次访问使用。在日长开发有很多场合，有一些数据量不是很大，不会经常改动，并且访问非常频繁。但是由于受限于硬盘IO的性能或者远程网络等原因获取可能非常的费时。会导致我们的程序非常缓慢，这在某些业务上是不能忍的！而缓存正是解决这类问题的神器！</p><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a><strong>正文</strong></h2><p>Guava Cache与ConcurrentMap很相似，但也不完全一样。最基本的区别是ConcurrentMap会一直保存所有添加的元素，直到显式地移除。相对地，Guava Cache为了限制内存占用，通常都设定为自动回收元素。在某些场景下，尽管LoadingCache 不回收元素，它也是很有用的，因为它会自动加载缓存</p><p>Guava Cache是在内存中缓存数据，相比较于数据库或redis存储，访问内存中的数据会更加高效。Guava官网介绍，下面的这几种情况可以考虑使用Guava Cache：</p><p>1.愿意消耗一些内存空间来提升速度。</p><p>2.预料到某些键会被多次查询。</p><p>3.缓存中存放的数据总量不会超出内存容量。</p><p>所以，可以将程序频繁用到的少量数据存储到Guava Cache中，以改善程序性能。下面对Guava Cache的用法进行详细的介绍。</p><h2 id="Maven依赖"><a href="#Maven依赖" class="headerlink" title="Maven依赖"></a><strong>Maven依赖</strong></h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>guava<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>23.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="构建缓存对象"><a href="#构建缓存对象" class="headerlink" title="构建缓存对象"></a><strong>构建缓存对象</strong></h2><p>接口Cache代表缓存，它有如下方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Cache</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function">V <span class="title">get</span><span class="params">(K key, Callable&lt;? extends V&gt; valueLoader)</span> <span class="keyword">throws</span> ExecutionException</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">ImmutableMap&lt;K, V&gt; <span class="title">getAllPresent</span><span class="params">(Iterable&lt;?&gt; keys)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">put</span><span class="params">(K key, V value)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">putAll</span><span class="params">(Map&lt;? extends K, ? extends V&gt; m)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">invalidate</span><span class="params">(Object key)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">invalidateAll</span><span class="params">(Iterable&lt;?&gt; keys)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">invalidateAll</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">size</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">CacheStats <span class="title">stats</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">ConcurrentMap&lt;K, V&gt; <span class="title">asMap</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">cleanUp</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以通过CacheBuilder类构建一个缓存对象，构建一个缓存对象代码如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Cache&lt;String,String&gt; cache = CacheBuilder.newBuilder().build();</span><br><span class="line">        cache.put(<span class="string">&quot;word&quot;</span>,<span class="string">&quot;Hello Guava Cache&quot;</span>);</span><br><span class="line">        System.out.println(cache.getIfPresent(<span class="string">&quot;word&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到Cache非常类似于JDK中的Map，但是相比于Map，Guava Cache提供了很多更强大的功能</p><h2 id="设置最大存储"><a href="#设置最大存储" class="headerlink" title="设置最大存储"></a><strong>设置最大存储</strong></h2><p>Guava Cache可以在构建缓存对象时指定缓存所能够存储的最大记录数量。当Cache中的记录数量达到最大值后再调用put方法向其中添加对象，Guava会先从当前缓存的对象记录中选择一条删除掉，腾出空间后再将新的对象存储到Cache中</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Cache&lt;String,String&gt; cache = CacheBuilder.newBuilder()</span><br><span class="line">                .maximumSize(<span class="number">2</span>)</span><br><span class="line">                .build();</span><br><span class="line">        cache.put(<span class="string">&quot;key1&quot;</span>, <span class="string">&quot;value1&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key2&quot;</span>, <span class="string">&quot;value2&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key3&quot;</span>, <span class="string">&quot;value3&quot;</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;第一个值：&quot;</span> + cache.getIfPresent(<span class="string">&quot;key1&quot;</span>));</span><br><span class="line">        System.out.println(<span class="string">&quot;第二个值：&quot;</span> + cache.getIfPresent(<span class="string">&quot;key2&quot;</span>));</span><br><span class="line">        System.out.println(<span class="string">&quot;第三个值：&quot;</span> + cache.getIfPresent(<span class="string">&quot;key3&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面代码在构造缓存对象时，通过CacheBuilder类的maximumSize方法指定Cache最多可以存储两个对象，然后调用Cache的put方法向其中添加了三个对象。程序执行结果如下图所示，可以看到第三条对象记录的插入，导致了第一条对象记录被删除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第一个值：null</span><br><span class="line">第二个值：value2</span><br><span class="line">第三个值：value3</span><br></pre></td></tr></table></figure><h2 id="设置过期时间"><a href="#设置过期时间" class="headerlink" title="设置过期时间"></a><strong>设置过期时间</strong></h2><p>在构建Cache对象时，可以通过CacheBuilder类的expireAfterAccess和expireAfterWrite两个方法为缓存中的对象指定过期时间，过期的对象将会被缓存自动删除。其中，expireAfterWrite方法指定对象被写入到缓存后多久过期，expireAfterAccess指定对象多久没有被访问后过期</p><h3 id="expireAfterWrite"><a href="#expireAfterWrite" class="headerlink" title="expireAfterWrite"></a>expireAfterWrite</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder()</span><br><span class="line">                .maximumSize(<span class="number">2</span>)</span><br><span class="line">                .expireAfterWrite(<span class="number">3</span>, TimeUnit.SECONDS)</span><br><span class="line">                .build();</span><br><span class="line">        cache.put(<span class="string">&quot;key1&quot;</span>, <span class="string">&quot;value1&quot;</span>);</span><br><span class="line">        <span class="keyword">int</span> time = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;第&quot;</span> + time++ + <span class="string">&quot;次取到key1的值为：&quot;</span> + cache.getIfPresent(<span class="string">&quot;key1&quot;</span>));</span><br><span class="line">            Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的代码在构造Cache对象时，通过CacheBuilder的expireAfterWrite方法指定put到Cache中的对象在3秒后会过期。在Cache对象中存储一条对象记录后，每隔1秒读取一次这条记录。程序运行结果如下图所示，可以看到，前三秒可以从Cache中获取到对象，超过三秒后，对象从Cache中被自动删除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">第1次取到key1的值为：value1</span><br><span class="line">第2次取到key1的值为：value1</span><br><span class="line">第3次取到key1的值为：value1</span><br><span class="line">第4次取到key1的值为：null</span><br><span class="line">第5次取到key1的值为：null</span><br><span class="line">第6次取到key1的值为：null</span><br><span class="line">第7次取到key1的值为：null</span><br><span class="line">第8次取到key1的值为：null</span><br></pre></td></tr></table></figure><h3 id="expireAfterAccess"><a href="#expireAfterAccess" class="headerlink" title="expireAfterAccess"></a>expireAfterAccess</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder()</span><br><span class="line">                .maximumSize(<span class="number">2</span>)</span><br><span class="line">                .expireAfterAccess(<span class="number">3</span>, TimeUnit.SECONDS)</span><br><span class="line">                .build();</span><br><span class="line">        cache.put(<span class="string">&quot;key1&quot;</span>, <span class="string">&quot;value1&quot;</span>);</span><br><span class="line">        <span class="keyword">double</span> time = <span class="number">1.5</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            Thread.sleep((<span class="keyword">long</span>) time * <span class="number">1000L</span>);</span><br><span class="line">            System.out.println(<span class="string">&quot;睡眠&quot;</span> + time++ + <span class="string">&quot;秒后取到key1的值为：&quot;</span> + cache.getIfPresent(<span class="string">&quot;key1&quot;</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过CacheBuilder的expireAfterAccess方法指定Cache中存储的对象如果超过3秒没有被访问就会过期。while中的代码每sleep一段时间就会访问一次Cache中存储的对象key1，每次访问key1之后下次sleep的时间会加长一秒。程序运行结果如下图所示，从结果中可以看出，当超过3秒没有读取key1对象之后，该对象会自动被Cache删除。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">睡眠1.5秒后取到key1的值为：value1</span><br><span class="line">睡眠2.5秒后取到key1的值为：value1</span><br><span class="line">睡眠3.5秒后取到key1的值为：null</span><br></pre></td></tr></table></figure><p>也可以同时用expireAfterAccess和expireAfterWrite方法指定过期时间，这时只要对象满足两者中的一个条件就会被自动过期删除。</p><h2 id="弱引用"><a href="#弱引用" class="headerlink" title="弱引用"></a><strong>弱引用</strong></h2><p>可以通过weakKeys和weakValues方法指定Cache只保存对缓存记录key和value的弱引用。这样当没有其他强引用指向key和value时，key和value对象就会被垃圾回收器回收</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Cache&lt;String, Object&gt; cache = CacheBuilder.newBuilder()</span><br><span class="line">                .maximumSize(<span class="number">2</span>)</span><br><span class="line">                .weakValues()</span><br><span class="line">                .build();</span><br><span class="line">        Object value = <span class="keyword">new</span> Object();</span><br><span class="line">        cache.put(<span class="string">&quot;key1&quot;</span>, value);</span><br><span class="line">        </span><br><span class="line">        value = <span class="keyword">new</span> Object(); <span class="comment">// 原对象不再有强引用</span></span><br><span class="line">        System.gc();</span><br><span class="line">        System.out.println(cache.getIfPresent(<span class="string">&quot;key1&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面代码的打印结果是null。构建Cache时通过weakValues方法指定Cache只保存记录值的一个弱引用。当给value引用赋值一个新的对象之后，就不再有任何一个强引用指向原对象。System.gc()触发垃圾回收后，原对象就被清除了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">null</span><br></pre></td></tr></table></figure><h2 id="显示清除"><a href="#显示清除" class="headerlink" title="显示清除"></a><strong>显示清除</strong></h2><p>可以调用Cache的invalidateAll或invalidate方法显示删除Cache中的记录。invalidate方法一次只能删除Cache中一个记录，接收的参数是要删除记录的key。invalidateAll方法可以批量删除Cache中的记录，当没有传任何参数时，invalidateAll方法将清除Cache中的全部记录。invalidateAll也可以接收一个Iterable类型的参数，参数中包含要删除记录的所有key值。下面代码对此做了示例</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder().build();</span><br><span class="line">        Object value = <span class="keyword">new</span> Object();</span><br><span class="line">        cache.put(<span class="string">&quot;key1&quot;</span>, <span class="string">&quot;value1&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key2&quot;</span>, <span class="string">&quot;value2&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key3&quot;</span>, <span class="string">&quot;value3&quot;</span>);</span><br><span class="line"></span><br><span class="line">        List&lt;String&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        list.add(<span class="string">&quot;key1&quot;</span>);</span><br><span class="line">        list.add(<span class="string">&quot;key2&quot;</span>);</span><br><span class="line"></span><br><span class="line">        cache.invalidateAll(list); <span class="comment">// 批量清除list中全部key对应的记录</span></span><br><span class="line">        System.out.println(cache.getIfPresent(<span class="string">&quot;key1&quot;</span>));</span><br><span class="line">        System.out.println(cache.getIfPresent(<span class="string">&quot;key2&quot;</span>));</span><br><span class="line">        System.out.println(cache.getIfPresent(<span class="string">&quot;key3&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>代码中构造了一个集合list用于保存要删除记录的key值，然后调用invalidateAll方法批量删除key1和key2对应的记录，只剩下key3对应的记录没有被删除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">null</span><br><span class="line">null</span><br><span class="line">value3</span><br></pre></td></tr></table></figure><h2 id="移除监听器"><a href="#移除监听器" class="headerlink" title="移除监听器"></a><strong>移除监听器</strong></h2><p>可以为Cache对象添加一个移除监听器，这样当有记录被删除时可以感知到这个事件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public class StudyGuavaCache &#123;</span><br><span class="line">    public static void main(String[] args) throws InterruptedException &#123;</span><br><span class="line">        RemovalListener&lt;String, String&gt; listener = notification -&gt;</span><br><span class="line">                System.out.println(<span class="string">&quot;[&quot;</span> + notification.getKey() + <span class="string">&quot;:&quot;</span> + notification.getValue() + <span class="string">&quot;] is removed!&quot;</span>);</span><br><span class="line">        Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder()</span><br><span class="line">                .maximumSize(3)</span><br><span class="line">                .removalListener(listener)</span><br><span class="line">                .build();</span><br><span class="line">        cache.put(<span class="string">&quot;key1&quot;</span>, <span class="string">&quot;value1&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key2&quot;</span>, <span class="string">&quot;value2&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key3&quot;</span>, <span class="string">&quot;value3&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key4&quot;</span>, <span class="string">&quot;value3&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key5&quot;</span>, <span class="string">&quot;value3&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key6&quot;</span>, <span class="string">&quot;value3&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key7&quot;</span>, <span class="string">&quot;value3&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key8&quot;</span>, <span class="string">&quot;value3&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>removalListener方法为Cache指定了一个移除监听器，这样当有记录从Cache中被删除时，监听器listener就会感知到这个事件。程序运行结果如下图所示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[key1:value1] is removed!</span><br><span class="line">[key2:value2] is removed!</span><br><span class="line">[key3:value3] is removed!</span><br><span class="line">[key4:value3] is removed!</span><br><span class="line">[key5:value3] is removed!</span><br></pre></td></tr></table></figure><h2 id="自动加载"><a href="#自动加载" class="headerlink" title="自动加载"></a><strong>自动加载</strong></h2><p>Cache的get方法有两个参数，第一个参数是要从Cache中获取记录的key，第二个记录是一个Callable对象。<br>当缓存中已经存在key对应的记录时，get方法直接返回key对应的记录。如果缓存中不包含key对应的记录，Guava会使用当前线程执行Callable对象中的call方法，call方法的返回值会作为key对应的值被存储到缓存中，并且被get方法返回。下面是一个多线程的例子：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = LoggerFactory.getLogger(StudyGuavaCache.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder()</span><br><span class="line">            .maximumSize(<span class="number">1</span>)</span><br><span class="line">            .build();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                LOGGER.info(<span class="string">&quot;1&quot;</span> + Thread.currentThread().getName());</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    String value = cache.get(<span class="string">&quot;key&quot;</span>, <span class="keyword">new</span> Callable&lt;String&gt;() &#123;</span><br><span class="line">                        <span class="function"><span class="keyword">public</span> String <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                            LOGGER.info(<span class="string">&quot;load1&quot;</span> + Thread.currentThread().getName()); <span class="comment">// 加载数据线程执行标志</span></span><br><span class="line">                            Thread.sleep(<span class="number">1000</span>); <span class="comment">// 模拟加载时间</span></span><br><span class="line">                            <span class="keyword">return</span> <span class="string">&quot;auto load by Callable1&quot;</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;);</span><br><span class="line">                    LOGGER.info(<span class="string">&quot;thread1 &quot;</span> + value);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                LOGGER.info(<span class="string">&quot;thread2&quot;</span>);</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    LOGGER.info(<span class="string">&quot;2&quot;</span> + Thread.currentThread().getName());</span><br><span class="line">                    String value = cache.get(<span class="string">&quot;key1&quot;</span>, <span class="keyword">new</span> Callable&lt;String&gt;() &#123;</span><br><span class="line">                        <span class="function"><span class="keyword">public</span> String <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                            LOGGER.info(<span class="string">&quot;load2&quot;</span> + Thread.currentThread().getName()); <span class="comment">// 加载数据线程执行标志</span></span><br><span class="line">                            Thread.sleep(<span class="number">1000</span>); <span class="comment">// 模拟加载时间</span></span><br><span class="line">                            <span class="keyword">return</span> <span class="string">&quot;auto load by Callable2&quot;</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;);</span><br><span class="line">                    LOGGER.info(<span class="string">&quot;thread2 &quot;</span> + value);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这段代码中有两个线程共享同一个Cache对象，两个线程同时调用get方法获取同一个key对应的记录。由于key对应的记录不存在，所以两个线程都在get方法处阻塞。此处在call方法中调用Thread.sleep(1000)模拟程序从外存加载数据的时间消耗</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">17:49:23.965 [Thread-2] INFO com.github.xxx.other.demo.guava.StudyGuavaCache - thread2</span><br><span class="line">17:49:23.965 [Thread-1] INFO com.github.xxx.other.demo.guava.StudyGuavaCache - 1Thread-1</span><br><span class="line">17:49:23.969 [Thread-2] INFO com.github.xxx.other.demo.guava.StudyGuavaCache - 2Thread-2</span><br><span class="line">17:49:23.983 [Thread-1] INFO com.github.xxx.other.demo.guava.StudyGuavaCache - load1Thread-1</span><br><span class="line">17:49:23.983 [Thread-2] INFO com.github.xxx.other.demo.guava.StudyGuavaCache - load2Thread-2</span><br></pre></td></tr></table></figure><p>从结果中可以看出，虽然是两个线程同时调用get方法，但只有一个get方法中的Callable会被执行(没有打印出load2)。<br>Guava可以保证当有多个线程同时访问Cache中的一个key时，如果key对应的记录不存在，Guava只会启动一个线程执行get方法中Callable参数对应的任务加载数据存到缓存。<br>当加载完数据后，任何线程中的get方法都会获取到key对应的值</p><h2 id="统计信息"><a href="#统计信息" class="headerlink" title="统计信息"></a><strong>统计信息</strong></h2><p>可以对Cache的命中率、加载数据时间等信息进行统计。在构建Cache对象时，可以通过CacheBuilder的recordStats方法开启统计信息的开关。<br>开关开启后Cache会自动对缓存的各种操作进行统计，调用Cache的stats方法可以查看统计后的信息</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder()</span><br><span class="line">                .maximumSize(<span class="number">3</span>)</span><br><span class="line">                .recordStats() <span class="comment">// 开启统计信息开关</span></span><br><span class="line">                .build();</span><br><span class="line">        cache.put(<span class="string">&quot;key1&quot;</span>, <span class="string">&quot;value1&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key2&quot;</span>, <span class="string">&quot;value2&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key3&quot;</span>, <span class="string">&quot;value3&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key4&quot;</span>, <span class="string">&quot;value4&quot;</span>);</span><br><span class="line"></span><br><span class="line">        cache.getIfPresent(<span class="string">&quot;key1&quot;</span>);</span><br><span class="line">        cache.getIfPresent(<span class="string">&quot;key2&quot;</span>);</span><br><span class="line">        cache.getIfPresent(<span class="string">&quot;key3&quot;</span>);</span><br><span class="line">        cache.getIfPresent(<span class="string">&quot;key4&quot;</span>);</span><br><span class="line">        cache.getIfPresent(<span class="string">&quot;key5&quot;</span>);</span><br><span class="line">        cache.getIfPresent(<span class="string">&quot;key6&quot;</span>);</span><br><span class="line">        </span><br><span class="line">        System.out.println(cache.stats()); <span class="comment">// 获取统计信息</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>程序执行结果如下所示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CacheStats&#123;hitCount=3, missCount=3, loadSuccessCount=0, loadExceptionCount=0, totalLoadTime=0, evictionCount=1&#125;</span><br></pre></td></tr></table></figure><p>这些统计信息对于调整缓存设置是至关重要的，在性能要求高的应用中应该密切关注这些数据</p><h2 id="LoadingCache"><a href="#LoadingCache" class="headerlink" title="LoadingCache"></a><strong>LoadingCache</strong></h2><p>LoadingCache是Cache的子接口，相比较于Cache，当从LoadingCache中读取一个指定key的记录时，如果该记录不存在，则LoadingCache可以自动执行加载数据到缓存的操作。<br>LoadingCache接口的定义如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">LoadingCache</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Cache</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt;, <span class="title">Function</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function">V <span class="title">get</span><span class="params">(K key)</span> <span class="keyword">throws</span> ExecutionException</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">V <span class="title">getUnchecked</span><span class="params">(K key)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">ImmutableMap&lt;K, V&gt; <span class="title">getAll</span><span class="params">(Iterable&lt;? extends K&gt; keys)</span> <span class="keyword">throws</span> ExecutionException</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">V <span class="title">apply</span><span class="params">(K key)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">refresh</span><span class="params">(K key)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function">ConcurrentMap&lt;K, V&gt; <span class="title">asMap</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>与构建Cache类型的对象类似，LoadingCache类型的对象也是通过CacheBuilder进行构建，不同的是，在调用CacheBuilder的build方法时，必须传递一个CacheLoader类型的参数，CacheLoader的load方法需要我们提供实现。<br>当调用LoadingCache的get方法时，如果缓存不存在对应key的记录，则CacheLoader中的load方法会被自动调用从外存加载数据，load方法的返回值会作为key对应的value存储到LoadingCache中，并从get方法返回</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException </span>&#123;</span><br><span class="line">        CacheLoader&lt;String, String&gt; loader = <span class="keyword">new</span> CacheLoader&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">load</span><span class="params">(String key)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                Thread.sleep(<span class="number">1000</span>); <span class="comment">// 休眠1s，模拟加载数据</span></span><br><span class="line">                System.out.println(key + <span class="string">&quot; is loaded from a cacheLoader!&quot;</span>);</span><br><span class="line">                <span class="keyword">return</span> key + <span class="string">&quot;&#x27;s value&quot;</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">        LoadingCache&lt;String, String&gt; loadingCache = CacheBuilder.newBuilder()</span><br><span class="line">                .maximumSize(<span class="number">3</span>)</span><br><span class="line">                .build(loader); <span class="comment">// 在构建时指定自动加载器</span></span><br><span class="line"></span><br><span class="line">        loadingCache.get(<span class="string">&quot;key1&quot;</span>);</span><br><span class="line">        loadingCache.get(<span class="string">&quot;key2&quot;</span>);</span><br><span class="line">        loadingCache.get(<span class="string">&quot;key3&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>程序执行结果如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">key1 is loaded from a cacheLoader!</span><br><span class="line">key2 is loaded from a cacheLoader!</span><br><span class="line">key3 is loaded from a cacheLoader!</span><br></pre></td></tr></table></figure><p>从LoadingCache查询的正规方式是使用get(K)方法。这个方法要么返回已经缓存的值，要么使用CacheLoader向缓存原子地加载新值（通过load(String key) 方法加载）。由于CacheLoader可能抛出异常，LoadingCache.get(K)也声明抛出ExecutionException异常。如果你定义的CacheLoader没有声明任何检查型异常，则可以通过getUnchecked(K)查找缓存；但必须注意，一旦CacheLoader声明了检查型异常，就不可以调用getUnchecked(K)。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="guava" scheme="https://yangyichao-mango.github.io/categories/guava/"/>
    
    
      <category term="guava" scheme="https://yangyichao-mango.github.io/tags/guava/"/>
    
  </entry>
  
  <entry>
    <title>apache-kafka:study-features</title>
    <link href="https://yangyichao-mango.github.io/2019/11/06/apache-kafka:study-features/"/>
    <id>https://yangyichao-mango.github.io/2019/11/06/apache-kafka:study-features/</id>
    <published>2019-11-06T01:57:17.000Z</published>
    <updated>2019-11-06T02:08:57.687Z</updated>
    
    <content type="html"><![CDATA[<h2 id="如何为Kafka集群选择合适的Partitions数量"><a href="#如何为Kafka集群选择合适的Partitions数量" class="headerlink" title="如何为Kafka集群选择合适的Partitions数量"></a><strong>如何为Kafka集群选择合适的Partitions数量</strong></h2><h3 id="越多的分区可以提供更高的吞吐量"><a href="#越多的分区可以提供更高的吞吐量" class="headerlink" title="越多的分区可以提供更高的吞吐量"></a>越多的分区可以提供更高的吞吐量</h3><p>首先我们需要明白以下事实：在kafka中，单个patition是kafka并行操作的最小单元。在producer和broker端，向每一个分区写入数据是可以完全并行化的，此时，可以通过加大硬件资源的利用率来提升系统的吞吐量，例如对数据进行压缩。在consumer段，kafka只允许单个partition的数据被一个consumer线程消费。因此，在consumer端，每一个Consumer Group内部的consumer并行度完全依赖于被消费的分区数量。综上所述，通常情况下，在一个Kafka集群中，partition的数量越多，意味着可以到达的吞吐量越大。</p><p>我们可以粗略地通过吞吐量来计算kafka集群的分区数量。假设对于单个partition，producer端的可达吞吐量为p，Consumer端的可达吞吐量为c，期望的目标吞吐量为t，那么集群所需要的partition数量至少为max(t/p,t/c)。在producer端，单个分区的吞吐量大小会受到批量大小、数据压缩方法、 确认类型（同步/异步）、复制因子等配置参数的影响。经过测试，在producer端，单个partition的吞吐量通常是在10MB/s左右。在consumer端，单个partition的吞吐量依赖于consumer端每个消息的应用逻辑处理速度。因此，我们需要对consumer端的吞吐量进行测量。</p><p>虽然随着时间的推移，我们能够对分区的数量进行添加，但是对于基于Key来生成的这一类消息需要我们重点关注。当producer向kafka写入基于key的消息时，kafka通过key的hash值来确定消息需要写入哪个具体的分区。通过这样的方案，kafka能够确保相同key值的数据可以写入同一个partition。kafka的这一能力对于一部分应用是极为重要的，例如对于同一个key的所有消息，consumer需要按消息的顺序进行有序消费。如果partition的数量发生改变，那么上面的有序性保证将不复存在。为了避免上述情况发生，通常的解决办法是多分配一些分区，以满足未来的需求。通常情况下，我们需要根据未来1到2年的目标吞吐量来设计kafka的分区数量。</p><p>一开始，我们可以基于当前的业务吞吐量为kafka集群分配较小的broker数量，随着时间的推移，我们可以向集群中增加更多的broker，然后在线方式将适当比例的partition转移到新增加的broker中去。通过这样的方法，我们可以在满足各种应用场景（包括基于key消息的场景）的情况下，保持业务吞吐量的扩展性。</p><p>在设计分区数时，除了吞吐量，还有一些其他因素值得考虑。正如我们后面即将看到的，对于一些应用场景，集群拥有过的分区将会带来负面的影响。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Kafka" scheme="https://yangyichao-mango.github.io/categories/Apache-Kafka/"/>
    
    
      <category term="Apache Kafka" scheme="https://yangyichao-mango.github.io/tags/Apache-Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：系统特性学习</title>
    <link href="https://yangyichao-mango.github.io/2019/11/03/apache-flink:study-features/"/>
    <id>https://yangyichao-mango.github.io/2019/11/03/apache-flink:study-features/</id>
    <published>2019-11-03T08:20:21.000Z</published>
    <updated>2019-11-11T07:18:07.272Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：系统特性学习</p><span id="more"></span><p><a href="https://github.com/flink-china/flink-training-course">教程</a></p><h2 id="window窗口"><a href="#window窗口" class="headerlink" title="window窗口"></a><strong>window窗口</strong></h2><h3 id="窗口大小"><a href="#窗口大小" class="headerlink" title="窗口大小"></a>窗口大小</h3><p>窗口大小是用户自己设定的，但是窗口的起始和结束时间点是系统根据窗口大小和自然数进行设定的，不会出现设置了一分钟的窗口，统计的数据是2:30到3:30的数据</p><p>[window_start_time, window_end_time)根据窗口大小和自然数进行设定</p><p>如果window大小是3秒，那么1分钟内会把window划分为如下的形式:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[00:00:00,00:00:03)</span><br><span class="line">[00:00:03,00:00:06)</span><br><span class="line">...</span><br><span class="line">[00:00:57,00:01:00)</span><br></pre></td></tr></table></figure><p>如果window大小是10秒，则window会被分为如下的形式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[00:00:00,00:00:10)</span><br><span class="line">[00:00:10,00:00:20)</span><br><span class="line">...</span><br><span class="line">[00:00:50,00:01:00)</span><br></pre></td></tr></table></figure><h3 id="watermark"><a href="#watermark" class="headerlink" title="watermark"></a>watermark</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> Watermark <span class="title">getCurrentWatermark</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// this guarantees that the watermark never goes backwards.</span></span><br><span class="line">    <span class="keyword">long</span> potentialWM = currentMaxTimestamp - maxOutOfOrderness;</span><br><span class="line">    <span class="keyword">if</span> (potentialWM &gt;= lastEmittedWatermark) &#123;</span><br><span class="line">        lastEmittedWatermark = potentialWM;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Watermark(lastEmittedWatermark);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>watermark = max( [当前已到达的时间戳最新的数据(currentMaxTimestamp)] - [最大乱序等待时间(maxOutOfOrderness)], watermark )</strong></p><h3 id="触发窗口运算条件"><a href="#触发窗口运算条件" class="headerlink" title="触发窗口运算条件"></a>触发窗口运算条件</h3><p>1.当前最新数据到达进行判断：当前到达event的time(timestamp) ＜ watermark则触发，表示数据是超过了最大等待时间，已经延迟到达的，则会触发</p><p>2.当前最新数据到达进行判断：最新的watermark &gt;= window_end_time（对于out-of-order以及正常的数据而言），在[window_start_time, window_end_time)中有数据存在</p><p>3.而且，这里要强调一点，watermark和currentMaxTimestamp是一个全局的值，不是某一个key下的值，所以即使不是同一个key的数据，其warmark也会增加</p><p>语义是：currentMaxTimestamp是当前到达的最大时间戳数据，代表时间戳为currentMaxTimestamp的数据已经到达了，所以所能等待的数据的时间戳（max_out_of_orderness）只能为watermark = currentMaxTimestamp - maxOutOfOrderness</p><h3 id="窗口计算"><a href="#窗口计算" class="headerlink" title="窗口计算"></a>窗口计算</h3><p>Window reduce，Window aggregate 和 Window Fold 是增量聚合，每来一条数据就计算一次，高效</p><p>Window apply（Window process 的老版本） 和 Window process 是全量聚合，触发窗口计算时全量计算</p><h3 id="被Keys化与非被Keys化Windows"><a href="#被Keys化与非被Keys化Windows" class="headerlink" title="被Keys化与非被Keys化Windows"></a>被Keys化与非被Keys化Windows</h3><p>要指定的第一件事是您的流是否应该使用keyedWindow，一般都与业务逻辑有关，比如说使用一分钟的窗口进行去重。使用keyBy(…)将您的无限流分成逻辑Key化的数据流。如果keyBy(…)未调用，则表示您的流不是被Keys化的。</p><p>对于被Key化的数据流，可以将传入数据（Object）的的任何属性用作键）。拥有被Key化的数据流将允许您的窗口计算由多个任务并行执行，因为每个Key化的数据流可以独立于其余任务进行处理。引用相同Keys的所有数据将被发送到同一个并行任务进行计算。</p><p>在非被Key化的数据流的情况下，您的原始流将不会被拆分为多个逻辑流，并且所有窗口逻辑将由单个任务执行，即并行度为1。</p><h2 id="sql"><a href="#sql" class="headerlink" title="sql"></a><strong>sql</strong></h2><p>1.在flinkSql中，如果使用groupBy，尽量使用窗口，否则会认为被groupBy的数据会默认人为整个窗口内的数据还没有到达，所以会一直等待，不会产出数据</p><p>update-mode: append / update</p><p>分为 update stream 模式和 append stream 模式</p><p>window聚合为append mode stream，groupby聚合为update mode stream</p><h2 id="Flink生成-Timestamps-和-Watermarks"><a href="#Flink生成-Timestamps-和-Watermarks" class="headerlink" title="Flink生成 Timestamps 和 Watermarks"></a><strong>Flink生成 Timestamps 和 Watermarks</strong></h2><p>为了让event time工作，Flink需要知道事件的时间戳，这意味着流中的每个元素都需要分配其事件时间戳。这个通常是通过抽取或者访问事件中某些字段的时间戳来获取的。</p><p>时间戳的分配伴随着水印的生成，告诉系统事件时间中的进度。</p><p>这里有两种方式来分配时间戳和生成水印:</p><ol><li>直接在数据流源中进行。</li><li>通过timestamp assigner和watermark generator生成:在Flink中，timestamp分配器也定义了用来发射的水印。</li></ol><h3 id="数据流源生成Timestamps和Watermarks"><a href="#数据流源生成Timestamps和Watermarks" class="headerlink" title="数据流源生成Timestamps和Watermarks"></a>数据流源生成Timestamps和Watermarks</h3><p>数据流源可以直接为它们产生的数据元素分配timestamp，并且他们也能发送水印。这样做的话，就没必要再去定义timestamp分配器了，需要注意的是:如果一个timestamp分配器被使用的话，由源提供的任何timestamp和watermark都会被重写。</p><h3 id="时间戳分配器-水印生成器（Timestamp-Assigners-Watermark-Generators）"><a href="#时间戳分配器-水印生成器（Timestamp-Assigners-Watermark-Generators）" class="headerlink" title="时间戳分配器/水印生成器（Timestamp Assigners / Watermark Generators）"></a>时间戳分配器/水印生成器（Timestamp Assigners / Watermark Generators）</h3><p>Timestamp分配器获取一个流并生成一个新的带有Timestamp元素和水印的流。如果原始流已经有时间戳和/或水印，则Timestamp分配程序将覆盖它们</p><p>Timestamp分配器通常在数据源之后立即指定，但这并不是严格要求的。通常是在timestamp分配器之前先解析(MapFunction)和过滤(FilterFunction)。在任何情况下，都需要在事件时间上的第一个操作(例如第一个窗口操作)之前指定timestamp分配程序。有一个特殊情况，当使用Kafka作为流作业的数据源时，Flink允许在源内部指定timestamp分配器和watermark生成器。更多关于如何进行的信息请参考Kafka Connector的文档。</p><p>直接在FlinkKafkaConsumer010上面使用assignTimestampsAndWatermarks可以根据kafka source的partitions的特性进行设置Timestamps和Watermarks，让用户做一些特殊的处理</p><p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka<br>partition, allows users to let them exploit the per-partition characteristics.</p><h2 id="log"><a href="#log" class="headerlink" title="log"></a><strong>log</strong></h2><h3 id="Web-UI查找log"><a href="#Web-UI查找log" class="headerlink" title="Web UI查找log"></a>Web UI查找log</h3><p>JobManger log： 展示整个作业的状态变化（例如，从create 到deploy到running再到failed），通过jobManger log可以查看作业历史失败的记录和直接原因。</p><p>TaskManager log： 调度到该TaskManager上的task 的打印的相关log。</p><h2 id="shuffle"><a href="#shuffle" class="headerlink" title="shuffle"></a><strong>shuffle</strong></h2><p>被keyBy的数据流中，相同的key的数据会被发送到同一个slot中运行（partitiner决定），也就是TaskManager中slot进行shuffle的过程<br>如果有多个producer并且producer的数量和partition数量相同，则每个producer写一个partition</p><h2 id="Savepoints和Checkpoints"><a href="#Savepoints和Checkpoints" class="headerlink" title="Savepoints和Checkpoints"></a><strong>Savepoints和Checkpoints</strong></h2><p>用 Data Stream API 编写的程序可以从 savepoint 继续执行。Savepoints 允许在不丢失任何状态的情况下升级程序和 Flink 集群。</p><p>Savepoints 是手动触发的 Checkpoints，它依靠常规的 Checkpoint 机制获取程序的快照并将其写入 state backend。在执行期间，程序会定期在 worker 节点上创建快照并生成 Checkpoints。对于恢复，Flink 仅需要最后完成的 Checkpoint，而一旦完成了新的 Checkpoint，旧的就可以被丢弃。</p><p>Savepoints 类似于这些定期的 Checkpoints，除了它们是由用户触发并且在新的 Checkpoints 完成时不会自动过期。你可以通过命令行 或在取消一个 job 时通过 REST API 来创建 Savepoints。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：实时场景下的应用</title>
    <link href="https://yangyichao-mango.github.io/2019/11/03/apache-flink:study-realtime-scenario/"/>
    <id>https://yangyichao-mango.github.io/2019/11/03/apache-flink:study-realtime-scenario/</id>
    <published>2019-11-03T05:30:05.000Z</published>
    <updated>2019-11-03T08:32:58.449Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：实时场景下的应用</p><span id="more"></span><h2 id="用户需求场景"><a href="#用户需求场景" class="headerlink" title="用户需求场景"></a><strong>用户需求场景</strong></h2><p>用户想要查看 版本，机型，国家，城市 等等维度下按照分钟的时间粒度的设备活跃，新增设备数，首次活跃设备数</p><h2 id="用户需求-gt-架构方案设计"><a href="#用户需求-gt-架构方案设计" class="headerlink" title="用户需求-&gt;架构方案设计"></a><strong>用户需求-&gt;架构方案设计</strong></h2><table><thead><tr><th align="center">数据所处阶段</th><th align="center">功能描述</th></tr></thead><tbody><tr><td align="center">数据source</td><td align="center">各种各样的打到kafka的用户行为数据的日志</td></tr><tr><td align="center">数据process</td><td align="center">实时引擎消费kafka，根据数据服务化提供的接口判断当前用户是否是新增，活跃，首次活跃，将用户的相关数据打到下游kafka</td></tr><tr><td align="center">数据sink</td><td align="center">结果kafka</td></tr><tr><td align="center">olap引擎</td><td align="center">消费sink kafka</td></tr><tr><td align="center">数据产品</td><td align="center">通过BI等的产品呈现给用户</td></tr></tbody></table><h2 id="架构设计-gt-选择实时计算引擎"><a href="#架构设计-gt-选择实时计算引擎" class="headerlink" title="架构设计-&gt;选择实时计算引擎"></a><strong>架构设计-&gt;选择实时计算引擎</strong></h2><p>为什么使用flink：</p><p>A.保证消费一次：checkpoint和savepoint 容错</p><p>B.时间属性：事件，注入，处理时间</p><p>优点：事件时间的属性可以被广泛应用，比如一般的分析场景都是分析用户某个时间段的用户相关指标，而不是事件处理某个时间段的用户相关指标</p><h2 id="flink实现方案"><a href="#flink实现方案" class="headerlink" title="flink实现方案"></a><strong>flink实现方案</strong></h2><h3 id="第一种方案"><a href="#第一种方案" class="headerlink" title="第一种方案"></a>第一种方案</h3><h4 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h4><p>新增场景下，每消费一条source kafka用户数据就判断一次是否为新增，判断方式可以选择自己维护历史全量数据，或者使用数据服务化提供的接口，最后将结果写入sink kafka</p><h4 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h4><table style="text-align: center;">    <thead>        <tr>            <th style="width: 20%">数据处理阶段</th>            <th style="width: 30%">问题</th>            <th style="width: 20%">结果（仅仅指当前问题会产生的结果）</th>            <th style="width: 10%">是否可解决</th>        </tr>    </thead>    <tbody>        <tr>            <td>数据source</td>            <td>同一个用户的行为数据到达时间间隔很小，几秒内就可能会产生几十条行为日志，判断是否为新增，活跃用户时可能会被重复判断</td>            <td>最终数据结果＞真实结果</td>            <td>可部分解决</td>        </tr>        <tr>            <td>数据process</td>            <td>自己维护全量数据<br/>1.每判断一条就更新历史全量数据就不存在问题<br/>2.如果历史全量数据更新有问题就会产生和数据服务化一样的下面两种问题</td>            <td></td>            <td>可部分解决</td>        </tr>        <tr>            <td>数据process</td>            <td>数据服务化维护全量数据且更新不及时<br/>在新增的场景下，一个新增用户使用app可能会在短时间内上报成百上千条行为日志，如果第一条数据判断出来这个用户是新增，下一条数据判断时，数据服务化提供的全量用户里还没有及时将这条新增用户数据添加进去，则这条数据也会被判断为新增，就会导致最终结果重复</td>            <td>最终数据结果＞真实结果</td>            <td>可部分解决</td>        </tr>        <tr>            <td>数据process</td>            <td>数据服务化维护全量数据且更新过快<br/>数据服务化更新速度快于flink消费source kafka的速度：就会导致本来是新增的设备被判断不是新增，导致最终结果漏判</td>            <td>最终数据结果＜真实结果</td>            <td>暂时无法解决</td>        </tr>    </tbody></table><h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><p>后续解决方法只讨论上述可部分解决的问题</p><table style="text-align: center;">    <thead>        <tr>            <th style="width: 10%">数据处理阶段</th>            <th style="width: 20%">问题</th>            <th style="width: 30%">解决方案</th>        </tr>    </thead>    <tbody>        <tr>            <td>数据source</td>            <td>同一个用户的行为数据到达时间间隔很小，可能会被重复判断</td>            <td rowspan="2">使用flink窗口解决部分问题<br/>使用滚动窗口解决，将一段时间内的用户行为收集，然后到达窗口结束时间处理后再进行上报。假设设置一小时的窗口，则将这一小时的用户行为数据只取一条进行判断是否为新增，则可以极大的保证当前用户判断为新增时，下一小时窗口中这个用户不太可能被判断为新增了，因为数据服务化没有那么慢</td>        </tr>        <tr>            <td>数据process</td>            <td>数据服务化维护全量数据且更新不及时，最终结果重复</td>        </tr>    </tbody></table><h3 id="第一种方案-gt-第二种方案"><a href="#第一种方案-gt-第二种方案" class="headerlink" title="第一种方案-&gt;第二种方案"></a>第一种方案-&gt;第二种方案</h3><h4 id="方案-1"><a href="#方案-1" class="headerlink" title="方案"></a>方案</h4><p>使用窗口可以部分解决在新增活跃等场景下用户行为数据重复的问题</p><p>但是使用了窗口也会引入问题，就是虽然大窗口可以保证尽可能去重，但是数据的实时性大大降低，所以窗口设置不能大也不能小，窗口大保证不了数据产出及时性，窗口小去重效果差，所以最大窗口就为一分钟，和用户期望看板中结果一致</p><h4 id="存在的问题-1"><a href="#存在的问题-1" class="headerlink" title="存在的问题"></a>存在的问题</h4><table style="text-align: center;">    <thead>        <tr>            <th style="width: 20%">数据处理阶段</th>            <th style="width: 30%">问题</th>            <th style="width: 20%">结果（仅仅指当前问题会产生的结果）</th>            <th style="width: 10%">是否可解决</th>        </tr>    </thead>    <tbody>        <tr>            <td>数据source</td>            <td rowspan="2">由于用户行为数据到达source，或者从source到达process阶段，由于网络延迟等的问题，会导致处理用户数据时有乱序情况<br/>比如计算实时活跃设备，上一分钟的数据如果下一分钟才到达，则该条数据就会被上一分钟漏算</td>            <td rowspan="2">最终数据结果＜真实结果</td>            <td rowspan="2">可部分解决</td>        </tr>        <tr>            <td>数据process</td>        </tr>    </tbody></table><h4 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h4><table style="text-align: center;">    <thead>        <tr>            <th style="width: 10%">数据处理阶段</th>            <th style="width: 20%">问题</th>            <th style="width: 30%">解决方案</th>        </tr>    </thead>    <tbody>        <tr>            <td>数据source</td>            <td rowspan="2">数据乱序延迟漏算</td>            <td rowspan="2">在flink窗口计算中，通过timestamp和watermark特性来尽可能解决</td>        </tr>        <tr>            <td>数据process</td>        </tr>    </tbody></table><h3 id="第二种方案-gt-第三种方案"><a href="#第二种方案-gt-第三种方案" class="headerlink" title="第二种方案-&gt;第三种方案"></a>第二种方案-&gt;第三种方案</h3><h4 id="方案-2"><a href="#方案-2" class="headerlink" title="方案"></a>方案</h4><p>设置一分钟的窗口，然后设置一分钟的最大延迟等待时间，其语义是保证数据最多延迟一分钟到达，只要可以保证这个语义可以保证最后数据的正确性</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
      <category term="实时计算" scheme="https://yangyichao-mango.github.io/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：BoundedOutOfOrdernessTimestampExtractor</title>
    <link href="https://yangyichao-mango.github.io/2019/11/02/apache-flink:study-BoundedOutOfOrdernessTimestampExtractor/"/>
    <id>https://yangyichao-mango.github.io/2019/11/02/apache-flink:study-BoundedOutOfOrdernessTimestampExtractor/</id>
    <published>2019-11-02T08:48:43.000Z</published>
    <updated>2019-11-02T11:03:54.828Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：BoundedOutOfOrdernessTimestampExtractor</p><span id="more"></span><h2 id="源码"><a href="#源码" class="headerlink" title="源码"></a><strong>源码</strong></h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Licensed to the Apache Software Foundation (ASF) under one</span></span><br><span class="line"><span class="comment"> * or more contributor license agreements.  See the NOTICE file</span></span><br><span class="line"><span class="comment"> * distributed with this work for additional information</span></span><br><span class="line"><span class="comment"> * regarding copyright ownership.  The ASF licenses this file</span></span><br><span class="line"><span class="comment"> * to you under the Apache License, Version 2.0 (the</span></span><br><span class="line"><span class="comment"> * &quot;License&quot;); you may not use this file except in compliance</span></span><br><span class="line"><span class="comment"> * with the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"> * See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"> * limitations under the License.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> org.apache.flink.streaming.api.functions.timestamps;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.watermark.Watermark;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * This is a &#123;<span class="doctag">@link</span> AssignerWithPeriodicWatermarks&#125; used to emit Watermarks that lag behind the element with</span></span><br><span class="line"><span class="comment"> * the maximum timestamp (in event time) seen so far by a fixed amount of time, &lt;code&gt;t_late&lt;/code&gt;. This can</span></span><br><span class="line"><span class="comment"> * help reduce the number of elements that are ignored due to lateness when computing the final result for a</span></span><br><span class="line"><span class="comment"> * given window, in the case where we know that elements arrive no later than &lt;code&gt;t_late&lt;/code&gt; units of time</span></span><br><span class="line"><span class="comment"> * after the watermark that signals that the system event-time has advanced past their (event-time) timestamp.</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">BoundedOutOfOrdernessTimestampExtractor</span>&lt;<span class="title">T</span>&gt; <span class="keyword">implements</span> <span class="title">AssignerWithPeriodicWatermarks</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** The current maximum timestamp seen so far. */</span></span><br><span class="line"><span class="comment">/** 数据流的最大时间戳 */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">long</span> currentMaxTimestamp;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** The timestamp of the last emitted watermark. */</span></span><br><span class="line"><span class="comment">/** 最后一次已提交的最新 [水印]（当前批次水印） */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">long</span> lastEmittedWatermark = Long.MIN_VALUE;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The (fixed) interval between the maximum seen timestamp seen in the records</span></span><br><span class="line"><span class="comment"> * and that of the watermark to be emitted.</span></span><br><span class="line"><span class="comment"> * 最大乱序时间间隔</span></span><br><span class="line"><span class="comment"> * 将要被提交的 [水印] 和 [数据流的最大时间戳] 的固定时间间隔</span></span><br><span class="line"><span class="comment"> * 如果 [数据流的最大时间戳] - [当前批次水印] &gt; [最大乱序时间间隔]</span></span><br><span class="line"><span class="comment"> * 则就会打上一个新的 [水印]</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> maxOutOfOrderness;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">BoundedOutOfOrdernessTimestampExtractor</span><span class="params">(Time maxOutOfOrderness)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (maxOutOfOrderness.toMilliseconds() &lt; <span class="number">0</span>) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;Tried to set the maximum allowed &quot;</span> +</span><br><span class="line"><span class="string">&quot;lateness to &quot;</span> + maxOutOfOrderness + <span class="string">&quot;. This parameter cannot be negative.&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">this</span>.maxOutOfOrderness = maxOutOfOrderness.toMilliseconds();</span><br><span class="line"><span class="keyword">this</span>.currentMaxTimestamp = Long.MIN_VALUE + <span class="keyword">this</span>.maxOutOfOrderness;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getMaxOutOfOrdernessInMillis</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> maxOutOfOrderness;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Extracts the timestamp from the given element.</span></span><br><span class="line"><span class="comment"> * 从当前数据流元素中获取 [时间戳] 字段，需要用户根据业务自定义</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> element The element that the timestamp is extracted from.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> The new timestamp.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(T element)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果 [当前数据流最大时间戳] - [最大乱序时间间隔] &gt;= [最后一次已提交的时间戳]</span></span><br><span class="line"><span class="comment"> * 则更新 [最后一次已提交的时间戳]</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> Watermark <span class="title">getCurrentWatermark</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">// this guarantees that the watermark never goes backwards.</span></span><br><span class="line"><span class="keyword">long</span> potentialWM = currentMaxTimestamp - maxOutOfOrderness;</span><br><span class="line"><span class="keyword">if</span> (potentialWM &gt;= lastEmittedWatermark) &#123;</span><br><span class="line">lastEmittedWatermark = potentialWM;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> Watermark(lastEmittedWatermark);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取数据流中当前最大时间戳</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(T element, <span class="keyword">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class="line"><span class="keyword">long</span> timestamp = extractTimestamp(element);</span><br><span class="line"><span class="keyword">if</span> (timestamp &gt; currentMaxTimestamp) &#123;</span><br><span class="line">currentMaxTimestamp = timestamp;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> timestamp;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id=""><a href="#" class="headerlink" title="****"></a>****</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Sets the time characteristic for all streams create from this environment, e.g., processing</span></span><br><span class="line"><span class="comment"> * time, event time, or ingestion time.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;If you set the characteristic to IngestionTime of EventTime this will set a default</span></span><br><span class="line"><span class="comment"> * watermark update interval of 200 ms. If this is not applicable for your application</span></span><br><span class="line"><span class="comment"> * you should change it using &#123;<span class="doctag">@link</span> ExecutionConfig#setAutoWatermarkInterval(long)&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> characteristic The time characteristic.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setStreamTimeCharacteristic</span><span class="params">(TimeCharacteristic characteristic)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.timeCharacteristic = Preconditions.checkNotNull(characteristic);</span><br><span class="line">    <span class="keyword">if</span> (characteristic == TimeCharacteristic.ProcessingTime) &#123;</span><br><span class="line">        getConfig().setAutoWatermarkInterval(<span class="number">0</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        getConfig().setAutoWatermarkInterval(<span class="number">200</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="window触发机制"><a href="#window触发机制" class="headerlink" title="window触发机制"></a><strong>window触发机制</strong></h2><p>window的触发机制，是先按照自然时间将window划分，如果window大小是3秒，那么1分钟内会把window划分为如下的形式:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[00:00:00,00:00:03)</span><br><span class="line">[00:00:03,00:00:06)</span><br><span class="line">...</span><br><span class="line">[00:00:57,00:01:00)</span><br></pre></td></tr></table></figure><p>如果window大小是10秒，则window会被分为如下的形式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[00:00:00,00:00:10)</span><br><span class="line">[00:00:10,00:00:20)</span><br><span class="line">...</span><br><span class="line">[00:00:50,00:01:00)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Hadoop 学习：hdfs shell 命令</title>
    <link href="https://yangyichao-mango.github.io/2019/10/30/apache-hadoop:study-hdfs-shell/"/>
    <id>https://yangyichao-mango.github.io/2019/10/30/apache-hadoop:study-hdfs-shell/</id>
    <published>2019-10-30T08:27:45.000Z</published>
    <updated>2019-10-30T08:32:07.350Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Hadoop 学习：hdfs shell 命令</p><span id="more"></span><h3 id="ls"><a href="#ls" class="headerlink" title="ls"></a><strong>ls</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -ls /</span><br></pre></td></tr></table></figure><h3 id="put"><a href="#put" class="headerlink" title="put"></a><strong>put</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put localfile /user/hadoop/hadoopfile</span><br></pre></td></tr></table></figure><h3 id="get"><a href="#get" class="headerlink" title="get"></a><strong>get</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -get /user/hadoop/hadoopfile localfile</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/categories/Apache-Hadoop/"/>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/tags/Apache-Hadoop/"/>
    
      <category term="Hdfs" scheme="https://yangyichao-mango.github.io/tags/Hdfs/"/>
    
  </entry>
  
  <entry>
    <title>Mac安装Nginx-1.17.3以及相关配置文件</title>
    <link href="https://yangyichao-mango.github.io/2019/10/28/nginx:1-17-3-mac-install-and-confs-set/"/>
    <id>https://yangyichao-mango.github.io/2019/10/28/nginx:1-17-3-mac-install-and-confs-set/</id>
    <published>2019-10-28T13:19:10.000Z</published>
    <updated>2019-10-29T02:56:24.767Z</updated>
    
    <content type="html"><![CDATA[<p>Mac安装Nginx-1.17.3以及相关配置文件</p><span id="more"></span><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a><strong>安装</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install nginx</span><br></pre></td></tr></table></figure><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a><strong>配置</strong></h2><p>nginx是一个功能非常强大的web服务器加反向代理服务器，同时又是邮件服务器等等</p><p>在项目使用中，使用最多的三个核心功能是反向代理、负载均衡和静态服务器</p><p>这三个不同的功能的使用，都跟nginx的配置密切相关，nginx服务器的配置信息主要集中在nginx.conf这个配置文件中，并且所有的可配置选项大致分为以下几个部分</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">main                                <span class="comment"># 全局配置</span></span><br><span class="line"></span><br><span class="line">events &#123;                            <span class="comment"># nginx工作模式配置</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;                                <span class="comment"># http设置</span></span><br><span class="line">    ....</span><br><span class="line"></span><br><span class="line">    server &#123;                        <span class="comment"># 服务器主机配置</span></span><br><span class="line">        ....</span><br><span class="line">        location &#123;                    <span class="comment"># 路由配置</span></span><br><span class="line">            ....</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        location path &#123;</span><br><span class="line">            ....</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        location otherpath &#123;</span><br><span class="line">            ....</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    server &#123;</span><br><span class="line">        ....</span><br><span class="line"></span><br><span class="line">        location &#123;</span><br><span class="line">            ....</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    upstream name &#123;                    <span class="comment"># 负载均衡配置</span></span><br><span class="line">        ....</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如上述配置文件所示，主要由6个部分组成：</p><p>1.main：用于进行nginx全局信息的配置<br>2.events：用于nginx工作模式的配置<br>3.http：用于进行http协议信息的一些配置<br>4.server：用于进行服务器访问信息的配置<br>5.location：用于进行访问路由的配置<br>6.upstream：用于进行负载均衡的配置</p><h2 id="main模块"><a href="#main模块" class="headerlink" title="main模块"></a><strong>main模块</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># user nobody nobody;</span></span><br><span class="line">worker_processes 2;</span><br><span class="line"><span class="comment"># error_log logs/error.log</span></span><br><span class="line"><span class="comment"># error_log logs/error.log notice</span></span><br><span class="line"><span class="comment"># error_log logs/error.log info</span></span><br><span class="line"><span class="comment"># pid logs/nginx.pid</span></span><br><span class="line">worker_rlimit_nofile 1024;</span><br></pre></td></tr></table></figure><p>上述配置都是存放在main全局配置模块中的配置项</p><p>1.user：用来指定nginx worker进程运行用户以及用户组，默认nobody账号运行<br>2.worker_processes：指定nginx要开启的子进程数量，运行过程中监控每个进程消耗内存(一般几M~几十M不等)根据实际情况进行调整，通常数量是CPU内核数量的整数倍<br>3.error_log：定义错误日志文件的位置及输出级别【debug / info / notice / warn / error / crit】<br>4.pid：用来指定进程id的存储文件的位置<br>5.worker_rlimit_nofile：用于指定一个进程可以打开最多文件数量的描述</p><h2 id="events模块"><a href="#events模块" class="headerlink" title="events模块"></a><strong>events模块</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">events &#123;</span><br><span class="line">    worker_connections 1024;</span><br><span class="line">    multi_accept on;</span><br><span class="line">    use epoll;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述配置是针对nginx服务器的工作模式的一些操作配置</p><p>1.worker_connections：指定最大可以同时接收的连接数量，这里一定要注意，最大连接数量是和worker processes共同决定的<br>2.multi_accept：配置指定nginx在收到一个新连接通知后尽可能多的接受更多的连接<br>3.use epoll：配置指定了线程轮询的方法，如果是linux2.6+，使用epoll，如果是BSD如Mac请使用Kqueue</p><h2 id="http模块"><a href="#http模块" class="headerlink" title="http模块"></a><strong>http模块</strong></h2><p>作为web服务器，http模块是nginx最核心的一个模块，配置项也是比较多的，项目中会设置到很多的实际业务场景，需要根据硬件信息进行适当的配置，常规情况下，使用默认配置即可！</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">    <span class="comment">##</span></span><br><span class="line">    <span class="comment"># 基础配置</span></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line"></span><br><span class="line">    sendfile on;</span><br><span class="line">    tcp_nopush on;</span><br><span class="line">    tcp_nodelay on;</span><br><span class="line">    keepalive_timeout 65;</span><br><span class="line">    types_hash_max_size 2048;</span><br><span class="line">    <span class="comment"># server_tokens off;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># server_names_hash_bucket_size 64;</span></span><br><span class="line">    <span class="comment"># server_name_in_redirect off;</span></span><br><span class="line"></span><br><span class="line">    include /etc/nginx/mime.types;</span><br><span class="line">    default_type application/octet-stream;</span><br><span class="line"></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line">    <span class="comment"># SSL证书配置</span></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line"></span><br><span class="line">    ssl_protocols TLSv1 TLSv1.1 TLSv1.2; <span class="comment"># Dropping SSLv3, ref: POODLE</span></span><br><span class="line">    ssl_prefer_server_ciphers on;</span><br><span class="line"></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line">    <span class="comment"># 日志配置</span></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line"></span><br><span class="line">    access_log /var/<span class="built_in">log</span>/nginx/access.log;</span><br><span class="line">    error_log /var/<span class="built_in">log</span>/nginx/error.log;</span><br><span class="line"></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line">    <span class="comment"># Gzip 压缩配置</span></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line"></span><br><span class="line">    gzip on;</span><br><span class="line">    gzip_disable <span class="string">&quot;msie6&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># gzip_vary on;</span></span><br><span class="line">    <span class="comment"># gzip_proxied any;</span></span><br><span class="line">    <span class="comment"># gzip_comp_level 6;</span></span><br><span class="line">    <span class="comment"># gzip_buffers 16 8k;</span></span><br><span class="line">    <span class="comment"># gzip_http_version 1.1;</span></span><br><span class="line">    <span class="comment"># gzip_types text/plain text/css application/json application/javascript</span></span><br><span class="line"> text/xml application/xml application/xml+rss text/javascript;</span><br><span class="line"></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line">    <span class="comment"># 虚拟主机配置</span></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line"></span><br><span class="line">    include /etc/nginx/conf.d/*.conf;</span><br><span class="line">    include /etc/nginx/sites-enabled/*;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="基础配置"><a href="#基础配置" class="headerlink" title="基础配置"></a>基础配置</h3><p>1.sendfile on：配置on让sendfile发挥作用，将文件的回写过程交给数据缓冲去去完成，而不是放在应用中完成，这样的话在性能提升有有好处<br>2.tc_nopush on：让nginx在一个数据包中发送所有的头文件，而不是一个一个单独发<br>3.tcp_nodelay on：让nginx不要缓存数据，而是一段一段发送，如果数据的传输有实时性的要求的话可以配置它，发送完一小段数据就立刻能得到返回值，但是不要滥用哦</p><p>4.keepalive_timeout 10：给客户端分配连接超时时间，服务器会在这个时间过后关闭连接。一般设置时间较短，可以让nginx工作持续性更好<br>5.client_header_timeout 10：设置请求头的超时时间<br>6.client_body_timeout 10：设置请求体的超时时间<br>7.send_timeout 10：指定客户端响应超时时间，如果客户端两次操作间隔超过这个时间，服务器就会关闭这个链接</p><p>8.limit_conn_zone $binary_remote_addr zone=addr:5m ：设置用于保存各种key的共享内存的参数，<br>9.limit_conn addr 100: 给定的key设置最大连接数</p><p>10.server_tokens：虽然不会让nginx执行速度更快，但是可以在错误页面关闭nginx版本提示，对于网站安全性的提升有好处哦<br>11.include /etc/nginx/mime.types：指定在当前文件中包含另一个文件的指令<br>12.default_type application/octet-stream：指定默认处理的文件类型可以是二进制<br>13.type_hash_max_size 2048：混淆数据，影响三列冲突率，值越大消耗内存越多，散列key冲突率会降低，检索速度更快；值越小key，占用内存较少，冲突率越高，检索速度变慢</p><h3 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h3><p>1.access_log logs/access.log：设置存储访问记录的日志<br>2.error_log logs/error.log：设置存储记录错误发生的日志</p><h3 id="压缩配置"><a href="#压缩配置" class="headerlink" title="压缩配置"></a>压缩配置</h3><p>1.gzip：是告诉nginx采用gzip压缩的形式发送数据。这将会减少我们发送的数据量。<br>2.gzip_disable：为指定的客户端禁用gzip功能。我们设置成IE6或者更低版本以使我们的方案能够广泛兼容。<br>3.gzip_static：告诉nginx在压缩资源之前，先查找是否有预先gzip处理过的资源。这要求你预先压缩你的文件（在这个例子中被注释掉了），从而允许你使用最高压缩比，这样nginx就不用再压缩这些文件了（想要更详尽的gzip_static的信息，请点击这里）。<br>4.gzip_proxied：允许或者禁止压缩基于请求和响应的响应流。我们设置为any，意味着将会压缩所有的请求。<br>5.gzip_min_length：设置对数据启用压缩的最少字节数。如果一个请求小于1000字节，我们最好不要压缩它，因为压缩这些小的数据会降低处理此请求的所有进程的速度。<br>6.gzip_comp_level：设置数据的压缩等级。这个等级可以是1-9之间的任意数值，9是最慢但是压缩比最大的。我们设置为4，这是一个比较折中的设置。<br>7.gzip_type：设置需要压缩的数据格式。上面例子中已经有一些了，你也可以再添加更多的格式。</p><h3 id="文件缓存配置"><a href="#文件缓存配置" class="headerlink" title="文件缓存配置"></a>文件缓存配置</h3><p>1.open_file_cache：打开缓存的同时也指定了缓存最大数目，以及缓存的时间。我们可以设置一个相对高的最大时间，这样我们可以在它们不活动超过20秒后清除掉。<br>2.open_file_cache_valid：在open_file_cache中指定检测正确信息的间隔时间。<br>3.open_file_cache_min_uses：定义了open_file_cache中指令参数不活动时间期间里最小的文件数。<br>4.open_file_cache_errors：指定了当搜索一个文件时是否缓存错误信息，也包括再次给配置中添加文件。我们也包括了服务器模块，这些是在不同文件中定义的。如果你的服务器模块不在这些位置，你就得修改这一行来指定正确的位置。</p><h3 id="server模块"><a href="#server模块" class="headerlink" title="server模块"></a>server模块</h3><p>server模块配置是http模块中的一个子模块，用来定义一个虚拟访问主机，也就是一个虚拟服务器的配置信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen        80;</span><br><span class="line">    server_name localhost    192.168.1.100;</span><br><span class="line">    root        /nginx/www;</span><br><span class="line">    index        index.php index.html index.html;</span><br><span class="line">    charset        utf-8;</span><br><span class="line">    access_log    logs/access.log;</span><br><span class="line">    error_log    logs/error.log;</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>核心配置信息如下：</p><p>1.server：一个虚拟主机的配置，一个http中可以配置多个server</p><p>2.server_name：用力啊指定ip地址或者域名，多个配置之间用空格分隔</p><p>3.root：表示整个server虚拟主机内的根目录，所有当前主机中web项目的根目录</p><p>4.index：用户访问web网站时的全局首页</p><p>5.charset：用于设置www/路径中配置的网页的默认编码格式</p><p>6.access_log：用于指定该虚拟主机服务器中的访问记录日志存放路径</p><p>7.error_log：用于指定该虚拟主机服务器中访问错误日志的存放路径</p><h3 id="location模块"><a href="#location模块" class="headerlink" title="location模块"></a>location模块</h3><p>location模块是nginx配置中出现最多的一个配置，主要用于配置路由访问信息</p><p>在路由访问信息配置中关联到反向代理、负载均衡等等各项功能，所以location模块也是一个非常重要的配置模块</p><h4 id="基本配置"><a href="#基本配置" class="headerlink" title="基本配置"></a>基本配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">location / &#123;</span><br><span class="line">    root    /nginx/www;</span><br><span class="line">    index    index.php index.html index.htm;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>location /：表示匹配访问根目录</p><p>root：用于指定访问根目录时，访问虚拟主机的web目录</p><p>index：在不指定访问具体资源时，默认展示的资源文件列表</p><h4 id="反向代理配置方式"><a href="#反向代理配置方式" class="headerlink" title="反向代理配置方式"></a>反向代理配置方式</h4><p>通过反向代理代理服务器访问模式，通过proxy_set配置让客户端访问透明化</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">location / &#123;</span><br><span class="line">    proxy_pass http://localhost:8888;</span><br><span class="line">    proxy_set_header X-real-ip <span class="variable">$remote_addr</span>;</span><br><span class="line">    proxy_set_header Host <span class="variable">$http_host</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="uwsgi配置"><a href="#uwsgi配置" class="headerlink" title="uwsgi配置"></a>uwsgi配置</h4><p>wsgi模式下的服务器配置访问方式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">location / &#123;</span><br><span class="line">    include uwsgi_params;</span><br><span class="line">    uwsgi_pass localhost:8888</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="upstream模块"><a href="#upstream模块" class="headerlink" title="upstream模块"></a>upstream模块</h3><p>upstream模块主要负责负载均衡的配置，通过默认的轮询调度方式来分发请求到后端服务器</p><p>简单的配置方式如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">upstream name &#123;</span><br><span class="line">    ip_hash;</span><br><span class="line">    server 192.168.1.100:8000;</span><br><span class="line">    server 192.168.1.100:8001 down;</span><br><span class="line">    server 192.168.1.100:8002 max_fails=3;</span><br><span class="line">    server 192.168.1.100:8003 fail_timeout=20s;</span><br><span class="line">    server 192.168.1.100:8004 max_fails=3 fail_timeout=20s;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>核心配置信息如下</p><p>1.ip_hash：指定请求调度算法，默认是weight权重轮询调度，可以指定</p><p>2.server host:port：分发服务器的列表配置</p><p>– down：表示该主机暂停服务</p><p>– max_fails：表示失败最大次数，超过失败最大次数暂停服务</p><p>– fail_timeout：表示如果请求受理失败，暂停指定的时间之后重新发起请求</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Nginx" scheme="https://yangyichao-mango.github.io/categories/Nginx/"/>
    
    
      <category term="Nginx" scheme="https://yangyichao-mango.github.io/tags/Nginx/"/>
    
  </entry>
  
  <entry>
    <title>Elastic-Job学习：分布式任务调度框架</title>
    <link href="https://yangyichao-mango.github.io/2019/10/26/elastic-job:study-distributed-scheduled-job-framework/"/>
    <id>https://yangyichao-mango.github.io/2019/10/26/elastic-job:study-distributed-scheduled-job-framework/</id>
    <published>2019-10-26T14:08:15.000Z</published>
    <updated>2019-10-28T03:23:31.499Z</updated>
    
    <content type="html"><![CDATA[<p>分布式任务调度框架学习</p><span id="more"></span><p><a href="http://elasticjob.io/docs/elastic-job-lite/00-overview/intro/">官方文档</a></p><p><a href="https://github.com/elasticjob">elastic-job github</a></p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="分布式调度框架" scheme="https://yangyichao-mango.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E8%B0%83%E5%BA%A6%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Apache Zookeeper" scheme="https://yangyichao-mango.github.io/tags/Apache-Zookeeper/"/>
    
      <category term="分布式调度框架" scheme="https://yangyichao-mango.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%B0%83%E5%BA%A6%E6%A1%86%E6%9E%B6/"/>
    
      <category term="SpringBoot" scheme="https://yangyichao-mango.github.io/tags/SpringBoot/"/>
    
  </entry>
  
  <entry>
    <title>Vue学习：Vue前端项目部署到SpringBoot工程下</title>
    <link href="https://yangyichao-mango.github.io/2019/10/26/vue:study-vue-project-deploy-in-springboot-project/"/>
    <id>https://yangyichao-mango.github.io/2019/10/26/vue:study-vue-project-deploy-in-springboot-project/</id>
    <published>2019-10-26T07:29:37.000Z</published>
    <updated>2019-10-26T07:52:06.519Z</updated>
    
    <content type="html"><![CDATA[<p>Vue项目部署到SpringBoot工程下</p><span id="more"></span><h2 id="Vue前端项目"><a href="#Vue前端项目" class="headerlink" title="Vue前端项目"></a><strong>Vue前端项目</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm run build</span><br></pre></td></tr></table></figure><p>运行上述命令，将前端Vue项目打包，命令运行完成之后会在项目根目录下生成一个生成一个dist文件夹, 编译好的静态文件就在这里面</p><h2 id="部署在SpringBoot项目下"><a href="#部署在SpringBoot项目下" class="headerlink" title="部署在SpringBoot项目下"></a><strong>部署在SpringBoot项目下</strong></h2><p>将前端项目中dist文件夹下的所有文件拷贝到SpringBoot工程的<strong>src/main/resources/static</strong>文件夹下</p><p>运行后端项目，可以看到控制台会有这样的输出，证明将静态页面加入了容器中，现在就可以访问到前端页面了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2019-10-26 15:47:09.607  INFO 1967 --- [           main] o.s.b.a.w.s.WelcomePageHandlerMapping    : Adding welcome page: class path resource [static/index.html]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Vue" scheme="https://yangyichao-mango.github.io/categories/Vue/"/>
    
    
      <category term="SpringBoot" scheme="https://yangyichao-mango.github.io/tags/SpringBoot/"/>
    
      <category term="Vue" scheme="https://yangyichao-mango.github.io/tags/Vue/"/>
    
  </entry>
  
  <entry>
    <title>Maven学习：使用maven-shade-plugin解决依赖包冲突</title>
    <link href="https://yangyichao-mango.github.io/2019/10/25/apache-maven:study-maven-shade-resolve-jar-conflicts/"/>
    <id>https://yangyichao-mango.github.io/2019/10/25/apache-maven:study-maven-shade-resolve-jar-conflicts/</id>
    <published>2019-10-25T06:23:20.000Z</published>
    <updated>2019-10-25T08:21:33.406Z</updated>
    
    <content type="html"><![CDATA[<p>java 依赖包冲突，使用maven-shade-plugin解决</p><span id="more"></span><h3 id="错误场景详情"><a href="#错误场景详情" class="headerlink" title="错误场景详情"></a><strong>错误场景详情</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoSuchMethodError: com.google.common.util.concurrent.MoreExecutors.directExecutor()Ljava/util/concurrent/Executor;</span><br></pre></td></tr></table></figure><h3 id="错误原因"><a href="#错误原因" class="headerlink" title="错误原因"></a><strong>错误原因</strong></h3><p>出现这样的错误详情一般是由于有下面这样的包依赖情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A - B - C(guava version 18)</span><br><span class="line">  \ D - C(guava version 23.6-jre)</span><br></pre></td></tr></table></figure><p><em>A</em>：代表我们所开发的当前项目<br><em>B和D</em>：代表当前项目所依赖的项目<br><em>C</em>：代表当前项目依赖的项目所依赖的项目</p><p>由于我们当前所开发的项目A依赖了B和D，B和D又依赖了项目C<br>我们打包运行项目时，maven只会将一个版本C(guava)打进包内（maven打包遇到相同依赖，最短路径优先，在路径相同时先在pom中声明优先）<br>比如此时打进包的版本是C(guava version 23.6-jre)，那么很有可能在运行B中的一个方法时，调用C的一个方法，这个方法是C(guava version 18)中的一个方法，在C(guava version 23.6-jre)中并不存在，这时候就会报出java.lang.NoSuchMethodError</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a><strong>解决方案</strong></h3><p>使用maven-shade-plugin将所有B项目依赖的包全部打进B.jar中，并且给guava包的路径重命名为我们的自定义路径\</p><h4 id="maven-shade-plugin基本功能"><a href="#maven-shade-plugin基本功能" class="headerlink" title="maven-shade-plugin基本功能"></a>maven-shade-plugin基本功能</h4><p>maven-shade-plugin提供了两大基本功能：</p><p>1、将依赖的jar包打包到当前jar包（常规打包是不会将所依赖jar包打进来的）<br>2、对依赖的jar包进行重命名（用于类的隔离，解决包冲突就是使用了这个功能）</p><h4 id="解决示例"><a href="#解决示例" class="headerlink" title="解决示例"></a>解决示例</h4><p>如下例，就可以在B项目中使用C(guava version 18)，只不过import路径变成了我们自定的路径</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span> <span class="attr">xmlns:xsi</span>=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>RpcModule<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>RpcModule<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">packaging</span>&gt;</span>jar<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>RpcModule<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">guava.version</span>&gt;</span>18<span class="tag">&lt;/<span class="name">guava.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>guava<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;guava.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">source</span>&gt;</span>8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">target</span>&gt;</span>8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">createDependencyReducedPom</span>&gt;</span>false<span class="tag">&lt;/<span class="name">createDependencyReducedPom</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="comment">&lt;!-- 给guava包的路径重命名为我们的自定义路径 --&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">relocations</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>shade.com.google.guava<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>org.joda<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>shade.com.google.joda<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>com.google.common<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>shade.com.google.common<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">relocations</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line">                                &lt;transformer</span><br><span class="line">                                        implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;/&gt;</span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><p>使用maven-shade-plugin之前</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> Entity.Request;</span><br><span class="line"><span class="keyword">import</span> Entity.Response;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.*;</span><br><span class="line"><span class="keyword">import</span> java.net.Socket;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.google.common.collect.ImmutableMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SocketClient</span> </span>&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用maven-shade-plugin之后编译后反编译结果</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> Entity.Request;</span><br><span class="line"><span class="keyword">import</span> Entity.Response;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.*;</span><br><span class="line"><span class="keyword">import</span> java.net.Socket;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> shade.com.google.common.collect.ImmutableMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SocketClient</span> </span>&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后将打好的包上传至我们的maven仓库，然后再在当前项目A中依赖，就没有依赖冲突了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A - B - C(guava version 18, shade.com.google.common.collect.ImmutableMap)</span><br><span class="line">  \ D - C(guava version 23.6-jre, com.google.common.collect.ImmutableMap)</span><br></pre></td></tr></table></figure><p>这样就可以做到将两个不同版本的包都引入使用，由于引入包路径不同，因此也没有冲突</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Maven" scheme="https://yangyichao-mango.github.io/categories/Apache-Maven/"/>
    
    
      <category term="Apache Maven" scheme="https://yangyichao-mango.github.io/tags/Apache-Maven/"/>
    
  </entry>
  
  <entry>
    <title>Apache Zookeeper 学习：Zookeeper实现统一配置管理中心</title>
    <link href="https://yangyichao-mango.github.io/2019/10/24/apache-zookeeper:study-zookeeper-implement-unified-configuration-management-center/"/>
    <id>https://yangyichao-mango.github.io/2019/10/24/apache-zookeeper:study-zookeeper-implement-unified-configuration-management-center/</id>
    <published>2019-10-24T10:40:24.000Z</published>
    <updated>2019-11-06T13:07:48.984Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Zookeeper 学习：模拟三台节点组成的Zookeeper集群实现的统一配置管理中心</p><span id="more"></span><h2 id="模拟Zookeeper集群"><a href="#模拟Zookeeper集群" class="headerlink" title="模拟Zookeeper集群"></a><strong>模拟Zookeeper集群</strong></h2><h3 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h3><p><img src="/blog-img/apache-zookeeper:study-zookeeper-implement-unified-configuration-management-center/zookeeper%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt="Zookeeper集群架构"></p><h3 id="创建zookeeper集群节点"><a href="#创建zookeeper集群节点" class="headerlink" title="创建zookeeper集群节点"></a>创建zookeeper集群节点</h3><p>模拟三台节点组成的zookeeper集群，需要在本机zookeeper目录下<br>创建三个zookeeper集群节点配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> conf/</span><br><span class="line">$ cp zoo_sample.cfg zoo1.cfg</span><br><span class="line">$ cp zoo_sample.cfg zoo2.cfg</span><br><span class="line">$ cp zoo_sample.cfg zoo3.cfg</span><br></pre></td></tr></table></figure><h3 id="创建所配置的各个文件夹"><a href="#创建所配置的各个文件夹" class="headerlink" title="创建所配置的各个文件夹"></a>创建所配置的各个文件夹</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir /tmp/zookeeper1</span><br><span class="line">$ mkdir /tmp/zookeeper1/data</span><br><span class="line">$ mkdir /tmp/zookeeper1/dataLog</span><br><span class="line">$ mkdir /tmp/zookeeper2</span><br><span class="line">$ mkdir /tmp/zookeeper2/data</span><br><span class="line">$ mkdir /tmp/zookeeper2/dataLog</span><br><span class="line">$ mkdir /tmp/zookeeper3</span><br><span class="line">$ mkdir /tmp/zookeeper3/data</span><br><span class="line">$ mkdir /tmp/zookeeper3/dataLog</span><br></pre></td></tr></table></figure><h3 id="tmp-zookeeperX-data文件夹下创建myid文件"><a href="#tmp-zookeeperX-data文件夹下创建myid文件" class="headerlink" title="/tmp/zookeeperX/data文件夹下创建myid文件"></a>/tmp/zookeeperX/data文件夹下创建myid文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> 1 &gt; /tmp/zookeeper1/data/myid</span><br><span class="line">$ <span class="built_in">echo</span> 2 &gt; /tmp/zookeeper2/data/myid</span><br><span class="line">$ <span class="built_in">echo</span> 3 &gt; /tmp/zookeeper3/data/myid</span><br></pre></td></tr></table></figure><h3 id="配置zookeeper节点信息"><a href="#配置zookeeper节点信息" class="headerlink" title="配置zookeeper节点信息"></a>配置zookeeper节点信息</h3><p><strong>zoo1.cfg</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The number of milliseconds of each tick</span></span><br><span class="line">tickTime=2000</span><br><span class="line"><span class="comment"># The number of ticks that the initial</span></span><br><span class="line"><span class="comment"># synchronization phase can take</span></span><br><span class="line">initLimit=10</span><br><span class="line"><span class="comment"># The number of ticks that can pass between</span></span><br><span class="line"><span class="comment"># sending a request and getting an acknowledgement</span></span><br><span class="line">syncLimit=5</span><br><span class="line"><span class="comment"># the directory where the snapshot is stored.</span></span><br><span class="line"><span class="comment"># do not use /tmp for storage, /tmp here is just</span></span><br><span class="line"><span class="comment"># example sakes.</span></span><br><span class="line">dataDir=/tmp/zookeeper1/data</span><br><span class="line">dataLogDir=/tmp/zookeeper1/dataLog</span><br><span class="line"><span class="comment"># the port at which the clients will connect</span></span><br><span class="line">clientPort=2181</span><br><span class="line"><span class="comment"># the maximum number of client connections.</span></span><br><span class="line"><span class="comment"># increase this if you need to handle more clients</span></span><br><span class="line"><span class="comment">#maxClientCnxns=60</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Be sure to read the maintenance section of the</span></span><br><span class="line"><span class="comment"># administrator guide before turning on autopurge.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The number of snapshots to retain in dataDir</span></span><br><span class="line"><span class="comment">#autopurge.snapRetainCount=3</span></span><br><span class="line"><span class="comment"># Purge task interval in hours</span></span><br><span class="line"><span class="comment"># Set to &quot;0&quot; to disable auto purge feature</span></span><br><span class="line"><span class="comment">#autopurge.purgeInterval=1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 格式:server.num=xxxx:port1:port2</span></span><br><span class="line"><span class="comment"># num对应myid中的内容，port1是zookeeper集群中各服务间的通信端口，port2是zookeeper集群选举leader的端口</span></span><br><span class="line">server.1=localhost:2888:3888</span><br><span class="line">server.2=localhost:2899:3899</span><br><span class="line">server.3=localhost:2877:3877</span><br></pre></td></tr></table></figure><p><strong>zoo2.cfg</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the directory where the snapshot is stored.</span></span><br><span class="line"><span class="comment"># do not use /tmp for storage, /tmp here is just</span></span><br><span class="line"><span class="comment"># example sakes.</span></span><br><span class="line">dataDir=/tmp/zookeeper2/data</span><br><span class="line">dataLogDir=/tmp/zookeeper2/dataLog</span><br><span class="line"><span class="comment"># the port at which the clients will connect</span></span><br><span class="line">clientPort=2182</span><br><span class="line">...</span><br><span class="line">server.1=localhost:2888:3888</span><br><span class="line">server.2=localhost:2899:3899</span><br><span class="line">server.3=localhost:2877:3877</span><br></pre></td></tr></table></figure><p><strong>zoo3.cfg</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the directory where the snapshot is stored.</span></span><br><span class="line"><span class="comment"># do not use /tmp for storage, /tmp here is just</span></span><br><span class="line"><span class="comment"># example sakes.</span></span><br><span class="line">dataDir=/tmp/zookeeper3/data</span><br><span class="line">dataLogDir=/tmp/zookeeper3/dataLog</span><br><span class="line"><span class="comment"># the port at which the clients will connect</span></span><br><span class="line">clientPort=2183</span><br><span class="line">...</span><br><span class="line">server.1=localhost:2888:3888</span><br><span class="line">server.2=localhost:2899:3899</span><br><span class="line">server.3=localhost:2877:3877</span><br></pre></td></tr></table></figure><p>配置文件中dataDir，dataLogDir，clientPort，三个zookeeper节点配置信息都不同<br>搭建zookeeper集群，需要在每个zookeeper安装目录下的data文件中创建名为myid的文件，修改zooX.cfg内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">server.1=xxx:2888:3888</span><br><span class="line">server.2=xxx:2899:3899</span><br><span class="line">server.3=xxx:2877:3877</span><br></pre></td></tr></table></figure><p>格式:server.num=xxxx:port1:port2<br>num对应myid中的内容，port1是zookeeper集群中各服务间的通信端口，port2是zookeeper集群选举leader的端口</p><h3 id="启动模拟集群节点"><a href="#启动模拟集群节点" class="headerlink" title="启动模拟集群节点"></a>启动模拟集群节点</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ ./zkServer.sh start ../conf/zoo1.cfg</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: ../conf/zoo1.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">$ ./zkServer.sh start ../conf/zoo2.cfg</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: ../conf/zoo2.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">$ ./zkServer.sh start ../conf/zoo3.cfg</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: ../conf/zoo3.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure><p>查看集群状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ ./zkServer.sh status ../conf/zoo1.cfg</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: ../conf/zoo1.cfg</span><br><span class="line">Mode: follower</span><br><span class="line">$ ./zkServer.sh status ../conf/zoo2.cfg</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: ../conf/zoo2.cfg</span><br><span class="line">Mode: leader</span><br><span class="line">$ ./zkServer.sh status ../conf/zoo3.cfg</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: ../conf/zoo3.cfg</span><br><span class="line">Mode: follower</span><br></pre></td></tr></table></figure><h2 id="创建zookeeper集群client"><a href="#创建zookeeper集群client" class="headerlink" title="创建zookeeper集群client"></a><strong>创建zookeeper集群client</strong></h2><h3 id="创建监听节点变化的server（zookeeper集群client）"><a href="#创建监听节点变化的server（zookeeper集群client）" class="headerlink" title="创建监听节点变化的server（zookeeper集群client）"></a>创建监听节点变化的server（zookeeper集群client）</h3><p>模拟监听节点变化server，启动两个BaseWatcher程序作为监听server（对zookeeper集群来说是client）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BaseWatcher</span> <span class="keyword">implements</span> <span class="title">Watcher</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> ZooKeeper zookeeper;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 超时时间</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> SESSION_TIME_OUT = <span class="number">2000</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> CountDownLatch defaultCountDownLatch = <span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> CountDownLatch childrenCountDownLatch = <span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> CountDownLatch dataCountDownLatch = <span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (event.getState() == KeeperState.SyncConnected) &#123;</span><br><span class="line">            LOGGER.info(<span class="string">&quot;Watch received event&quot;</span>);</span><br><span class="line">            defaultCountDownLatch.countDown();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (event.getType() == EventType.NodeCreated) &#123;</span><br><span class="line">            LOGGER.info(<span class="string">&quot;创建节点&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (event.getType() == EventType.NodeDataChanged) &#123;</span><br><span class="line">            LOGGER.info(<span class="string">&quot;节点改变&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (event.getType() == EventType.NodeChildrenChanged) &#123;</span><br><span class="line">            LOGGER.info(<span class="string">&quot;子节点节点改变&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (event.getType() == EventType.NodeDeleted) &#123;</span><br><span class="line">            LOGGER.info(<span class="string">&quot;节点删除&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 连接zookeeper</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> host</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception </span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">connectZookeeper</span><span class="params">(String host, Watcher defaultWatcher)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        zookeeper = <span class="keyword">new</span> ZooKeeper(host, SESSION_TIME_OUT, defaultWatcher);</span><br><span class="line">        defaultCountDownLatch.await();</span><br><span class="line">        LOGGER.info(<span class="string">&quot;zookeeper connection success&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取路径下所有子节点</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> path</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> KeeperException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;String&gt; <span class="title">getChildren</span><span class="params">(String path, Watcher childrenWatcher)</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> zookeeper.getChildren(path, childrenWatcher);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取节点上面的数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> path  路径</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> KeeperException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException </span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">getData</span><span class="params">(String path, Watcher dataWatcher)</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">byte</span>[] data = zookeeper.getData(path, dataWatcher, <span class="keyword">null</span>);</span><br><span class="line">        <span class="keyword">if</span> (data == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> String(data);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 关闭连接</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">closeConnection</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (zookeeper != <span class="keyword">null</span>) &#123;</span><br><span class="line">            zookeeper.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String HOST = <span class="string">&quot;localhost:2181,localhost:2182,localhost:2183&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = LoggerFactory.getLogger(BaseWatcher.class);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 运行程序之前需要启动zookeeper服务端，&#123;<span class="doctag">@link</span> BaseWatcher.HOST&#125; 根据zookeeper服务端具体配置去修改</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * 除了默认watcher外其他watcher一旦触发就会失效，需要充新注册，本示例中因为</span></span><br><span class="line"><span class="comment">         * 还未想到比较好的重新注册watcher方式(考虑到如果在Watcher中持有一个zk客户端的</span></span><br><span class="line"><span class="comment">         * 实例可能存在循环引用的问题)，因此暂不实现watcher失效后重新注册watcher的问题，</span></span><br><span class="line"><span class="comment">         * 后续可以查阅curator重新注册watcher的实现方法。</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        BaseWatcher defaultWatcher = <span class="keyword">new</span> BaseWatcher();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 连接zookeeper并设置一个默认的watcher监听zookeeper文件节点的变化</span></span><br><span class="line">        connectZookeeper(HOST, defaultWatcher);</span><br><span class="line">        TimeUnit.SECONDS.sleep(<span class="number">1000000</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="创建改变节点数据的server（zookeeper集群client）"><a href="#创建改变节点数据的server（zookeeper集群client）" class="headerlink" title="创建改变节点数据的server（zookeeper集群client）"></a>创建改变节点数据的server（zookeeper集群client）</h3><p>如果一个节点向被监听节点中写数据，其他节点就会接受到zookeeper的 <strong>NodeDataChanged</strong> event</p><p>模拟改变数据server，启动一个BaseWatcher程序作为改变数据server（对zookeeper集群来说是client）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 运行程序之前需要启动zookeeper服务端，&#123;<span class="doctag">@link</span> BaseWatcher.HOST&#125; 根据zookeeper服务端具体配置去修改</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 除了默认watcher外其他watcher一旦触发就会失效，需要充新注册，本示例中因为</span></span><br><span class="line"><span class="comment">     * 还未想到比较好的重新注册watcher方式(考虑到如果在Watcher中持有一个zk客户端的</span></span><br><span class="line"><span class="comment">     * 实例可能存在循环引用的问题)，因此暂不实现watcher失效后重新注册watcher的问题，</span></span><br><span class="line"><span class="comment">     * 后续可以查阅curator重新注册watcher的实现方法。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    BaseWatcher defaultWatcher = <span class="keyword">new</span> BaseWatcher();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 连接zookeeper并设置一个默认的watcher监听zookeeper文件节点的变化</span></span><br><span class="line">    connectZookeeper(HOST, defaultWatcher);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 向/GetChildren节点写数据之前需要先创建此文件夹</span></span><br><span class="line">    <span class="comment">// 向/GetChildren节点写数据，则监听程序就会收到zookeeper的 [NodeDataChanged] event</span></span><br><span class="line">    setData(<span class="string">&quot;/GetChildren&quot;</span>, <span class="string">&quot;8&quot;</span>);</span><br><span class="line">    TimeUnit.SECONDS.sleep(<span class="number">1000000</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h3><p>两个监听程序收到zookeeper的 <strong>NodeDataChanged</strong> event，log如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">11:24:27.295 [main-SendThread(localhost:2183)] DEBUG org.apache.zookeeper.ClientCnxn - Got notification sessionid:0x300001251b50003</span><br><span class="line">11:24:27.297 [main-SendThread(localhost:2183)] DEBUG org.apache.zookeeper.ClientCnxn - Got WatchedEvent state:SyncConnected <span class="built_in">type</span>:NodeDataChanged path:/GetChildren <span class="keyword">for</span> sessionid 0x300001251b50003</span><br><span class="line">11:24:27.297 [main-EventThread] INFO com.github.xxx.bigdata.demo.zookeeper.BaseWatcher - Watch received event</span><br><span class="line">11:24:27.297 [main-EventThread] INFO com.github.xxx.bigdata.demo.zookeeper.BaseWatcher - 节点改变</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Zookeeper" scheme="https://yangyichao-mango.github.io/categories/Apache-Zookeeper/"/>
    
    
      <category term="Apache Zookeeper" scheme="https://yangyichao-mango.github.io/tags/Apache-Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Apache Druid 学习：组件以及查询类型</title>
    <link href="https://yangyichao-mango.github.io/2019/10/22/apache-druid:study-components-and-query-types/"/>
    <id>https://yangyichao-mango.github.io/2019/10/22/apache-druid:study-components-and-query-types/</id>
    <published>2019-10-22T02:40:12.000Z</published>
    <updated>2019-10-22T13:18:53.844Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Druid 学习：组件以及查询类型</p><span id="more"></span><h2 id="OLAP"><a href="#OLAP" class="headerlink" title="OLAP"></a><strong>OLAP</strong></h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>维度(Dimension): 指的是观察数据的一个角度，是考虑问题的一类属性，这些属性的集合统称为一个维。<br>维的级别(Level): 对数据的观察还存在细节程度的不同，在druid中一般表示为时间的粒度(granularity)，比如一秒，一分钟，一小时，一天…… <br>度量(Measure): 度量是用来聚合分析计算的数字信息，在druid中称为”metrics”，它可以是存储在数据库中，也可以是通过策略计算得出的。比如一篇文章的点击数、或者是根据评论数、点击数、转发数计算出的热点值</p><h3 id="对于数据处理"><a href="#对于数据处理" class="headerlink" title="对于数据处理"></a>对于数据处理</h3><p>向下钻取(Drill-down)/上卷(Roll-up): <strong>改变维的层次和级别，变换分析的粒度</strong>。Roll-up在于提升维的级别（或者称粒度）或者减少维度来聚合数据，展现总览，Drill-down反之，降低维的级别(或者称粒度)或增加维度来查看细节<br>切片(slice)和切块(dice): 当维度为两个时，我们对获取数据(查询)的操作称之为切片，当维度的数量大于两个时，我们称之为切块<br>旋转(Pivoting): 变换维的方向，例如表格中的行列互换</p><h2 id="查询条件参数"><a href="#查询条件参数" class="headerlink" title="查询条件参数"></a><strong>查询条件参数</strong></h2><table><thead><tr><th align="center">字段名</th><th align="center">描述</th><th align="center">是否必须</th></tr></thead><tbody><tr><td align="center">queryType</td><td align="center">查询类型，对应聚合查询下的类型值：timeseries、topN、groupBy等</td><td align="center">是</td></tr><tr><td align="center">dataSource</td><td align="center">数据源，类似关系数据库中表的概念，对应数据导入时Json配置属性dataSource值</td><td align="center">是</td></tr><tr><td align="center">descending</td><td align="center">返回结果是否逆序，默认值为否（正序）</td><td align="center">否</td></tr><tr><td align="center">intervals</td><td align="center">查询时间区间</td><td align="center">是</td></tr><tr><td align="center">filter</td><td align="center">对Dimension进行过滤，可以根据情况对几个维度组合不同的filter类型(and、or、not、bound)，还可以根据需要定义javascript function进行过滤</td><td align="center">否</td></tr><tr><td align="center">aggregations</td><td align="center">指定度量在聚合时候的计算策略，例如相加、或者求平均值、又或者取最后一个值，在内置类型不满足的情况下可以使用javascript。比如某手游中我统计了我每一局击杀小怪数量，以及野怪的数量，通过聚合策略sum，我能知道我从开号以来击杀了多少小怪和野怪。</td><td align="center">否</td></tr><tr><td align="center">postAggregations</td><td align="center">后聚合策略，提供了多个度量组合生成新度量的能力，主要有利于聚合计算的抽象，避免对一些指标的重复计算。举个例子，假如我需要一个度量，是我击杀小怪和野怪的总和，那么，我只需要在后聚合阶段计算，只需要拿小怪和野怪的数量相加一次，大大地提高了计算效率。</td><td align="center">否</td></tr><tr><td align="center">granularity</td><td align="center">查询的时间粒度，最细粒度为秒，最大粒度为all，提供了时间维度级别的调整并对数据进行上卷和向下钻取的能力</td><td align="center">是</td></tr><tr><td align="center">dimensionSpec</td><td align="center">提供了维度在聚合前输出展示值定制的能力，比如在Dimension age一列中，拿到的是字符串类型的数字，我希望转成数字类型，又或者定制一个javascript function，统一以 ${age} year old的形式展现</td><td align="center">否</td></tr><tr><td align="center">limit</td><td align="center">返回结果数量限制</td><td align="center">否</td></tr><tr><td align="center">context</td><td align="center">表示对当前查询本身的一些配置，比如设置查询超时的时间，又比如是否使用缓存，在通用的配置基础上，每种查询类型还有特定的配置，详见文档</td><td align="center">否</td></tr></tbody></table><h2 id="基本组件"><a href="#基本组件" class="headerlink" title="基本组件"></a><strong>基本组件</strong></h2><h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><p>过滤器，在查询语句中是一个json对象，<strong>用来对维度进行筛选</strong>，表示满足filter的是我们需要的数据。类似于SQL中的where。</p><table><thead><tr><th align="center">类型</th><th align="center">功能</th></tr></thead><tbody><tr><td align="center">SelectorFilter</td><td align="center">功能类似于SQL中的where key=value</td></tr><tr><td align="center">AndFilter, OrFilter, NotFilter</td><td align="center">功能类似于SQL中and、or、not三种过滤器。支持递归嵌套，可以构造出丰富的逻辑表达式</td></tr><tr><td align="center">RegexFilter</td><td align="center">正则表达式，支持任意维度值的java正则</td></tr><tr><td align="center">SearchFilter</td><td align="center">通过字符串匹配维度，支持多种表达式</td></tr><tr><td align="center">InFilter</td><td align="center">功能类似于SQL中where key in (value1, value2)</td></tr><tr><td align="center">IntervalFilter</td><td align="center">针对于时间维度过滤</td></tr><tr><td align="center">BoundFilter</td><td align="center">功能类似于SQL中的大于、小于、等于三种算子</td></tr><tr><td align="center">JavaScriptFilter</td><td align="center">上述filter均不能满足可以自己写JavaScript来过滤维度</td></tr></tbody></table><h3 id="aggregator"><a href="#aggregator" class="headerlink" title="aggregator"></a>aggregator</h3><p>聚合可以在采集数据时规格部分的一种方式，汇总数据进入Druid之前提供。<br>聚合也可以被指定为在查询时多查询的部分，聚合类型如下：</p><table><thead><tr><th align="center">类型</th><th align="center">功能</th></tr></thead><tbody><tr><td align="center">CountAggregator</td><td align="center">SQL count(key)</td></tr><tr><td align="center">SumAggregator</td><td align="center">SQL sum(key)</td></tr><tr><td align="center">MaxAggregator, MinAggregator</td><td align="center">SQL max(key), min(key)</td></tr><tr><td align="center">DistinctCountAggregator</td><td align="center">SQL count(distinct key)</td></tr><tr><td align="center">JavaScriptAggregator</td><td align="center">上述aggregator均不能满足可以自己写JavaScript来定义计算</td></tr></tbody></table><h3 id="post-aggregator"><a href="#post-aggregator" class="headerlink" title="post-aggregator"></a>post-aggregator</h3><table><thead><tr><th align="center">类型</th><th align="center">功能</th></tr></thead><tbody><tr><td align="center">ArithmeticPostAggregator</td><td align="center">支持对聚合后<strong>指标</strong>进行”+ - * / quotient”计算</td></tr><tr><td align="center">FieldAccessPostAggregator</td><td align="center">直接获取聚合的字段（维度，指标）</td></tr><tr><td align="center">ConstantPostAggregator</td><td align="center">返回常数</td></tr><tr><td align="center">JavaScriptPostAggregator</td><td align="center">上述postAggregator均不能满足可以自己写JavaScript来定义计算</td></tr></tbody></table><h2 id="查询类型"><a href="#查询类型" class="headerlink" title="查询类型"></a><strong>查询类型</strong></h2><h3 id="聚合查询"><a href="#聚合查询" class="headerlink" title="聚合查询"></a>聚合查询</h3><h4 id="timeseries"><a href="#timeseries" class="headerlink" title="timeseries"></a>timeseries</h4><p>时序查询，实际上即是对数据基于时间点(timestamp)的一次上卷。适合用来看某几个度量在一个时间段内的趋势。排序可按时间降序或升序</p><table><thead><tr><th align="center">字段名</th><th align="center">描述</th><th align="center">是否必须</th></tr></thead><tbody><tr><td align="center">queryType</td><td align="center">查询类型，必须为 “timeseries”</td><td align="center">是</td></tr><tr><td align="center">dataSource</td><td align="center">数据源，比如 “wikipedia”</td><td align="center">是</td></tr><tr><td align="center">descending</td><td align="center">返回结果是否逆序，默认值为否（正序）</td><td align="center">否</td></tr><tr><td align="center">intervals</td><td align="center">查询时间区间</td><td align="center">是</td></tr><tr><td align="center">granularity</td><td align="center">聚合粒度，粒度决定如何在跨时间维度得到数据块</td><td align="center">是</td></tr><tr><td align="center">filter</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">aggregations</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">postAggregations</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">limit</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">context</td><td align="center"></td><td align="center">否</td></tr></tbody></table><p>context：</p><p>1.grandTotal</p><p>2.零填充<br>如果时间范围内没有值，则会填充0<br>时间序列查询通常用零填充空的内部时间段。例如，如果您对间隔2012-01-01 / 2012-01-04发出“天”粒度时间序列查询，而2012-01-02没有数据存在，您将收到：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">&quot;timestamp&quot;</span>: <span class="string">&quot;2012-01-01T00:00:00.000Z&quot;</span>,</span><br><span class="line">    <span class="string">&quot;result&quot;</span>: &#123; <span class="string">&quot;sample_name1&quot;</span>: &lt;some_value&gt; &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">   <span class="string">&quot;timestamp&quot;</span>: <span class="string">&quot;2012-01-02T00:00:00.000Z&quot;</span>,</span><br><span class="line">   <span class="string">&quot;result&quot;</span>: &#123; <span class="string">&quot;sample_name1&quot;</span>: 0 &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">&quot;timestamp&quot;</span>: <span class="string">&quot;2012-01-03T00:00:00.000Z&quot;</span>,</span><br><span class="line">    <span class="string">&quot;result&quot;</span>: &#123; <span class="string">&quot;sample_name1&quot;</span>: &lt;some_value&gt; &#125;</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h4 id="topN"><a href="#topN" class="headerlink" title="topN"></a>topN</h4><p>在时间点的基础上，又增加了一个维度(OLAP的概念算两个维度)，进而对源数据进行切片，切片之后分别上卷，最后返回一个聚合集，你可以指定某个指标作为排序的依据。官方文档称这对比单个druid dimension 的groupBy 更高效。适合看某个维度下的时间趋势，（比如美国和中国十年内GDP的增长趋势比对，在这里除了时间外国家就是另外一个维度）</p><table><thead><tr><th align="center">字段名</th><th align="center">描述</th><th align="center">是否必须</th></tr></thead><tbody><tr><td align="center">queryType</td><td align="center">查询类型，必须为 “topN”</td><td align="center">是</td></tr><tr><td align="center">dataSource</td><td align="center">数据源，比如 “wikipedia”</td><td align="center">是</td></tr><tr><td align="center">intervals</td><td align="center">查询时间区间</td><td align="center">是</td></tr><tr><td align="center">granularity</td><td align="center">聚合粒度，粒度决定如何在跨时间维度得到数据块</td><td align="center">是</td></tr><tr><td align="center">filter</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">aggregations</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">postAggregations</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">dimension</td><td align="center">除了时间之外聚合的维度，只能定义一个维度</td><td align="center">是</td></tr><tr><td align="center">threshold</td><td align="center">topN中的N，例如：希望查询到top2，则值为2</td><td align="center">是</td></tr><tr><td align="center">metric</td><td align="center">topN中用来排序的指标</td><td align="center">是</td></tr><tr><td align="center">context</td><td align="center"></td><td align="center">否</td></tr></tbody></table><h4 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h4><p>适用于两个维度以上的查询，druid会根据维度切块，并且分别上卷，最后返回聚合集。相对于topN而言，这是一个向下钻取的操作，每多一个维度意味着保留更多的细节。(比如增加一个行业的维度，就可以知道美国和中国十年内，每一年不同行业贡献GDP的占比)<br><strong>注意：如果要使用时间作为唯一分组进行聚合，或者在单个维度上使用有序groupBy进行聚合，请优先考虑使用Timeseries和TopN查询。在某些情况下，它们的性能可能会更好。有关更多详细信息，请参见下面的替代方法。</strong></p><table><thead><tr><th align="center">字段名</th><th align="center">描述</th><th align="center">是否必须</th></tr></thead><tbody><tr><td align="center">queryType</td><td align="center">查询类型，必须为 “groupBy”</td><td align="center">是</td></tr><tr><td align="center">dataSource</td><td align="center">数据源，比如 “wikipedia”</td><td align="center">是</td></tr><tr><td align="center">dimensions</td><td align="center">需要聚合的所有维度</td><td align="center">是</td></tr><tr><td align="center">limitSpec</td><td align="center">同关系型数据库中的limit</td><td align="center">否</td></tr><tr><td align="center">having</td><td align="center">同关系型数据库中的having</td><td align="center">否</td></tr><tr><td align="center">granularity</td><td align="center">聚合粒度，粒度决定如何在跨时间维度得到数据块</td><td align="center">是</td></tr><tr><td align="center">filter</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">aggregations</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">postAggregations</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">intervals</td><td align="center">查询时间区间</td><td align="center">是</td></tr><tr><td align="center">subtotalsSpec</td><td align="center">类似于的grouping sets</td><td align="center">否</td></tr><tr><td align="center">context</td><td align="center"></td><td align="center">否</td></tr></tbody></table><h3 id="普通查询"><a href="#普通查询" class="headerlink" title="普通查询"></a>普通查询</h3><h4 id="select"><a href="#select" class="headerlink" title="select"></a>select</h4><p>类似SQL中的select操作，select用来查看Druid中存储的数据，并支持按照指定过滤器和时间查看指定维度和指标。不支持aggregations和post aggregations</p><p><strong>注意：建议您尽可能使用scan查询类型而不是select。在涉及大量segment的情况下，select查询可能具有很高的内存和性能开销，但是scan查询没有此问题。<br>两者之间的主要区别是“扫描”查询不支持分页。但是，即使没有分页，scan查询类型也能够返回几乎无限数量的结果，使得分页在许多情况下是不必要的。</strong></p><table><thead><tr><th align="center">字段名</th><th align="center">描述</th><th align="center">是否必须</th></tr></thead><tbody><tr><td align="center">queryType</td><td align="center">查询类型，必须为 “select”</td><td align="center">是</td></tr><tr><td align="center">dataSource</td><td align="center">数据源，比如 “wikipedia”</td><td align="center">是</td></tr><tr><td align="center">intervals</td><td align="center">查询时间区间</td><td align="center">是</td></tr><tr><td align="center">descending</td><td align="center">返回结果是否逆序，默认值为否（正序）</td><td align="center">否</td></tr><tr><td align="center">filter</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">dimensions</td><td align="center">需要查询的维度列表</td><td align="center">否</td></tr><tr><td align="center">metrics</td><td align="center">需要查询的指标列表</td><td align="center">否</td></tr><tr><td align="center">granularity</td><td align="center">聚合粒度，粒度决定如何在跨时间维度得到数据块，默认是Granularity.ALL</td><td align="center">否</td></tr><tr><td align="center">pagingSpec</td><td align="center">分页</td><td align="center">是</td></tr><tr><td align="center">context</td><td align="center"></td><td align="center">否</td></tr></tbody></table><h4 id="scan"><a href="#scan" class="headerlink" title="scan"></a>scan</h4><p>扫描查询以流模式返回行，Select查询和Scan查询之间的最大区别是，Scan查询子返回给客户端数据之前不会将所有行数据保留在内存中<br>而select查询将把行保留在内存中，如果返回太多行，则会导致内存压力。扫描查询可以返回所有行，而无需发出另一个分页查询。</p><p>除了将scan查询发送给server的用法外，还可以直接向historical历史记录进程或streaming ingestion流式提取任务发出扫描查询。如果要并行检索大量数据，这将很有用。</p><table><thead><tr><th align="center">字段名</th><th align="center">描述</th><th align="center">是否必须</th></tr></thead><tbody><tr><td align="center">queryType</td><td align="center">查询类型，必须为 “scan”</td><td align="center">是</td></tr><tr><td align="center">dataSource</td><td align="center">数据源，比如 “wikipedia”</td><td align="center">是</td></tr><tr><td align="center">intervals</td><td align="center">查询时间区间</td><td align="center">是</td></tr><tr><td align="center">resultFormat</td><td align="center">返回结果类型：list，compactedList或valueVector。目前仅list和compactedList受支持。默认是list</td><td align="center">否</td></tr><tr><td align="center">filter</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">columns</td><td align="center">需要scan的维度和指标，默认为所有</td><td align="center">否</td></tr><tr><td align="center">batchSize</td><td align="center">返回数据之前默认缓存最多多少行</td><td align="center">否</td></tr><tr><td align="center">limit</td><td align="center">查询返回最大的数据条目，如果不指定，则返回所有的数据</td><td align="center">否</td></tr><tr><td align="center">order</td><td align="center">返回数据的order，基于timestamp，并且只有__time被包含在columns中才生效</td><td align="center">否</td></tr><tr><td align="center">legacy</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">context</td><td align="center"></td><td align="center">否</td></tr></tbody></table><h4 id="search"><a href="#search" class="headerlink" title="search"></a>search</h4><p>类似SQL中的Like操作，但是支持更多的匹配操作</p><table><thead><tr><th align="center">字段名</th><th align="center">描述</th><th align="center">是否必须</th></tr></thead><tbody><tr><td align="center">queryType</td><td align="center">查询类型，必须为 “search”</td><td align="center">是</td></tr><tr><td align="center">dataSource</td><td align="center">数据源，比如 “wikipedia”</td><td align="center">是</td></tr><tr><td align="center">granularity</td><td align="center">聚合粒度，粒度决定如何在跨时间维度得到数据块</td><td align="center">是</td></tr><tr><td align="center">filter</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">limit</td><td align="center">每个历史进程的最大查询返回数据条目（默认是1000）</td><td align="center">否</td></tr><tr><td align="center">intervals</td><td align="center">查询时间区间</td><td align="center">是</td></tr><tr><td align="center">searchDimensions</td><td align="center">需要search的维度（默认是所有维度），key like value中的key</td><td align="center">否</td></tr><tr><td align="center">query</td><td align="center">search维度需要匹配的value，key like value中的value</td><td align="center">是</td></tr><tr><td align="center">sort</td><td align="center">指定应如何对搜索结果进行排序，包括字典编排（默认排序），字母数字，strlen和数字排序</td><td align="center">否</td></tr><tr><td align="center">context</td><td align="center"></td><td align="center">否</td></tr></tbody></table><h3 id="元数据查询"><a href="#元数据查询" class="headerlink" title="元数据查询"></a>元数据查询</h3><h4 id="time-bounding"><a href="#time-bounding" class="headerlink" title="time bounding"></a>time bounding</h4><h4 id="segment-metadata"><a href="#segment-metadata" class="headerlink" title="segment metadata"></a>segment metadata</h4><h4 id="dataSource-metadata"><a href="#dataSource-metadata" class="headerlink" title="dataSource metadata"></a>dataSource metadata</h4>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/categories/Apache-Druid/"/>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/tags/Apache-Druid/"/>
    
  </entry>
  
  <entry>
    <title>Apache Druid 学习：kafka to druid</title>
    <link href="https://yangyichao-mango.github.io/2019/10/21/apache-druid:study-kafka-to-druid/"/>
    <id>https://yangyichao-mango.github.io/2019/10/21/apache-druid:study-kafka-to-druid/</id>
    <published>2019-10-21T07:08:45.000Z</published>
    <updated>2019-10-21T07:26:54.920Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Druid 学习：kafka to druid demo</p><span id="more"></span><h3 id="Druid-Web操作"><a href="#Druid-Web操作" class="headerlink" title="Druid Web操作"></a><strong>Druid Web操作</strong></h3><p><a href="http://druid.apache.org/docs/latest/tutorials/tutorial-kafka.html">官网教程</a></p><h3 id="Java-Demo"><a href="#Java-Demo" class="headerlink" title="Java Demo"></a><strong>Java Demo</strong></h3>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/categories/Apache-Druid/"/>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/tags/Apache-Druid/"/>
    
      <category term="Apache Kafka" scheme="https://yangyichao-mango.github.io/tags/Apache-Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Mac安装Apache Druid-0.16.0-incubating</title>
    <link href="https://yangyichao-mango.github.io/2019/10/21/apache-druid:0.16.0-incubating-mac-install/"/>
    <id>https://yangyichao-mango.github.io/2019/10/21/apache-druid:0.16.0-incubating-mac-install/</id>
    <published>2019-10-21T06:21:53.000Z</published>
    <updated>2019-10-22T02:39:59.582Z</updated>
    
    <content type="html"><![CDATA[<p>Mac安装Apache Druid-0.16.0-incubating教程</p><span id="more"></span><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a><strong>安装</strong></h3><p>参考<a href="http://druid.apache.org/docs/latest/tutorials/index.html">官网教程</a></p><h4 id="brew安装"><a href="#brew安装" class="headerlink" title="brew安装"></a>brew安装</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install druid</span><br></pre></td></tr></table></figure><p>quer</p><h4 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ curl https://www-us.apache.org/dist/incubator/druid/0.16.0-incubating/apache-druid-0.16.0-incubating-bin.tar.gz</span><br><span class="line">$ tar -xzf apache-druid-0.16.0-incubating-bin.tar.gz</span><br><span class="line">$ <span class="built_in">cd</span> apache-druid-0.16.0-incubating</span><br></pre></td></tr></table></figure><h3 id="配置启动"><a href="#配置启动" class="headerlink" title="配置启动"></a><strong>配置启动</strong></h3><p>启动druid服务之前需要先启动zookeeper，下面有两种方式启动和使用zookeeper</p><h4 id="使用集成zookeeper"><a href="#使用集成zookeeper" class="headerlink" title="使用集成zookeeper"></a>使用集成zookeeper</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> <span class="variable">$DRUID_HOME</span> // 需要在~/.bash_profile中进行配置</span><br><span class="line">$ curl https://archive.apache.org/dist/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz -o zookeeper-3.4.14.tar.gz</span><br><span class="line">$ tar -xzf zookeeper-3.4.14.tar.gz</span><br><span class="line">$ mv zookeeper-3.4.14 zk</span><br></pre></td></tr></table></figure><h4 id="使用外部zookeeper"><a href="#使用外部zookeeper" class="headerlink" title="使用外部zookeeper"></a>使用外部zookeeper</h4><h5 id="修改-DRUID-HOME-conf-supervise-single-server-micro-quickstart-conf-中的配置"><a href="#修改-DRUID-HOME-conf-supervise-single-server-micro-quickstart-conf-中的配置" class="headerlink" title="修改 $DRUID_HOME/conf/supervise/single-server/micro-quickstart.conf 中的配置"></a>修改 <strong>$DRUID_HOME/conf/supervise/single-server/micro-quickstart.conf</strong> 中的配置</h5><p>将 <strong>!p10 zk bin/run-zk conf</strong> 注释掉</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ vi <span class="variable">$DRUID_HOME</span>/conf/supervise/single-server/micro-quickstart.conf</span><br><span class="line"></span><br><span class="line">:verify bin/verify-java</span><br><span class="line">:verify bin/verify-default-ports</span><br><span class="line">:kill-timeout 10</span><br><span class="line"></span><br><span class="line"><span class="comment"># !p10 zk bin/run-zk conf // 这里是运行集成zookeeper的代码，所以要注释掉</span></span><br><span class="line">coordinator-overlord bin/run-druid coordinator-overlord conf/druid/single-server/micro-quickstart</span><br><span class="line">broker bin/run-druid broker conf/druid/single-server/micro-quickstart</span><br><span class="line">router bin/run-druid router conf/druid/single-server/micro-quickstart</span><br><span class="line">historical bin/run-druid historical conf/druid/single-server/micro-quickstart</span><br><span class="line">!p90 middleManager bin/run-druid middleManager conf/druid/single-server/micro-quickstart</span><br><span class="line"></span><br><span class="line"><span class="comment"># Uncomment to use Tranquility Server</span></span><br><span class="line"><span class="comment">#!p95 tranquility-server tranquility/bin/tranquility server -configFile conf/tranquility/wikipedia-server.json -Ddruid.extensions.loadList=[]</span></span><br></pre></td></tr></table></figure><h5 id="修改-DRUID-HOME-conf-druid-single-server-micro-quickstart-common-common-runtime-properties-中的配置"><a href="#修改-DRUID-HOME-conf-druid-single-server-micro-quickstart-common-common-runtime-properties-中的配置" class="headerlink" title="修改 $DRUID_HOME/conf/druid/single-server/micro-quickstart/_common/common.runtime.properties 中的配置"></a>修改 <strong>$DRUID_HOME/conf/druid/single-server/micro-quickstart/_common/common.runtime.properties</strong> 中的配置</h5><p>修改zookeeper的client配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ vim <span class="variable">$DRUID_HOME</span>/conf/druid/single-server/micro-quickstart/_common/common.runtime.properties</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置外部zookeeper的信息</span></span><br><span class="line"><span class="comment"># zookeeper，大概在46~55行中间，对zk进行配置</span></span><br><span class="line"><span class="comment"># zookeeper的server运行在2181端口上</span></span><br><span class="line">druid.zk.service.host=127.0.0.1:2181</span><br><span class="line">druid.zk.paths.base=/druid</span><br></pre></td></tr></table></figure><h5 id="修改-DRUID-HOME-bin-verify-default-ports-中的配置"><a href="#修改-DRUID-HOME-bin-verify-default-ports-中的配置" class="headerlink" title="修改 $DRUID_HOME/bin/verify-default-ports 中的配置"></a>修改 <strong>$DRUID_HOME/bin/verify-default-ports</strong> 中的配置</h5><p>因为使用了外部zookeeper，并且外部zookeeper的ip:port为127.0.0.1:2181<br>所以需要将zookeeper的2181端口删除，否则会校验本机2181端口是否被占用，因为本机zookeeper已经将其占用，则会报错，服务不能启动<br>如果使用的zookeeper的不在本机部署，则可以不注释2181</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ vi <span class="variable">$DRUID_HOME</span>/bin/verify-default-ports</span><br><span class="line"></span><br><span class="line"><span class="comment"># my @ports = (1527, 2181, 8081, 8082, 8083, 8090, 8091, 8200, 9095);</span></span><br><span class="line"></span><br><span class="line">my @ports = (1527, 8081, 8082, 8083, 8090, 8091, 8200, 9095);</span><br></pre></td></tr></table></figure><h4 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/start-micro-quickstart</span><br></pre></td></tr></table></figure><p>可以到<a href="http://localhost:8888">http://localhost:8888</a>查看是否启动成功</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/categories/Apache-Druid/"/>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/tags/Apache-Druid/"/>
    
      <category term="Apache Zookeeper" scheme="https://yangyichao-mango.github.io/tags/Apache-Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：hbase作为sink</title>
    <link href="https://yangyichao-mango.github.io/2019/10/20/apache-flink:study-hbase-sink/"/>
    <id>https://yangyichao-mango.github.io/2019/10/20/apache-flink:study-hbase-sink/</id>
    <published>2019-10-20T06:27:49.000Z</published>
    <updated>2019-10-25T03:40:04.204Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：hbase作为sink的demo</p><span id="more"></span><h3 id="依赖项"><a href="#依赖项" class="headerlink" title="依赖项"></a><strong>依赖项</strong></h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hbase.version</span>&gt;</span>2.0.5<span class="tag">&lt;/<span class="name">hbase.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hbase.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="demo代码"><a href="#demo代码" class="headerlink" title="demo代码"></a><strong>demo代码</strong></h3><p>使用了hbase作为source，hbase作为sink</p><p><strong>运行之前需要运行hadoop集群（zookeeper集群），hbase集群</strong><br><strong>flink根据部署的集群信息（比如zookeeper的ip:port为127.0.0.1:2181等的信息）去连接hbase</strong></p><h4 id="hbase-site-xml"><a href="#hbase-site-xml" class="headerlink" title="hbase-site.xml"></a>hbase-site.xml</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--Autogenerated by Cloudera Manager--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- zk configuration --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>zookeeper.session.timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>60000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>zookeeper.znode.parent<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>127.0.0.1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.clientPort<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="HBaseReader"><a href="#HBaseReader" class="headerlink" title="HBaseReader"></a>HBaseReader</h4><p>HBase作为source</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseReader</span> <span class="keyword">extends</span> <span class="title">RichSourceFunction</span>&lt;<span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = LoggerFactory.getLogger(HBaseReader.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> HBaseClient hBaseClient;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_HBASE_SOURCE_TABLE_NAME = <span class="string">&quot;student&quot;</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_HBASE_SOURCE_START_ROW = <span class="string">&quot;row1&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_HBASE_SOURCE_STOP_ROW = <span class="string">&quot;row1&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">                    hBaseClient = <span class="keyword">new</span> HBaseClient();</span><br><span class="line">                    hBaseClient.initialize();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;String&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        List&lt;<span class="keyword">byte</span>[]&gt; results = hBaseClient.scan(</span><br><span class="line">                DEFAULT_HBASE_SOURCE_TABLE_NAME</span><br><span class="line">                , DEFAULT_HBASE_SOURCE_START_ROW</span><br><span class="line">                , DEFAULT_HBASE_SOURCE_STOP_ROW);</span><br><span class="line">        results.forEach(result -&gt; ctx.collect(<span class="keyword">new</span> String(result)));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        hBaseClient.destroy();</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                        LOGGER.error(<span class="string">&quot;&quot;</span>, e);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="HBaseWriter"><a href="#HBaseWriter" class="headerlink" title="HBaseWriter"></a>HBaseWriter</h4><p>HBase作为sink</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseWriter</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = LoggerFactory.getLogger(HBaseWriter.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> HBaseClient hBaseClient;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">                    hBaseClient = <span class="keyword">new</span> HBaseClient();</span><br><span class="line">                    hBaseClient.initialize();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(Tuple2&lt;String, Integer&gt; value, Context context)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        String result = value.toString();</span><br><span class="line">        Put put = hBaseClient.createPut(<span class="string">&quot;row2&quot;</span>);</span><br><span class="line">        hBaseClient.addValueOnPut(put, <span class="string">&quot;description&quot;</span>, <span class="string">&quot;age&quot;</span>, <span class="string">&quot;19&quot;</span>);</span><br><span class="line">        hBaseClient.put(<span class="string">&quot;student&quot;</span>, put);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.close();</span><br><span class="line">        <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        hBaseClient.destroy();</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                        LOGGER.error(<span class="string">&quot;&quot;</span>, e);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="定义dag"><a href="#定义dag" class="headerlink" title="定义dag"></a>定义dag</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> SourceFunction&lt;String&gt; source;</span><br><span class="line">    <span class="keyword">final</span> ParameterTool params = ParameterTool.fromArgs(args);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/******************************* hbase source *******************************/</span></span><br><span class="line">    source = <span class="keyword">new</span> HBaseReader();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/******************************* define dag *******************************/</span></span><br><span class="line">    <span class="comment">// create the environment to create streams and configure execution</span></span><br><span class="line">    <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    <span class="comment">// make parameters available in the web interface</span></span><br><span class="line">    env.getConfig().setGlobalJobParameters(params);</span><br><span class="line">    DataStream&lt;String&gt; sentenceStream = env.addSource(source);</span><br><span class="line">    DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; wordCountStream = sentenceStream</span><br><span class="line">            .flatMap(<span class="keyword">new</span> LineSplitter())</span><br><span class="line">            .keyBy(<span class="number">0</span>)</span><br><span class="line">            .sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/******************************* hbase sink *******************************/</span></span><br><span class="line">    wordCountStream.addSink(<span class="keyword">new</span> HBaseWriter());</span><br><span class="line">    wordCountStream.print();</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">&quot;Java hbase Word Count&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="查看hbase文件"><a href="#查看hbase文件" class="headerlink" title="查看hbase文件"></a><strong>查看hbase文件</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; scan <span class="string">&#x27;student&#x27;</span></span><br><span class="line">ROW                   COLUMN+CELL</span><br><span class="line"> row1                 column=description:age, timestamp=1571460125600, value=18</span><br><span class="line"> row1                 column=description:name, timestamp=1571460129987, value=li</span><br><span class="line">                      u</span><br><span class="line"> <span class="comment"># 记录以及被写入hbase</span></span><br><span class="line"> row2                 column=description:age, timestamp=1571576517072, value=19</span><br><span class="line">2 row(s) <span class="keyword">in</span> 0.2010 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):002:0&gt;</span><br></pre></td></tr></table></figure><p>发现columnFamilyName为<strong>description</strong>，columnName为<strong>age</strong>，rowkey为<strong>row2</strong>，value为<strong>19</strong>的记录已经被写入hbase</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
      <category term="Apache HBase" scheme="https://yangyichao-mango.github.io/tags/Apache-HBase/"/>
    
  </entry>
  
  <entry>
    <title>Mac安装Apache Zookeeper-3.4.12</title>
    <link href="https://yangyichao-mango.github.io/2019/10/20/apache-zookeeper:3.4.12-mac-install/"/>
    <id>https://yangyichao-mango.github.io/2019/10/20/apache-zookeeper:3.4.12-mac-install/</id>
    <published>2019-10-20T03:23:17.000Z</published>
    <updated>2019-10-25T03:41:16.346Z</updated>
    
    <content type="html"><![CDATA[<p>Mac安装Apache Zookeeper-3.4.12教程</p><span id="more"></span><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a><strong>安装</strong></h3><h4 id="安装方式1-brew安装"><a href="#安装方式1-brew安装" class="headerlink" title="安装方式1-brew安装"></a>安装方式1-brew安装</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew install zookeeper</span><br></pre></td></tr></table></figure><h4 id="安装方式2-下载压缩包"><a href="#安装方式2-下载压缩包" class="headerlink" title="安装方式2-下载压缩包"></a>安装方式2-下载压缩包</h4><p>从此地址下载<strong><a href="http://mirrors.hust.edu.cn/apache/zookeeper/stable/">http://mirrors.hust.edu.cn/apache/zookeeper/stable/</a></strong></p><p>解压配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ tar -zxvf zookeeper-3.4.12.tar.gz // 解压</span><br><span class="line">$ <span class="built_in">cd</span> zookeeper-3.4.12/conf // 切换到配置目录下</span><br><span class="line">$ mv zoo_sample.cfg zoo.cfg // 更改默认配置文件名称</span><br><span class="line">$ vi zoo.cfg // 编辑配置文件，自定义dataDir</span><br></pre></td></tr></table></figure><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a><strong>启动</strong></h3><h4 id="启动sever端"><a href="#启动sever端" class="headerlink" title="启动sever端"></a>启动sever端</h4><p>切换到bin目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">pwd</span></span><br><span class="line">/user/<span class="built_in">local</span>/Celler/zookeeper/3.4.12/bin</span><br><span class="line"></span><br><span class="line">$ ls</span><br><span class="line">README.txtzkCli.cmdzkEnv.cmdzkServer.cmdzookeeper.out</span><br><span class="line">zkCleanup.shzkCli.shzkEnv.shzkServer.sh</span><br><span class="line"></span><br><span class="line">$ ./zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /user/<span class="built_in">local</span>/Celler/zookeeper/3.4.12/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure><h4 id="启动client端"><a href="#启动client端" class="headerlink" title="启动client端"></a>启动client端</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">./zkCli.sh -server 127.0.0.1:2181</span><br><span class="line">Connecting to 127.0.0.1:2181</span><br><span class="line">2019-10-20 12:17:25,861 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.12-e5259e437540f349646870ea94dc2658c4e44b3b, built on 03/27/2018 03:55 GMT</span><br><span class="line">2019-10-20 12:17:25,864 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=localhost</span><br><span class="line">2019-10-20 12:17:25,864 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.8.0_191</span><br><span class="line">2019-10-20 12:17:25,866 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation</span><br><span class="line">2019-10-20 12:17:25,866 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_191.jdk/Contents/Home/jre</span><br><span class="line">2019-10-20 12:17:25,868 [myid:] - INFO  [main:ZooKeeper@441] - Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain<span class="variable">$MyWatcher</span>@799f7e29</span><br><span class="line">Welcome to ZooKeeper!</span><br><span class="line">2019-10-20 12:17:25,896 [myid:] - INFO  [main-SendThread(127.0.0.1:2181):ClientCnxn<span class="variable">$SendThread</span>@1028] - Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)</span><br><span class="line">JLine support is enabled</span><br><span class="line">2019-10-20 12:17:25,959 [myid:] - INFO  [main-SendThread(127.0.0.1:2181):ClientCnxn<span class="variable">$SendThread</span>@878] - Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session</span><br><span class="line">2019-10-20 12:17:25,966 [myid:] - INFO  [main-SendThread(127.0.0.1:2181):ClientCnxn<span class="variable">$SendThread</span>@1302] - Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x10000112e160008, negotiated timeout = 30000</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected <span class="built_in">type</span>:None path:null</span><br><span class="line">[zk: 127.0.0.1:2181(CONNECTED) 0] ls /</span><br><span class="line">[zookeeper, hbase]</span><br></pre></td></tr></table></figure><h4 id="停止server端"><a href="#停止server端" class="headerlink" title="停止server端"></a>停止server端</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; ./zkServer.sh stop //停止后，如果CLi没有关闭，将报错</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: zookeeper-3.4.12/bin/../conf/zoo.cfg</span><br><span class="line">Stopping zookeeper ... STOPPED</span><br></pre></td></tr></table></figure><h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a><strong>配置文件</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The number of milliseconds of each tick</span></span><br><span class="line">tickTime=2000</span><br><span class="line"><span class="comment"># The number of ticks that the initial</span></span><br><span class="line"><span class="comment"># synchronization phase can take</span></span><br><span class="line">initLimit=10</span><br><span class="line"><span class="comment"># The number of ticks that can pass between</span></span><br><span class="line"><span class="comment"># sending a request and getting an acknowledgement</span></span><br><span class="line">syncLimit=5</span><br><span class="line"><span class="comment"># the directory where the snapshot is stored.</span></span><br><span class="line"><span class="comment"># do not use /tmp for storage, /tmp here is just</span></span><br><span class="line"><span class="comment"># example sakes. </span></span><br><span class="line"><span class="comment"># 内存数据快照的保存目录；如果没有自定义Log也使用该目录</span></span><br><span class="line">dataDir=/tmp/zookeeper</span><br><span class="line"><span class="comment"># the port at which the clients will connect</span></span><br><span class="line"><span class="comment"># zookeeper服务端的端口，客户端启动时需要连接的端口</span></span><br><span class="line">clientPort=2181</span><br><span class="line"><span class="comment"># the maximum number of client connections.</span></span><br><span class="line"><span class="comment"># increase this if you need to handle more clients</span></span><br><span class="line"><span class="comment">#maxClientCnxns=60</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Be sure to read the maintenance section of the</span></span><br><span class="line"><span class="comment"># administrator guide before turning on autopurge.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The number of snapshots to retain in dataDir</span></span><br><span class="line"><span class="comment">#autopurge.snapRetainCount=3</span></span><br><span class="line"><span class="comment"># Purge task interval in hours</span></span><br><span class="line"><span class="comment"># Set to &quot;0&quot; to disable auto purge feature</span></span><br><span class="line"><span class="comment">#autopurge.purgeInterval=1</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Zookeeper" scheme="https://yangyichao-mango.github.io/categories/Apache-Zookeeper/"/>
    
    
      <category term="Apache Zookeeper" scheme="https://yangyichao-mango.github.io/tags/Apache-Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：hdfs作为sink</title>
    <link href="https://yangyichao-mango.github.io/2019/10/19/apache-flink:study-hdfs-sink/"/>
    <id>https://yangyichao-mango.github.io/2019/10/19/apache-flink:study-hdfs-sink/</id>
    <published>2019-10-19T11:33:38.000Z</published>
    <updated>2019-10-25T03:40:04.196Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：hdfs作为source和sink的demo</p><span id="more"></span><h3 id="依赖项"><a href="#依赖项" class="headerlink" title="依赖项"></a><strong>依赖项</strong></h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">flink.version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">flink.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-filesystem_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span> </span><br></pre></td></tr></table></figure><h3 id="demo代码"><a href="#demo代码" class="headerlink" title="demo代码"></a><strong>demo代码</strong></h3><p>使用了kafka作为source，hdfs作为sink<br><strong>运行之前需要运行kafka集群，hadoop集群（zookeeper集群）</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/******************************* define dag *******************************/</span></span><br><span class="line"><span class="comment">// create the environment to create streams and configure execution</span></span><br><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// make parameters available in the web interface</span></span><br><span class="line">env.getConfig().setGlobalJobParameters(params);</span><br><span class="line">DataStream&lt;String&gt; sentenceStream = env.addSource(source);</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; wordCountStream = sentenceStream</span><br><span class="line">        .flatMap(<span class="keyword">new</span> LineSplitter())</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        .sum(<span class="number">1</span>);</span><br><span class="line">wordCountStream.print();</span><br><span class="line"></span><br><span class="line">DataStream&lt;String&gt; kafkaSinkStream = wordCountStream</span><br><span class="line">        .map(<span class="keyword">new</span> WordBuilder());</span><br><span class="line"></span><br><span class="line"><span class="comment">/******************************* hdfs sink *******************************/</span></span><br><span class="line"></span><br><span class="line">BucketingSink&lt;String&gt; bucketingSink = <span class="keyword">new</span> BucketingSink&lt;&gt;(<span class="string">&quot;/user/xxx/flink/from-kafka&quot;</span>); <span class="comment">//hdfs上的路径</span></span><br><span class="line">bucketingSink.setWriter(<span class="keyword">new</span> StringWriter&lt;&gt;())</span><br><span class="line">        .setBatchSize(<span class="number">1024</span> * <span class="number">1024L</span>)</span><br><span class="line">        .setBatchRolloverInterval(<span class="number">2000</span>)</span><br><span class="line">        .setInactiveBucketThreshold(<span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">kafkaSinkStream.addSink(bucketingSink);</span><br></pre></td></tr></table></figure><p>上面例子将创建一个 Sink，写入遵循下面格式的分桶文件中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/base/path/&#123;date-time&#125;/_part-&#123;parallel-task&#125;-&#123;count&#125;</span><br></pre></td></tr></table></figure><p><strong>date-time</strong>：<br>是从setBucketer()自定义的日期/时间格式的字符串，如果不进行设置，默认Bucketer是DateTimeBucketer，默认值是yyyy-MM-dd–HH（DateTimeBucketer.DEFAULT_FORMAT_STRING）</p><p><strong>_part-{parallel-task}-{count}：</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_VALID_PREFIX = <span class="string">&quot;_&quot;</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_PART_PREFIX = <span class="string">&quot;part&quot;</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_PENDING_SUFFIX = <span class="string">&quot;.pending&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">openNewPartFile</span><span class="params">(Path bucketPath, BucketState&lt;T&gt; bucketState)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Path partPath = assemblePartPath(bucketPath, subtaskIndex, bucketState.partCounter);</span><br><span class="line">    Path inProgressPath = getInProgressPathFor(partPath);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> Path <span class="title">assemblePartPath</span><span class="params">(Path bucket, <span class="keyword">int</span> subtaskIndex, <span class="keyword">int</span> partIndex)</span> </span>&#123;</span><br><span class="line">    String localPartSuffix = partSuffix != <span class="keyword">null</span> ? partSuffix : <span class="string">&quot;&quot;</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Path(bucket, String.format(<span class="string">&quot;%s-%s-%s%s&quot;</span>, partPrefix, subtaskIndex, partIndex, localPartSuffix));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> Path <span class="title">getInProgressPathFor</span><span class="params">(Path path)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Path(path.getParent(), inProgressPrefix + path.getName()).suffix(inProgressSuffix);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="查看hdfs文件"><a href="#查看hdfs文件" class="headerlink" title="查看hdfs文件"></a><strong>查看hdfs文件</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ ./hdfs dfs -ls /user/xxx/flink/from-kafka/2019-10-19--19</span><br><span class="line">2019-10-19 20:08:31,244 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">Found 15 items</span><br><span class="line">-rw-r--r--   1 xxx supergroup          4 2019-10-19 19:45 /user/xxx/flink/from-kafka/2019-10-19--19/_part-0-0.pending</span><br><span class="line">-rw-r--r--   1 xxx supergroup          2 2019-10-19 19:45 /user/xxx/flink/from-kafka/2019-10-19--19/_part-1-0.pending</span><br><span class="line">-rw-r--r--   1 xxx supergroup          5 2019-10-19 19:44 /user/xxx/flink/from-kafka/2019-10-19--19/_part-2-0.pending</span><br><span class="line">-rw-r--r--   1 xxx supergroup         11 2019-10-19 19:45 /user/xxx/flink/from-kafka/2019-10-19--19/_part-2-1.pending</span><br><span class="line">-rw-r--r--   1 xxx supergroup         10 2019-10-19 19:44 /user/xxx/flink/from-kafka/2019-10-19--19/_part-3-0.pending</span><br><span class="line">-rw-r--r--   1 xxx supergroup         13 2019-10-19 19:45 /user/xxx/flink/from-kafka/2019-10-19--19/_part-3-1.pending</span><br><span class="line">-rw-r--r--   1 xxx supergroup          9 2019-10-19 19:45 /user/xxx/flink/from-kafka/2019-10-19--19/_part-4-0.pending</span><br><span class="line">-rw-r--r--   1 xxx supergroup         10 2019-10-19 19:44 /user/xxx/flink/from-kafka/2019-10-19--19/_part-5-0.pending</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/tags/Apache-Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Mac安装Apache HBase-1.3.5</title>
    <link href="https://yangyichao-mango.github.io/2019/10/18/apache-hbase:1.3.5-mac-install/"/>
    <id>https://yangyichao-mango.github.io/2019/10/18/apache-hbase:1.3.5-mac-install/</id>
    <published>2019-10-18T15:14:59.000Z</published>
    <updated>2019-10-25T03:40:04.199Z</updated>
    
    <content type="html"><![CDATA[<p>Mac安装Apache HBase-1.3.5教程</p><span id="more"></span><h3 id="HBase安装"><a href="#HBase安装" class="headerlink" title="HBase安装"></a><strong>HBase安装</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew install hbase</span><br></pre></td></tr></table></figure><p>安装在<font color=red><strong>/usr/local/Cellar/hbase/1.3.5</strong></font></p><h3 id="HBase配置"><a href="#HBase配置" class="headerlink" title="HBase配置"></a><strong>HBase配置</strong></h3><h4 id="hbase-env-sh"><a href="#hbase-env-sh" class="headerlink" title="hbase-env.sh"></a>hbase-env.sh</h4><p>在<font color=red>conf/hbase-env.sh设置JAVA_HOME</font></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/Cellar/hbase/1.3.5/libexec/conf</span><br><span class="line">$ vim hbase-env.sh</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=<span class="string">&quot;<span class="subst">$(/usr/libexec/java_home --version 1.8)</span>&quot;</span></span><br></pre></td></tr></table></figure><p>Apache HBase-1.3.5中JAVA_HOME已经默认被配置好了<br>如果JAVA_HOME没有配置好，则需要设置JAVA_HOME，可以通过下面的命令查看JAVA_HOME</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/libexec/java_home</span><br><span class="line">/Library/Java/JavaVirtualMachines/jdk1.8.0_191.jdk/Contents/Home</span><br></pre></td></tr></table></figure><h4 id="hbase-site-xml"><a href="#hbase-site-xml" class="headerlink" title="hbase-site.xml"></a>hbase-site.xml</h4><p>在<font color=red>conf/hbase-site.xml</font>设置HBase的核心配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ vim hbase-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">    // 这里设置让HBase存储文件的地方</span><br><span class="line">    &lt;value&gt;file:///usr/<span class="built_in">local</span>/Cellar/hbase/tmp/hbase&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</span><br><span class="line">      // 这里设置让HBase存储内建zookeeper文件的地方</span><br><span class="line">      &lt;value&gt;/usr/<span class="built_in">local</span>/Cellar/hbase/tmp/zookeeper&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="启动HBase"><a href="#启动HBase" class="headerlink" title="启动HBase"></a>启动HBase</h4><p><font color=red>/usr/local/Cellar/hbase/1.3.5/bin/start-hbase.sh</font>提供HBase的启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ./start-hbase.sh</span><br><span class="line">starting master, logging to /usr/<span class="built_in">local</span>/var/<span class="built_in">log</span>/hbase/hbase-xxx-master-xxx.local.out</span><br></pre></td></tr></table></figure><h4 id="验证是否安装成功"><a href="#验证是否安装成功" class="headerlink" title="验证是否安装成功"></a>验证是否安装成功</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ jps</span><br><span class="line">722 Launcher</span><br><span class="line">1142 HMaster</span><br><span class="line">726 Launcher</span><br><span class="line">1256 Jps</span><br></pre></td></tr></table></figure><h4 id="启动HBase-Shell"><a href="#启动HBase-Shell" class="headerlink" title="启动HBase Shell"></a>启动HBase Shell</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ ./hbase shell</span><br><span class="line">2019-10-19 11:58:34,879 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">HBase Shell; enter <span class="string">&#x27;help&lt;RETURN&gt;&#x27;</span> <span class="keyword">for</span> list of supported commands.</span><br><span class="line">Type <span class="string">&quot;exit&lt;RETURN&gt;&quot;</span> to leave the HBase Shell</span><br><span class="line">Version 1.3.5, rb59afe7b1dc650ff3a86034477b563734e8799a9, Wed Jun  5 15:57:14 PDT 2019</span><br><span class="line"></span><br><span class="line">hbase(main):001:0&gt;</span><br></pre></td></tr></table></figure><h4 id="停止HBase运行"><a href="#停止HBase运行" class="headerlink" title="停止HBase运行"></a>停止HBase运行</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ./stop-hbase.sh</span><br><span class="line">stopping hbase...............</span><br></pre></td></tr></table></figure><h3 id="伪分布式模式"><a href="#伪分布式模式" class="headerlink" title="伪分布式模式"></a><strong>伪分布式模式</strong></h3><p>必须先关闭HBase</p><h4 id="修改hbase-env-sh"><a href="#修改hbase-env-sh" class="headerlink" title="修改hbase-env.sh"></a>修改hbase-env.sh</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HBASE_MANAGE_ZK = <span class="literal">true</span></span><br></pre></td></tr></table></figure><h4 id="修改hbase-site-xml"><a href="#修改hbase-site-xml" class="headerlink" title="修改hbase-site.xml"></a>修改hbase-site.xml</h4><p>设置HBase使用分布式模式运行</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    // Here you have to set the path where you want HBase to store its files.</span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><font color=red><strong>hbase.rootdir路径一定要跟hadoop中core-site.xml中fs.default.name相同</strong></font></p><p>change the hbase.rootdir from the local filesystem to the address of your HDFS instance —offical quick start</p><p>如何两处设置不同会引起ERROR: <font color=red>Can’t get master address from ZooKeeper; znode data == null错误错误</font></p><p>在启动HBase之前, 请先启动Hadoop, 使之运行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ ./start-hbase.sh</span><br><span class="line">localhost: starting zookeeper, logging to /usr/<span class="built_in">local</span>/var/<span class="built_in">log</span>/hbase/hbase-xxx-zookeeper-xxx.local.out</span><br><span class="line">starting master, logging to /usr/<span class="built_in">local</span>/var/<span class="built_in">log</span>/hbase/hbase-xxx-master-xxx.local.out</span><br><span class="line">starting regionserver, logging to /usr/<span class="built_in">local</span>/var/<span class="built_in">log</span>/hbase/hbase-xxx-1-regionserver-xxx.local.out</span><br><span class="line"></span><br><span class="line">$ jps  <span class="comment">#验证是否启动成功, 包含HMaster和HRegionServer说明启动成功</span></span><br><span class="line">5614 HRegionServer</span><br><span class="line">2222 NameNode</span><br><span class="line">722 Launcher</span><br><span class="line">2323 DataNode</span><br><span class="line">5461 HMaster</span><br><span class="line">726 Launcher</span><br><span class="line">2650 ResourceManager</span><br><span class="line">2747 NodeManager</span><br><span class="line">2459 SecondaryNameNode</span><br><span class="line">5405 HQuorumPeer</span><br><span class="line">285</span><br><span class="line">5726 Jps</span><br></pre></td></tr></table></figure><p>查看hdfs中文件夹</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ./hdfs dfs -ls /</span><br><span class="line">2019-10-19 12:39:03,895 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">Found 3 items</span><br><span class="line">drwxr-xr-x   - xxx supergroup          0 2019-10-19 12:38 /hbase</span><br><span class="line">drwxrwxr-x   - xxx supergroup          0 2019-10-17 14:41 /tmp</span><br><span class="line">drwxr-xr-x   - xxx supergroup          0 2019-10-17 11:44 /user</span><br></pre></td></tr></table></figure><h3 id="HBase-Shell"><a href="#HBase-Shell" class="headerlink" title="HBase Shell"></a><strong>HBase Shell</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$ hbase shell  <span class="comment">#启动HBase Shell</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建表</span></span><br><span class="line">&gt; create <span class="string">&#x27;student&#x27;</span>, <span class="string">&#x27;description&#x27;</span>, <span class="string">&#x27;course&#x27;</span>  <span class="comment">#创建表名为student的表, 指明两个列名, 分别为description和course</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 信息明细</span></span><br><span class="line">&gt; list <span class="string">&#x27;student&#x27;</span>  <span class="comment">#列出list表信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 插入数据</span></span><br><span class="line"><span class="comment"># 意思为在student表row1处插入description:age的数据为18</span></span><br><span class="line"><span class="comment"># rowKey为row1，columnFamilyName为description，columnName为age</span></span><br><span class="line">&gt; put <span class="string">&#x27;student&#x27;</span>, <span class="string">&#x27;row1&#x27;</span>, <span class="string">&#x27;description:age&#x27;</span>, <span class="string">&#x27;18&#x27;</span></span><br><span class="line">&gt; put <span class="string">&#x27;student&#x27;</span>, <span class="string">&#x27;row1&#x27;</span>, <span class="string">&#x27;description:name&#x27;</span>, <span class="string">&#x27;liu&#x27;</span></span><br><span class="line">&gt; put <span class="string">&#x27;student&#x27;</span>, <span class="string">&#x27;row1&#x27;</span>, <span class="string">&#x27;course:chinese&#x27;</span>, <span class="string">&#x27;100&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一次扫描所有数据</span></span><br><span class="line">&gt; scan <span class="string">&#x27;student&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使表失效 / 有效</span></span><br><span class="line">&gt; <span class="built_in">disable</span> <span class="string">&#x27;student&#x27;</span></span><br><span class="line">&gt; <span class="built_in">enable</span> <span class="string">&#x27;student&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除表(要先disable)</span></span><br><span class="line">&gt;  drop <span class="string">&#x27;student&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 退出shell</span></span><br><span class="line">&gt; quit</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache HBase" scheme="https://yangyichao-mango.github.io/categories/Apache-HBase/"/>
    
    
      <category term="Apache HBase" scheme="https://yangyichao-mango.github.io/tags/Apache-HBase/"/>
    
      <category term="Mac安装" scheme="https://yangyichao-mango.github.io/tags/Mac%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>IntelliJ IDEA 中如何查看一个类的所有继承关系</title>
    <link href="https://yangyichao-mango.github.io/2019/10/18/idea-mac:show-class-hierarchy/"/>
    <id>https://yangyichao-mango.github.io/2019/10/18/idea-mac:show-class-hierarchy/</id>
    <published>2019-10-18T06:09:17.000Z</published>
    <updated>2019-10-25T08:40:51.878Z</updated>
    
    <content type="html"><![CDATA[<p>IntelliJ IDEA 中如何查看一个类的所有继承关系，包括父类与子类</p><span id="more"></span><h3 id="查看方式"><a href="#查看方式" class="headerlink" title="查看方式"></a><strong>查看方式</strong></h3><p>IntelliJ IDEA 中最上端的Navigate，下拉选择Type Hierarchy，就会出现层级关系列表</p><h4 id="关于该类的父类和子类继承关系"><a href="#关于该类的父类和子类继承关系" class="headerlink" title="关于该类的父类和子类继承关系"></a>关于该类的父类和子类继承关系</h4><p><img src="/blog-img/idea-mac:show-class-hierarchy/%E7%88%B6%E7%B1%BB%E5%92%8C%E5%AD%90%E7%B1%BB%E7%BB%A7%E6%89%BF%E5%85%B3%E7%B3%BB.png" alt="父类和子类继承关系"></p><h4 id="关于该类的父类继承关系"><a href="#关于该类的父类继承关系" class="headerlink" title="关于该类的父类继承关系"></a>关于该类的父类继承关系</h4><p><img src="/blog-img/idea-mac:show-class-hierarchy/%E7%88%B6%E7%B1%BB%E7%BB%A7%E6%89%BF%E5%85%B3%E7%B3%BB.png" alt="父类继承关系"></p><h4 id="关于该类的子类继承关系"><a href="#关于该类的子类继承关系" class="headerlink" title="关于该类的子类继承关系"></a>关于该类的子类继承关系</h4><p><img src="/blog-img/idea-mac:show-class-hierarchy/%E5%AD%90%E7%B1%BB%E7%BB%A7%E6%89%BF%E5%85%B3%E7%B3%BB.png" alt="子类继承关系"></p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="IntelliJ IDEA" scheme="https://yangyichao-mango.github.io/categories/IntelliJ-IDEA/"/>
    
    
      <category term="IntelliJ IDEA" scheme="https://yangyichao-mango.github.io/tags/IntelliJ-IDEA/"/>
    
  </entry>
  
  <entry>
    <title>Apache Hive环境搭建错误：java.lang.IllegalArgumentException: java.net.URISyntaxException:...</title>
    <link href="https://yangyichao-mango.github.io/2019/10/17/apache-hive:error-URISyntaxException:Relative-path-in-absolute-URI:%7Bsystem:java.io.tmpdir%7D%7Bsystem-user-name%7D/"/>
    <id>https://yangyichao-mango.github.io/2019/10/17/apache-hive:error-URISyntaxException:Relative-path-in-absolute-URI:%7Bsystem:java.io.tmpdir%7D%7Bsystem-user-name%7D/</id>
    <published>2019-10-17T07:44:01.000Z</published>
    <updated>2019-10-25T03:40:04.210Z</updated>
    
    <content type="html"><![CDATA[<p>出现错误：Exception in thread “main” java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D</p><span id="more"></span><h3 id="错误场景详情"><a href="#错误场景详情" class="headerlink" title="错误场景详情"></a><strong>错误场景详情</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HIVE_HOME</span>/bin/hive</span><br><span class="line">Hive Session ID = 41e2ad09-81b3-4700-9b87-f42b25a29731</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration <span class="keyword">in</span> jar:file:/usr/<span class="built_in">local</span>/Cellar/hive/3.1.2/libexec/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: <span class="literal">true</span></span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">&quot;main&quot;</span> java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path <span class="keyword">in</span> absolute URI: <span class="variable">$&#123;system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D</span></span><br><span class="line"><span class="variable">at org.apache.hadoop.fs.Path.initialize(Path.java:263)</span></span><br><span class="line"><span class="variable">at org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:221)</span></span><br><span class="line"><span class="variable">at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:710)</span></span><br><span class="line"><span class="variable">at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:627)</span></span><br><span class="line"><span class="variable">at org.apache.hadoop.hive.ql.session.SessionState.beginStart(SessionState.java:591)</span></span><br><span class="line"><span class="variable">at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:747)</span></span><br><span class="line"><span class="variable">at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)</span></span><br><span class="line"><span class="variable">at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span></span><br><span class="line"><span class="variable">at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span></span><br><span class="line"><span class="variable">at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span></span><br><span class="line"><span class="variable">at java.lang.reflect.Method.invoke(Method.java:498)</span></span><br><span class="line"><span class="variable">at org.apache.hadoop.util.RunJar.run(RunJar.java:323)</span></span><br><span class="line"><span class="variable">at org.apache.hadoop.util.RunJar.main(RunJar.java:236)</span></span><br><span class="line"><span class="variable">Caused by: java.net.URISyntaxException: Relative path in absolute URI: <span class="variable">$&#123;system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D</span></span></span><br><span class="line"><span class="variable"><span class="variable">at java.net.URI.checkPath(URI.java:1823)</span></span></span><br><span class="line"><span class="variable"><span class="variable">at java.net.URI.&lt;init&gt;(URI.java:745)</span></span></span><br><span class="line"><span class="variable"><span class="variable">at org.apache.hadoop.fs.Path.initialize(Path.java:260)</span></span></span><br><span class="line"><span class="variable"><span class="variable">... 12 more</span></span></span><br></pre></td></tr></table></figure><h3 id="错误原因"><a href="#错误原因" class="headerlink" title="错误原因"></a><strong>错误原因</strong></h3><p>hive-site.xml里的临时目录没有设置好，一共有三个</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>Hive.exec.local.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;system:Java.io.tmpdir&#125;/$&#123;system:user.name&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Local scratch space for Hive jobs<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.downloaded.resources.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;system:java.io.tmpdir&#125;/$&#123;hive.session.id&#125;_resources<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Temporary local directory for added resources in the remote file system.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.logging.operation.log.location<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;system:Java.io.tmpdir&#125;/$&#123;system:user.name&#125;/operation_logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Top level directory where operation logs are stored if logging functionality is enabled<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a><strong>解决方案</strong></h3><p>将hive-site.xml文件中的${system:java.io.tmpdir}替换为hive的临时目录<br>例如我替换为<font color=red>/usr/local/Cellar/hive/tmp</font>，该目录如果不存在则要自己手工创建，并且赋予读写权限</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>Hive.exec.local.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/Cellar/hive/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Local scratch space for Hive jobs<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.downloaded.resources.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/Cellar/hive/tmp/$&#123;hive.session.id&#125;_resources<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Temporary local directory for added resources in the remote file system.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.logging.operation.log.location<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/Cellar/hive/tmp/root/operation_logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Top level directory where operation logs are stored if logging functionality is enabled<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/categories/Apache-Hive/"/>
    
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/tags/Apache-Hive/"/>
    
  </entry>
  
  <entry>
    <title>Mac安装Apache Hive-3.2.1</title>
    <link href="https://yangyichao-mango.github.io/2019/10/17/apache-hive:3.1.2-mac-install/"/>
    <id>https://yangyichao-mango.github.io/2019/10/17/apache-hive:3.1.2-mac-install/</id>
    <published>2019-10-17T06:58:34.000Z</published>
    <updated>2019-10-25T03:40:04.207Z</updated>
    
    <content type="html"><![CDATA[<p>Mac安装Apache Hive-3.2.1教程</p><span id="more"></span><h2 id="安装Hadoop"><a href="#安装Hadoop" class="headerlink" title="安装Hadoop"></a><strong>安装Hadoop</strong></h2><p>下载包进行安装，则hadoop需要独立安装</p><h2 id="安装Hive"><a href="#安装Hive" class="headerlink" title="安装Hive"></a><strong>安装Hive</strong></h2><h3 id="brew安装"><a href="#brew安装" class="headerlink" title="brew安装"></a>brew安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew install hive</span><br></pre></td></tr></table></figure><p>此命令会把hive依赖的hadoop安装，所以就不需要单独进行安装hadoop<br>该命令默认安装的版本较新，我的hive是3.1.2，hadoop是3.2.1，安装位置：<font color=red>/usr/local/Cellar/hive/</font></p><h3 id="环境变量修改"><a href="#环境变量修改" class="headerlink" title="环境变量修改"></a>环境变量修改</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ vim ~/.bash_profile</span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/usr/<span class="built_in">local</span>/Cellar/hive/3.1.2</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="string">&quot;<span class="variable">$HIVE_HOME</span>/bin:<span class="variable">$PATH</span>&quot;</span></span><br><span class="line"></span><br><span class="line">$ <span class="built_in">source</span> ~/.bash_profile</span><br></pre></td></tr></table></figure><h3 id="使用mysql作为hive元数据存储"><a href="#使用mysql作为hive元数据存储" class="headerlink" title="使用mysql作为hive元数据存储"></a>使用mysql作为hive元数据存储</h3><p>在mysql中为hive 创建用户，及初始化数据库<br>以下在mysql 中操作，注意：这里创建的用户名是 hadoop， 密码 mysql<br>第一行：创建数据库<br>第二、三行 创建用户，赋予权限<br>第四行 权限生效</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create database hive;</span><br><span class="line">CREATE USER  <span class="string">&#x27;hadoop&#x27;</span>@<span class="string">&#x27;%&#x27;</span>  IDENTIFIED BY <span class="string">&#x27;mysql&#x27;</span>;</span><br><span class="line">GRANT ALL PRIVILEGES ON  *.* TO <span class="string">&#x27;hadoop&#x27;</span>@<span class="string">&#x27;%&#x27;</span> WITH GRANT OPTION;</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure><p>查看权限是否已经存储</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM mysql.user;</span><br></pre></td></tr></table></figure><h3 id="修改配置文件hive-site-xml"><a href="#修改配置文件hive-site-xml" class="headerlink" title="修改配置文件hive-site.xml"></a>修改配置文件hive-site.xml</h3><p>修改hive配置文件，我的配置文件位置在 <font color=red>/usr/local/Cellar/hive/3.1.2/libexec/conf</font><br>如果不存在hive-site.xml文件，则使用下面这个命令创建一个默认的hive-site.xml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cp hive-default.xml.template hive-site.xml</span><br></pre></td></tr></table></figure><p>修改配置文件</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span>mysql</span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://localhost:3306/hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>javax.jdo.option.ConnectionUserName – 连接mysql的账号，hadoop<br>javax.jdo.option.ConnectionPassword – 连接mysql的密码，mysql<br>javax.jdo.option.ConnectionURL – 对应上一步创建的数据库，localhost:3306/hive</p><h3 id="hadoop中创建hive所需仓库"><a href="#hadoop中创建hive所需仓库" class="headerlink" title="hadoop中创建hive所需仓库"></a>hadoop中创建hive所需仓库</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -mkdir       /tmp</span><br><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -mkdir   -p  /user/hive/warehouse</span><br><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -chmod g+w   /tmp</span><br><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -chmod g+w   /user/hive/warehouse</span><br></pre></td></tr></table></figure><p>$HADOOP_HOME – 代表您的hadoop工作目录</p><h3 id="hive初始化mysql中的数据库hive"><a href="#hive初始化mysql中的数据库hive" class="headerlink" title="hive初始化mysql中的数据库hive"></a>hive初始化mysql中的数据库hive</h3><h4 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HIVE_HOME</span>/bin/schematool -dbType msyql -initSchema</span><br></pre></td></tr></table></figure><h4 id="可能出现错误1：java-lang-NoSuchMethodError-com-google-common…"><a href="#可能出现错误1：java-lang-NoSuchMethodError-com-google-common…" class="headerlink" title="可能出现错误1：java.lang.NoSuchMethodError: com.google.common…"></a>可能出现错误1：java.lang.NoSuchMethodError: com.google.common…</h4><p>解决方案：<a href="https://yangyichao-mango.github.io/2019/10/17/apache-hive-error-NoSuchMethodError:com.google.common.base.Preconditions.checkArgument/">java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument</a></p><h4 id="可能出现错误2：java-lang-ClassNotFoundException-com-mysql…"><a href="#可能出现错误2：java-lang-ClassNotFoundException-com-mysql…" class="headerlink" title="可能出现错误2：java.lang.ClassNotFoundException: com.mysql…"></a>可能出现错误2：java.lang.ClassNotFoundException: com.mysql…</h4><p>解决方案：<a href="https://yangyichao-mango.github.io/2019/10/17/apache-hive-error-ClassNotFoundException:com.mysql.jdbc.driver/">java.lang.ClassNotFoundException: com.mysql.jdbc.Driver</a></p><h3 id="启动Hive的Metastore-Server服务进程"><a href="#启动Hive的Metastore-Server服务进程" class="headerlink" title="启动Hive的Metastore Server服务进程"></a>启动Hive的Metastore Server服务进程</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HIVE_HOME</span>/bin/hive --service metastore &amp;</span><br></pre></td></tr></table></figure><h3 id="登录hive客户端"><a href="#登录hive客户端" class="headerlink" title="登录hive客户端"></a>登录hive客户端</h3><h4 id="命令-1"><a href="#命令-1" class="headerlink" title="命令"></a>命令</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HIVE_HOME</span>/bin/ hive </span><br></pre></td></tr></table></figure><h4 id="可能出现的错误1：java-lang-IllegalArgumentException-java-net-URISyntaxException-…"><a href="#可能出现的错误1：java-lang-IllegalArgumentException-java-net-URISyntaxException-…" class="headerlink" title="可能出现的错误1：java.lang.IllegalArgumentException: java.net.URISyntaxException:…"></a>可能出现的错误1：java.lang.IllegalArgumentException: java.net.URISyntaxException:…</h4><p>解决方案：<a href="https://yangyichao-mango.github.io/2019/10/17/apache-hive-error-URISyntaxException:Relative-path-in-absolute-URI:%7Bsystem:java.io.tmpdir%7D%7Bsystem-user-name%7D/">Exception in thread “main” java.lang.IllegalArgumentException: java.net.URISyntaxException:</a></p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/categories/Apache-Hive/"/>
    
    
      <category term="Mac安装" scheme="https://yangyichao-mango.github.io/tags/Mac%E5%AE%89%E8%A3%85/"/>
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/tags/Apache-Hive/"/>
    
  </entry>
  
  <entry>
    <title>Apache Hive环境搭建错误：com.mysql.jdbc.Driver was not found in the CLASSPATH</title>
    <link href="https://yangyichao-mango.github.io/2019/10/17/apache-hive:error-ClassNotFoundException:com.mysql.jdbc.driver/"/>
    <id>https://yangyichao-mango.github.io/2019/10/17/apache-hive:error-ClassNotFoundException:com.mysql.jdbc.driver/</id>
    <published>2019-10-17T06:35:11.000Z</published>
    <updated>2019-10-19T04:52:31.852Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Hive环境搭建错误：com.mysql.jdbc.Driver was not found in the CLASSPATH</p><span id="more"></span><h3 id="错误场景详情"><a href="#错误场景详情" class="headerlink" title="错误场景详情"></a><strong>错误场景详情</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HIVE_HOME</span>/bin/schematool -dbType mysql -initSchema</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/usr/<span class="built_in">local</span>/Cellar/hive/3.1.2/libexec/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/usr/<span class="built_in">local</span>/Cellar/hadoop/3.2.1/libexec/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html<span class="comment">#multiple_bindings for an explanation.</span></span><br><span class="line">SLF4J: Actual binding is of <span class="built_in">type</span> [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Metastore connection URL: jdbc:mysql://localhost:3306/hive?characterEncoding=UTF-8</span><br><span class="line">Metastore Connection Driver : com.mysql.jdbc.Driver</span><br><span class="line">Metastore connection User: hadoop</span><br><span class="line">org.apache.hadoop.hive.metastore.HiveMetaException: Failed to load driver</span><br><span class="line">Underlying cause: java.lang.ClassNotFoundException : com.mysql.jdbc.Driver</span><br><span class="line">Use --verbose <span class="keyword">for</span> detailed stacktrace.</span><br><span class="line">*** schemaTool failed ***</span><br></pre></td></tr></table></figure><h3 id="错误原因"><a href="#错误原因" class="headerlink" title="错误原因"></a><strong>错误原因</strong></h3><p>在配置hive-site.xml文件时配置了mysql驱动，而hive/lib目录下没有mysql驱动包。</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a><strong>解决方案</strong></h3><p>官网下载mysql驱动下载地址 (<a href="https://dev.mysql.com/downloads/connector/j/">https://dev.mysql.com/downloads/connector/j/</a>)<br>把下载好的压缩包（mysql-connector-java-8.0.18.zip）进行解压<br>unzip mysql-connector-java-8.0.18.zip<br>复制到hive/lib下<br>cp mysql-connector-java-8.0.18/mysql-connector-java-8.0.18.jar hive/lib</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/categories/Apache-Hive/"/>
    
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/tags/Apache-Hive/"/>
    
  </entry>
  
  <entry>
    <title>Apache Hive环境搭建错误：java.lang.NoSuchMethodError: com.google.common...</title>
    <link href="https://yangyichao-mango.github.io/2019/10/17/apache-hive:error-NoSuchMethodError:com.google.common.base.Preconditions.checkArgument/"/>
    <id>https://yangyichao-mango.github.io/2019/10/17/apache-hive:error-NoSuchMethodError:com.google.common.base.Preconditions.checkArgument/</id>
    <published>2019-10-17T06:14:27.000Z</published>
    <updated>2019-10-19T04:52:31.858Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Hive环境搭建错误：java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument</p><span id="more"></span><h3 id="错误场景详情"><a href="#错误场景详情" class="headerlink" title="错误场景详情"></a><strong>错误场景详情</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HIVE_HOME</span>/bin/schematool -dbType mysql -initSchema</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/usr/<span class="built_in">local</span>/Cellar/hive/3.1.2/libexec/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/usr/<span class="built_in">local</span>/Cellar/hadoop/3.2.1/libexec/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html<span class="comment">#multiple_bindings for an explanation.</span></span><br><span class="line">SLF4J: Actual binding is of <span class="built_in">type</span> [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">&quot;main&quot;</span> java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V</span><br><span class="line">at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)</span><br><span class="line">at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)</span><br><span class="line">at org.apache.hadoop.mapred.JobConf.setJar(JobConf.java:536)</span><br><span class="line">at org.apache.hadoop.mapred.JobConf.setJarByClass(JobConf.java:554)</span><br><span class="line">at org.apache.hadoop.mapred.JobConf.&lt;init&gt;(JobConf.java:448)</span><br><span class="line">at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:5141)</span><br><span class="line">at org.apache.hadoop.hive.conf.HiveConf.&lt;init&gt;(HiveConf.java:5104)</span><br><span class="line">at org.apache.hive.beeline.HiveSchemaTool.&lt;init&gt;(HiveSchemaTool.java:96)</span><br><span class="line">at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:1473)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class="line">at org.apache.hadoop.util.RunJar.run(RunJar.java:323)</span><br><span class="line">at org.apache.hadoop.util.RunJar.main(RunJar.java:236)</span><br></pre></td></tr></table></figure><h3 id="错误原因"><a href="#错误原因" class="headerlink" title="错误原因"></a><strong>错误原因</strong></h3><p>这是因为hive内依赖的guava.jar和hadoop内的版本不一致造成的。</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a><strong>解决方案</strong></h3><p>查看hadoop安装目录下share/hadoop/common/lib内guava.jar版本<br>查看hive安装目录下lib内guava.jar的版本，如果两者不一致，删除版本低的，并拷贝高版本的，问题解决！</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/categories/Apache-Hive/"/>
    
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/tags/Apache-Hive/"/>
    
  </entry>
  
  <entry>
    <title>Apache Hadoop错误：Unable to load native-hadoop library for your platform</title>
    <link href="https://yangyichao-mango.github.io/2019/10/17/apache-hadoop:error-unable-to-load-native-hadoop-library-from-you-platform/"/>
    <id>https://yangyichao-mango.github.io/2019/10/17/apache-hadoop:error-unable-to-load-native-hadoop-library-from-you-platform/</id>
    <published>2019-10-17T03:34:57.000Z</published>
    <updated>2019-10-25T03:40:04.184Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Hadoop错误：Unable to load native-hadoop library for your platform</p><span id="more"></span><h3 id="错误场景详情"><a href="#错误场景详情" class="headerlink" title="错误场景详情"></a><strong>错误场景详情</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -ls /</span><br><span class="line">WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br></pre></td></tr></table></figure><h3 id="错误原因"><a href="#错误原因" class="headerlink" title="错误原因"></a><strong>错误原因</strong></h3><p>Hadoop是使用Java语言开发的,但是有一些需求和操作并不适合使用java所以会引入了本地库（Native Libraries）的概念，通过本地库，Hadoop可以更加高效地执行某一些操作.<br>当我们在linux 输入 hdoop fs -ls / 去查看 hdfs 文件系统上的资源时会出现下面错误</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a><strong>解决方案</strong></h3><h4 id="解决方案1"><a href="#解决方案1" class="headerlink" title="解决方案1"></a>解决方案1</h4><p>在Hadoop的配置文件core-site.xml中可以设置是否使用本地库：（Hadoop默认的配置为启用本地库）</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.native.lib<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Should native hadoop libraries, if present, be used.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="解决方案2"><a href="#解决方案2" class="headerlink" title="解决方案2"></a>解决方案2</h4><p>有博客说可以直接下载编译好的位包，替换原来的native包<br>由于在我本地安装的Apache Hadoop 3.2.1版本中没有找到lib文件夹，所以在3.2.1版本中暂时不能使用此种方法</p><p>执行查看文件命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -ls /</span><br><span class="line">2019-10-17 11:33:09,369 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - xxx supergroup          0 2019-10-17 11:23 /tmp</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/categories/Apache-Hadoop/"/>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/tags/Apache-Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Mac安装Apache Hadoop-3.2.1</title>
    <link href="https://yangyichao-mango.github.io/2019/10/17/apache-hadoop:3.2.1-mac-install/"/>
    <id>https://yangyichao-mango.github.io/2019/10/17/apache-hadoop:3.2.1-mac-install/</id>
    <published>2019-10-17T02:12:31.000Z</published>
    <updated>2019-10-25T08:40:28.142Z</updated>
    
    <content type="html"><![CDATA[<p>Mac安装Apache hadoop-3.2.1教程</p><span id="more"></span><h2 id="Java环境配置"><a href="#Java环境配置" class="headerlink" title="Java环境配置"></a><strong>Java环境配置</strong></h2><h3 id="安装Java，查看Java版本以测试是否安装成功"><a href="#安装Java，查看Java版本以测试是否安装成功" class="headerlink" title="安装Java，查看Java版本以测试是否安装成功"></a>安装Java，查看Java版本以测试是否安装成功</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ java -version</span><br><span class="line">java version <span class="string">&quot;1.8.0_191&quot;</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_191-b12)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode)</span><br></pre></td></tr></table></figure><h3 id="查看Java安装位置信息，之后配置Hadoop运行环境需要使用"><a href="#查看Java安装位置信息，之后配置Hadoop运行环境需要使用" class="headerlink" title="查看Java安装位置信息，之后配置Hadoop运行环境需要使用"></a>查看Java安装位置信息，之后配置Hadoop运行环境需要使用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/libexec/java_home</span><br><span class="line">/Library/Java/JavaVirtualMachines/jdk1.8.0_191.jdk/Contents/Home</span><br></pre></td></tr></table></figure><h2 id="ssh配置"><a href="#ssh配置" class="headerlink" title="ssh配置"></a><strong>ssh配置</strong></h2><h3 id="配置ssh"><a href="#配置ssh" class="headerlink" title="配置ssh"></a>配置ssh</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line">$ chmod 0600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><h3 id="创建ssh公钥"><a href="#创建ssh公钥" class="headerlink" title="创建ssh公钥"></a>创建ssh公钥</h3><p>如果没有ssh公钥，执行以下命令创建</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa</span><br></pre></td></tr></table></figure><h3 id="开启远程登录"><a href="#开启远程登录" class="headerlink" title="开启远程登录"></a>开启远程登录</h3><p><img src="/blog-img/apache-hadoop:3.2.1-mac-install/remote-login.png" alt="系统偏好设置-&gt;共享"></p><h3 id="测试远程登录是否开启"><a href="#测试远程登录是否开启" class="headerlink" title="测试远程登录是否开启"></a>测试远程登录是否开启</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh localhost</span><br></pre></td></tr></table></figure><h2 id="安装hadoop"><a href="#安装hadoop" class="headerlink" title="安装hadoop"></a><strong>安装hadoop</strong></h2><h3 id="brew安装hadoop"><a href="#brew安装hadoop" class="headerlink" title="brew安装hadoop"></a>brew安装hadoop</h3><p>brew安装的一般都是最新的hadoop，我这里是hadoop 3.2.1 <br>如果需要安装其他版本的hadoop，通过<a href="https://www.jianshu.com/p/aadb54eac0a8">brew安装指定版本的软件</a>进行安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ brew install hadoop</span><br><span class="line">Updating Homebrew...</span><br><span class="line">==&gt; Downloading https://www.apache.org/dyn/closer.cgi?path=hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz</span><br><span class="line">==&gt; Downloading from http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz</span><br><span class="line"><span class="comment">######################################################################## 100.0%</span></span><br><span class="line">🍺  /usr/<span class="built_in">local</span>/Cellar/hadoop/3.2.1: 21,686 files, 774.1MB, built <span class="keyword">in</span> 10 minutes 1 second</span><br></pre></td></tr></table></figure><p>注意上面的下载信息中 <br>默认brew是会从apache官方的镜像中下载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">==&gt; Downloading https://www.apache.org/dyn/closer.cgi?path=hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz</span><br></pre></td></tr></table></figure><p>如果下载很慢，可以配置国内镜像进行下载(<a href="https://mirror.tuna.tsinghua.edu.cn/help/homebrew/">清华大学开源软件镜像站</a>)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">==&gt; Downloading from http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz</span><br></pre></td></tr></table></figure><p>安装完之后查看hadoop安装位置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ brew info hadoop</span><br><span class="line">hadoop: stable 3.2.1</span><br><span class="line">Framework <span class="keyword">for</span> distributed processing of large data sets</span><br><span class="line">https://hadoop.apache.org/</span><br><span class="line">Conflicts with:</span><br><span class="line">  yarn (because both install `yarn` binaries)</span><br><span class="line">/usr/<span class="built_in">local</span>/Cellar/hadoop/hdfs (20 files, 1MB)</span><br><span class="line">  Built from <span class="built_in">source</span></span><br><span class="line">/usr/<span class="built_in">local</span>/Cellar/hadoop/3.2.1 (22,408 files, 815.8MB)</span><br><span class="line">  Built from <span class="built_in">source</span> on 2019-10-17 at 09:46:37</span><br><span class="line">From: https://github.com/Homebrew/homebrew-core/blob/master/Formula/hadoop.rb</span><br><span class="line">==&gt; Requirements</span><br><span class="line">Required: java &gt;= 1.8 ✔</span><br><span class="line">==&gt; Analytics</span><br><span class="line">install: 4,572 (30 days), 10,774 (90 days), 44,762 (365 days)</span><br><span class="line">install_on_request: 3,822 (30 days), 9,128 (90 days), 38,206 (365 days)</span><br><span class="line">build_error: 0 (30 days)</span><br></pre></td></tr></table></figure><h3 id="配置hadoop"><a href="#配置hadoop" class="headerlink" title="配置hadoop"></a>配置hadoop</h3><p>需要修改的配置文件都在<font color=red>/usr/local/Cellar/hadoop/3.2.1/libexec/etc/hadoop</font>这个目录下</p><h4 id="hadoop-env-sh"><a href="#hadoop-env-sh" class="headerlink" title="hadoop-env.sh"></a>hadoop-env.sh</h4><p>配置 export JAVA_HOME</p><p>将/usr/libexec/java_home查到的 Java 路径配置进去，记得去掉注释 #。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_191.jdk/Contents/Home</span><br></pre></td></tr></table></figure><h4 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h4><p>修改core-site.xml 文件参数,配置NameNode的主机名和端口号</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/Cellar/hadoop/hdfs/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>A base for other temporary directories<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h4><p>变量dfs.replication指定了每个HDFS数据库的复制次数。 通常为3, 由于我们只有一台主机和一个伪分布式模式的DataNode，将此值修改为1</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="格式化"><a href="#格式化" class="headerlink" title="格式化"></a>格式化</h4><p>格式化hdfs操作只要第一次才使用，否则会造成数据全部丢失</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hdfs namenode -format</span><br></pre></td></tr></table></figure><h3 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h3><p>启动服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./start-all.sh</span><br></pre></td></tr></table></figure><p>启动成功后，可以在<br><a href="http://localhost:9870/">http://localhost:9870/</a><br><a href="http://localhost:8088/cluster">http://localhost:8088/cluster</a><br>进行查看</p><p>关闭服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./stop-all.sh</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/categories/Apache-Hadoop/"/>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/tags/Apache-Hadoop/"/>
    
      <category term="Mac安装" scheme="https://yangyichao-mango.github.io/tags/Mac%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>useful-api-for-java</title>
    <link href="https://yangyichao-mango.github.io/2019/10/16/java-api:useful-api/"/>
    <id>https://yangyichao-mango.github.io/2019/10/16/java-api:useful-api/</id>
    <published>2019-10-16T02:16:10.000Z</published>
    <updated>2019-10-19T04:52:48.324Z</updated>
    
    <content type="html"><![CDATA[<h2 id="API"><a href="#API" class="headerlink" title="API"></a><strong>API</strong></h2><ul><li><a href="https://www.jianshu.com/p/865e9ae667a0">Retrofit（http请求工具包）</a></li><li><a href="https://www.jianshu.com/p/4271ebc40be8">Resilience4j（接口重试，限流，熔断器工具）</a></li></ul>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Java Api" scheme="https://yangyichao-mango.github.io/categories/Java-Api/"/>
    
    
      <category term="Java Api" scheme="https://yangyichao-mango.github.io/tags/Java-Api/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 零基础入门（四）：DataStream API 编程 学习心得</title>
    <link href="https://yangyichao-mango.github.io/2019/10/15/apache-flink:study-4-datastream-api/"/>
    <id>https://yangyichao-mango.github.io/2019/10/15/apache-flink:study-4-datastream-api/</id>
    <published>2019-10-15T07:57:16.000Z</published>
    <updated>2019-11-06T13:07:48.987Z</updated>
    
    <content type="html"><![CDATA[<p>学习心得</p><span id="more"></span><h2 id="DataStream"><a href="#DataStream" class="headerlink" title="DataStream"></a><strong>DataStream</strong></h2><h3 id="RichParallelSourceFunction"><a href="#RichParallelSourceFunction" class="headerlink" title="RichParallelSourceFunction"></a>RichParallelSourceFunction</h3><p>用户通过实现SourceFunction自定义DataSource</p><p>如果设置了并行度，则会产生指定并行度个数的DataSource消费客户端去消费DataSource</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment.setParallelism(int)</span><br></pre></td></tr></table></figure><p>举例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">GroupedProcessingTimeWindow</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = LoggerFactory.getLogger(GroupedProcessingTimeWindow.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">DataSource</span> <span class="keyword">extends</span> <span class="title">RichParallelSourceFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isRunning = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Tuple2&lt;String, Integer&gt;&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            Random random = <span class="keyword">new</span> Random();</span><br><span class="line">            <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">                TimeUnit.MILLISECONDS.sleep((getRuntimeContext().getIndexOfThisSubtask() + <span class="number">1</span>) * <span class="number">1000</span> * <span class="number">5</span>);</span><br><span class="line">                String key = <span class="string">&quot;类别&quot;</span> + (<span class="keyword">char</span>) (<span class="string">&#x27;A&#x27;</span> + random.nextInt(<span class="number">3</span>));</span><br><span class="line">                <span class="keyword">int</span> value = random.nextInt(<span class="number">10</span>) + <span class="number">1</span>;</span><br><span class="line">                LOGGER.info(<span class="string">&quot;Thread: &#123;&#125;, key: &#123;&#125;, value: &#123;&#125;, dataSource object: &#123;&#125;)&quot;</span></span><br><span class="line">                        , Thread.currentThread().getName()</span><br><span class="line">                        , key</span><br><span class="line">                        , value</span><br><span class="line">                        , <span class="keyword">this</span>);</span><br><span class="line">                ctx.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(key, value));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            isRunning = <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        DataSource dataSource = <span class="keyword">new</span> DataSource();</span><br><span class="line">        LOGGER.info(<span class="string">&quot;dataSource object: &#123;&#125;&quot;</span>, dataSource);</span><br><span class="line"></span><br><span class="line">        DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; ds = env.addSource(dataSource);</span><br><span class="line">        KeyedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple&gt; keyedStream = ds.keyBy(<span class="number">0</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// KeyedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple&gt; keyedStream = ds.keyBy(&quot;f0&quot;); 通过指定字段名 f0</span></span><br><span class="line"></span><br><span class="line">        keyedStream</span><br><span class="line">            .sum(<span class="number">1</span>)</span><br><span class="line">            <span class="comment">// .sum(&quot;f1&quot;) 通过制定字段名 f1</span></span><br><span class="line">            .keyBy((KeySelector&lt;Tuple2&lt;String, Integer&gt;, Object&gt;) stringIntegerTuple2 -&gt; StringUtils.EMPTY)</span><br><span class="line">            .fold(<span class="keyword">new</span> HashMap&lt;String, Integer&gt;(),</span><br><span class="line">                    <span class="keyword">new</span> FoldFunction&lt;Tuple2&lt;String, Integer&gt;, HashMap&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> HashMap&lt;String, Integer&gt; <span class="title">fold</span><span class="params">(HashMap&lt;String, Integer&gt; accumulator,</span></span></span><br><span class="line"><span class="function"><span class="params">                                Tuple2&lt;String, Integer&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                            accumulator.put(value.f0, value.f1);</span><br><span class="line">                            <span class="keyword">return</span> accumulator;</span><br><span class="line">                        &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            .addSink(<span class="keyword">new</span> SinkFunction&lt;HashMap&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(HashMap&lt;String, Integer&gt; value, Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    <span class="comment">// 每个类型的商品成交量</span></span><br><span class="line">                    LOGGER.info(<span class="string">&quot;&#123;&#125;&quot;</span></span><br><span class="line">                            , value);</span><br><span class="line">                    <span class="comment">// 商品成交总量</span></span><br><span class="line">                    LOGGER.info(<span class="string">&quot;&#123;&#125;&quot;</span></span><br><span class="line">                            , value.values().stream().mapToInt(v -&gt; v).sum());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过查看dataSource object:的log就会发现上面这个例子中国产生了3个DataSource实例。</p><h3 id="Evictor"><a href="#Evictor" class="headerlink" title="Evictor"></a>Evictor</h3><p>CountEvictor：保持窗口内元素数量符合用户指定数量，如果多于用户指定的数量，从窗口缓冲区的开头丢弃剩余的元素。<br>DeltaEvictor：使用 DeltaFunction和 一个阈值，计算窗口缓冲区中的最后一个元素与其余每个元素之间的 delta 值，并删除 delta 值大于或等于阈值的元素。<br>TimeEvictor：以毫秒为单位的时间间隔作为参数，对于给定的窗口，找到元素中的最大的时间戳max_ts，并删除时间戳小于max_ts - interval的所有元素。</p><h2 id="keyedStream"><a href="#keyedStream" class="headerlink" title="keyedStream"></a><strong>keyedStream</strong></h2><h3 id="KeyedStream-fold-R-initialValue-FoldFunction-lt-T-R-gt-folder"><a href="#KeyedStream-fold-R-initialValue-FoldFunction-lt-T-R-gt-folder" class="headerlink" title="KeyedStream.fold(R initialValue, FoldFunction&lt;T, R&gt; folder)"></a>KeyedStream.fold(R initialValue, FoldFunction&lt;T, R&gt; folder)</h3><p>添加一个合并key分组的算子，FoldFunction会接收到同一key的value，只有key相同的值才会被分发到同一个folder。</p><h2 id="可能出现的问题"><a href="#可能出现的问题" class="headerlink" title="可能出现的问题"></a><strong>可能出现的问题</strong></h2><h3 id="Apache-Flink-Return-type-of-function-could-not-be-determined-automatically-due-to-type-erasure"><a href="#Apache-Flink-Return-type-of-function-could-not-be-determined-automatically-due-to-type-erasure" class="headerlink" title="Apache Flink: Return type of function could not be determined automatically due to type erasure"></a><font color=red>Apache Flink: Return type of function could not be determined automatically due to type erasure</font></h3><p>错误场景：<br>在用户定义DAG图算子的时候，可能会出现不支持lambda表达式的情况</p><p>原因：<br>为了执行程序，Flink需要知道要处理的值的类型，因为它需要序列化和反序列化数据。<br>Flink的类型系统基于描述数据类型的TypeInformation进行序列化和反序列化，会将Java中的基本类型以及Object类型与TypeInformation进行映射。<br>当您指定一个函数时，Flink会尝试推断该函数的返回类型。<br>但是某些Lambda函数由于类型擦除而丢失了此信息（可以自己编译后再对编译成的.class文件进行反编译，然后查看函数签名，发现函数签名具体类型被擦除），<br>因此Flink无法通过此自动推断类型。<br><a href="https://flink.sojb.cn/dev/java_lambdas.html">Flink Java Lambda表达式</a></p><p>因此，必须显式声明返回类型。</p><p>解决方案1：用户自己定义返回类型</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; wordDataStream = dataStream.flatMap(</span><br><span class="line">    (String sentence, Collector&lt;String&gt; out) -&gt; &#123;</span><br><span class="line">        <span class="keyword">for</span>(String word: sentence.split(<span class="string">&quot;\\W+&quot;</span>)) &#123;</span><br><span class="line">            out.collect(word); <span class="comment">// collect objects of type String</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">).returns(Types.STRING);</span><br></pre></td></tr></table></figure><p>解决方案2：显示声明返回类型</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; wordDataStream = dataStream.flatMap(</span><br><span class="line">    <span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String sentence, Collector&lt;String&gt; out)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// normalize and split the line</span></span><br><span class="line">            String[] words = sentence.toLowerCase().split(<span class="string">&quot;\\W+&quot;</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// emit the pairs</span></span><br><span class="line">            <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                <span class="keyword">if</span> (word.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                    out.collect(word);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://yangyichao-mango.github.io/2019/10/14/hello-world/"/>
    <id>https://yangyichao-mango.github.io/2019/10/14/hello-world/</id>
    <published>2019-10-14T12:54:25.000Z</published>
    <updated>2019-10-19T11:37:10.532Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><span id="more"></span><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
    
  </entry>
  
</feed>
