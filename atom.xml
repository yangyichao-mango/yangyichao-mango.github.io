<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>yangyichao-mango&#39;s blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://yangyichao-mango.github.io/"/>
  <updated>2020-09-06T02:04:08.084Z</updated>
  <id>https://yangyichao-mango.github.io/</id>
  
  <author>
    <name>yangyichao-mango</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>生产实践 | 基于 Flink 的短视频生产消费监控</title>
    <link href="https://yangyichao-mango.github.io/2020/09/01/monitor/"/>
    <id>https://yangyichao-mango.github.io/2020/09/01/monitor/</id>
    <published>2020-09-01T06:21:53.000Z</published>
    <updated>2020-09-06T02:04:08.084Z</updated>
    
    <content type="html"><![CDATA[<h1 id="生产实践-基于-Flink-的短视频生产消费监控"><a href="#生产实践-基于-Flink-的短视频生产消费监控" class="headerlink" title="生产实践 | 基于 Flink 的短视频生产消费监控"></a>生产实践 | 基于 Flink 的短视频生产消费监控</h1><blockquote><p>实时监控类指标整个链路开发过程中的一些经验。</p></blockquote><h2 id="短视频生产消费监控"><a href="#短视频生产消费监控" class="headerlink" title="短视频生产消费监控"></a>短视频生产消费监控</h2><p>短视频带来了全新的传播场域和节目形态，小屏幕、快节奏成为行业潮流的同时，也催生了新的用户消费习惯，为创作者和商户带来收益。而多元化的短视频也可以为品牌方提供营销机遇。</p><p>其中对于垂类生态短视频的生产消费热点的监控分析目前成为了实时数据处理很常见的一个应用场景，比如对某个圈定的垂类生态下的视频生产或者视频消费进行监控，对热点视频生成对应的优化推荐策略，促进热点视频的生产或者消费，构建整个生产消费数据链路的闭环，从而提高创作者收益以及消费者留存。</p><p>本文将完整分析整条数据链路，并基于 Flink 提供几种对于垂类视频监控的架构方案设计。通过本文，你可以了解到：</p><ul><li><p><strong>垂类生态短视频生产消费数据链路闭环</strong></p></li><li><p><strong>实时监控架构设计</strong></p></li><li><p><strong>不同场景下的代码实现</strong></p></li><li><p><strong>flink 学习资料</strong></p></li></ul><h2 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h2><p>垂类生态短视频生产消费数据链路流转架构图如下：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/monitor/data_transform.png" alt="链路" title>                </div>                <div class="image-caption">链路</div>            </figure><p>在上述场景中，用户生产和消费短视频，从而客户端、服务端以及数据库会产生相应的行为操作日志，这些日志会通过日志抽取中间件抽取到消息队列中，我们目前的场景中是使用 Kafka 作为消息队列，然后使用<br>Flink 对垂类生态中的视频进行消费监控，最后将其消费数据产出到下游，下游有可能为数据服务，实时看板，运营同学或者自动化工具会帮助我们分析当前垂类下的生产或者消费热点，从而生成推荐策略。</p><h3 id="架构方案设计"><a href="#架构方案设计" class="headerlink" title="架构方案设计"></a>架构方案设计</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/monitor/monitor_flink.png" alt="架构" title>                </div>                <div class="image-caption">架构</div>            </figure><p>其中数据源如下：</p><ul><li><strong>Kafka</strong> 为全量内容生产和内容消费的日志。</li><li><strong>Rpc/Http/Mysql/配置中心/Redis/HBase</strong> 为需要监控的垂类生态内容 id 池（生产则为作者 id 池，消费则为视频 id 池），主要是提供给运营同学动态配置需要监控的 id 范围，这部分内容可以在<br>flink 中进行实时查询，解析运营同学想要的监控指标范围，以及监控的指标和计算方式，然后加工数据产出，可以支持随时配置，实时数据随时计算最后展示在 BI 平台。</li></ul><p>其中数据汇为聚类好的内容生产或者消费热点话题或者事件时间。</p><ul><li><strong>Redis/HBase</strong> 主要是以低延迟并且高 QPS 提供数据服务，给 Server 端或者线上用户提供低延迟的数据查询。</li><li><strong>Druid</strong> 可以做为 OLAP 引擎为 BI 提供灵活的上卷下钻聚合分析能力。</li><li><strong>Kafka</strong> 可以以流式数据产出，从而提供给下游继续消费或者进行特征提取，进行二次消费分析。</li></ul><p>其中代码示例为 ProcessWindowFunction，也可以使用 AggregateFunction 代替，其中主要监控逻辑都相同。</p><h3 id="方案-1"><a href="#方案-1" class="headerlink" title="方案 1"></a>方案 1</h3><p>适合监控 id 数据量小的场景（<strong>几千 id</strong>），其实现方式是在 flink 任务初始化时将需要监控的 id 池或动态配置中心的 id 池加载到内存当中，之后只需要在内存中判断内容生产或者消费数据是否在这个监控池当中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">ProcessWindowFunction p = <span class="keyword">new</span> ProcessWindowFunction&lt;CommonModel, CommonModel, Long, TimeWindow&gt;() &#123;</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">private</span> Config&lt;Set&lt;Long&gt;&gt; needMonitoredIdsConfig;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.needMonitoredIdsConfig = ConfigBuilder</span><br><span class="line">                .buildSet(<span class="string">"needMonitoredIds"</span>, Long.class);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Long bucket, Context context, Iterable&lt;CommonModel&gt; iterable, Collector&lt;CommonModel&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Set&lt;Long&gt; needMonitoredIds = needMonitoredIdsConfig.get();</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 判断 commonModel 中的 id 是否在 needMonitoredIds 池中</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="方案-2"><a href="#方案-2" class="headerlink" title="方案 2"></a>方案 2</h3><p>适合监控 id 数据量适中（<strong>几十万 id</strong>），监控数据范围会不定时发生变动的场景。其实现方式是在 flink 算子中定时访问接口获取最新的监控 id 池，以获取最新监控数据范围。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">ProcessWindowFunction p = <span class="keyword">new</span> ProcessWindowFunction&lt;CommonModel, CommonModel, Long, TimeWindow&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> lastRefreshTimestamp;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Set&lt;Long&gt; needMonitoredIds;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        <span class="keyword">this</span>.refreshNeedMonitoredIds(System.currentTimeMillis());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Long bucket, Context context, Iterable&lt;CommonModel&gt; iterable, Collector&lt;CommonModel&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> windowStart = context.window().getStart();</span><br><span class="line">        <span class="keyword">this</span>.refreshNeedMonitoredIds(windowStart);</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 判断 commonModel 中的 id 是否在 needMonitoredIds 池中</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">refreshNeedMonitoredIds</span><span class="params">(<span class="keyword">long</span> windowStart)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 每隔 10 秒访问一次</span></span><br><span class="line">        <span class="keyword">if</span> (windowStart - <span class="keyword">this</span>.lastRefreshTimestamp &gt;= <span class="number">10000L</span>) &#123;</span><br><span class="line">            <span class="keyword">this</span>.lastRefreshTimestamp = windowStart;</span><br><span class="line">            <span class="keyword">this</span>.needMonitoredIds = Rpc.get(...)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="方案-3"><a href="#方案-3" class="headerlink" title="方案 3"></a>方案 3</h3><p>方案 3 对方案 2 的一个优化（<strong>几十万 id，我们生产环境中最常用的</strong>）。其实现方式是在 flink 中使用 broadcast 算子定时访问监控 id 池，并将 id 池以广播的形式下发给下游参与计算的各个算子。其优化点在于：比如任务的并行度为 500，每 1s 访问一次，采用方案 2 则访问监控 id 池接口的 QPS 为 500，在使用 broadcast 算子之后，其访问 QPS 可以减少到 1，可以大大减少对接口的访问量，减轻接口压力。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Example</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Slf</span>4j</span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">NeedMonitorIdsSource</span> <span class="keyword">implements</span> <span class="title">SourceFunction</span>&lt;<span class="title">Map</span>&lt;<span class="title">Long</span>, <span class="title">Set</span>&lt;<span class="title">Long</span>&gt;&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isCancel;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Map&lt;Long, Set&lt;Long&gt;&gt;&gt; sourceContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">while</span> (!<span class="keyword">this</span>.isCancel) &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    TimeUnit.SECONDS.sleep(<span class="number">1</span>);</span><br><span class="line">                    Set&lt;Long&gt; needMonitorIds = Rpc.get(...);</span><br><span class="line">                    <span class="comment">// 可以和上一次访问的数据做比较查看是否有变化，如果有变化，才发送出去</span></span><br><span class="line">                    <span class="keyword">if</span> (CollectionUtils.isNotEmpty(needMonitorIds)) &#123;</span><br><span class="line">                        sourceContext.collect(<span class="keyword">new</span> HashMap&lt;Long, Set&lt;Long&gt;&gt;() &#123;&#123;</span><br><span class="line">                            put(<span class="number">0L</span>, needMonitorIds);</span><br><span class="line">                        &#125;&#125;);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">                    <span class="comment">// 防止接口访问失败导致的错误导致 flink job 挂掉</span></span><br><span class="line">                    log.error(<span class="string">"need monitor ids error"</span>, e);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.isCancel = <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        ParameterTool parameterTool = ParameterTool.fromArgs(args);</span><br><span class="line">        InputParams inputParams = <span class="keyword">new</span> InputParams(parameterTool);</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> MapStateDescriptor&lt;Long, Set&lt;Long&gt;&gt; broadcastMapStateDescriptor = <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line">                <span class="string">"config-keywords"</span>,</span><br><span class="line">                BasicTypeInfo.LONG_TYPE_INFO,</span><br><span class="line">                TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Set&lt;Long&gt;&gt;() &#123;</span><br><span class="line">                &#125;));</span><br><span class="line"></span><br><span class="line">        <span class="comment">/********************* kafka source *********************/</span></span><br><span class="line">        BroadcastStream&lt;Map&lt;Long, Set&lt;Long&gt;&gt;&gt; broadcastStream = env</span><br><span class="line">                .addSource(<span class="keyword">new</span> NeedMonitorIdsSource()) <span class="comment">// redis photoId 数据广播</span></span><br><span class="line">                .setParallelism(<span class="number">1</span>)</span><br><span class="line">                .broadcast(broadcastMapStateDescriptor);</span><br><span class="line"></span><br><span class="line">        DataStream&lt;CommonModel&gt; logSourceDataStream = SourceFactory.getSourceDataStream(...);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/********************* dag *********************/</span></span><br><span class="line">        DataStream&lt;CommonModel&gt; resultDataStream = logSourceDataStream</span><br><span class="line">                .keyBy(KeySelectorFactory.getStringKeySelector(CommonModel::getKeyField))</span><br><span class="line">                .connect(broadcastStream)</span><br><span class="line">                .process(<span class="keyword">new</span> KeyedBroadcastProcessFunction&lt;String, CommonModel, Map&lt;Long, Set&lt;Long&gt;&gt;, CommonModel&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">private</span> Set&lt;Long&gt; needMonitoredIds;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                        <span class="keyword">this</span>.needMonitoredIds = Rpc.get(...)</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(CommonModel commonModel, ReadOnlyContext readOnlyContext, Collector&lt;CommonModel&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="comment">// 判断 commonModel 中的 id 是否在 needMonitoredIds 池中</span></span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processBroadcastElement</span><span class="params">(Map&lt;Long, Set&lt;Long&gt;&gt; longSetMap, Context context, Collector&lt;CommonModel&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="comment">// 需要监控的字段</span></span><br><span class="line">                        Set&lt;Long&gt; needMonitorIds = longSetMap.get(<span class="number">0L</span>);</span><br><span class="line">                        <span class="keyword">if</span> (CollectionUtils.isNotEmpty(needMonitorIds)) &#123;</span><br><span class="line">                            <span class="keyword">this</span>.needMonitoredIds = needMonitorIds;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/********************* kafka sink *********************/</span></span><br><span class="line">        SinkFactory.setSinkDataStream(...);</span><br><span class="line">        </span><br><span class="line">        env.execute(inputParams.jobName);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="方案-4"><a href="#方案-4" class="headerlink" title="方案 4"></a>方案 4</h3><p>适合于超大监控范围的数据（<strong>几百万，我们自己的生产实践中使用扩量到 500 万</strong>）。其原理是将监控范围接口按照 id 按照一定规则分桶。flink 消费到日志数据后将 id 按照 监控范围接口 id 相同的分桶方法进行分桶 keyy，这样在下游算子中每个算子中就可以按照桶名称，从接口中拿到对应桶的监控 id 数据，这样 flink 中并行的每个算子只需要获取到自己对应的桶的数据，可以大大减少请求的压力。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Example</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        ParameterTool parameterTool = ParameterTool.fromArgs(args);</span><br><span class="line">        InputParams inputParams = <span class="keyword">new</span> InputParams(parameterTool);</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> MapStateDescriptor&lt;Long, Set&lt;Long&gt;&gt; broadcastMapStateDescriptor = <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line">                <span class="string">"config-keywords"</span>,</span><br><span class="line">                BasicTypeInfo.LONG_TYPE_INFO,</span><br><span class="line">                TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Set&lt;Long&gt;&gt;() &#123;</span><br><span class="line">                &#125;));</span><br><span class="line"></span><br><span class="line">        <span class="comment">/********************* kafka source *********************/</span></span><br><span class="line"></span><br><span class="line">        DataStream&lt;CommonModel&gt; logSourceDataStream = SourceFactory.getSourceDataStream(...);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/********************* dag *********************/</span></span><br><span class="line">        DataStream&lt;CommonModel&gt; resultDataStream = logSourceDataStream</span><br><span class="line">                .keyBy(KeySelectorFactory.getLongKeySelector(CommonModel::getKeyField))</span><br><span class="line">                .timeWindow(Time.seconds(inputParams.accTimeWindowSeconds))</span><br><span class="line">                .process(<span class="keyword">new</span> ProcessWindowFunction&lt;CommonModel, CommonModel, Long, TimeWindow&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">private</span> <span class="keyword">long</span> lastRefreshTimestamp;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">private</span> Set&lt;Long&gt; oneBucketNeedMonitoredIds;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Long bucket, Context context, Iterable&lt;CommonModel&gt; iterable, Collector&lt;CommonModel&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">long</span> windowStart = context.window().getStart();</span><br><span class="line">                        <span class="keyword">this</span>.refreshNeedMonitoredIds(windowStart, bucket);</span><br><span class="line">                        <span class="comment">/**</span></span><br><span class="line"><span class="comment">                         * 判断 commonModel 中的 id 是否在 needMonitoredIds 池中</span></span><br><span class="line"><span class="comment">                         */</span></span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">refreshNeedMonitoredIds</span><span class="params">(<span class="keyword">long</span> windowStart, <span class="keyword">long</span> bucket)</span> </span>&#123;</span><br><span class="line">                        <span class="comment">// 每隔 10 秒访问一次</span></span><br><span class="line">                        <span class="keyword">if</span> (windowStart - <span class="keyword">this</span>.lastRefreshTimestamp &gt;= <span class="number">10000L</span>) &#123;</span><br><span class="line">                            <span class="keyword">this</span>.lastRefreshTimestamp = windowStart;</span><br><span class="line">                            <span class="keyword">this</span>.oneBucketNeedMonitoredIds = Rpc.get(bucket, ...)</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/********************* kafka sink *********************/</span></span><br><span class="line">        SinkFactory.setSinkDataStream(...);</span><br><span class="line"></span><br><span class="line">        env.execute(inputParams.jobName);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="学习资料"><a href="#学习资料" class="headerlink" title="学习资料"></a>学习资料</h2><h3 id="flink"><a href="#flink" class="headerlink" title="flink"></a>flink</h3><ul><li><a href="https://github.com/flink-china/flink-training-course/blob/master/README.md" target="_blank" rel="noopener">https://github.com/flink-china/flink-training-course/blob/master/README.md</a></li><li><a href="https://ververica.cn/developers-resources/" target="_blank" rel="noopener">https://ververica.cn/developers-resources/</a></li><li><a href="https://space.bilibili.com/33807709" target="_blank" rel="noopener">https://space.bilibili.com/33807709</a></li></ul>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>实时开发需求确认模板</title>
    <link href="https://yangyichao-mango.github.io/2020/09/01/demand/"/>
    <id>https://yangyichao-mango.github.io/2020/09/01/demand/</id>
    <published>2020-09-01T06:21:53.000Z</published>
    <updated>2020-09-06T02:00:13.392Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实时开发需求确认模板"><a href="#实时开发需求确认模板" class="headerlink" title="实时开发需求确认模板"></a>实时开发需求确认模板</h1><blockquote><p>实时指标整个链路开发过程中的一些经验。</p></blockquote><h2 id="需求评估"><a href="#需求评估" class="headerlink" title="需求评估"></a>需求评估</h2><p>分析实时指标是否符合该需求场景，以及在该场景中实时指标能够发挥的价值。</p><h3 id="1-实时指标所能提供的分析能力"><a href="#1-实时指标所能提供的分析能力" class="headerlink" title="1.实时指标所能提供的分析能力"></a>1.实时指标所能提供的分析能力</h3><p>实时计算的输出内容，以及提供的分析能力：OLAP 分析，key-value 实时数据服务，维度填充，数据打标等。</p><h3 id="2-产出维度，指标的合理性"><a href="#2-产出维度，指标的合理性" class="headerlink" title="2.产出维度，指标的合理性"></a>2.产出维度，指标的合理性</h3><p>从需求出发，评估实时指标产出的维度和指标的合理性。</p><h3 id="3-需求可配置变量"><a href="#3-需求可配置变量" class="headerlink" title="3.需求可配置变量"></a>3.需求可配置变量</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">评估维度</th>            <th style="width: 20%; text-align: center;">评估指标</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">监控数据范围</td>            <td>监控数据范围动态配置改变？</td>        </tr>        <tr>            <td style="text-align: center;">监控数据起止时间</td>            <td>监控数据的起止时间动态配置改变？</td>        </tr>        <tr>            <td style="text-align: center;">监控数据的某些配置变量</td>            <td>当变量发生变动时，可能会对产出的实时数据有什么影响，对计算链路有什么影响，会决定实时计算链路的实现方式。</td>        </tr>    </tbody></table><h3 id="4-面向用户范围"><a href="#4-面向用户范围" class="headerlink" title="4.面向用户范围"></a>4.面向用户范围</h3><p>评估 SLA 等服务质量等级保障，以及提供的实时数据服务的可用性等级，并且提前和业务方确认可用性和出现故障时的恢复时间等问题。</p><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">评估维度</th>            <th style="width: 20%; text-align: center;">评估指标</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">时间语义</td>            <td>事件时间、处理时间？事件时间：可以通过获取数据的时间戳，使用处理时间来真实反映和还原事件，但是可能会出现数据条数过小窗口不能触发，或者在一些有截止日期的活动结束的最后一个窗口不能触发的问题；处理时间：处理时间一般和事件时间差距很小，经验值一般 diff 小于 1%，只能反映流式框架处理数据时的时间戳，但是不会出现上述事件时间的两个问题。因此需要评估需求的逻辑精确度是否要求很高？</td>        </tr>        <tr>            <td style="text-align: center;">数据一致性</td>            <td>至少一次、精确一次？至少一次：受限于目前的上下游以及依赖中间件的能力，比如 010 版本及以下的 Kafka 不支持两阶段提交，所以只能达到至少一次的语义；精确一次：整条实时计算链路中的所有组件都需要支持精确一次的语义（从技术层面或者业务逻辑层面达到精确一次）。评估需求逻辑是否可接受任务发生失败时有重复数据产生？</td>        </tr>        <tr>            <td style="text-align: center;">SLA 要求</td>            <td>评估需求的 SLA 要求，整条实时计算链路的 SLA 要求，产出数据最多延迟多长时间？数据准确率几个9？提供的接口服务是否需要考虑跨集群、机房备份、双写；是否需要建立多条计算链路以供故障切换？一旦发生故障，下游消费方能容忍的最大故障时长？下游消费方在发生故障时的兜底策略？</td>        </tr>    </tbody></table><h2 id="可行性评估"><a href="#可行性评估" class="headerlink" title="可行性评估"></a>可行性评估</h2><h3 id="1-技术可行性"><a href="#1-技术可行性" class="headerlink" title="1.技术可行性"></a>1.技术可行性</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">评估维度</th>            <th style="width: 20%; text-align: center;">评估指标</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">QPS</td>            <td>确定 QPS 以评估实时上下游以及依赖组件的选型以及能力。</td>        </tr>        <tr>            <td style="text-align: center;">数据输入</td>            <td>首先确定数据输入是否能够计算实时指标，然后评估上游提供的数据在计算实时指标时整个实时计算链路的逻辑以及复杂度。比如：是否需要用到双流 join，需要评估双流 join 所存在的误差是否在可接受范围内，一般可通过离线误差对比或经验值给出结论。常见输入中间件：消息队列，接口等，常用中间件：Kafka，rpc，http。</td>        </tr>        <tr>            <td style="text-align: center;">数据依赖</td>            <td>调研目前可用的哪些中间件可以提供能力来支持当前指标计算？举例：key-value等，常用依赖中间件：Redis。</td>        </tr>        <tr>            <td style="text-align: center;">数据输出</td>            <td>确定输出下游消费方的消费需求以及能力，以评估实时产出的数据以及存储组件是否能够满足其需求？常见输出中间件：消息队列，OLAP，key-value，接口等，常用中间件：Kafka，Druid，Redis，rpc。产出维度，一般场景下，维度值不建议是大数量级别的数据，比如说使用 user_id，device_id。</td>        </tr>    </tbody></table><h3 id="2-成本可行性"><a href="#2-成本可行性" class="headerlink" title="2.成本可行性"></a>2.成本可行性</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">评估维度</th>            <th style="width: 20%; text-align: center;">评估指标</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">QPS</td>            <td>确定 QPS 以评估实时上下游以及依赖中间件的资源消耗。确定资源消耗是否在可接受范围之内？</td>        </tr>        <tr>            <td style="text-align: center;">数据输入</td>            <td>确定整个实时计算链路的逻辑以及复杂度，来评估可能的资源消耗。</td>        </tr>        <tr>            <td style="text-align: center;">数据依赖</td>            <td>确定整个实时计算链路的逻辑以及复杂度，来评估可能的资源消耗。</td>        </tr>        <tr>            <td style="text-align: center;">数据输出</td>            <td>由输出内容以及存储组件来评估下游存储中间件的资源消耗。举例：维度值不建议是大数量级别的数据，比如说使用 user_id，device_id 作为维度或者产出明细数据，虽然使用 OLAP 在维度聚合场景下很灵活，但是这些场景下使用 OLAP 可能会造成很大的资源消耗。</td>        </tr>    </tbody></table><p>综合以上技术和成本可行性以及需求收益等指标，以评估实时指标的 ROI。</p><h3 id="3-数据输入"><a href="#3-数据输入" class="headerlink" title="3.数据输入"></a>3.数据输入</h3><h4 id="消息队列日志"><a href="#消息队列日志" class="headerlink" title="消息队列日志"></a>消息队列日志</h4><p>最常见的实时数据源就是消息队列日志，首先我们需要确定日志类型，不同的日志类型决定了指标或者维度字段是否可以产出以及其准确性，一般情况下有以下三种类型日志：</p><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">日志类型</th>            <th style="width: 20%; text-align: center;">备注</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">埋点日志</td>            <td>维度最全，数据准确</td>        </tr>        <tr>            <td style="text-align: center;">web server log</td>            <td>维度次全，数据准确度一般</td>        </tr>        <tr>            <td style="text-align: center;">binlog</td>            <td>数据库真实数据，反映的是真实数据，数据最准确，维度信息一般很少</td>        </tr>    </tbody></table><h4 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h4><p>这里的接口一般是用来根据需求圈定一批需要进行监控的数据内容。接口的提供方式可以是http，配置中心配置，rpc接口等。</p><h3 id="4-数据依赖"><a href="#4-数据依赖" class="headerlink" title="4.数据依赖"></a>4.数据依赖</h3><p>实时一般情况下都会或多或少依赖到外部组件，最常见的就是 key-value 存储。<br>场景：很常见的一类需求就是对数据源中的数据进行打标然后产出，这里的标签数据就会存储在 key-value 中间件中。<br>需要评估访问外存的 QPS，以及外存提供的能力。</p><h3 id="5-数据输出"><a href="#5-数据输出" class="headerlink" title="5.数据输出"></a>5.数据输出</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">输出组件</th>            <th style="width: 20%; text-align: center;">备注</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">消息队列</td>            <td>常见中间件：kafka等</td>        </tr>        <tr>            <td style="text-align: center;">key-value存储</td>            <td>常见中间件：Redis，HBase，服务化接口等</td>        </tr>        <tr>            <td style="text-align: center;">OLAP</td>            <td>常见中间件：Druid，ClickHouse等</td>        </tr>    </tbody></table><h2 id="技术方案评估"><a href="#技术方案评估" class="headerlink" title="技术方案评估"></a>技术方案评估</h2>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>实时新增类指标标准化处理方案</title>
    <link href="https://yangyichao-mango.github.io/2020/09/01/new/"/>
    <id>https://yangyichao-mango.github.io/2020/09/01/new/</id>
    <published>2020-09-01T06:21:53.000Z</published>
    <updated>2020-09-06T02:04:08.078Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实时新增类指标标准化处理方案"><a href="#实时新增类指标标准化处理方案" class="headerlink" title="实时新增类指标标准化处理方案"></a>实时新增类指标标准化处理方案</h1><blockquote><p>实时指标整个链路开发过程中的一些经验。</p></blockquote><h2 id="实时新增类指标"><a href="#实时新增类指标" class="headerlink" title="实时新增类指标"></a>实时新增类指标</h2><p>大体上可以将实时新增类指标以以下两种维度进行分类。</p><h3 id="identity-id-类型维度"><a href="#identity-id-类型维度" class="headerlink" title="identity id 类型维度"></a>identity id 类型维度</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">identity id 类型</th>            <th style="width: 20%; text-align: center;">备注</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">number(long) 类型 identity id</td>            <td>数值类型 identity id 的好处在于可以使用 Bitmap 类组件做到精确去重。</td>        </tr>        <tr>            <td style="text-align: center;">字符类型 identity id</td>            <td>字符类型 identity id 去重相对复杂，有两种方式，在误差允许范围之内使用 BloomFilter 进行去重，或者使用 key-value 组件进行精确去重。</td>        </tr>    </tbody></table><h3 id="产出数据类型维度"><a href="#产出数据类型维度" class="headerlink" title="产出数据类型维度"></a>产出数据类型维度</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">产出数据类型</th>            <th style="width: 20%; text-align: center;">备注</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">明细类数据</td>            <td>此类数据一般是要求将新增的数据明细产出，uv 的含义是做过滤，产出的明细数据中的 identity id 不会有重复。输出明细数据的好处在于，我们可以在下游使用 OLAP 引擎对明细数据进行各种维度的聚合计算，从而很方便的产出不同维度下的 uv 数据。</td>        </tr>        <tr>            <td style="text-align: center;">聚合类数据</td>            <td>将一个时间窗口内的 uv 进行聚合，并且可以计算出分维度的 uv，其产出数据一般都是[维度 + uv_count]，但是这里的维度一般情况下是都是固定维度。如果需要拓展则需要改动源码。</td>        </tr>    </tbody></table><h2 id="计算链路"><a href="#计算链路" class="headerlink" title="计算链路"></a>计算链路</h2><p>因此新增产出的链路多数就是以上两种维度因子的相互组合。</p><h3 id="number-long-类型-identity-id"><a href="#number-long-类型-identity-id" class="headerlink" title="number(long) 类型 identity id"></a>number(long) 类型 identity id</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/new/new_uid_roaringbitmap.png" alt="使用 RoaringBitmap 的 uv 计算链路" title>                </div>                <div class="image-caption">使用 RoaringBitmap 的 uv 计算链路</div>            </figure><p>代码示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">RoaringBitmapDuplicateable</span>&lt;<span class="title">Model</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> DEFAULT_DUPLICATE_MILLS = <span class="number">24</span> * <span class="number">3600</span> * <span class="number">1000L</span>;</span><br><span class="line"></span><br><span class="line">    BiPredicate&lt;Long, Long&gt; ROARING_BIT_MAP_CLEAR_BI_PREDICATE =</span><br><span class="line">            (start, end) -&gt; end - start &gt;= DEFAULT_DUPLICATE_MILLS;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化</span></span><br><span class="line">    <span class="keyword">default</span> ValueState&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt; getBitMapValueState(String name) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.getRuntimeContext().getState(</span><br><span class="line">                <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(name, TypeInformation.of(</span><br><span class="line">                        <span class="keyword">new</span> TypeHint&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt;() &#123; &#125;))</span><br><span class="line">        );</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">RuntimeContext <span class="title">getRuntimeContext</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">getLongId</span><span class="params">(Model model)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">Optional&lt;Logger&gt; <span class="title">getLogger</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> BiPredicate&lt;Long, Long&gt; <span class="title">roaringBitMapClearBiPredicate</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> ROARING_BIT_MAP_CLEAR_BI_PREDICATE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> List&lt;Model&gt; <span class="title">duplicateAndGet</span><span class="params">(List&lt;Model&gt; models, <span class="keyword">long</span> windowStartTimestamp</span></span></span><br><span class="line"><span class="function"><span class="params">            , ValueState&lt;Tuple2&lt;Date, Roaring64NavigableMap&gt;&gt; bitMapValueState)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Tuple2&lt;Date, Roaring64NavigableMap&gt; bitMap = checkAndGetState(windowStartTimestamp, bitMapValueState);</span><br><span class="line"></span><br><span class="line">        Map&lt;Long, Model&gt; idModelsMap = models</span><br><span class="line">                .stream()</span><br><span class="line">                .collect(Collectors.toMap(<span class="keyword">this</span>::getLongId, Function.identity(), (oldOne, newOne) -&gt; oldOne));</span><br><span class="line"></span><br><span class="line">        Set&lt;Long&gt; ids = idModelsMap.keySet();</span><br><span class="line"></span><br><span class="line">        List&lt;Model&gt; newModels = Lists.newArrayList();</span><br><span class="line">        <span class="keyword">for</span> (Long id : ids) &#123;</span><br><span class="line">            <span class="keyword">if</span> (!bitMap.f1.contains(id)) &#123;</span><br><span class="line">                <span class="keyword">if</span> (idModelsMap.containsKey(id)) &#123;</span><br><span class="line">                    newModels.add(idModelsMap.get(id));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        newModels.stream()</span><br><span class="line">                .map(<span class="keyword">this</span>::getLongId)</span><br><span class="line">                .forEach(bitMap.f1::add);</span><br><span class="line">        bitMapValueState.update(bitMap);</span><br><span class="line">        <span class="keyword">return</span> newModels;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> <span class="keyword">long</span> <span class="title">duplicateAndCount</span><span class="params">(List&lt;Model&gt; models, <span class="keyword">long</span> windowStartTimestamp</span></span></span><br><span class="line"><span class="function"><span class="params">            , ValueState&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt; bitMapValueState)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Tuple2&lt;Long, Roaring64NavigableMap&gt; bitMap = checkAndGetState(windowStartTimestamp, bitMapValueState);</span><br><span class="line"></span><br><span class="line">        Set&lt;Long&gt; ids = models</span><br><span class="line">                .stream()</span><br><span class="line">                .map(<span class="keyword">this</span>::getLongId)</span><br><span class="line">                .collect(Collectors.toSet());</span><br><span class="line"></span><br><span class="line">        List&lt;Long&gt; newIds = Lists.newArrayList();</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (Long id : ids) &#123;</span><br><span class="line">            <span class="keyword">if</span> (!bitMap.f1.contains(id)) &#123;</span><br><span class="line">                newIds.add(id);</span><br><span class="line">                count++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        newIds.forEach(bitMap.f1::add);</span><br><span class="line">        bitMapValueState.update(bitMap);</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> Tuple2&lt;Long, Roaring64NavigableMap&gt; <span class="title">checkAndGetState</span><span class="params">(<span class="keyword">long</span> windowStartTimestamp</span></span></span><br><span class="line"><span class="function"><span class="params">            , ValueState&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt; bitMapValueState)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Tuple2&lt;Long, Roaring64NavigableMap&gt; bitmap = bitMapValueState.value();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> == bitmap) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">this</span>.getLogger().ifPresent(logger -&gt;</span><br><span class="line">                    logger.info(<span class="string">"New RoaringBitMapValueState Timestamp=&#123;&#125;"</span>, windowStartTimestamp));</span><br><span class="line">            Tuple2&lt;Long, Roaring64NavigableMap&gt; newBitMap = Tuple2.of(windowStartTimestamp, <span class="keyword">new</span> Roaring64NavigableMap());</span><br><span class="line">            bitMapValueState.update(newBitMap);</span><br><span class="line">            <span class="keyword">return</span> newBitMap;</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="keyword">this</span>.roaringBitMapClearBiPredicate().test(bitmap.f0, windowStartTimestamp)) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">this</span>.getLogger().ifPresent(logger -&gt;</span><br><span class="line">                    logger.info(<span class="string">"Clear RoaringBitMapValueState, from start=&#123;&#125; to end=&#123;&#125;"</span>, bitmap.f0, windowStartTimestamp));</span><br><span class="line"></span><br><span class="line">            bitMapValueState.clear();</span><br><span class="line">            bitmap.f1.clear();</span><br><span class="line">            Tuple2&lt;Long, Roaring64NavigableMap&gt; newBitMap = Tuple2.of(windowStartTimestamp, <span class="keyword">new</span> Roaring64NavigableMap());</span><br><span class="line">            bitMapValueState.update(newBitMap);</span><br><span class="line">            <span class="keyword">return</span> newBitMap;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> bitmap;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="字符类型-identity-id"><a href="#字符类型-identity-id" class="headerlink" title="字符类型 identity id"></a>字符类型 identity id</h3><h4 id="使用-Flink-state"><a href="#使用-Flink-state" class="headerlink" title="使用 Flink state"></a>使用 Flink state</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/new/new_did_flink_state.png" alt="使用 flink state 的 uv 计算链路" title>                </div>                <div class="image-caption">使用 flink state 的 uv 计算链路</div>            </figure><h4 id="使用-key-value-外存"><a href="#使用-key-value-外存" class="headerlink" title="使用 key-value 外存"></a>使用 key-value 外存</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/new/new_did_key_value.png" alt="使用 key-value 的 uv 计算链路" title>                </div>                <div class="image-caption">使用 key-value 的 uv 计算链路</div>            </figure><p>如果选用的是 Redis 作为 key-value 过滤，那么这里会有一个巧用 Redis bit 特性的优化。举一个一般场景下的方案与使用 Redis bit 特性的方案做对比：</p><p>场景：假如需要同一天有几十场活动，并且都希望计算出这几十场活动的 uv，那么我们就可以按照下图设计 Redis bit 结构。</p><p>通常方案：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/new/new_did_redis.png" alt="使用 Redis 的 多 uv 指标计算链路" title>                </div>                <div class="image-caption">使用 Redis 的 多 uv 指标计算链路</div>            </figure><p>这种场景下，如果有 1 亿用户，需要同时计算 50 个活动或者 50 个不同维度下的 uv。那么理论上最大 key 数量为 1 亿 * 50 = 50 亿个 key。</p><p>Redis bit 方案：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/new/new_did_redis_bit.png" alt="使用 Redis bit 特性的 多 uv 指标计算链路" title>                </div>                <div class="image-caption">使用 Redis bit 特性的 多 uv 指标计算链路</div>            </figure><p>这样做的一个优点，就是这几十场活动的 uv 计算都使用了相同的 Redis key 来计算，可以大幅度减少 Redis 的容量占用。使用此方案的话，以上述相同的用户和活动场数，理论上最大<br>key 数量仅仅为 1 亿，只是 value 数量会多占几十个 bit。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yangyichao-mango.github.io/2020/09/01/broadcast/"/>
    <id>https://yangyichao-mango.github.io/2020/09/01/broadcast/</id>
    <published>2020-09-01T06:21:53.000Z</published>
    <updated>2020-09-06T02:00:13.372Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实时新增类指标标准化处理方案"><a href="#实时新增类指标标准化处理方案" class="headerlink" title="实时新增类指标标准化处理方案"></a>实时新增类指标标准化处理方案</h1><blockquote><p>实时指标整个链路开发过程中的一些经验。</p></blockquote><h2 id="实时新增类指标"><a href="#实时新增类指标" class="headerlink" title="实时新增类指标"></a>实时新增类指标</h2><p>大体上可以将实时新增类指标以以下两种维度进行分类。</p><h3 id="identity-id-类型维度"><a href="#identity-id-类型维度" class="headerlink" title="identity id 类型维度"></a>identity id 类型维度</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">identity id 类型</th>            <th style="width: 20%; text-align: center;">备注</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">number(long) 类型 identity id</td>            <td>数值类型 identity id 的好处在于可以使用 Bitmap 类组件做到精确去重。</td>        </tr>        <tr>            <td style="text-align: center;">字符类型 identity id</td>            <td>字符类型 identity id 去重相对复杂，有两种方式，在误差允许范围之内使用 BloomFilter 进行去重，或者使用 key-value 组件进行精确去重。</td>        </tr>    </tbody></table><h3 id="产出数据类型维度"><a href="#产出数据类型维度" class="headerlink" title="产出数据类型维度"></a>产出数据类型维度</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">产出数据类型</th>            <th style="width: 20%; text-align: center;">备注</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">明细类数据</td>            <td>此类数据一般是要求将新增的数据明细产出，uv 的含义是做过滤，产出的明细数据中的 identity id 不会有重复。输出明细数据的好处在于，我们可以在下游使用 OLAP 引擎对明细数据进行各种维度的聚合计算，从而很方便的产出不同维度下的 uv 数据。</td>        </tr>        <tr>            <td style="text-align: center;">聚合类数据</td>            <td>将一个时间窗口内的 uv 进行聚合，并且可以计算出分维度的 uv，其产出数据一般都是[维度 + uv_count]，但是这里的维度一般情况下是都是固定维度。如果需要拓展则需要改动源码。</td>        </tr>    </tbody></table><h2 id="计算链路"><a href="#计算链路" class="headerlink" title="计算链路"></a>计算链路</h2><p>因此新增产出的链路多数就是以上两种维度因子的相互组合。</p><h3 id="number-long-类型-identity-id"><a href="#number-long-类型-identity-id" class="headerlink" title="number(long) 类型 identity id"></a>number(long) 类型 identity id</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="new_uid_roaringbitmap.png" alt="使用 RoaringBitmap 的 uv 计算链路" title>                </div>                <div class="image-caption">使用 RoaringBitmap 的 uv 计算链路</div>            </figure><p>代码示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">RoaringBitmapDuplicateable</span>&lt;<span class="title">Model</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> DEFAULT_DUPLICATE_MILLS = <span class="number">24</span> * <span class="number">3600</span> * <span class="number">1000L</span>;</span><br><span class="line"></span><br><span class="line">    BiPredicate&lt;Long, Long&gt; ROARING_BIT_MAP_CLEAR_BI_PREDICATE =</span><br><span class="line">            (start, end) -&gt; end - start &gt;= DEFAULT_DUPLICATE_MILLS;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化</span></span><br><span class="line">    <span class="keyword">default</span> ValueState&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt; getBitMapValueState(String name) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.getRuntimeContext().getState(</span><br><span class="line">                <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(name, TypeInformation.of(</span><br><span class="line">                        <span class="keyword">new</span> TypeHint&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt;() &#123; &#125;))</span><br><span class="line">        );</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">RuntimeContext <span class="title">getRuntimeContext</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">getLongId</span><span class="params">(Model model)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">Optional&lt;Logger&gt; <span class="title">getLogger</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> BiPredicate&lt;Long, Long&gt; <span class="title">roaringBitMapClearBiPredicate</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> ROARING_BIT_MAP_CLEAR_BI_PREDICATE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> List&lt;Model&gt; <span class="title">duplicateAndGet</span><span class="params">(List&lt;Model&gt; models, <span class="keyword">long</span> windowStartTimestamp</span></span></span><br><span class="line"><span class="function"><span class="params">            , ValueState&lt;Tuple2&lt;Date, Roaring64NavigableMap&gt;&gt; bitMapValueState)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Tuple2&lt;Date, Roaring64NavigableMap&gt; bitMap = checkAndGetState(windowStartTimestamp, bitMapValueState);</span><br><span class="line"></span><br><span class="line">        Map&lt;Long, Model&gt; idModelsMap = models</span><br><span class="line">                .stream()</span><br><span class="line">                .collect(Collectors.toMap(<span class="keyword">this</span>::getLongId, Function.identity(), (oldOne, newOne) -&gt; oldOne));</span><br><span class="line"></span><br><span class="line">        Set&lt;Long&gt; ids = idModelsMap.keySet();</span><br><span class="line"></span><br><span class="line">        List&lt;Model&gt; newModels = Lists.newArrayList();</span><br><span class="line">        <span class="keyword">for</span> (Long id : ids) &#123;</span><br><span class="line">            <span class="keyword">if</span> (!bitMap.f1.contains(id)) &#123;</span><br><span class="line">                <span class="keyword">if</span> (idModelsMap.containsKey(id)) &#123;</span><br><span class="line">                    newModels.add(idModelsMap.get(id));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        newModels.stream()</span><br><span class="line">                .map(<span class="keyword">this</span>::getLongId)</span><br><span class="line">                .forEach(bitMap.f1::add);</span><br><span class="line">        bitMapValueState.update(bitMap);</span><br><span class="line">        <span class="keyword">return</span> newModels;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> <span class="keyword">long</span> <span class="title">duplicateAndCount</span><span class="params">(List&lt;Model&gt; models, <span class="keyword">long</span> windowStartTimestamp</span></span></span><br><span class="line"><span class="function"><span class="params">            , ValueState&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt; bitMapValueState)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Tuple2&lt;Long, Roaring64NavigableMap&gt; bitMap = checkAndGetState(windowStartTimestamp, bitMapValueState);</span><br><span class="line"></span><br><span class="line">        Set&lt;Long&gt; ids = models</span><br><span class="line">                .stream()</span><br><span class="line">                .map(<span class="keyword">this</span>::getLongId)</span><br><span class="line">                .collect(Collectors.toSet());</span><br><span class="line"></span><br><span class="line">        List&lt;Long&gt; newIds = Lists.newArrayList();</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (Long id : ids) &#123;</span><br><span class="line">            <span class="keyword">if</span> (!bitMap.f1.contains(id)) &#123;</span><br><span class="line">                newIds.add(id);</span><br><span class="line">                count++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        newIds.forEach(bitMap.f1::add);</span><br><span class="line">        bitMapValueState.update(bitMap);</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> Tuple2&lt;Long, Roaring64NavigableMap&gt; <span class="title">checkAndGetState</span><span class="params">(<span class="keyword">long</span> windowStartTimestamp</span></span></span><br><span class="line"><span class="function"><span class="params">            , ValueState&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt; bitMapValueState)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Tuple2&lt;Long, Roaring64NavigableMap&gt; bitmap = bitMapValueState.value();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> == bitmap) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">this</span>.getLogger().ifPresent(logger -&gt;</span><br><span class="line">                    logger.info(<span class="string">"New RoaringBitMapValueState Timestamp=&#123;&#125;"</span>, windowStartTimestamp));</span><br><span class="line">            Tuple2&lt;Long, Roaring64NavigableMap&gt; newBitMap = Tuple2.of(windowStartTimestamp, <span class="keyword">new</span> Roaring64NavigableMap());</span><br><span class="line">            bitMapValueState.update(newBitMap);</span><br><span class="line">            <span class="keyword">return</span> newBitMap;</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="keyword">this</span>.roaringBitMapClearBiPredicate().test(bitmap.f0, windowStartTimestamp)) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">this</span>.getLogger().ifPresent(logger -&gt;</span><br><span class="line">                    logger.info(<span class="string">"Clear RoaringBitMapValueState, from start=&#123;&#125; to end=&#123;&#125;"</span>, bitmap.f0, windowStartTimestamp));</span><br><span class="line"></span><br><span class="line">            bitMapValueState.clear();</span><br><span class="line">            bitmap.f1.clear();</span><br><span class="line">            Tuple2&lt;Long, Roaring64NavigableMap&gt; newBitMap = Tuple2.of(windowStartTimestamp, <span class="keyword">new</span> Roaring64NavigableMap());</span><br><span class="line">            bitMapValueState.update(newBitMap);</span><br><span class="line">            <span class="keyword">return</span> newBitMap;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> bitmap;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="字符类型-identity-id"><a href="#字符类型-identity-id" class="headerlink" title="字符类型 identity id"></a>字符类型 identity id</h3><h4 id="使用-Flink-state"><a href="#使用-Flink-state" class="headerlink" title="使用 Flink state"></a>使用 Flink state</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="new_did_flink_state.png" alt="使用 flink state 的 uv 计算链路" title>                </div>                <div class="image-caption">使用 flink state 的 uv 计算链路</div>            </figure><h4 id="使用-key-value-外存"><a href="#使用-key-value-外存" class="headerlink" title="使用 key-value 外存"></a>使用 key-value 外存</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="new_did_key_value.png" alt="使用 key-value 的 uv 计算链路" title>                </div>                <div class="image-caption">使用 key-value 的 uv 计算链路</div>            </figure><p>如果选用的是 Redis 作为 key-value 过滤，那么这里会有一个巧用 Redis bit 特性的优化。举一个一般场景下的方案与使用 Redis bit 特性的方案做对比：</p><p>场景：假如需要同一天有几十场活动，并且都希望计算出这几十场活动的 uv，那么我们就可以按照下图设计 Redis bit 结构。</p><p>通常方案：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="new_did_redis.png" alt="使用 Redis 的 多 uv 指标计算链路" title>                </div>                <div class="image-caption">使用 Redis 的 多 uv 指标计算链路</div>            </figure><p>这种场景下，如果有 1 亿用户，需要同时计算 50 个活动或者 50 个不同维度下的 uv。那么理论上最大 key 数量为 1 亿 * 50 = 50 亿个 key。</p><p>Redis bit 方案：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="new_did_redis_bit.png" alt="使用 Redis bit 特性的 多 uv 指标计算链路" title>                </div>                <div class="image-caption">使用 Redis bit 特性的 多 uv 指标计算链路</div>            </figure><p>这样做的一个优点，就是这几十场活动的 uv 计算都使用了相同的 Redis key 来计算，可以大幅度减少 Redis 的容量占用。使用此方案的话，以上述相同的用户和活动场数，理论上最大<br>key 数量仅仅为 1 亿，只是 value 数量会多占几十个 bit。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/categories/Apache-Druid/"/>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/tags/Apache-Druid/"/>
    
      <category term="Apache Zookeeper" scheme="https://yangyichao-mango.github.io/tags/Apache-Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>实时开发标准化处理方案</title>
    <link href="https://yangyichao-mango.github.io/2020/09/01/realtime/"/>
    <id>https://yangyichao-mango.github.io/2020/09/01/realtime/</id>
    <published>2020-09-01T06:21:53.000Z</published>
    <updated>2020-09-06T02:00:13.380Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实时开发标准化处理方案"><a href="#实时开发标准化处理方案" class="headerlink" title="实时开发标准化处理方案"></a>实时开发标准化处理方案</h1><blockquote><p>实时指标整个链路开发过程中的一些经验。</p></blockquote><h2 id="指标类型"><a href="#指标类型" class="headerlink" title="指标类型"></a>指标类型</h2><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">指标类型</th>            <th style="width: 20%; text-align: center;">备注</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">pv</td>            <td>简单 pv 类型指标，来一条日志信息加一，计算 count</td>        </tr>        <tr>            <td style="text-align: center;">uv</td>            <td>uv 类型指标，需要在一段时间范围内（一小时、一天、一场活动）正对 user_id 等 identity_id 去重计数。</td>        </tr>        <tr>            <td style="text-align: center;">监控圈定集合内的 identity_id 数据表现</td>            <td>有一组鉴定的 identity_id 集合，实时的监控或者计算这组 identity_id 相关的数据</td>        </tr>        <tr>            <td style="text-align: center;">排名</td>            <td>实时监控某些数据并根据某些指标进行排序</td>        </tr>    </tbody></table>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>为什么hashmap的数组初始化大小都是2的次方大小时，hashmap的效率最高</title>
    <link href="https://yangyichao-mango.github.io/2020/01/06/java:study-hashmap/"/>
    <id>https://yangyichao-mango.github.io/2020/01/06/java:study-hashmap/</id>
    <published>2020-01-06T12:39:35.000Z</published>
    <updated>2020-01-07T06:32:24.844Z</updated>
    
    <content type="html"><![CDATA[<p>为什么hashmap的数组初始化大小都是2的次方大小时，hashmap的效率最高</p><a id="more"></a><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">indexFor</span><span class="params">(<span class="keyword">int</span> hashcode, <span class="keyword">int</span> length)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> hashcode &amp; (length-<span class="number">1</span>);  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="怎样提高get-key-效率？"><a href="#怎样提高get-key-效率？" class="headerlink" title="怎样提高get(key)效率？"></a>怎样提高get(key)效率？</h1><p>怎样提高get(key)效率 = 怎样提高确定key的所在hashmap中数组index的效率</p><p>hashmap的数据结构是数组和链表的结合，所以我们当然希望这个hashmap里面的元素位置尽量的分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，而不用再去遍历链表。</p><p>看下图，左边两组是数组长度为16（2的4次方），右边两组是数组长度为15。两组的hashcode均为8和9，但是很明显，当它们和1110“与”的时候，产生了相同的结果，也就是说它们会定位到数组中的同一个位置上去，这就产生了碰撞，8和9会被放到同一个链表上，那么查询的时候就需要遍历这个链表，得到8或者9，这样就降低了查询的效率。同时，我们也可以发现，当数组长度为15的时候，hashcode的值会与14（1110）进行“与”，那么最后一位永远是0，而0001，0011，0101，1001，1011，0111，1101这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组长度小了很多，这意味着进一步增加了碰撞的几率，减慢了查询的效率！ </p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/java:study-hashmap/hashmap-index-for.jpg" alt="hashmap-index" title>                </div>                <div class="image-caption">hashmap-index</div>            </figure><p>1.假设key的hashcode为h，数组长度为length，为了将数据打散，使hashmap中的数组下标对应的Entry链表都有数据，首先想到的就是对length取模，计算方法如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">indexFor</span><span class="params">(<span class="keyword">int</span> h, <span class="keyword">int</span> length)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> h % length;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>因此确定了hashmap的indexFor函数的计算方式。</p><h1 id="怎样确定hashmap数组的length"><a href="#怎样确定hashmap数组的length" class="headerlink" title="怎样确定hashmap数组的length"></a>怎样确定hashmap数组的length</h1><p>“模”运算的消耗还是比较大的，能不能找一种更快速，消耗更小的方式？我们发现做位运算的消耗是很小的，所以尝试将取模运算转换成位预算，由此发现length为2的n次方时会有</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">indexFor</span><span class="params">(<span class="keyword">int</span> h, <span class="keyword">int</span> length)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> h % length; <span class="comment">// 等价于 h &amp; (length - 1);</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于 &amp; 运算符计算效率大大高于 % 运算符，所以上述计算转换为：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">indexFor</span><span class="params">(<span class="keyword">int</span> h, <span class="keyword">int</span> length)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> h &amp; (length - <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>也规定了 length 必须是2的n次方（n&gt;0）<br>除此之外，我们也可以发现，当数组长度为15的时候，hashcode的值会与14（1110）进行“与”，那么最后一位永远是0，而0001，0011，0101，1001，1011，0111，1101这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组长度小了很多，这意味着进一步增加了碰撞的几率，减慢了查询的效率！ </p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="JAVA" scheme="https://yangyichao-mango.github.io/categories/JAVA/"/>
    
    
      <category term="JAVA" scheme="https://yangyichao-mango.github.io/tags/JAVA/"/>
    
  </entry>
  
  <entry>
    <title>apache-flink:study-flink</title>
    <link href="https://yangyichao-mango.github.io/2019/11/22/apache-flink:study-flink/"/>
    <id>https://yangyichao-mango.github.io/2019/11/22/apache-flink:study-flink/</id>
    <published>2019-11-22T03:30:41.000Z</published>
    <updated>2020-05-10T10:38:05.892Z</updated>
    
    <content type="html"><![CDATA[<p>所有operator中的初始化如果写在构造函数当中就会出错，问题是序列化时的问题<br>flink从jobmanager序列化到各个 taskmanager时可能会出问题</p><p>split组件功能可以减少多个flatmap的性能损失<br>多个flatmap数据每个都是使用全部的流进行filter<br>split一个就可以满足</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;所有operator中的初始化如果写在构造函数当中就会出错，问题是序列化时的问题&lt;br&gt;flink从jobmanager序列化到各个 taskmanager时可能会出问题&lt;/p&gt;
&lt;p&gt;split组件功能可以减少多个flatmap的性能损失&lt;br&gt;多个flatmap数据每个
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：Jobs 和 Scheduling</title>
    <link href="https://yangyichao-mango.github.io/2019/11/20/apache-flink:study-flink-jobs-and-scheduling/"/>
    <id>https://yangyichao-mango.github.io/2019/11/20/apache-flink:study-flink-jobs-and-scheduling/</id>
    <published>2019-11-20T03:27:03.000Z</published>
    <updated>2019-11-20T03:35:25.990Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：Jobs 和 Scheduling</p><a id="more"></a><h1 id="Scheduling"><a href="#Scheduling" class="headerlink" title="Scheduling"></a><strong>Scheduling</strong></h1><p>Flink中的执行资源是通过 Task Slots 定义的。每个 TaskManager 都有一个或多个 Task Slots，每个 Slot 可以运行一个并行任务流。<br>并行任务流由多个连续的任务组成，例如 MapFunction 的第n个并行实例和 ReduceFunction 的第n个并行实例。请注意，Flink 经常并发地执行连续的任务：对于流式程序，基本上都会使用并行任务，对于批处理程序，也会经常使用并行任务。</p><p>下图说明了这一点。一个具有数据源、MapFunction 和 ReduceFunction 的程序。源函数和 MapFunction 的并行度为4，而 ReduceFunction 的并行度为3。流由 Source - Map - Reduce 组成。<br>在这个集群中，有两个 TaskManager，每个 TaskManager 有三个 slot，则程序将按如下所述执行。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：DataStream Api 中 State 和容错——State 的使用</title>
    <link href="https://yangyichao-mango.github.io/2019/11/19/apache-flink:study-flink-datastream-state-and-fault-tolerance-working-with-state/"/>
    <id>https://yangyichao-mango.github.io/2019/11/19/apache-flink:study-flink-datastream-state-and-fault-tolerance-working-with-state/</id>
    <published>2019-11-19T07:48:16.000Z</published>
    <updated>2019-11-20T03:29:23.261Z</updated>
    
    <content type="html"><![CDATA[<p>flink有两种基本的state，分别是Keyed State以及Operator State(non-keyed state)；其中Keyed State只能在KeyedStream上的functions及operators上使用；每个operator state会跟parallel operator中的一个实例绑定；Operator State支持parallelism变更时进行redistributing<br>Keyed State及Operator State都分别有managed及raw两种形式，managed由flink runtime来管理，由runtime负责encode及写入checkpoint；raw形式的state由operators自己管理，flink runtime无法了解该state的数据结构，将其视为raw bytes；所有的datastream function都可以使用managed state，而raw state一般仅限于自己实现operators来使用<br>stateful function可以通过CheckpointedFunction接口或者ListCheckpointed接口来使用managed operator state；CheckpointedFunction定义了snapshotState、initializeState两个方法；每当checkpoint执行的时候，snapshotState会被调用；而initializeState方法在每次用户定义的function初始化的时候(第一次初始化或者从前一次checkpoint recover的时候)被调用，该方法不仅可以用来初始化state，还可以用于处理state recovery的逻辑<br>对于manageed operator state，目前仅仅支持list-style的形式，即要求state是serializable objects的List结构，方便在rescale的时候进行redistributed；关于redistribution schemes的模式目前有两种，分别是Even-split redistribution(在restore/redistribution的时候每个operator仅仅得到整个state的sublist)及Union redistribution(在restore/redistribution的时候每个operator得到整个state的完整list)<br>FunctionSnapshotContext继承了ManagedSnapshotContext接口，它定义了getCheckpointId、getCheckpointTimestamp方法；FunctionInitializationContext继承了ManagedInitializationContext接口，它定义了isRestored、getOperatorStateStore、getKeyedStateStore方法，可以用来判断是否是在前一次execution的snapshot中restored，以及获取OperatorStateStore、KeyedStateStore对象</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Hadoop 学习：hdfs架构</title>
    <link href="https://yangyichao-mango.github.io/2019/11/13/apache-hadoop:study-hadoop-hdfs-design/"/>
    <id>https://yangyichao-mango.github.io/2019/11/13/apache-hadoop:study-hadoop-hdfs-design/</id>
    <published>2019-11-13T06:04:52.000Z</published>
    <updated>2019-11-13T07:24:26.271Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Hadoop 学习：hdfs架构</p><a id="more"></a><h1 id="文件系统-Namespace"><a href="#文件系统-Namespace" class="headerlink" title="文件系统 Namespace"></a><strong>文件系统 Namespace</strong></h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-hadoop:study-hadoop-hdfs-design/hdfs架构.png" alt="hdfs架构" title>                </div>                <div class="image-caption">hdfs架构</div>            </figure><p>HDFS支持传统的分层文件组织。用户或应用程序可以在这些目录中创建目录并存储文件。文件系统命名空间层次结构与大多数其他现有文件系统相似；可以创建和删除文件，将文件从一个目录移动到另一个目录，或者重命名文件。HDFS支持用户配额和访问权限。HDFS不支持硬链接或软链接。然而，HDFS体系结构并不排除实现这些特性。</p><p>虽然HDFS遵循文件系统的命名约定，但某些路径和名称（例如/.reserved和.snapshot）是保留的。透明加密和快照等功能使用保留路径。</p><p>NameNode维护文件系统名称空间。对文件系统命名空间或其属性的任何更改都由NameNode记录。应用程序可以指定HDFS应该维护的文件副本的数量。文件的副本数称为该文件的复制因子。此信息由NameNode存储。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/categories/Apache-Hadoop/"/>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/tags/Apache-Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：Table Api &amp; SQL</title>
    <link href="https://yangyichao-mango.github.io/2019/11/12/apache-flink:study-flink-table-api-and-sql/"/>
    <id>https://yangyichao-mango.github.io/2019/11/12/apache-flink:study-flink-table-api-and-sql/</id>
    <published>2019-11-12T01:51:01.000Z</published>
    <updated>2019-11-13T01:53:02.813Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：Table Api &amp; SQL</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：DataStream Api 中 Operators（算子）——Joining</title>
    <link href="https://yangyichao-mango.github.io/2019/11/11/apache-flink:study-flink-datastream-operators-joining/"/>
    <id>https://yangyichao-mango.github.io/2019/11/11/apache-flink:study-flink-datastream-operators-joining/</id>
    <published>2019-11-11T10:30:13.000Z</published>
    <updated>2019-11-13T02:14:11.303Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：DataStream Api 中 Operators（算子）——Joining</p><a id="more"></a><h1 id="Window-Join"><a href="#Window-Join" class="headerlink" title="Window Join"></a><strong>Window Join</strong></h1><p>Window Join 可以将两个流中相同key并且在同一个窗口中的元素进行链接。窗口可以使用 Window Assigner 进行定义，并且对来自不同的流的元素进行计算。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">stream.join(otherStream)</span><br><span class="line">    .where(&lt;KeySelector&gt;)</span><br><span class="line">    .equalTo(&lt;KeySelector&gt;)</span><br><span class="line">    .window(&lt;WindowAssigner&gt;)</span><br><span class="line">    .apply(&lt;JoinFunction&gt;)</span><br></pre></td></tr></table></figure><p>关于一些语义的解释：<br>1.两个流的成对组合的过程类似于 Inner Join，意味着如果一个流中的元素没有另一个流的元素要与之连接，则不会发出这些元素。</p><p>2.那些被连接的元素的时间戳是位于相应窗口中的最大时间戳。例如，以[5，10)为边界的窗口，则进行连接的元素的时间戳为9。</p><h2 id="Tumbling-Window-Join"><a href="#Tumbling-Window-Join" class="headerlink" title="Tumbling Window Join"></a>Tumbling Window Join</h2><p>执行 Tumbling Window Join，在相同 key，相同时间窗口内的元素会进行笛卡尔积组合，这种组合类似于 inner join，如果一个流的对应流的同一窗口中没有元素，则这个流的当前窗口数据不会发出去。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-flink-datastream-operators-joining/tumbling-window-join.svg" alt="tumbling-window-join" title>                </div>                <div class="image-caption">tumbling-window-join</div>            </figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"> </span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">DataStream&lt;Integer&gt; orangeStream = ...</span><br><span class="line">DataStream&lt;Integer&gt; greenStream = ...</span><br><span class="line"></span><br><span class="line">orangeStream.join(greenStream)</span><br><span class="line">    .where(&lt;KeySelector&gt;)</span><br><span class="line">    .equalTo(&lt;KeySelector&gt;)</span><br><span class="line">    .window(TumblingEventTimeWindows.of(Time.milliseconds(<span class="number">2</span>)))</span><br><span class="line">    .apply (<span class="keyword">new</span> JoinFunction&lt;Integer, Integer, String&gt; ()&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">join</span><span class="params">(Integer first, Integer second)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> first + <span class="string">","</span> + second;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><h2 id="Sliding-Window-Join"><a href="#Sliding-Window-Join" class="headerlink" title="Sliding Window Join"></a>Sliding Window Join</h2><p>执行 Sliding Window Join，具有公共 key 和公共滑动窗口的所有元素都作为成对组合进行连接，并传递给 JoinFunction 或 FlatJoinFunction。也是 inner join，当前流窗口匹配不到对应流窗口的元素则不会发送数据到下游！请注意，某些元素可能在一个滑动窗口中连接，在另一个滑动窗口中不会进行连接！</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-flink-datastream-operators-joining/sliding-window-join.svg" alt="sliding-window-join" title>                </div>                <div class="image-caption">sliding-window-join</div>            </figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">DataStream&lt;Integer&gt; orangeStream = ...</span><br><span class="line">DataStream&lt;Integer&gt; greenStream = ...</span><br><span class="line"></span><br><span class="line">orangeStream.join(greenStream)</span><br><span class="line">    .where(&lt;KeySelector&gt;)</span><br><span class="line">    .equalTo(&lt;KeySelector&gt;)</span><br><span class="line">    .window(SlidingEventTimeWindows.of(Time.milliseconds(<span class="number">2</span>) <span class="comment">/* size */</span>, Time.milliseconds(<span class="number">1</span>) <span class="comment">/* slide */</span>))</span><br><span class="line">    .apply (<span class="keyword">new</span> JoinFunction&lt;Integer, Integer, String&gt; ()&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">join</span><span class="params">(Integer first, Integer second)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> first + <span class="string">","</span> + second;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><h2 id="Session-Window-Join"><a href="#Session-Window-Join" class="headerlink" title="Session Window Join"></a>Session Window Join</h2><p>执行 Session Window Join，具有相同 key 的所有元素（当“组合”满足会话条件时）将以成对组合联接，并传递给JoinFunction或FlatJoinFunction。同样，也是 inner join，当前流窗口匹配不到对应流窗口的元素则不会发送数据到下游！</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-flink-datastream-operators-joining/session-window-join.svg" alt="session-window-join" title>                </div>                <div class="image-caption">session-window-join</div>            </figure><h1 id="Interval-Join"><a href="#Interval-Join" class="headerlink" title="Interval Join"></a><strong>Interval Join</strong></h1><p>interval join 用一个公共 key 连接两个流的元素（流A和流B），其中流B的元素具有与流A中元素的时间戳相对时间间隔内的时间戳，那么这个时间间隔内两个流的元素就会 join。</p><p>即：<strong>b.timestamp ∈ [a.timestamp + lowerBound; a.timestamp + upperBound] or a.timestamp + lowerBound &lt;= b.timestamp &lt;= a.timestamp + upperBound</strong></p><p>其中 lowerBound 和 upperBound 可正可负，只要 lowerBound &lt;= upperBound。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-flink-datastream-operators-joining/interval-join.svg" alt="interval-join" title>                </div>                <div class="image-caption">interval-join</div>            </figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.co.ProcessJoinFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">DataStream&lt;Integer&gt; orangeStream = ...</span><br><span class="line">DataStream&lt;Integer&gt; greenStream = ...</span><br><span class="line"></span><br><span class="line">orangeStream</span><br><span class="line">    .keyBy(&lt;KeySelector&gt;)</span><br><span class="line">    .intervalJoin(greenStream.keyBy(&lt;KeySelector&gt;))</span><br><span class="line">    .between(Time.milliseconds(-<span class="number">2</span>), Time.milliseconds(<span class="number">1</span>))</span><br><span class="line">    .process (<span class="keyword">new</span> ProcessJoinFunction&lt;Integer, Integer, String()&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Integer left, Integer right, Context ctx, Collector&lt;String&gt; out)</span> </span>&#123;</span><br><span class="line">            out.collect(first + <span class="string">","</span> + second);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>apache-flink:study-allowedLateness-and-maxOutOfOrderness</title>
    <link href="https://yangyichao-mango.github.io/2019/11/11/apache-flink:study-allowedLateness-and-maxOutOfOrderness/"/>
    <id>https://yangyichao-mango.github.io/2019/11/11/apache-flink:study-allowedLateness-and-maxOutOfOrderness/</id>
    <published>2019-11-11T02:32:17.000Z</published>
    <updated>2019-11-11T02:51:52.244Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Apache Kafka 学习：kafka在数据处理中的应用</title>
    <link href="https://yangyichao-mango.github.io/2019/11/10/apache-kafka:study-kafka-in-data-process/"/>
    <id>https://yangyichao-mango.github.io/2019/11/10/apache-kafka:study-kafka-in-data-process/</id>
    <published>2019-11-10T15:12:14.000Z</published>
    <updated>2019-11-11T01:42:14.697Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Kafka 学习：kafka在数据处理中的应用的一些个人理解。</p><a id="more"></a><h1 id="离线数据处理"><a href="#离线数据处理" class="headerlink" title="离线数据处理"></a><strong>离线数据处理</strong></h1><h2 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h2><p>在离线数据处理的过程中。<br>如果不使用消息队列，在并发量很小的情况下，所有的客户端数据日志直接向hdfs写数据暂时不会产生什么问题。<br>如果不使用消息队列，在并发量很大的情况下，向 hdfs 写数据就会出现问题，首先，向hdfs写数据会有锁竞争的情况，可能会导致大部分写请求很长时间得不到锁，导致大量请求延迟或者超时，这是不能接受的；并且 hdfs 作为文件系统不能承受太大的并发量，在并发很高的情况下，集群可能会崩溃。</p><h2 id="问题原因总结"><a href="#问题原因总结" class="headerlink" title="问题原因总结"></a>问题原因总结</h2><p>总的来说，在这种情况下，问题的根本在于高并发情况下，hdfs 作为文件系统不能承受高并发请求的问题。</p><h2 id="实施方案"><a href="#实施方案" class="headerlink" title="实施方案"></a>实施方案</h2><p>所以需要一种工具可以将客户端日志这种高并发请求转换为低并发的请求，这时候就可以使用kafka这样的消息队列，客户端的高并发请求直接写入 kafka，然后 hdfs 以低并发消费 kafka 中的数据。这样就解决了文件系统不能支持高并发的情况。并且由于 kafka 的HA特性，可以保证数据的正确性。</p><h2 id="kafka作用"><a href="#kafka作用" class="headerlink" title="kafka作用"></a>kafka作用</h2><p>将高并发请求以低并发方式处理。这种方式中解耦效果不明显，下面的实时数据处理使用到的解耦效果比较明显。</p><h1 id="实时数据处理"><a href="#实时数据处理" class="headerlink" title="实时数据处理"></a><strong>实时数据处理</strong></h1><h2 id="场景-1"><a href="#场景-1" class="headerlink" title="场景"></a>场景</h2><p>在实时数据处理的过程中。<br>如果不使用消息队列，上游数据写入到类似 flink 这样的实时处理引擎当中，flink处理完成后向下游 olap 引擎（druid，clickhouse等）或者hdfs，hive，es等的文件系统写数据时，就需要为每一种 olap 引擎开发一种 connector，这样的情况下，每出现一种 olap 引擎或者每当下游的 olap 引擎升级版本引入新特性时，就需要 flink 开发工程师开发一种新的 connector 或者跟随 olap 引擎的升级而升级自己的 connector，这样 flink 开发工程师的维护成本之后就会特别高。<br>这里有同学可能会说可以在数据处理的过程中使用下游 olap 等的引擎提供的 sdk，这种方法是可以的，但是实时处理打不风情况下并发量很高，olap 引擎提供的 sdk 应对这种高并发的场景可能会有很多问题。</p><h2 id="问题原因总结-1"><a href="#问题原因总结-1" class="headerlink" title="问题原因总结"></a>问题原因总结</h2><p>问题的根本在于实时处理引擎和下游之间的耦合问题，这就需要一种HA的中间件来将各个模块进行解耦，kafka这样的消息队列可以很好的解决这中模块之间高度耦合的情况。</p><h2 id="实施方案-1"><a href="#实施方案-1" class="headerlink" title="实施方案"></a>实施方案</h2><p>在 flink 和 druid中间使用 kafka 进行解耦，让 flink 向 kafka 生成数据，druid 消费 kafka 的数据。<br>这样就使得 flink 可以只开发和维护一套针对于 kafka 的 connector，druid也只用开发和维护一套针对 kafka 的 connector，这样无论是实时处理引擎的升级或替换，或者是实时处理引擎下游的模块的升级或替换，都不会互相影响，并且这些模块的工程师只需要对消息队列的 connector 进行维护即可，并且可以根据其特性进行更好的优化。</p><h2 id="kafka作用-1"><a href="#kafka作用-1" class="headerlink" title="kafka作用"></a>kafka作用</h2><p>模块之间的解耦。让各个模块各司其职。<br>拓展：<br>1.Java 虚拟机在 Java 语言和各个系统之间的作用。<br>2.Sql 进行三范式优化，不需要将所有数据都放在一张表当中，将n对n的表拆分出一张维表进行解耦。将维度拆分为维表可以减少数据量，更好划分和使用维度数据。<br>3.经典网络五层模型，划分为五层，则在每一层中更新或者新建协议栈只需要对上下层进行兼容即可，不需要对整个网络架构模型做调整。<br>4.Maven multiModule 划分。<br>5.springMvc。<br>等等。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Kafka" scheme="https://yangyichao-mango.github.io/categories/Apache-Kafka/"/>
    
    
      <category term="Apache Kafka" scheme="https://yangyichao-mango.github.io/tags/Apache-Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：DataStream Api 中 Operators（算子）——窗口</title>
    <link href="https://yangyichao-mango.github.io/2019/11/09/apache-flink:study-flink-datastream-operators-windows/"/>
    <id>https://yangyichao-mango.github.io/2019/11/09/apache-flink:study-flink-datastream-operators-windows/</id>
    <published>2019-11-09T14:48:53.000Z</published>
    <updated>2019-11-11T10:45:12.300Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：DataStream Api 中 Operators（算子）——窗口</p><a id="more"></a><p>Keyed Windows</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">       .keyBy(...)               &lt;-  keyed versus non-keyed windows</span><br><span class="line">       .window(...)              &lt;-  required: <span class="string">"assigner"</span></span><br><span class="line">      [.trigger(...)]            &lt;-  optional: <span class="string">"trigger"</span> (<span class="keyword">else</span> <span class="keyword">default</span> trigger)</span><br><span class="line">      [.evictor(...)]            &lt;-  optional: <span class="string">"evictor"</span> (<span class="keyword">else</span> no evictor)</span><br><span class="line">      [.allowedLateness(...)]    &lt;-  optional: <span class="string">"lateness"</span> (<span class="keyword">else</span> zero)</span><br><span class="line">      [.sideOutputLateData(...)] &lt;-  optional: <span class="string">"output tag"</span> (<span class="keyword">else</span> no side output <span class="keyword">for</span> late data)</span><br><span class="line">       .reduce/aggregate/fold/apply()      &lt;-  required: <span class="string">"function"</span></span><br><span class="line">      [.getSideOutput(...)]      &lt;-  optional: <span class="string">"output tag"</span></span><br></pre></td></tr></table></figure><p>Non-Keyed Windows</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">       .windowAll(...)           &lt;-  required: <span class="string">"assigner"</span></span><br><span class="line">      [.trigger(...)]            &lt;-  optional: <span class="string">"trigger"</span> (<span class="keyword">else</span> <span class="keyword">default</span> trigger)</span><br><span class="line">      [.evictor(...)]            &lt;-  optional: <span class="string">"evictor"</span> (<span class="keyword">else</span> no evictor)</span><br><span class="line">      [.allowedLateness(...)]    &lt;-  optional: <span class="string">"lateness"</span> (<span class="keyword">else</span> zero)</span><br><span class="line">      [.sideOutputLateData(...)] &lt;-  optional: <span class="string">"output tag"</span> (<span class="keyword">else</span> no side output <span class="keyword">for</span> late data)</span><br><span class="line">       .reduce/aggregate/fold/apply()      &lt;-  required: <span class="string">"function"</span></span><br><span class="line">      [.getSideOutput(...)]      &lt;-  optional: <span class="string">"output tag"</span></span><br></pre></td></tr></table></figure><h1 id="Window-生命周期"><a href="#Window-生命周期" class="headerlink" title="Window 生命周期"></a><strong>Window 生命周期</strong></h1><p>简而言之，当属于该窗口的第一个元素到达时，将会创建一个窗口，并且当时间（Event Time 或者 Processing Time）超过其结束时间戳加上用户指定的允许延迟时间时，将完全删除该窗口，<strong>注意窗口都是左开右闭，比如：[0, 5)</strong>。<br>Flink 保证只删除基于时间的窗口，而不删除其他类型的窗口，例如 Global Window。例如，使用基于事件时间的窗口，并且创建一个窗口大小为5分钟的滚动（Tumbing）窗口，并且允许延迟1分钟。当时间戳属于12:00到12:05之间的第一个元素到达时，Flink 将创建一个新窗口，当 Watermark 通过12:06时间戳时，就会把这个窗口删除。</p><p>此外，每个窗口都包含一个 Trigger 和一个函数（ProcessWindowFunction, ReduceFunction, AggregateFunction or FoldFunction）。函数包含了要应用于窗口内容的计算，而 Trigger 制定了什么情况下才触发执行这些函数。比如，触发策略可能类似于“当窗口中的元素数超过4时”或“当 WaterMark 通过窗口结束时”进行触发。Trigger 还可以决定什么时候删除窗口中的元素。</p><p>除上述内容外，您还可以指定一个 Evictor，该 Evictor 将能够在 Trigger 触发后、应用函数之前和/或之后从窗口中移除元素。</p><p>下面例子中的窗口都是按照 Event Time 或者 Processing Time进行指定。</p><h1 id="Keyed-vs-Non-Keyed-Windows"><a href="#Keyed-vs-Non-Keyed-Windows" class="headerlink" title="Keyed vs Non-Keyed Windows"></a><strong>Keyed vs Non-Keyed Windows</strong></h1><p>首先要指定的是是否应该为流设置 key。使用keyBy（…）可以将无限流拆分为逻辑 keyed 流。</p><p>在 Keyed Stream 的情况下，传入 event 的任何属性都可以用作键。拥有一个 Keyed Stream 将允许您的窗口计算由多个任务并行执行，因为每个逻辑 Keyed Stream 都可以独立于其他任务进行处理。所有引用同一个键的 event 都将被发送到同一个并行任务（通过 partitioner 完成）。</p><p>如果不是 Keyed Stream，则不会将原始流拆分为多个逻辑流，所有窗口逻辑将由单个任务执行，即并行度为1。</p><h1 id="Tumbling-Windows"><a href="#Tumbling-Windows" class="headerlink" title="Tumbling Windows"></a><strong>Tumbling Windows</strong></h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-flink-datastream-operators-windows/滚动窗口.svg" alt="滚动窗口" title>                </div>                <div class="image-caption">滚动窗口</div>            </figure><p>滚动窗口，如果你指定的滚动窗口大小为一天计算一次，并且你需要更具你本地的时间的 00:00:00 开始，则必须按照时区来指定窗口。<br>可以看到上图中，无论是多少个 key，每个 key 的窗口的起始和截止时间都相同。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Creates a new &#123;<span class="doctag">@code</span> TumblingEventTimeWindows&#125; &#123;<span class="doctag">@link</span> WindowAssigner&#125; that assigns</span></span><br><span class="line"><span class="comment"> * elements to time windows based on the element timestamp and offset.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 可以根据 时间戳 以及 偏移量 来指定 窗口的范围</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * &lt;p&gt;For example, if you want window a stream by hour,but window begins at the 15th minutes</span></span><br><span class="line"><span class="comment"> * of each hour, you can use &#123;<span class="doctag">@code</span> of(Time.hours(1),Time.minutes(15))&#125;,then you will get</span></span><br><span class="line"><span class="comment"> * time windows start at 0:15:00,1:15:00,2:15:00,etc.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 比如，如果需要一个窗口大小为一个小时，从每个小时的第15分钟开始计数的窗口，则可以使用下面的代码实现</span></span><br><span class="line"><span class="comment"> * of(Time.hours(1),Time.minutes(15))</span></span><br><span class="line"><span class="comment"> * 这样获得的窗口就是 0:15:00，1:15:00，2:15:00 ...</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;Rather than that,if you are living in somewhere which is not using UTC±00:00 time,</span></span><br><span class="line"><span class="comment"> * such as China which is using UTC+08:00,and you want a time window with size of one day,</span></span><br><span class="line"><span class="comment"> * and window begins at every 00:00:00 of local time,you may use &#123;<span class="doctag">@code</span> of(Time.days(1),Time.hours(-8))&#125;.</span></span><br><span class="line"><span class="comment"> * The parameter of offset is &#123;<span class="doctag">@code</span> Time.hours(-8))&#125; since UTC+08:00 is 8 hours earlier than UTC time.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 除此之外，如果您的时区不是 UTC±00:00 时间，比如在 中国（时区是 UTC+08:00），并且你需要一个一天大小的窗口，</span></span><br><span class="line"><span class="comment"> * 并且窗口时间是本地 00:00:00开始，则可以使用下面的代码实现</span></span><br><span class="line"><span class="comment"> * of(Time.days(1), Time.hours(-8))</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> size The size of the generated windows.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> offset The offset which window start would be shifted by.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> The time policy.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> TumblingEventTimeWindows <span class="title">of</span><span class="params">(Time size, Time offset)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> TumblingEventTimeWindows(size.toMilliseconds(), offset.toMilliseconds());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// daily tumbling event-time windows offset by -8 hours.</span></span><br><span class="line">    input</span><br><span class="line">        .keyBy(&lt;key selector&gt;)</span><br><span class="line">        .window(TumblingEventTimeWindows.of(Time.days(<span class="number">1</span>), Time.hours(-<span class="number">8</span>)))</span><br><span class="line">        .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Sliding-Windows"><a href="#Sliding-Windows" class="headerlink" title="Sliding Windows"></a><strong>Sliding Windows</strong></h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-flink-datastream-operators-windows/滑动窗口.svg" alt="滑动窗口" title>                </div>                <div class="image-caption">滑动窗口</div>            </figure><p>滑动窗口，如果你指定窗口大小和滑动步长一样，那么和滚动窗口的作用一样，滑动窗口的一个明显的特征就是：<strong>窗口可能会重叠，即同一个元素可能会属于不同的窗口</strong>。</p><p>和滚动窗口相同，如果你指定的滚动窗口大小为一天计算一次，你需要更具你本地的时间的 00:00:00 开始，则必须按照时区来指定窗口。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// sliding processing-time windows offset by -8 hours</span></span><br><span class="line">    input</span><br><span class="line">        .keyBy(&lt;key selector&gt;)</span><br><span class="line">        .window(SlidingProcessingTimeWindows.of(Time.hours(<span class="number">12</span>), Time.hours(<span class="number">1</span>), Time.hours(-<span class="number">8</span>)))</span><br><span class="line">        .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Session-Windows"><a href="#Session-Windows" class="headerlink" title="Session Windows"></a><strong>Session Windows</strong></h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-flink-datastream-operators-windows/会话窗口.svg" alt="会话窗口" title>                </div>                <div class="image-caption">会话窗口</div>            </figure><p>会话窗口，根据会话来指定窗口，与滚动和滑动窗口相比，会话窗口不重叠，并且没有固定的开始和结束时间。会话窗口可以指定静态指定会话间隔，或者可以让用户动态指定会话间隔。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;T&gt; input = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">// event-time session windows with static gap</span></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(EventTimeSessionWindows.withGap(Time.minutes(<span class="number">10</span>)))</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br><span class="line">    </span><br><span class="line"><span class="comment">// event-time session windows with dynamic gap</span></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(EventTimeSessionWindows.withDynamicGap((element) -&gt; &#123;</span><br><span class="line">        <span class="comment">// determine and return session gap</span></span><br><span class="line">    &#125;))</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br></pre></td></tr></table></figure><h1 id="Global-Windows"><a href="#Global-Windows" class="headerlink" title="Global Windows"></a><strong>Global Windows</strong></h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-flink-datastream-operators-windows/全局窗口.svg" alt="全局窗口" title>                </div>                <div class="image-caption">全局窗口</div>            </figure><p>全局窗口（即无窗口），代表所有元素斗数以一个全局窗口，如果你不指定 Trigger，那么永远也不会生产出数据，因为全局窗口没有窗口开始和窗口结束的概念。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;T&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">    .keyBy(<span class="xml"><span class="tag">&lt;<span class="name">key</span> <span class="attr">selector</span>&gt;</span>)</span></span><br><span class="line"><span class="xml">    .window(GlobalWindows.create())</span></span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br></pre></td></tr></table></figure><h1 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a><strong>窗口函数</strong></h1><p>在定义 window assigner 之后，我们需要指定要在每个窗口上执行的计算。当系统确定窗口准备好处理时（Trigger决定），这些窗口函数就可以用于处理每个（Keyed / Non-Keyed）窗口的元素。</p><p>窗口函数可以是ReduceFunction、AggregateFunction、FoldFunction或ProcessWindowFunction之一。前两个函数执行起来会更高效，因为 Flink 可以在每个窗口中的元素到达时递增地聚合元素。ProcessWindowFunction获取包含在窗口中的所有元素的Iterable，以及有关元素所属窗口的其他元信息。</p><p>使用ProcessWindowFunction的窗口函数不能像其他函数那样高效地执行，因为Flink在调用函数之前必须在内部缓冲窗口的所有元素。这可以通过将ProcessWindowFunction与ReduceFunction、AggregateFunction或FoldFunction组合使用来提高效率，从而使得其他窗口元数据或者窗口元素的进行增量聚合。</p><h2 id="ReduceFunction"><a href="#ReduceFunction" class="headerlink" title="ReduceFunction"></a>ReduceFunction</h2><p>变量：两个输入生成一个输出，三个变量的类型必须相同。<br>触发时间：在每个窗口的元素到来的时候进行增量聚合。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(&lt;window assigner&gt;)</span><br><span class="line">    .reduce(<span class="keyword">new</span> ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt; &#123;</span><br><span class="line">      <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Long&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Long&gt; v1, Tuple2&lt;String, Long&gt; v2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(v1.f0, v1.f1 + v2.f1);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><h2 id="AggregateFunction"><a href="#AggregateFunction" class="headerlink" title="AggregateFunction"></a>AggregateFunction</h2><p>AggregateFunction 是 ReduceFunction 的一个扩展版本。</p><p>变量：三个变量，一个是输入，一个是 accumulator 累加器，还有一个是输出，三个变量的类型可以不同。<br>触发时间：在每个窗口的元素到来的时候进行增量聚合。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The accumulator is used to keep a running sum and a count. The &#123;<span class="doctag">@code</span> getResult&#125; method</span></span><br><span class="line"><span class="comment"> * computes the average.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">AverageAggregate</span></span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;, <span class="title">Tuple2</span>&lt;<span class="title">Long</span>, <span class="title">Long</span>&gt;, <span class="title">Double</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">0L</span>, <span class="number">0L</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">add</span><span class="params">(Tuple2&lt;String, Long&gt; value, Tuple2&lt;Long, Long&gt; accumulator)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(accumulator.f0 + value.f1, accumulator.f1 + <span class="number">1L</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Double <span class="title">getResult</span><span class="params">(Tuple2&lt;Long, Long&gt; accumulator)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> ((<span class="keyword">double</span>) accumulator.f0) / accumulator.f1;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">merge</span><span class="params">(Tuple2&lt;Long, Long&gt; a, Tuple2&lt;Long, Long&gt; b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(a.f0 + b.f0, a.f1 + b.f1);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(&lt;window assigner&gt;)</span><br><span class="line">    .aggregate(<span class="keyword">new</span> AverageAggregate());</span><br></pre></td></tr></table></figure><h2 id="FoldFunction"><a href="#FoldFunction" class="headerlink" title="FoldFunction"></a>FoldFunction</h2><p>FoldFunction 是 AggregateFunction 的一个简易版本。</p><p>变量：三个个变量，一个是输出值的初始化值，一个是输入，还有一个是输出，三个变量中输入和输出的类型可以不同，但是输出和输出初始化值必须相同。<br>触发时间：在每个窗口的元素到来的时候进行增量聚合。</p><p>FoldFunction 指定如何将窗口的输入元素与输出类型的元素组合。对添加到窗口的每个元素和当前输出值增量调用 FoldFunction。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(&lt;window assigner&gt;)</span><br><span class="line">    .fold(<span class="string">""</span>, <span class="keyword">new</span> FoldFunction&lt;Tuple2&lt;String, Long&gt;, String&gt;&gt; &#123;</span><br><span class="line">       <span class="function"><span class="keyword">public</span> String <span class="title">fold</span><span class="params">(String acc, Tuple2&lt;String, Long&gt; value)</span> </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> acc + value.f1;</span><br><span class="line">       &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><h2 id="ProcessWindowFunction"><a href="#ProcessWindowFunction" class="headerlink" title="ProcessWindowFunction"></a>ProcessWindowFunction</h2><p>ProcessWindowFunction 可以得到一个包含窗口的所有元素的迭代器，以及一个访问时间和状态信息的上下文对象，使得它能够提供比其他窗口函数更好的灵活性。<br>但是这是以性能和资源消耗为代价的，因为元素不能增量聚合，而是需要在内部缓冲，直到窗口可以处理为止。</p><p>变量：四个变量，一个是窗口的key，一个是包含了窗口信息的上下文，一个是窗口内所有元素的迭代器，一个是输出数据的收集器。<br>触发时间：窗口内有数据并且 Watermark 到达了窗口结束时间时触发。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">IN</span>, <span class="title">OUT</span>, <span class="title">KEY</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">implements</span> <span class="title">Function</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Evaluates the window and outputs none or several elements.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key The key for which this window is evaluated.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context The context in which the window is being evaluated.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> elements The elements in the window being evaluated.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> out A collector for emitting elements.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception The function may throw exceptions to fail the program and trigger recovery.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            KEY key,</span></span></span><br><span class="line"><span class="function"><span class="params">            Context context,</span></span></span><br><span class="line"><span class="function"><span class="params">            Iterable&lt;IN&gt; elements,</span></span></span><br><span class="line"><span class="function"><span class="params">            Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * The context holding window metadata.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Context</span> <span class="keyword">implements</span> <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span> </span>&#123;</span><br><span class="line">       <span class="comment">/**</span></span><br><span class="line"><span class="comment">        * Returns the window that is being evaluated.</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">       <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> W <span class="title">window</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">       <span class="comment">/** Returns the current processing time. */</span></span><br><span class="line">       <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">long</span> <span class="title">currentProcessingTime</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">       <span class="comment">/** Returns the current event-time watermark. */</span></span><br><span class="line">       <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">long</span> <span class="title">currentWatermark</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">       <span class="comment">/**</span></span><br><span class="line"><span class="comment">        * State accessor for per-key and per-window state.</span></span><br><span class="line"><span class="comment">        *</span></span><br><span class="line"><span class="comment">        * &lt;p&gt;&lt;b&gt;<span class="doctag">NOTE:</span>&lt;/b&gt;If you use per-window state you have to ensure that you clean it up</span></span><br><span class="line"><span class="comment">        * by implementing &#123;<span class="doctag">@link</span> ProcessWindowFunction#clear(Context)&#125;.</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">       <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> KeyedStateStore <span class="title">windowState</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">       <span class="comment">/**</span></span><br><span class="line"><span class="comment">        * State accessor for per-key global state.</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">       <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> KeyedStateStore <span class="title">globalState</span><span class="params">()</span></span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">  .keyBy(t -&gt; t.f0)</span><br><span class="line">  .timeWindow(Time.minutes(<span class="number">5</span>))</span><br><span class="line">  .process(<span class="keyword">new</span> MyProcessWindowFunction());</span><br><span class="line"></span><br><span class="line"><span class="comment">/* ... */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProcessWindowFunction</span> </span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;, <span class="title">String</span>, <span class="title">String</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String key, Context context, Iterable&lt;Tuple2&lt;String, Long&gt;&gt; input, Collector&lt;String&gt; out)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (Tuple2&lt;String, Long&gt; in: input) &#123;</span><br><span class="line">      count++;</span><br><span class="line">    &#125;</span><br><span class="line">    out.collect(<span class="string">"Window: "</span> + context.window() + <span class="string">"count: "</span> + count);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用 ProcessWindowFunction 进行元素 count 是非常低效的，下面会讲到怎样将 ReduceFunction 或者 AggregateFunction 与 ProcessWindowFunction 结合使用将增量的数据的与 ProcessWindowFunction 配合进行使用。</p><p>为什么需要用到 ProcessWindowFunction：如果必要的话，一般的业务逻辑是没必要使用到 ProcessWindowFunction 的，但是有的需求需要获取到当前元素时间戳，窗口开始结束等等的信息，这时就需要使用 ProcessWindowFunction 来获取这些信息了。</p><h3 id="ProcessWindowFunction-与-ReduceFunction"><a href="#ProcessWindowFunction-与-ReduceFunction" class="headerlink" title="ProcessWindowFunction 与 ReduceFunction"></a>ProcessWindowFunction 与 ReduceFunction</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;SensorReading&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">  .keyBy(&lt;key selector&gt;)</span><br><span class="line">  .timeWindow(&lt;duration&gt;)</span><br><span class="line">  .reduce(<span class="keyword">new</span> MyReduceFunction(), <span class="keyword">new</span> MyProcessWindowFunction());</span><br><span class="line"></span><br><span class="line"><span class="comment">// Function definitions</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReduceFunction</span> <span class="keyword">implements</span> <span class="title">ReduceFunction</span>&lt;<span class="title">SensorReading</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> SensorReading <span class="title">reduce</span><span class="params">(SensorReading r1, SensorReading r2)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> r1.value() &gt; r2.value() ? r2 : r1;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProcessWindowFunction</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">SensorReading</span>, <span class="title">Tuple2</span>&lt;<span class="title">Long</span>, <span class="title">SensorReading</span>&gt;, <span class="title">String</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String key,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Context context,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Iterable&lt;SensorReading&gt; minReadings,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Collector&lt;Tuple2&lt;Long, SensorReading&gt;&gt; out)</span> </span>&#123;</span><br><span class="line">      SensorReading min = minReadings.iterator().next();</span><br><span class="line">      out.collect(<span class="keyword">new</span> Tuple2&lt;Long, SensorReading&gt;(context.window().getStart(), min));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="ProcessWindowFunction-与-AggregateFunction"><a href="#ProcessWindowFunction-与-AggregateFunction" class="headerlink" title="ProcessWindowFunction 与 AggregateFunction"></a>ProcessWindowFunction 与 AggregateFunction</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">  .keyBy(&lt;key selector&gt;)</span><br><span class="line">  .timeWindow(&lt;duration&gt;)</span><br><span class="line">  .aggregate(<span class="keyword">new</span> AverageAggregate(), <span class="keyword">new</span> MyProcessWindowFunction());</span><br><span class="line"></span><br><span class="line"><span class="comment">// Function definitions</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The accumulator is used to keep a running sum and a count. The &#123;<span class="doctag">@code</span> getResult&#125; method</span></span><br><span class="line"><span class="comment"> * computes the average.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">AverageAggregate</span></span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;, <span class="title">Tuple2</span>&lt;<span class="title">Long</span>, <span class="title">Long</span>&gt;, <span class="title">Double</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">0L</span>, <span class="number">0L</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">add</span><span class="params">(Tuple2&lt;String, Long&gt; value, Tuple2&lt;Long, Long&gt; accumulator)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(accumulator.f0 + value.f1, accumulator.f1 + <span class="number">1L</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Double <span class="title">getResult</span><span class="params">(Tuple2&lt;Long, Long&gt; accumulator)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> ((<span class="keyword">double</span>) accumulator.f0) / accumulator.f1;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">merge</span><span class="params">(Tuple2&lt;Long, Long&gt; a, Tuple2&lt;Long, Long&gt; b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(a.f0 + b.f0, a.f1 + b.f1);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProcessWindowFunction</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">Double</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Double</span>&gt;, <span class="title">String</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String key,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Context context,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Iterable&lt;Double&gt; averages,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Collector&lt;Tuple2&lt;String, Double&gt;&gt; out)</span> </span>&#123;</span><br><span class="line">      Double average = averages.iterator().next();</span><br><span class="line">      out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(key, average));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="ProcessWindowFunction-与-FoldFunction"><a href="#ProcessWindowFunction-与-FoldFunction" class="headerlink" title="ProcessWindowFunction 与 FoldFunction"></a>ProcessWindowFunction 与 FoldFunction</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;SensorReading&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">  .keyBy(&lt;key selector&gt;)</span><br><span class="line">  .timeWindow(&lt;duration&gt;)</span><br><span class="line">  .fold(<span class="keyword">new</span> Tuple3&lt;String, Long, Integer&gt;(<span class="string">""</span>,<span class="number">0L</span>, <span class="number">0</span>), <span class="keyword">new</span> MyFoldFunction(), <span class="keyword">new</span> MyProcessWindowFunction())</span><br><span class="line"></span><br><span class="line"><span class="comment">// Function definitions</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyFoldFunction</span></span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">FoldFunction</span>&lt;<span class="title">SensorReading</span>, <span class="title">Tuple3</span>&lt;<span class="title">String</span>, <span class="title">Long</span>, <span class="title">Integer</span>&gt; &gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Tuple3&lt;String, Long, Integer&gt; <span class="title">fold</span><span class="params">(Tuple3&lt;String, Long, Integer&gt; acc, SensorReading s)</span> </span>&#123;</span><br><span class="line">      Integer cur = acc.getField(<span class="number">2</span>);</span><br><span class="line">      acc.setField(cur + <span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line">      <span class="keyword">return</span> acc;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProcessWindowFunction</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">Tuple3</span>&lt;<span class="title">String</span>, <span class="title">Long</span>, <span class="title">Integer</span>&gt;, <span class="title">Tuple3</span>&lt;<span class="title">String</span>, <span class="title">Long</span>, <span class="title">Integer</span>&gt;, <span class="title">String</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String key,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Context context,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Iterable&lt;Tuple3&lt;String, Long, Integer&gt;&gt; counts,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Collector&lt;Tuple3&lt;String, Long, Integer&gt;&gt; out)</span> </span>&#123;</span><br><span class="line">    Integer count = counts.iterator().next().getField(<span class="number">2</span>);</span><br><span class="line">    out.collect(<span class="keyword">new</span> Tuple3&lt;String, Long, Integer&gt;(key, context.window().getEnd(),count));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="ProcessWindowFunction-中使用-state"><a href="#ProcessWindowFunction-中使用-state" class="headerlink" title="ProcessWindowFunction 中使用 state"></a>ProcessWindowFunction 中使用 state</h3><h2 id="WindowFunction（遗留）"><a href="#WindowFunction（遗留）" class="headerlink" title="WindowFunction（遗留）"></a>WindowFunction（遗留）</h2><p>在一些可以使用 ProcessWindowFunction 的地方，你也可以使用 WindowFunction，这是较旧版本的 ProcessWindowFunction，它提供的上下文信息较少，并且没有一些高级功能，例如 per-window keyed state。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">WindowFunction</span>&lt;<span class="title">IN</span>, <span class="title">OUT</span>, <span class="title">KEY</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">extends</span> <span class="title">Function</span>, <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Evaluates the window and outputs none or several elements.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> key The key for which this window is evaluated.</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> window The window that is being evaluated.</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> input The elements in the window being evaluated.</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> out A collector for emitting elements.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@throws</span> Exception The function may throw exceptions to fail the program and trigger recovery.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">apply</span><span class="params">(KEY key, W window, Iterable&lt;IN&gt; input, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(&lt;window assigner&gt;)</span><br><span class="line">    .apply(<span class="keyword">new</span> MyWindowFunction());</span><br></pre></td></tr></table></figure><h1 id="Triggers"><a href="#Triggers" class="headerlink" title="Triggers"></a><strong>Triggers</strong></h1><p>触发器决定了窗口函数什么时候处理窗口中的数据。每一个 WindowAssigner 都会带有一个默认的 Trigger，如果默认的 Trigger 不符合需求，你可以使用 trigger(…) 指定你需要的触发器。</p><p>一个 Trigger 接口有五个方法，可以通过编写函数指定如何对不同的 event 做出相应。</p><table>    <thead>        <tr>            <th style="width: 10%">Function</th>            <th style="width: 20%">作用</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;"><strong>TriggerResult onElement(T element, long timestamp, W window, TriggerContext ctx)</strong>            </td>            <td>每向窗口添加一个元素时触发一次。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>TriggerResult onEventTime(long time, W window, TriggerContext ctx)</strong>            </td>            <td>Event Time timer 触发时调用。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>TriggerResult onProcessingTime(long time, W window, TriggerContext ctx)</strong>            </td>            <td>Processing Time timer 调用时触发。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>void onMerge(W window, OnMergeContext ctx)</strong>            </td>            <td>方法与有状态 Trigger 相关，并在两个触发器的相应窗口合并时合并它们的状态，例如在使用会话窗口时（会话窗口每添加一个元素就会产生一个窗口）。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>void clear(W window, TriggerContext ctx)</strong>            </td>            <td>将当前窗口的 state 清除。            </td>        </tr>    </tbody></table><p>前三个函数通过 TriggerResult 决定如何处理它们的调用事件。</p><table>    <thead>        <tr>            <th style="width: 10%">TriggerResult</th>            <th style="width: 20%">作用</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;"><strong>TriggerResult.CONTINUE</strong>            </td>            <td>什么都不做。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>TriggerResult.FIRE</strong>            </td>            <td>触发计算。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>TriggerResult.PURGE</strong>            </td>            <td>清除窗口内的元素。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>TriggerResult.FIRE_AND_PURGE</strong>            </td>            <td>触发计算，并且在此之后清除窗口内的元素。            </td>        </tr>    </tbody></table><h2 id="触发运算并且清除元素"><a href="#触发运算并且清除元素" class="headerlink" title="触发运算并且清除元素"></a>触发运算并且清除元素</h2><h2 id="WindowAssigners-的默认-Triggers"><a href="#WindowAssigners-的默认-Triggers" class="headerlink" title="WindowAssigners 的默认 Triggers"></a>WindowAssigners 的默认 Triggers</h2><p>很多 WindowAssigners 的默认 Triggers是适用于很多场景的。例如，所有的 event-time window assigners 都将 EventTimeTrigger 作为默认的 Trigger。这个 Trigger 的作用就是当 Watermark 到达了窗口结束时间时就触发。<br><strong>提示1：GlobalWindow 的默认 Trigger 是永远不会触发的 NeverTrigger。</strong><br><strong>提示2：通过使用 trigger() 指定触发器，您将覆盖 WindowAssigner 的默认触发器。例如，如果为 TumblingEventTimeWindows 指定 CountTrigger，则不会再根据时间进度而仅按 count 触发窗口。</strong></p><h2 id="通用的-Triggers"><a href="#通用的-Triggers" class="headerlink" title="通用的 Triggers"></a>通用的 Triggers</h2><table>    <thead>        <tr>            <th style="width: 10%">Trigger</th>            <th style="width: 20%">作用</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;"><strong>EventTimeTrigger</strong>            </td>            <td>根据由 Watermark 计算的 Event Time 进度触发。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>ProcessingTimeTrigger</strong>            </td>            <td>根据 Processing Time 触发。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>CountTrigger</strong>            </td>            <td>在窗口中的元素数超过给定限额时触发。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>PurgingTrigger</strong>            </td>            <td>将另一个 Trigger 作为参数，并将其转换为清除触发器。            </td>        </tr>    </tbody></table><h1 id="Evictors"><a href="#Evictors" class="headerlink" title="Evictors"></a><strong>Evictors</strong></h1><p>Flink 的窗口模型中允许指定除 WindowAssigner 和 Trigger 之外的可选逐出器（Evictor）。可以使用exictor(…)方法指定。Exictor 能够在触发器触发之后，并在使用窗口函数之前或者之后移除元素。为此，逐出器接口有两个方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Optionally evicts elements. Called before windowing function.</span></span><br><span class="line"><span class="comment"> * 在执行窗口函数之前执行。</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> elements The elements currently in the pane.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> size The current number of elements in the pane.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> window The &#123;<span class="doctag">@link</span> Window&#125;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> evictorContext The context for the Evictor</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">evictBefore</span><span class="params">(Iterable&lt;TimestampedValue&lt;T&gt;&gt; elements, <span class="keyword">int</span> size, W window, EvictorContext evictorContext)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Optionally evicts elements. Called after windowing function.</span></span><br><span class="line"><span class="comment"> * 在执行窗口函数之后执行。</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> elements The elements currently in the pane.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> size The current number of elements in the pane.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> window The &#123;<span class="doctag">@link</span> Window&#125;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> evictorContext The context for the Evictor</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">evictAfter</span><span class="params">(Iterable&lt;TimestampedValue&lt;T&gt;&gt; elements, <span class="keyword">int</span> size, W window, EvictorContext evictorContext)</span></span>;</span><br></pre></td></tr></table></figure><table>    <thead>        <tr>            <th style="width: 10%">Evictor</th>            <th style="width: 20%">作用</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;"><strong>CountEvictor</strong>            </td>            <td>保持窗口内元素数量符合用户指定数量，如果多于用户指定的数量，从窗口缓冲区的开头丢弃剩余的元素。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>DeltaEvictor</strong>            </td>            <td>使用 DeltaFunction 和一个阈值，计算窗口缓冲区中的最后一个元素与其余每个元素之间的 delta 值，并删除 delta 值大于或等于阈值的元素。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>TimeEvictor</strong>            </td>            <td>以毫秒为单位的时间间隔作为参数，对于给定的窗口，找到元素中的最大的时间戳max_ts，并删除时间戳小于max_ts - interval的所有元素。            </td>        </tr>    </tbody></table><p>默认情况下，都只会在执行 WindowFunction 之前执行 Evictor。</p><p><strong>提示：Flink 不能保证窗口中元素的顺序。这意味着尽管逐出器可能会从窗口的开头移除元素，但这些元素不一定是最先到达或最后到达的元素。</strong></p><h1 id="Allowed-Lateness"><a href="#Allowed-Lateness" class="headerlink" title="Allowed Lateness"></a><strong>Allowed Lateness</strong></h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>使用 Event Time 窗口时，可能会发生元素到达晚的情况，即 Flink 用于跟踪 Event Time 进度的 Watermark 已超过元素所属窗口的结束时间戳。</p><p>默认情况下，当发现 Watermark 已经超过到达的元素所属的窗口结束时间时，将删除这个延迟元素。但是 Flink 可以给窗口算子指定一个最大允许延迟时间。Allowed lateness 指定元素在被删除之前可以延迟多少时间，其默认值为0。</p><p>在迟到元素到达时，如果 Watermark 大于其所属窗口的结束时间时，并且 Watermark 小于窗口结束时间加上 allowed lateness，这个迟到的元素仍然可以被加到这个窗口内进行运算。有的触发器会出现在延迟但是没有丢弃的元素到达时，使得窗口再次计算，比如 EventTimeTrigger。</p><p><strong>提示1：在 assignTimestampsAndWatermarks 时有一个 maxOutOfOrderness 的概念，maxOutOfOrderness 是生成 Watermark 所需要的，是指元素最大无序时间。而 Allowed Lateness 是指在 Watermark 到达窗口结束时间之后允许延迟多长时间，两个概念不一样。</strong><br><strong>提示2：<br>a.通过 watermark 机制来处理 out-of-order 的问题，属于第一层防护，属于全局性的防护，通常说的乱序问题的解决办法，就是指这类；<br>b.通过窗口上的 allowedLateness 机制来处理 out-of-order 的问题，属于第二层防护，属于特定 window operator 的防护，late element 的问题就是指这类</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;T&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(&lt;window assigner&gt;)</span><br><span class="line">    .allowedLateness(&lt;time&gt;)</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br></pre></td></tr></table></figure><p><strong>提示：当使用 GlobalWindows 时，没有元素会被认为是迟到的，因为这个窗口哦的结束时间时 Long.MAX_VALUE。</strong></p><h2 id="迟到的数据做旁路输出（side-output）"><a href="#迟到的数据做旁路输出（side-output）" class="headerlink" title="迟到的数据做旁路输出（side output）"></a>迟到的数据做旁路输出（side output）</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> OutputTag&lt;T&gt; lateOutputTag = <span class="keyword">new</span> OutputTag&lt;T&gt;(<span class="string">"late-data"</span>)&#123;&#125;;</span><br><span class="line"></span><br><span class="line">DataStream&lt;T&gt; input = ...;</span><br><span class="line"></span><br><span class="line">SingleOutputStreamOperator&lt;T&gt; result = input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(&lt;window assigner&gt;)</span><br><span class="line">    .allowedLateness(&lt;time&gt;)</span><br><span class="line">    .sideOutputLateData(lateOutputTag)</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br><span class="line"></span><br><span class="line">DataStream&lt;T&gt; lateStream = result.getSideOutput(lateOutputTag);</span><br></pre></td></tr></table></figure><h2 id="拓展思考"><a href="#拓展思考" class="headerlink" title="拓展思考"></a>拓展思考</h2><p>当指定 Allowed Lateness &gt; 0 时，在 Watermark 通过窗口结束时间后，<strong>将保留窗口及其内容</strong>。在这种情况下，当一个延迟但未被丢弃的元素到达时，它可能会再次触发窗口运算。这些被触发运算的被称为 late firing。在使用会话窗口时，它们可能会将两个预先存在的未合并窗口进行合并，下面是一个例子。</p><p>比如：有一个会话窗口且 Gap 为3分钟，现在有两个窗口，第一个窗口起始和结束时间为（01:00:00，01:00:05），第二个窗口起始和结束时间为（01:00:09，01:00:15），如果我们在此时不设置 Allowed Lateness 时，那么如果不保存第一个窗口的数据，运算第二个窗口的数据时，不会有什么问题，但是如果我们设置了 Allowed Lateness = 5 min，那么这时就会有问题了，比如有迟到元素01:00:15才到达，元素自己的时间戳为01:00:07，这样这个元素就可以将两个窗口的数据结合为一个窗口。</p><p><strong>提示：延迟数据触发的运算应该将之前的计算结果更新，所以如果下游 sink 使用了 kafka，则这种情况不是很适用（除非消费 kafka 的是一些 updateable dfs），否则，你将会得到很多的对相同组数据计算的结果。</strong></p><h1 id="窗口结果的使用"><a href="#窗口结果的使用" class="headerlink" title="窗口结果的使用"></a><strong>窗口结果的使用</strong></h1><p>窗口计算的结果也会转化为一个数据流，这份结果中不会包含窗口操作的任何信息，所以如果后续计算中需要这些信息，你必须使用 ProcessWindowFunction 将这些信息通过编码传输进去。</p><h2 id="窗口和-Watermark-的联系"><a href="#窗口和-Watermark-的联系" class="headerlink" title="窗口和 Watermark 的联系"></a>窗口和 Watermark 的联系</h2><p>当 Watermark 到达窗口算子处时，会触发两个事件：<br>1.Watermark 会触发所有的窗口中的最大时间戳（窗口结束时间戳 - 1）&lt; 到达的最新 Watermark的窗口运算。<br>2.将 Watermark 发送到下游算子。<br>Intuitively, a watermark “flushes” out any windows that would be considered late in downstream operations once they receive that watermark.</p><h2 id="连续的窗口算子"><a href="#连续的窗口算子" class="headerlink" title="连续的窗口算子"></a>连续的窗口算子</h2><h2 id="设置窗口时产生的状态大小的注意事项"><a href="#设置窗口时产生的状态大小的注意事项" class="headerlink" title="设置窗口时产生的状态大小的注意事项"></a>设置窗口时产生的状态大小的注意事项</h2><p>窗口大小可以定义的很大（如天、周或月），因此可能会累积非常大的状态（state）。所以在估计窗口计算的存储需求时，需要记住以下几个规则：</p><p>1.Flink 会为每个元素所属的窗口创建一个副本。因此，滚动的窗口保留每个元素的一个副本（一个元素只属于一个窗口）。相反，滑动窗口可能会创建每个元素中的几个副本。因此，1天大小的窗口，1秒滑动步长的滑动窗口可能不是一个好主意。</p><p>2.ReduceFunction、AggregateFunction 和 FoldFunction 可以显著减少存储需求，因为它们在元素到达时就聚合元素，并且每个窗口只存储一个值。相反，仅仅使用 ProcessWindowFunction 就需要累积所有元素。</p><p>3.使用 Evictor 可防止任何预聚合，因为在应用计算之前，窗口的所有元素都必须通过 Evictor 传递。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：DataStream Api 中 Operators（算子）——概览</title>
    <link href="https://yangyichao-mango.github.io/2019/11/09/apache-flink:study-flink-datastream-operators-overview/"/>
    <id>https://yangyichao-mango.github.io/2019/11/09/apache-flink:study-flink-datastream-operators-overview/</id>
    <published>2019-11-09T08:22:30.000Z</published>
    <updated>2019-11-09T14:47:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>描述了基本 Operators 的 Transformations，应用这些转换后如何进行 physical partitioning（物理分区），以及对Flink算子链的深入了解。</p><a id="more"></a><h1 id="DataStream-Transformations"><a href="#DataStream-Transformations" class="headerlink" title="DataStream Transformations"></a><strong>DataStream Transformations</strong></h1><h1 id="Physical-partitioning"><a href="#Physical-partitioning" class="headerlink" title="Physical partitioning"></a><strong>Physical partitioning</strong></h1><table>    <thead>        <tr>            <th style="width: 10%">Transformation</th>            <th style="width: 20%">Description</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">Custom partitioning<br>                DataStream → DataStream<br>                <strong>CustomPartitionerWrapper<<t>k, T></t></strong>            </td>            <td>使用自定义的 partitioner 为每一条 record 选择下一个 task<br>                <strong>dataStream.partitionCustom(partitioner, "someKey");</strong><br>                <strong>dataStream.partitionCustom(partitioner, 0);</strong>            </td>        </tr>        <tr>            <td style="text-align: center;">Random partitioning<br>                DataStream → DataStream<br>                <strong>ShufflePartitioner<<t>T></t></strong>            </td>            <td>按随机均匀划分元素<br>                <strong>dataStream.shuffle();</strong>            </td>        </tr>        <tr>            <td style="text-align: center;">Rebalancing (Round-robin partitioning)<br>                DataStream → DataStream<br>                <strong>RebalancePartitioner<<t>T></t></strong>            </td>            <td>分区循环划分元素，为每个下游创建相等的负载。对于存在数据倾斜的性能优化非常有用。<br>                <strong>dataStream.rebalance();</strong>            </td>        </tr>        <tr>            <td style="text-align: center;">Rescaling<br>                DataStream → DataStream<br>                <strong>RescalePartitioner<<t>T></t></strong>            </td>            <td>将元素循环（round robin）分配到下游 operator 的子集。如果你的 pipeline 是一下的情况，那么这种方式会非常有用。<br>                例如，将并行数据源的每个实例的数据传输到的下游多个算子（operators）一个子集以分配负载。但不希望 full rebalance，则这非常有用。<br>                如果合理配置 TaskManager 的 slot数量，则数据传输只需要本地传输，而不需要通过网络传输数据。<br><br>                上游 operators 向的下游 operators 发送 record 取决于上游 operators 和下游 operators 的并行度。<br>                例如，如果上游 operator 的并行度为2，而下游 operator 的并行度为6，则一个上游 operator 将 record 分配给三个下游 operator，而另一个上游 operator 将 record 分配给其他三个下游 operator。相反，如果下游 operator 的并行度为2，而上游 operator 的并行度为6，则三个上游 operator 将分配给一个下游 operator，而其他三个上游 operator 将分配给另一个下游 operator。<br><br>                如果上下游算子的并行度不是彼此的倍数，则一个或多个下游 operator 将具有来自上游 operator 的不同数量的输入。                如下图：                <figure>                    <div class="img-lightbox">                        <div class="overlay"></div>                        <img src="/blog-img/apache-flink:study-flink-datastream-operators-overview/rescale_partition.svg" alt="rescale_partition" title>                    </div>                </figure>                <strong>dataStream.rescale();</strong>            </td>        </tr>        <tr>            <td style="text-align: center;">Broadcasting<br>                DataStream → DataStream<br>                <strong>BroadcastPartitioner<<t>T></t></strong>            </td>            <td>广播数据到下游的每个partition。<br>                <strong>dataStream.broadcast();</strong>            </td>        </tr>        <tr>            <td style="text-align: center;">Local Forward<br>                DataStream → DataStream<br>                <strong>ForwardPartitioner<<t>T></t></strong>            </td>            <td>数据传输到本地的下游算子。<br>                <strong>dataStream.forward();</strong>            </td>        </tr>        <tr>            <td style="text-align: center;">GlobalPartitioner<br>                DataStream → DataStream<br>                <strong>GlobalPartitioner<<t>T></t></strong>            </td>            <td>数据传输到下游子任务id为0的task中。<br>                <strong>dataStream.forward();</strong>            </td>        </tr>        <tr>            <td style="text-align: center;">Key Groups<br>                DataStream → DataStream<br>                <strong>KeyGroupStreamPartitioner<<t>T, K></t></strong>            </td>            <td>相同key的值会传输到同一个下游。类似于Rescaling，但是不用再api中指定，再使用keyBy时会自动指定此方法。<br>                <strong>dataStream.keyBy();</strong>            </td>        </tr>    </tbody></table><h1 id="Task-chaining-和-资源组"><a href="#Task-chaining-和-资源组" class="headerlink" title="Task chaining 和 资源组"></a><strong>Task chaining 和 资源组</strong></h1><p>链接两个 Transformations 意味着可以将它们共同放在在同一个线程中执行以获得更好的性能。<br>默认情况下，Flink会尽可能链接两个算子（例如，两个 Map Transformations）。如果需要，可以使用API对 Task chaining 进行细粒度控制：</p><p>在 Flink 中，一个 slot 就是一个资源组。如果需要的话，你可以通过使用api把上下游算子隔离在不同的 slot 中运行。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：DataStream Api 中 EventTime</title>
    <link href="https://yangyichao-mango.github.io/2019/11/09/apache-flink:study-flink-datastream-eventtime/"/>
    <id>https://yangyichao-mango.github.io/2019/11/09/apache-flink:study-flink-datastream-eventtime/</id>
    <published>2019-11-09T04:09:56.000Z</published>
    <updated>2019-11-09T07:41:48.694Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：DataStream Api 中 EventTime</p><a id="more"></a><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-flink-datastream-eventtime/时间.svg" alt="时间" title>                </div>                <div class="image-caption">时间</div>            </figure><h1 id="Processing-time"><a href="#Processing-time" class="headerlink" title="Processing time"></a><strong>Processing time</strong></h1><p>处理时间是指执行相应操作的机器的系统时间</p><h1 id="Event-time"><a href="#Event-time" class="headerlink" title="Event time"></a><strong>Event time</strong></h1><p>事件时间是每个事件在其生产设备（生产event的设备，手机等的源头设备）上发生的时间</p><h2 id="Event-Time-和-Watermark"><a href="#Event-Time-和-Watermark" class="headerlink" title="Event Time 和 Watermark"></a>Event Time 和 Watermark</h2><p>Flink中测量事件时间进度的机制是Watermark。Watermark作为数据流的一部分流动，并带有时间戳t。Watermark（t）声明该流中的 Event Time 已达到时间t，这意味着流中不应再有时间戳t’&lt;=t的元素（即 Event Time 早于或等于 Watermark 的事件）</p><p>下图显示了具有时间戳的事件流，以及内联流动的水印。在这个例子中，事件是有序的，这意味着 Watermark 只是流中的周期性标记。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-flink-datastream-eventtime/顺序流的Watermark.svg" alt="顺序流的Watermark" title>                </div>                <div class="image-caption">顺序流的Watermark</div>            </figure><p><strong>Watermark对于无序流是至关重要的</strong>，如下所示，其中事件到达顺序不是按时间戳顺序。Watermark代表通过流中的该点，这个时间戳之前的事件都应该到达了。一旦Watermark到达算子，算子就可以将其内部事件时钟更新到Watermark的值。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-flink-datastream-eventtime/无序流的Watermark.svg" alt="无序流的Watermark" title>                </div>                <div class="image-caption">无序流的Watermark</div>            </figure><h2 id="并行流的-Watermarks"><a href="#并行流的-Watermarks" class="headerlink" title="并行流的 Watermarks"></a>并行流的 Watermarks</h2><p>Watermark 在源数据 Function 处生成，或直接在源数据 Function 之后生成。源数据 Function 的每个并行子任务通常独立生成其 Watermark。这些 Watermark 定义并行源数据的 Event Time。</p><p>当 Watermark 流过程序时，会更新到达算子的 Event Time，当一个 Watermark 更新了算子的 Event Time 时，它会为其下游的后续算子生成一个新的 Watermark。</p><p>某些算子消费多个输入流，例如：union 算子或者跟在 keyBy，partition 算子的之后的算子。这样的算子的 Event Time 是所有输入 stream 中最小的 Watermark，即：<br><strong>Operator Event Time = min(Input Stream 1 Watermark, Input Stream 2 Watermark…)</strong><br>算子会跟着输入流 Watermark 的更新来更新算子自己的 Event Time。</p><p>下图显示了流经并行流的事件和 Watermark 以及算子获取 Event Time 的示例。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-flink-datastream-eventtime/并行流的Watermarks.svg" alt="并行流的Watermarks" title>                </div>                <div class="image-caption">并行流的Watermarks</div>            </figure><p>注意，Kafka支持分区 Watermark</p><h1 id="Ingestion-time"><a href="#Ingestion-time" class="headerlink" title="Ingestion time"></a><strong>Ingestion time</strong></h1><p>注入时间是事件进入Flink Job的时间。在源Operator处，每条Operator获取源数据的时间作为Ingestion time时间戳</p><h1 id="生成-Timestamps-和-Watermarks"><a href="#生成-Timestamps-和-Watermarks" class="headerlink" title="生成 Timestamps 和 Watermarks"></a><strong>生成 Timestamps 和 Watermarks</strong></h1><h2 id="分配-Timestamps"><a href="#分配-Timestamps" class="headerlink" title="分配 Timestamps"></a>分配 Timestamps</h2><h3 id="数据流源生成-Timestamps-和-Watermarks"><a href="#数据流源生成-Timestamps-和-Watermarks" class="headerlink" title="数据流源生成 Timestamps 和 Watermarks"></a>数据流源生成 Timestamps 和 Watermarks</h3><p>数据源可以直接为它们产生的数据分配 Timestamp，并且他们也能发送 Watermark。这样做的话，在后面的处理中就没必要再去定义 Timestamp 分配器了，需要注意的是：如果在后面的处理中使用了一个 timestamp 分配器，由数据源提供的任何 timestamp 和 watermark 都会被重写。</p><h3 id="Timestamp-分配器-Watermark生成器"><a href="#Timestamp-分配器-Watermark生成器" class="headerlink" title="Timestamp 分配器 / Watermark生成器"></a>Timestamp 分配器 / Watermark生成器</h3><p>Timestamp 分配器获取一个流并生成一个新的带有 Timestamp 元素和 Watermark 的流。如果上游的原始数据流已经有 Timestamp 或 Watermark，则 Timestamp 分配器将覆盖上游的 Timestamp 或 Watermark</p><p>Timestamp 分配器通常在数据源之后立即指定，但这并不是严格要求的。通常是在 Timestamp 分配器之前先解析（MapFunction）和过滤（FilterFunction）数据源。在任何情况下，都需要在基于 Event Time 算子（例如 window 操作）运行之前指定 Timestamp 分配程序。<br>有一个特殊情况，当使用 Kafka 作为流作业的数据源时，Flink 允许在数据源内部指定 Timestamp 分配器和 Watermark 生成器。更多关于如何进行的信息请参考Kafka Connector的文档。</p><p>直接在FlinkKafkaConsumer010上面使用assignTimestampsAndWatermarks可以根据kafka source的partitions的特性进行设置Timestamps和Watermarks，让用户做一些特殊的处理</p><p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka<br>partition, allows users to let them exploit the per-partition characteristics.</p><h1 id="Kafka-分区的-Timestamp"><a href="#Kafka-分区的-Timestamp" class="headerlink" title="Kafka 分区的 Timestamp"></a><strong>Kafka 分区的 Timestamp</strong></h1><p>当使用 Apache Kafka 作为数据源时，每个 Kafka 分区可能有一个简单的 Event Time 模式（递增的时间戳或有界无序）。然而，当 Flink Job 使用来自Kafka的流时，多个分区常常并行消费，每一个 operator 算子并行消费时就会破坏各个分区的时间模式（这是 Kafka 客户端消费 Kafka 数据必然发生的）。</p><p>在这种情况下，可以使用 Flink’s Kafka-partition-aware watermark generation，使用该功能，每个 Kafka 分区在 Kafka consumer 内部生成 Watermark，每个分区合并 Watermark 的方式与流 shuffles 时合并 Watermark 的方式相同。</p><p>例如，如果事件时间戳严格按照 Kafka 分区递增，则使用递增时间戳 Watermark 生成器生成每个分区的 Watermark 将是完美的全局 Watermark。</p><p>下图显示了如何使用 Flink’s Kafka-partition-aware watermark generation，以及在这种情况下 Watermark 如何通过流数据流传播。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-flink-datastream-eventtime/KafkaSource多分区Watermark.svg" alt="KafkaSource多分区Watermark" title>                </div>                <div class="image-caption">KafkaSource多分区Watermark</div>            </figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：slot和parallelism设置的关系</title>
    <link href="https://yangyichao-mango.github.io/2019/11/08/apache-flink:study-flink-slot-parallelism/"/>
    <id>https://yangyichao-mango.github.io/2019/11/08/apache-flink:study-flink-slot-parallelism/</id>
    <published>2019-11-08T07:31:08.000Z</published>
    <updated>2019-11-08T15:55:53.544Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：slot和parallelism设置的关系</p><a id="more"></a><h1 id="如何设置-parallelism"><a href="#如何设置-parallelism" class="headerlink" title="如何设置 parallelism"></a><strong>如何设置 parallelism</strong></h1><h2 id="flink-conf-yaml"><a href="#flink-conf-yaml" class="headerlink" title="flink-conf.yaml"></a>flink-conf.yaml</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cat flink-conf.yaml | grep parallelism</span><br><span class="line"></span><br><span class="line"><span class="comment"># The parallelism used for programs that did not specify and other parallelism.</span></span><br><span class="line">parallelism.default: 1</span><br></pre></td></tr></table></figure><h2 id="命令行启动"><a href="#命令行启动" class="headerlink" title="命令行启动"></a>命令行启动</h2><p>如果你是用命令行启动你的 Flink job，那么你也可以这样设置并行度(使用 -p 并行度)：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/flink run -p 10 ../word-count.jar</span><br></pre></td></tr></table></figure><h2 id="代码设置整个程序的并行度"><a href="#代码设置整个程序的并行度" class="headerlink" title="代码设置整个程序的并行度"></a>代码设置整个程序的并行度</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setParallelism(<span class="number">10</span>);</span><br></pre></td></tr></table></figure><p>注意：这样设置的并行度是你整个程序的并行度，那么后面如果你的每个算子不单独设置并行度覆盖的话，那么后面每个算子的并行度就都是这里设置的并行度的值了。</p><h2 id="每个算子单独设置并行度"><a href="#每个算子单独设置并行度" class="headerlink" title="每个算子单独设置并行度"></a>每个算子单独设置并行度</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data.keyBy(<span class="keyword">new</span> xxxKey())</span><br><span class="line">    .flatMap(<span class="keyword">new</span> XxxFlatMapFunction()).setParallelism(<span class="number">5</span>)</span><br><span class="line">    .map(<span class="keyword">new</span> XxxMapFunction).setParallelism(<span class="number">5</span>)</span><br><span class="line">    .addSink(<span class="keyword">new</span> XxxSink()).setParallelism(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>如上，就是在每个算子后面单独的设置并行度，这样的话，就算你前面设置了 env.setParallelism(10) 也是会被覆盖的。</p><p>这也说明优先级是：算子设置并行度 &gt; env 设置并行度 &gt; 配置文件默认并行度</p><h1 id="slot"><a href="#slot" class="headerlink" title="slot"></a><strong>slot</strong></h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-flink-slot-parallelism/FlinkJob运行架构.svg" alt="FlinkJob运行架构" title>                </div>                <div class="image-caption">FlinkJob运行架构</div>            </figure><p>图中 Task Manager 是从 Job Manager 处接收需要部署的 Task，任务的并行性由每个 Task Manager 上可用的 slot 决定。每个任务代表分配给任务槽的一组资源，slot 在 Flink 里面可以认为是资源组，Flink 将每个任务分成子任务并且将这些子任务分配到 slot 来并行执行程序。</p><p>例如，如果 Task Manager 有四个 slot，那么它将为每个 slot 分配 25％ 的内存。 可以在一个 slot 中运行一个或多个线程。 同一 slot 中的线程共享相同的 JVM。 同一 JVM 中的任务共享 TCP 连接和心跳消息。Task Manager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离。默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 task 的 subtask，只要它们来自相同的 job。这种共享可以有更好的资源利用率。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-flink-slot-parallelism/TaskSlots执行.svg" alt="TaskSlots执行" title>                </div>                <div class="image-caption">TaskSlots执行</div>            </figure><p>默认情况下，Flink 允许 subtasks 共享 slots，即使它们是不同 tasks 的 subtasks，只要它们来自同一个 job。因此，一个 slot 可能会负责这个 job 的整个管道（pipeline）。允许 slot sharing 有两个好处：</p><p>1.Flink 集群需要与 job 中使用的最高并行度一样多的 slots。这样不需要计算作业总共包含多少个 tasks（具有不同并行度）。</p><p>2.更好的资源利用率。在没有 slot sharing 的情况下，简单的 subtasks（source/map()）将会占用和复杂的 subtasks （window）一样多的资源。通过 slot sharing，将示例中的并行度从 2 增加到 6 可以充分利用 slot 的资源，同时确保繁重的 subtask 在 TaskManagers 之间公平地获取资源。</p><p>下图即为Flink subtasks 共享 slots的模式：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-flink-slot-parallelism/TaskSlotsSharing执行.svg" alt="TaskSlotsSharing执行" title>                </div>                <div class="image-caption">TaskSlotsSharing执行</div>            </figure><p>上面图片中有两个 Task Manager，每个 Task Manager 有三个 slot，这样我们的算子最大并行度那么就可以达到 6 个，在同一个 slot 里面可以执行 1 至多个子任务。</p><p>那么再看上面的图片，source/map/keyby/window/apply 最大可以有 6 个并行度，sink 只用了 1 个并行。</p><p>每个 Flink TaskManager 在集群中提供 slot。 slot 的数量通常与每个 TaskManager 的可用 CPU 内核数成比例。一般情况下你的 slot 数是你每个 TaskManager 的 cpu 的核数</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：Flink Job ExecutionGraph生成过程</title>
    <link href="https://yangyichao-mango.github.io/2019/11/08/apache-flink:study-flink-job-ExecutionGraph/"/>
    <id>https://yangyichao-mango.github.io/2019/11/08/apache-flink:study-flink-job-ExecutionGraph/</id>
    <published>2019-11-08T01:49:18.000Z</published>
    <updated>2019-11-08T02:37:23.427Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：Flink Job 执行计划生成过程</p><a id="more"></a><h1 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a><strong>Transformations</strong></h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-flink-job-ExecutionGraph/TransformationClasses.png" alt="TransformationClasses" title>                </div>                <div class="image-caption">TransformationClasses</div>            </figure><p>并不是每一个 Transformation 都会转换成runtime层中的物理操作。有一些只是逻辑概念，比如union、split/select、partition等。如下图所示的转换树，在运行时会优化成下方的操作图。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-flink-job-ExecutionGraph/Transformations.png" alt="Transformations" title>                </div>                <div class="image-caption">Transformations</div>            </figure><h1 id="执行计划转换过程"><a href="#执行计划转换过程" class="headerlink" title="执行计划转换过程"></a><strong>执行计划转换过程</strong></h1><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-flink-job-ExecutionGraph/4层转化.png" alt="4层转换" title>                </div>                <div class="image-caption">4层转换</div>            </figure><p>1.转换过程 StreamExecutionEnvironment 存放的 transformation -&gt; StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; 物理执行图<br>2.StreamExecutionEnvironment 存放的 transformation -&gt; StreamGraph -&gt; JobGraph 在<strong>客户端</strong>完成，然后提交 JobGraph 到 JobManager<br>3.JobManager 的主节点 JobMaster，将 JobGraph 转化为 ExecutionGraph，然后发送到不同的 taskManager，得到实际的物理执行图</p><h2 id="LocalStreamEnvironment-中-parallelism"><a href="#LocalStreamEnvironment-中-parallelism" class="headerlink" title="LocalStreamEnvironment 中 parallelism"></a><strong>LocalStreamEnvironment 中 parallelism</strong></h2><p>其中 LocalStreamEnvironment Task 中的 parallelism 数量是根据以下代码生成的</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Public</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamExecutionEnvironment</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> defaultLocalParallelism = Runtime.getRuntime().availableProcessors();</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Creates a &#123;<span class="doctag">@link</span> LocalStreamEnvironment&#125;. The local execution environment</span></span><br><span class="line"><span class="comment">     * will run the program in a multi-threaded fashion in the same JVM as the</span></span><br><span class="line"><span class="comment">     * environment was created in. The default parallelism of the local</span></span><br><span class="line"><span class="comment">     * environment is the number of hardware contexts (CPU cores / threads),</span></span><br><span class="line"><span class="comment">     * unless it was specified differently by &#123;<span class="doctag">@link</span> #setParallelism(int)&#125;.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> A local execution environment.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> LocalStreamEnvironment <span class="title">createLocalEnvironment</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> createLocalEnvironment(defaultLocalParallelism);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Creates a &#123;<span class="doctag">@link</span> LocalStreamEnvironment&#125;. The local execution environment</span></span><br><span class="line"><span class="comment">     * will run the program in a multi-threaded fashion in the same JVM as the</span></span><br><span class="line"><span class="comment">     * environment was created in. It will use the parallelism specified in the</span></span><br><span class="line"><span class="comment">     * parameter.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parallelism</span></span><br><span class="line"><span class="comment">     * The parallelism for the local environment.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> A local execution environment with the specified parallelism.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> LocalStreamEnvironment <span class="title">createLocalEnvironment</span><span class="params">(<span class="keyword">int</span> parallelism)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> createLocalEnvironment(parallelism, <span class="keyword">new</span> Configuration());</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：异步IO之RichAsyncFunction</title>
    <link href="https://yangyichao-mango.github.io/2019/11/06/apache-flink:study-async-io-RichAsyncFunction/"/>
    <id>https://yangyichao-mango.github.io/2019/11/06/apache-flink:study-async-io-RichAsyncFunction/</id>
    <published>2019-11-06T10:45:59.000Z</published>
    <updated>2019-11-08T02:42:18.399Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：异步IO之RichAsyncFunction</p><a id="more"></a><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a><strong>问题</strong></h2><h3 id="设置kafka-consumer并行度的语义"><a href="#设置kafka-consumer并行度的语义" class="headerlink" title="设置kafka consumer并行度的语义"></a>设置kafka consumer并行度的语义</h3><p>1.如果设置kafka consumer的并发度为100，并且申请到集群中资源的Task Manager的slot个数也为100个，则每个slot中运行的任务都生成这么多数量的kafka consumer，还是每个slot一个kafka consumer?</p><p>2.场景：一个keyBy过后设置了一分钟的窗口dataStream中，如果保证每次触发这个窗口时，窗口的数据永远只有一条的话，并且在保证窗口为1分钟大小的情况下，接口返回速度保证在10秒，使用Async IO是否就没有意义了，因为当前请求队列里面只有一条数据</p><p>3.flink 默认执行一个Job的slot中线程数为什么是8，在哪里设置的</p><h3 id="使用AsyncIO需要考虑的指标"><a href="#使用AsyncIO需要考虑的指标" class="headerlink" title="使用AsyncIO需要考虑的指标"></a>使用AsyncIO需要考虑的指标</h3><p>1.每个slot中Flink Job的线程数<br>2.如果需要使用时间窗口：时间窗口的大小，几分钟的窗口<br>2.如果需要keyBy：每个slot中Flink Job的大概key的个数（什么情况使用，什么情况不使用）</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a><strong>简介</strong></h2><p>我们知道flink对于外部数据源的操作可以通过自带的连接器，或者自定义sink和source实现数据的交互，那么为啥还需要异步IO呢？<br>那时因为对于实时处理，当我们需要使用外部存储数据参与计算时，与外部系统之间的交互延迟对流处理的整个工作进度起决定性的影响。<br>如果我们是使用传统方式mapfunction等算子里访问外部存储，实际上该交互过程是同步的，比如下图中：请求a发送到数据库，那么function会一直等待响应。在很多案例中，这个等待过程是非常浪费函数时间的。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-async-io-RichAsyncFunction/异步IO.jpeg" alt="异步IO" title>                </div>                <div class="image-caption">异步IO</div>            </figure><p>图中棕色的长条表示等待时间，可以发现网络等待时间极大地阻碍了吞吐和延迟。为了解决同步访问的问题，异步模式可以并发地处理多个请求和回复。<br>也就是说，你可以连续地向数据库发送用户a、b、c等的请求，与此同时，哪个请求的回复先返回了就处理哪个回复，从而连续的请求之间不需要阻塞等待，如上图右边所示。<br>这也正是 Async I/O 的实现原理。</p><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a><strong>目的</strong></h2><p>将MapFunction或者FlatMapFunction中的同步访问外部存储设备的方法通过AsyncFunction替换以实现异步访问<br>在执行过程中，如果使用了keyBy，则相同的key整个执行周期都使用同一个线程，但是不同的key也可以使用同一个线程</p><h2 id="如何使用Async-I-O"><a href="#如何使用Async-I-O" class="headerlink" title="如何使用Async I/O"></a><strong>如何使用Async I/O</strong></h2><p>我们需要自定义一个类实现RichAsyncFunction这个抽象类，实现其中的抽象方法，这点和自定义source很像。<br>主要是的抽象方法如下，然后在asyncInvoke()使用CompletableFuture执行异步操作（CompletableFuture会提供一个ForkJoinPool作为请求线程池）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">asyncInvoke</span><span class="params">(IN var1, ResultFuture&lt;OUT&gt; var2)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">default</span> <span class="keyword">void</span> <span class="title">timeout</span><span class="params">(IN input, ResultFuture&lt;OUT&gt; resultFuture)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    resultFuture.completeExceptionally(<span class="keyword">new</span> TimeoutException(<span class="string">"Async function call has timed out."</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后在AsyncDataStream中使用我们定义好的类，去实现主流异步的访问外部数据源</p><h2 id="原理实现"><a href="#原理实现" class="headerlink" title="原理实现"></a><strong>原理实现</strong></h2><p>AsyncDataStream.(un)orderedWait 的主要工作就是创建了一个 AsyncWaitOperator。<br>AsyncWaitOperator 是支持异步 IO 访问的算子实现，该算子会运行 AsyncFunction 并处理异步返回的结果，其内部原理如下图所示</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-async-io-RichAsyncFunction/异步原理.jpeg" alt="异步原理" title>                </div>                <div class="image-caption">异步原理</div>            </figure><p>如图所示，AsyncWaitOperator 主要由两部分组成：StreamElementQueue 和 Emitter。<br>StreamElementQueue 是一个 Promise 队列，所谓 Promise 是一种异步抽象表示将来会有一个值（参考 Scala Promise 了解更多），这个队列是未完成的 Promise 队列，也就是进行中的请求队列。<br>Emitter 是一个单独的线程，负责发送消息（收到的异步回复）给下游。</p><p>图中E5表示进入该算子的第五个元素（”Element-5”），在执行过程中首先会将其包装成一个 “Promise” P5，然后将P5放入队列。<br>最后调用 AsyncFunction 的 ayncInvoke 方法，该方法会向外部服务发起一个异步的请求，并注册回调。<br>该回调会在异步请求成功返回时调用 AsyncCollector.collect 方法将返回的结果交给框架处理。<br>实际上 AsyncCollector 也一个 Promise，也就是 P5，在调用 collect 的时候会标记 Promise 为完成状态，并通知 Emitter 线程有完成的消息可以发送了。<br>Emitter 就会从队列中拉取完成的 Promise ，并从 Promise 中取出消息发送给下游。</p><h2 id="消息的顺序性"><a href="#消息的顺序性" class="headerlink" title="消息的顺序性"></a><strong>消息的顺序性</strong></h2><p>上文提到 Async I/O 提供了两种输出模式。<br>其实细分有三种模式: 有序，ProcessingTime 无序，EventTime 无序。<br>Flink 使用队列来实现不同的输出模式，并抽象出一个队列的接口（StreamElementQueue），这种分层设计使得AsyncWaitOperator和Emitter不用关心消息的顺序问题。<br>StreamElementQueue有两种具体实现，分别是 OrderedStreamElementQueue 和 UnorderedStreamElementQueue。<br>UnorderedStreamElementQueue 比较有意思，它使用了一套逻辑巧妙地实现完全无序和 EventTime 无序</p><h3 id="有序"><a href="#有序" class="headerlink" title="有序"></a>有序</h3><p>有序比较简单，使用一个队列就能实现。<br>所有新进入该算子的元素（包括 watermark），都会包装成 Promise 并按到达顺序放入该队列。<br>如下图所示，尽管P4的结果先返回，但并不会发送，只有 P1 （队首）的结果返回了才会触发 Emitter 拉取队首元素进行发送</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-async-io-RichAsyncFunction/有序.jpeg" alt="有序" title>                </div>                <div class="image-caption">有序</div>            </figure><h3 id="ProcessingTime-无序"><a href="#ProcessingTime-无序" class="headerlink" title="ProcessingTime 无序"></a>ProcessingTime 无序</h3><p>ProcessingTime 无序也比较简单，因为没有 watermark，不需要协调 watermark 与消息的顺序性，所以使用两个队列就能实现，一个 uncompletedQueue 一个 completedQueue。<br>所有新进入该算子的元素，同样的包装成 Promise 并放入 uncompletedQueue 队列，当uncompletedQueue队列中任意的Promise返回了数据，则将该 Promise 移到 completedQueue 队列中，并通知 Emitter 消费。<br>如下图所示：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-async-io-RichAsyncFunction/ProcessingTime无序.jpeg" alt="ProcessingTime无序" title>                </div>                <div class="image-caption">ProcessingTime无序</div>            </figure><h3 id="EventTime-无序"><a href="#EventTime-无序" class="headerlink" title="EventTime 无序"></a>EventTime 无序</h3><p>EventTime 无序类似于有序与 ProcessingTime 无序的结合体。<br>因为有 watermark，需要协调 watermark 与消息之间的顺序性，所以uncompletedQueue中存放的元素从原先的 Promise 变成了 Promise 集合。<br>如果进入算子的是消息元素，则会包装成 Promise 放入队尾的集合中。<br>如果进入算子的是 watermark，也会包装成 Promise 并放到一个独立的集合中，再将该集合加入到 uncompletedQueue 队尾，最后再创建一个空集合加到 uncompletedQueue 队尾。<br>这样，watermark 就成了消息顺序的边界。<br>只有处在队首的集合中的 Promise 返回了数据，才能将该 Promise 移到 completedQueue 队列中，由 Emitter 消费发往下游。<br>只有队首集合空了，才能处理第二个集合。这样就保证了当且仅当某个 watermark 之前所有的消息都已经被发送了，该 watermark 才能被发送。过程如下图所示：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-flink:study-async-io-RichAsyncFunction/EventTime无序.jpeg" alt="EventTime无序" title>                </div>                <div class="image-caption">EventTime无序</div>            </figure><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a><strong>说明</strong></h2><p>1、AsyncDataStream有2个方法，unorderedWait表示数据不需要关注顺序，处理完立即发送，orderedWait表示数据需要关注顺序，为了实现该目标，操作算子会在该结果记录之前的记录为发送之前缓存该记录。这往往会引入额外的延迟和一些Checkpoint负载，因为相比于无序模式结果记录会保存在Checkpoint状态内部较长的时间。<br>2、Timeout配置，主要是为了处理死掉或者失败的任务，防止资源被长期阻塞占用。<br>3、最后一个参数Capacity表示同时最多有多少个异步请求在处理，异步IO的方式会导致更高的吞吐量，但是对于实时应用来说该操作也是一个瓶颈。限制并发请求数，算子不会积压过多的未处理请求，但是一旦超过容量的显示会触发背压。<br>该参数可以不配置，但是默认是100</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Common 包学习：常用集合类Collections4学习</title>
    <link href="https://yangyichao-mango.github.io/2019/11/06/apache-common:study-apache-common-collections4/"/>
    <id>https://yangyichao-mango.github.io/2019/11/06/apache-common:study-apache-common-collections4/</id>
    <published>2019-11-06T08:04:06.000Z</published>
    <updated>2019-11-06T08:57:35.325Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Common 包学习：常用集合类Collections4学习</p><a id="more"></a><h2 id="Maven依赖"><a href="#Maven依赖" class="headerlink" title="Maven依赖"></a><strong>Maven依赖</strong></h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.commons<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-collections4<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="CollectionUtils"><a href="#CollectionUtils" class="headerlink" title="CollectionUtils"></a><strong>CollectionUtils</strong></h2><h3 id="lt-O-gt-Collection-lt-O-gt-subtract-final-Iterable-lt-extends-O-gt-a-final-Iterable-lt-extends-O-gt-b"><a href="#lt-O-gt-Collection-lt-O-gt-subtract-final-Iterable-lt-extends-O-gt-a-final-Iterable-lt-extends-O-gt-b" class="headerlink" title="&lt;O&gt; Collection&lt;O&gt; subtract(final Iterable&lt;? extends O&gt; a, final Iterable&lt;? extends O&gt; b)"></a>&lt;O<o>&gt; Collection&lt;O<o>&gt; subtract(final Iterable&lt;? extends O&gt; a, final Iterable&lt;? extends O&gt; b)</o></o></h3><p>a是做差集运算的左集，b是做差集运算的右集，下面是一个例子</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = LoggerFactory.getLogger(Demo.class);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Set&lt;Pair&lt;String, String&gt;&gt; allProductDevices = Sets.newHashSet(Pair.of(<span class="string">"a"</span>, <span class="string">"b"</span>), Pair.of(<span class="string">"c"</span>, <span class="string">"d"</span>));</span><br><span class="line"></span><br><span class="line">        Set&lt;Pair&lt;String, String&gt;&gt; oldProductDevices = Sets.newHashSet(Pair.of(<span class="string">"a"</span>, <span class="string">"b"</span>), Pair.of(<span class="string">"e"</span>, <span class="string">"f"</span>));</span><br><span class="line"></span><br><span class="line">        List&lt;Pair&lt;String, String&gt;&gt; newProductDevices =</span><br><span class="line">                (ArrayList&lt;Pair&lt;String, String&gt;&gt;) CollectionUtils.subtract(allProductDevices, oldProductDevices);</span><br><span class="line"></span><br><span class="line">        List&lt;String&gt; list1 = Lists.newArrayList(<span class="string">"a"</span>, <span class="string">"b"</span>);</span><br><span class="line"></span><br><span class="line">        List&lt;String&gt; list2 = Lists.newArrayList(<span class="string">"c"</span>, <span class="string">"b"</span>);</span><br><span class="line"></span><br><span class="line">        List&lt;String&gt; list3 = (ArrayList&lt;String&gt;) CollectionUtils.subtract(list1, list2);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Common包" scheme="https://yangyichao-mango.github.io/categories/Apache-Common%E5%8C%85/"/>
    
    
      <category term="Apache Common包" scheme="https://yangyichao-mango.github.io/tags/Apache-Common%E5%8C%85/"/>
    
  </entry>
  
  <entry>
    <title>Google Guava 学习：guava cache缓存学习</title>
    <link href="https://yangyichao-mango.github.io/2019/11/06/google-guava:study-guava-cache/"/>
    <id>https://yangyichao-mango.github.io/2019/11/06/google-guava:study-guava-cache/</id>
    <published>2019-11-06T08:03:38.000Z</published>
    <updated>2019-11-06T10:38:23.662Z</updated>
    
    <content type="html"><![CDATA[<p>Google Guava 学习：guava cache缓存学习</p><a id="more"></a><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a><strong>背景</strong></h2><p>缓存的主要作用是暂时在内存中保存业务系统的数据处理结果，并且等待下次访问使用。在日长开发有很多场合，有一些数据量不是很大，不会经常改动，并且访问非常频繁。但是由于受限于硬盘IO的性能或者远程网络等原因获取可能非常的费时。会导致我们的程序非常缓慢，这在某些业务上是不能忍的！而缓存正是解决这类问题的神器！</p><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a><strong>正文</strong></h2><p>Guava Cache与ConcurrentMap很相似，但也不完全一样。最基本的区别是ConcurrentMap会一直保存所有添加的元素，直到显式地移除。相对地，Guava Cache为了限制内存占用，通常都设定为自动回收元素。在某些场景下，尽管LoadingCache 不回收元素，它也是很有用的，因为它会自动加载缓存</p><p>Guava Cache是在内存中缓存数据，相比较于数据库或redis存储，访问内存中的数据会更加高效。Guava官网介绍，下面的这几种情况可以考虑使用Guava Cache：</p><p>1.愿意消耗一些内存空间来提升速度。</p><p>2.预料到某些键会被多次查询。</p><p>3.缓存中存放的数据总量不会超出内存容量。</p><p>所以，可以将程序频繁用到的少量数据存储到Guava Cache中，以改善程序性能。下面对Guava Cache的用法进行详细的介绍。</p><h2 id="Maven依赖"><a href="#Maven依赖" class="headerlink" title="Maven依赖"></a><strong>Maven依赖</strong></h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>guava<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>23.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="构建缓存对象"><a href="#构建缓存对象" class="headerlink" title="构建缓存对象"></a><strong>构建缓存对象</strong></h2><p>接口Cache代表缓存，它有如下方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Cache</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function">V <span class="title">get</span><span class="params">(K key, Callable&lt;? extends V&gt; valueLoader)</span> <span class="keyword">throws</span> ExecutionException</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">ImmutableMap&lt;K, V&gt; <span class="title">getAllPresent</span><span class="params">(Iterable&lt;?&gt; keys)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">put</span><span class="params">(K key, V value)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">putAll</span><span class="params">(Map&lt;? extends K, ? extends V&gt; m)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">invalidate</span><span class="params">(Object key)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">invalidateAll</span><span class="params">(Iterable&lt;?&gt; keys)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">invalidateAll</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">size</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">CacheStats <span class="title">stats</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">ConcurrentMap&lt;K, V&gt; <span class="title">asMap</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">cleanUp</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以通过CacheBuilder类构建一个缓存对象，构建一个缓存对象代码如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Cache&lt;String,String&gt; cache = CacheBuilder.newBuilder().build();</span><br><span class="line">        cache.put(<span class="string">"word"</span>,<span class="string">"Hello Guava Cache"</span>);</span><br><span class="line">        System.out.println(cache.getIfPresent(<span class="string">"word"</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到Cache非常类似于JDK中的Map，但是相比于Map，Guava Cache提供了很多更强大的功能</p><h2 id="设置最大存储"><a href="#设置最大存储" class="headerlink" title="设置最大存储"></a><strong>设置最大存储</strong></h2><p>Guava Cache可以在构建缓存对象时指定缓存所能够存储的最大记录数量。当Cache中的记录数量达到最大值后再调用put方法向其中添加对象，Guava会先从当前缓存的对象记录中选择一条删除掉，腾出空间后再将新的对象存储到Cache中</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Cache&lt;String,String&gt; cache = CacheBuilder.newBuilder()</span><br><span class="line">                .maximumSize(<span class="number">2</span>)</span><br><span class="line">                .build();</span><br><span class="line">        cache.put(<span class="string">"key1"</span>, <span class="string">"value1"</span>);</span><br><span class="line">        cache.put(<span class="string">"key2"</span>, <span class="string">"value2"</span>);</span><br><span class="line">        cache.put(<span class="string">"key3"</span>, <span class="string">"value3"</span>);</span><br><span class="line">        System.out.println(<span class="string">"第一个值："</span> + cache.getIfPresent(<span class="string">"key1"</span>));</span><br><span class="line">        System.out.println(<span class="string">"第二个值："</span> + cache.getIfPresent(<span class="string">"key2"</span>));</span><br><span class="line">        System.out.println(<span class="string">"第三个值："</span> + cache.getIfPresent(<span class="string">"key3"</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面代码在构造缓存对象时，通过CacheBuilder类的maximumSize方法指定Cache最多可以存储两个对象，然后调用Cache的put方法向其中添加了三个对象。程序执行结果如下图所示，可以看到第三条对象记录的插入，导致了第一条对象记录被删除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第一个值：null</span><br><span class="line">第二个值：value2</span><br><span class="line">第三个值：value3</span><br></pre></td></tr></table></figure><h2 id="设置过期时间"><a href="#设置过期时间" class="headerlink" title="设置过期时间"></a><strong>设置过期时间</strong></h2><p>在构建Cache对象时，可以通过CacheBuilder类的expireAfterAccess和expireAfterWrite两个方法为缓存中的对象指定过期时间，过期的对象将会被缓存自动删除。其中，expireAfterWrite方法指定对象被写入到缓存后多久过期，expireAfterAccess指定对象多久没有被访问后过期</p><h3 id="expireAfterWrite"><a href="#expireAfterWrite" class="headerlink" title="expireAfterWrite"></a>expireAfterWrite</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder()</span><br><span class="line">                .maximumSize(<span class="number">2</span>)</span><br><span class="line">                .expireAfterWrite(<span class="number">3</span>, TimeUnit.SECONDS)</span><br><span class="line">                .build();</span><br><span class="line">        cache.put(<span class="string">"key1"</span>, <span class="string">"value1"</span>);</span><br><span class="line">        <span class="keyword">int</span> time = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">"第"</span> + time++ + <span class="string">"次取到key1的值为："</span> + cache.getIfPresent(<span class="string">"key1"</span>));</span><br><span class="line">            Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的代码在构造Cache对象时，通过CacheBuilder的expireAfterWrite方法指定put到Cache中的对象在3秒后会过期。在Cache对象中存储一条对象记录后，每隔1秒读取一次这条记录。程序运行结果如下图所示，可以看到，前三秒可以从Cache中获取到对象，超过三秒后，对象从Cache中被自动删除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">第1次取到key1的值为：value1</span><br><span class="line">第2次取到key1的值为：value1</span><br><span class="line">第3次取到key1的值为：value1</span><br><span class="line">第4次取到key1的值为：null</span><br><span class="line">第5次取到key1的值为：null</span><br><span class="line">第6次取到key1的值为：null</span><br><span class="line">第7次取到key1的值为：null</span><br><span class="line">第8次取到key1的值为：null</span><br></pre></td></tr></table></figure><h3 id="expireAfterAccess"><a href="#expireAfterAccess" class="headerlink" title="expireAfterAccess"></a>expireAfterAccess</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder()</span><br><span class="line">                .maximumSize(<span class="number">2</span>)</span><br><span class="line">                .expireAfterAccess(<span class="number">3</span>, TimeUnit.SECONDS)</span><br><span class="line">                .build();</span><br><span class="line">        cache.put(<span class="string">"key1"</span>, <span class="string">"value1"</span>);</span><br><span class="line">        <span class="keyword">double</span> time = <span class="number">1.5</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            Thread.sleep((<span class="keyword">long</span>) time * <span class="number">1000L</span>);</span><br><span class="line">            System.out.println(<span class="string">"睡眠"</span> + time++ + <span class="string">"秒后取到key1的值为："</span> + cache.getIfPresent(<span class="string">"key1"</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过CacheBuilder的expireAfterAccess方法指定Cache中存储的对象如果超过3秒没有被访问就会过期。while中的代码每sleep一段时间就会访问一次Cache中存储的对象key1，每次访问key1之后下次sleep的时间会加长一秒。程序运行结果如下图所示，从结果中可以看出，当超过3秒没有读取key1对象之后，该对象会自动被Cache删除。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">睡眠1.5秒后取到key1的值为：value1</span><br><span class="line">睡眠2.5秒后取到key1的值为：value1</span><br><span class="line">睡眠3.5秒后取到key1的值为：null</span><br></pre></td></tr></table></figure><p>也可以同时用expireAfterAccess和expireAfterWrite方法指定过期时间，这时只要对象满足两者中的一个条件就会被自动过期删除。</p><h2 id="弱引用"><a href="#弱引用" class="headerlink" title="弱引用"></a><strong>弱引用</strong></h2><p>可以通过weakKeys和weakValues方法指定Cache只保存对缓存记录key和value的弱引用。这样当没有其他强引用指向key和value时，key和value对象就会被垃圾回收器回收</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Cache&lt;String, Object&gt; cache = CacheBuilder.newBuilder()</span><br><span class="line">                .maximumSize(<span class="number">2</span>)</span><br><span class="line">                .weakValues()</span><br><span class="line">                .build();</span><br><span class="line">        Object value = <span class="keyword">new</span> Object();</span><br><span class="line">        cache.put(<span class="string">"key1"</span>, value);</span><br><span class="line">        </span><br><span class="line">        value = <span class="keyword">new</span> Object(); <span class="comment">// 原对象不再有强引用</span></span><br><span class="line">        System.gc();</span><br><span class="line">        System.out.println(cache.getIfPresent(<span class="string">"key1"</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面代码的打印结果是null。构建Cache时通过weakValues方法指定Cache只保存记录值的一个弱引用。当给value引用赋值一个新的对象之后，就不再有任何一个强引用指向原对象。System.gc()触发垃圾回收后，原对象就被清除了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">null</span><br></pre></td></tr></table></figure><h2 id="显示清除"><a href="#显示清除" class="headerlink" title="显示清除"></a><strong>显示清除</strong></h2><p>可以调用Cache的invalidateAll或invalidate方法显示删除Cache中的记录。invalidate方法一次只能删除Cache中一个记录，接收的参数是要删除记录的key。invalidateAll方法可以批量删除Cache中的记录，当没有传任何参数时，invalidateAll方法将清除Cache中的全部记录。invalidateAll也可以接收一个Iterable类型的参数，参数中包含要删除记录的所有key值。下面代码对此做了示例</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder().build();</span><br><span class="line">        Object value = <span class="keyword">new</span> Object();</span><br><span class="line">        cache.put(<span class="string">"key1"</span>, <span class="string">"value1"</span>);</span><br><span class="line">        cache.put(<span class="string">"key2"</span>, <span class="string">"value2"</span>);</span><br><span class="line">        cache.put(<span class="string">"key3"</span>, <span class="string">"value3"</span>);</span><br><span class="line"></span><br><span class="line">        List&lt;String&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        list.add(<span class="string">"key1"</span>);</span><br><span class="line">        list.add(<span class="string">"key2"</span>);</span><br><span class="line"></span><br><span class="line">        cache.invalidateAll(list); <span class="comment">// 批量清除list中全部key对应的记录</span></span><br><span class="line">        System.out.println(cache.getIfPresent(<span class="string">"key1"</span>));</span><br><span class="line">        System.out.println(cache.getIfPresent(<span class="string">"key2"</span>));</span><br><span class="line">        System.out.println(cache.getIfPresent(<span class="string">"key3"</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>代码中构造了一个集合list用于保存要删除记录的key值，然后调用invalidateAll方法批量删除key1和key2对应的记录，只剩下key3对应的记录没有被删除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">null</span><br><span class="line">null</span><br><span class="line">value3</span><br></pre></td></tr></table></figure><h2 id="移除监听器"><a href="#移除监听器" class="headerlink" title="移除监听器"></a><strong>移除监听器</strong></h2><p>可以为Cache对象添加一个移除监听器，这样当有记录被删除时可以感知到这个事件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public class StudyGuavaCache &#123;</span><br><span class="line">    public static void main(String[] args) throws InterruptedException &#123;</span><br><span class="line">        RemovalListener&lt;String, String&gt; listener = notification -&gt;</span><br><span class="line">                System.out.println(<span class="string">"["</span> + notification.getKey() + <span class="string">":"</span> + notification.getValue() + <span class="string">"] is removed!"</span>);</span><br><span class="line">        Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder()</span><br><span class="line">                .maximumSize(3)</span><br><span class="line">                .removalListener(listener)</span><br><span class="line">                .build();</span><br><span class="line">        cache.put(<span class="string">"key1"</span>, <span class="string">"value1"</span>);</span><br><span class="line">        cache.put(<span class="string">"key2"</span>, <span class="string">"value2"</span>);</span><br><span class="line">        cache.put(<span class="string">"key3"</span>, <span class="string">"value3"</span>);</span><br><span class="line">        cache.put(<span class="string">"key4"</span>, <span class="string">"value3"</span>);</span><br><span class="line">        cache.put(<span class="string">"key5"</span>, <span class="string">"value3"</span>);</span><br><span class="line">        cache.put(<span class="string">"key6"</span>, <span class="string">"value3"</span>);</span><br><span class="line">        cache.put(<span class="string">"key7"</span>, <span class="string">"value3"</span>);</span><br><span class="line">        cache.put(<span class="string">"key8"</span>, <span class="string">"value3"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>removalListener方法为Cache指定了一个移除监听器，这样当有记录从Cache中被删除时，监听器listener就会感知到这个事件。程序运行结果如下图所示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[key1:value1] is removed!</span><br><span class="line">[key2:value2] is removed!</span><br><span class="line">[key3:value3] is removed!</span><br><span class="line">[key4:value3] is removed!</span><br><span class="line">[key5:value3] is removed!</span><br></pre></td></tr></table></figure><h2 id="自动加载"><a href="#自动加载" class="headerlink" title="自动加载"></a><strong>自动加载</strong></h2><p>Cache的get方法有两个参数，第一个参数是要从Cache中获取记录的key，第二个记录是一个Callable对象。<br>当缓存中已经存在key对应的记录时，get方法直接返回key对应的记录。如果缓存中不包含key对应的记录，Guava会使用当前线程执行Callable对象中的call方法，call方法的返回值会作为key对应的值被存储到缓存中，并且被get方法返回。下面是一个多线程的例子：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = LoggerFactory.getLogger(StudyGuavaCache.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder()</span><br><span class="line">            .maximumSize(<span class="number">1</span>)</span><br><span class="line">            .build();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                LOGGER.info(<span class="string">"1"</span> + Thread.currentThread().getName());</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    String value = cache.get(<span class="string">"key"</span>, <span class="keyword">new</span> Callable&lt;String&gt;() &#123;</span><br><span class="line">                        <span class="function"><span class="keyword">public</span> String <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                            LOGGER.info(<span class="string">"load1"</span> + Thread.currentThread().getName()); <span class="comment">// 加载数据线程执行标志</span></span><br><span class="line">                            Thread.sleep(<span class="number">1000</span>); <span class="comment">// 模拟加载时间</span></span><br><span class="line">                            <span class="keyword">return</span> <span class="string">"auto load by Callable1"</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;);</span><br><span class="line">                    LOGGER.info(<span class="string">"thread1 "</span> + value);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                LOGGER.info(<span class="string">"thread2"</span>);</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    LOGGER.info(<span class="string">"2"</span> + Thread.currentThread().getName());</span><br><span class="line">                    String value = cache.get(<span class="string">"key1"</span>, <span class="keyword">new</span> Callable&lt;String&gt;() &#123;</span><br><span class="line">                        <span class="function"><span class="keyword">public</span> String <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                            LOGGER.info(<span class="string">"load2"</span> + Thread.currentThread().getName()); <span class="comment">// 加载数据线程执行标志</span></span><br><span class="line">                            Thread.sleep(<span class="number">1000</span>); <span class="comment">// 模拟加载时间</span></span><br><span class="line">                            <span class="keyword">return</span> <span class="string">"auto load by Callable2"</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;);</span><br><span class="line">                    LOGGER.info(<span class="string">"thread2 "</span> + value);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这段代码中有两个线程共享同一个Cache对象，两个线程同时调用get方法获取同一个key对应的记录。由于key对应的记录不存在，所以两个线程都在get方法处阻塞。此处在call方法中调用Thread.sleep(1000)模拟程序从外存加载数据的时间消耗</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">17:49:23.965 [Thread-2] INFO com.github.xxx.other.demo.guava.StudyGuavaCache - thread2</span><br><span class="line">17:49:23.965 [Thread-1] INFO com.github.xxx.other.demo.guava.StudyGuavaCache - 1Thread-1</span><br><span class="line">17:49:23.969 [Thread-2] INFO com.github.xxx.other.demo.guava.StudyGuavaCache - 2Thread-2</span><br><span class="line">17:49:23.983 [Thread-1] INFO com.github.xxx.other.demo.guava.StudyGuavaCache - load1Thread-1</span><br><span class="line">17:49:23.983 [Thread-2] INFO com.github.xxx.other.demo.guava.StudyGuavaCache - load2Thread-2</span><br></pre></td></tr></table></figure><p>从结果中可以看出，虽然是两个线程同时调用get方法，但只有一个get方法中的Callable会被执行(没有打印出load2)。<br>Guava可以保证当有多个线程同时访问Cache中的一个key时，如果key对应的记录不存在，Guava只会启动一个线程执行get方法中Callable参数对应的任务加载数据存到缓存。<br>当加载完数据后，任何线程中的get方法都会获取到key对应的值</p><h2 id="统计信息"><a href="#统计信息" class="headerlink" title="统计信息"></a><strong>统计信息</strong></h2><p>可以对Cache的命中率、加载数据时间等信息进行统计。在构建Cache对象时，可以通过CacheBuilder的recordStats方法开启统计信息的开关。<br>开关开启后Cache会自动对缓存的各种操作进行统计，调用Cache的stats方法可以查看统计后的信息</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder()</span><br><span class="line">                .maximumSize(<span class="number">3</span>)</span><br><span class="line">                .recordStats() <span class="comment">// 开启统计信息开关</span></span><br><span class="line">                .build();</span><br><span class="line">        cache.put(<span class="string">"key1"</span>, <span class="string">"value1"</span>);</span><br><span class="line">        cache.put(<span class="string">"key2"</span>, <span class="string">"value2"</span>);</span><br><span class="line">        cache.put(<span class="string">"key3"</span>, <span class="string">"value3"</span>);</span><br><span class="line">        cache.put(<span class="string">"key4"</span>, <span class="string">"value4"</span>);</span><br><span class="line"></span><br><span class="line">        cache.getIfPresent(<span class="string">"key1"</span>);</span><br><span class="line">        cache.getIfPresent(<span class="string">"key2"</span>);</span><br><span class="line">        cache.getIfPresent(<span class="string">"key3"</span>);</span><br><span class="line">        cache.getIfPresent(<span class="string">"key4"</span>);</span><br><span class="line">        cache.getIfPresent(<span class="string">"key5"</span>);</span><br><span class="line">        cache.getIfPresent(<span class="string">"key6"</span>);</span><br><span class="line">        </span><br><span class="line">        System.out.println(cache.stats()); <span class="comment">// 获取统计信息</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>程序执行结果如下所示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CacheStats&#123;hitCount=3, missCount=3, loadSuccessCount=0, loadExceptionCount=0, totalLoadTime=0, evictionCount=1&#125;</span><br></pre></td></tr></table></figure><p>这些统计信息对于调整缓存设置是至关重要的，在性能要求高的应用中应该密切关注这些数据</p><h2 id="LoadingCache"><a href="#LoadingCache" class="headerlink" title="LoadingCache"></a><strong>LoadingCache</strong></h2><p>LoadingCache是Cache的子接口，相比较于Cache，当从LoadingCache中读取一个指定key的记录时，如果该记录不存在，则LoadingCache可以自动执行加载数据到缓存的操作。<br>LoadingCache接口的定义如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">LoadingCache</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Cache</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt;, <span class="title">Function</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function">V <span class="title">get</span><span class="params">(K key)</span> <span class="keyword">throws</span> ExecutionException</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">V <span class="title">getUnchecked</span><span class="params">(K key)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">ImmutableMap&lt;K, V&gt; <span class="title">getAll</span><span class="params">(Iterable&lt;? extends K&gt; keys)</span> <span class="keyword">throws</span> ExecutionException</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">V <span class="title">apply</span><span class="params">(K key)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">refresh</span><span class="params">(K key)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function">ConcurrentMap&lt;K, V&gt; <span class="title">asMap</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>与构建Cache类型的对象类似，LoadingCache类型的对象也是通过CacheBuilder进行构建，不同的是，在调用CacheBuilder的build方法时，必须传递一个CacheLoader类型的参数，CacheLoader的load方法需要我们提供实现。<br>当调用LoadingCache的get方法时，如果缓存不存在对应key的记录，则CacheLoader中的load方法会被自动调用从外存加载数据，load方法的返回值会作为key对应的value存储到LoadingCache中，并从get方法返回</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException </span>&#123;</span><br><span class="line">        CacheLoader&lt;String, String&gt; loader = <span class="keyword">new</span> CacheLoader&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">load</span><span class="params">(String key)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                Thread.sleep(<span class="number">1000</span>); <span class="comment">// 休眠1s，模拟加载数据</span></span><br><span class="line">                System.out.println(key + <span class="string">" is loaded from a cacheLoader!"</span>);</span><br><span class="line">                <span class="keyword">return</span> key + <span class="string">"'s value"</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">        LoadingCache&lt;String, String&gt; loadingCache = CacheBuilder.newBuilder()</span><br><span class="line">                .maximumSize(<span class="number">3</span>)</span><br><span class="line">                .build(loader); <span class="comment">// 在构建时指定自动加载器</span></span><br><span class="line"></span><br><span class="line">        loadingCache.get(<span class="string">"key1"</span>);</span><br><span class="line">        loadingCache.get(<span class="string">"key2"</span>);</span><br><span class="line">        loadingCache.get(<span class="string">"key3"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>程序执行结果如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">key1 is loaded from a cacheLoader!</span><br><span class="line">key2 is loaded from a cacheLoader!</span><br><span class="line">key3 is loaded from a cacheLoader!</span><br></pre></td></tr></table></figure><p>从LoadingCache查询的正规方式是使用get(K)方法。这个方法要么返回已经缓存的值，要么使用CacheLoader向缓存原子地加载新值（通过load(String key) 方法加载）。由于CacheLoader可能抛出异常，LoadingCache.get(K)也声明抛出ExecutionException异常。如果你定义的CacheLoader没有声明任何检查型异常，则可以通过getUnchecked(K)查找缓存；但必须注意，一旦CacheLoader声明了检查型异常，就不可以调用getUnchecked(K)。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="guava" scheme="https://yangyichao-mango.github.io/categories/guava/"/>
    
    
      <category term="guava" scheme="https://yangyichao-mango.github.io/tags/guava/"/>
    
  </entry>
  
  <entry>
    <title>apache-kafka:study-features</title>
    <link href="https://yangyichao-mango.github.io/2019/11/06/apache-kafka:study-features/"/>
    <id>https://yangyichao-mango.github.io/2019/11/06/apache-kafka:study-features/</id>
    <published>2019-11-06T01:57:17.000Z</published>
    <updated>2019-11-06T02:08:57.687Z</updated>
    
    <content type="html"><![CDATA[<h2 id="如何为Kafka集群选择合适的Partitions数量"><a href="#如何为Kafka集群选择合适的Partitions数量" class="headerlink" title="如何为Kafka集群选择合适的Partitions数量"></a><strong>如何为Kafka集群选择合适的Partitions数量</strong></h2><h3 id="越多的分区可以提供更高的吞吐量"><a href="#越多的分区可以提供更高的吞吐量" class="headerlink" title="越多的分区可以提供更高的吞吐量"></a>越多的分区可以提供更高的吞吐量</h3><p>首先我们需要明白以下事实：在kafka中，单个patition是kafka并行操作的最小单元。在producer和broker端，向每一个分区写入数据是可以完全并行化的，此时，可以通过加大硬件资源的利用率来提升系统的吞吐量，例如对数据进行压缩。在consumer段，kafka只允许单个partition的数据被一个consumer线程消费。因此，在consumer端，每一个Consumer Group内部的consumer并行度完全依赖于被消费的分区数量。综上所述，通常情况下，在一个Kafka集群中，partition的数量越多，意味着可以到达的吞吐量越大。</p><p>我们可以粗略地通过吞吐量来计算kafka集群的分区数量。假设对于单个partition，producer端的可达吞吐量为p，Consumer端的可达吞吐量为c，期望的目标吞吐量为t，那么集群所需要的partition数量至少为max(t/p,t/c)。在producer端，单个分区的吞吐量大小会受到批量大小、数据压缩方法、 确认类型（同步/异步）、复制因子等配置参数的影响。经过测试，在producer端，单个partition的吞吐量通常是在10MB/s左右。在consumer端，单个partition的吞吐量依赖于consumer端每个消息的应用逻辑处理速度。因此，我们需要对consumer端的吞吐量进行测量。</p><p>虽然随着时间的推移，我们能够对分区的数量进行添加，但是对于基于Key来生成的这一类消息需要我们重点关注。当producer向kafka写入基于key的消息时，kafka通过key的hash值来确定消息需要写入哪个具体的分区。通过这样的方案，kafka能够确保相同key值的数据可以写入同一个partition。kafka的这一能力对于一部分应用是极为重要的，例如对于同一个key的所有消息，consumer需要按消息的顺序进行有序消费。如果partition的数量发生改变，那么上面的有序性保证将不复存在。为了避免上述情况发生，通常的解决办法是多分配一些分区，以满足未来的需求。通常情况下，我们需要根据未来1到2年的目标吞吐量来设计kafka的分区数量。</p><p>一开始，我们可以基于当前的业务吞吐量为kafka集群分配较小的broker数量，随着时间的推移，我们可以向集群中增加更多的broker，然后在线方式将适当比例的partition转移到新增加的broker中去。通过这样的方法，我们可以在满足各种应用场景（包括基于key消息的场景）的情况下，保持业务吞吐量的扩展性。</p><p>在设计分区数时，除了吞吐量，还有一些其他因素值得考虑。正如我们后面即将看到的，对于一些应用场景，集群拥有过的分区将会带来负面的影响。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Kafka" scheme="https://yangyichao-mango.github.io/categories/Apache-Kafka/"/>
    
    
      <category term="Apache Kafka" scheme="https://yangyichao-mango.github.io/tags/Apache-Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：系统特性学习</title>
    <link href="https://yangyichao-mango.github.io/2019/11/03/apache-flink:study-features/"/>
    <id>https://yangyichao-mango.github.io/2019/11/03/apache-flink:study-features/</id>
    <published>2019-11-03T08:20:21.000Z</published>
    <updated>2019-11-11T07:18:07.272Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：系统特性学习</p><a id="more"></a><p><a href="https://github.com/flink-china/flink-training-course" target="_blank" rel="noopener">教程</a></p><h2 id="window窗口"><a href="#window窗口" class="headerlink" title="window窗口"></a><strong>window窗口</strong></h2><h3 id="窗口大小"><a href="#窗口大小" class="headerlink" title="窗口大小"></a>窗口大小</h3><p>窗口大小是用户自己设定的，但是窗口的起始和结束时间点是系统根据窗口大小和自然数进行设定的，不会出现设置了一分钟的窗口，统计的数据是2:30到3:30的数据</p><p>[window_start_time, window_end_time)根据窗口大小和自然数进行设定</p><p>如果window大小是3秒，那么1分钟内会把window划分为如下的形式:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[00:00:00,00:00:03)</span><br><span class="line">[00:00:03,00:00:06)</span><br><span class="line">...</span><br><span class="line">[00:00:57,00:01:00)</span><br></pre></td></tr></table></figure><p>如果window大小是10秒，则window会被分为如下的形式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[00:00:00,00:00:10)</span><br><span class="line">[00:00:10,00:00:20)</span><br><span class="line">...</span><br><span class="line">[00:00:50,00:01:00)</span><br></pre></td></tr></table></figure><h3 id="watermark"><a href="#watermark" class="headerlink" title="watermark"></a>watermark</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> Watermark <span class="title">getCurrentWatermark</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// this guarantees that the watermark never goes backwards.</span></span><br><span class="line">    <span class="keyword">long</span> potentialWM = currentMaxTimestamp - maxOutOfOrderness;</span><br><span class="line">    <span class="keyword">if</span> (potentialWM &gt;= lastEmittedWatermark) &#123;</span><br><span class="line">        lastEmittedWatermark = potentialWM;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Watermark(lastEmittedWatermark);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>watermark = max( [当前已到达的时间戳最新的数据(currentMaxTimestamp)] - [最大乱序等待时间(maxOutOfOrderness)], watermark )</strong></p><h3 id="触发窗口运算条件"><a href="#触发窗口运算条件" class="headerlink" title="触发窗口运算条件"></a>触发窗口运算条件</h3><p>1.当前最新数据到达进行判断：当前到达event的time(timestamp) ＜ watermark则触发，表示数据是超过了最大等待时间，已经延迟到达的，则会触发</p><p>2.当前最新数据到达进行判断：最新的watermark &gt;= window_end_time（对于out-of-order以及正常的数据而言），在[window_start_time, window_end_time)中有数据存在</p><p>3.而且，这里要强调一点，watermark和currentMaxTimestamp是一个全局的值，不是某一个key下的值，所以即使不是同一个key的数据，其warmark也会增加</p><p>语义是：currentMaxTimestamp是当前到达的最大时间戳数据，代表时间戳为currentMaxTimestamp的数据已经到达了，所以所能等待的数据的时间戳（max_out_of_orderness）只能为watermark = currentMaxTimestamp - maxOutOfOrderness</p><h3 id="窗口计算"><a href="#窗口计算" class="headerlink" title="窗口计算"></a>窗口计算</h3><p>Window reduce，Window aggregate 和 Window Fold 是增量聚合，每来一条数据就计算一次，高效</p><p>Window apply（Window process 的老版本） 和 Window process 是全量聚合，触发窗口计算时全量计算</p><h3 id="被Keys化与非被Keys化Windows"><a href="#被Keys化与非被Keys化Windows" class="headerlink" title="被Keys化与非被Keys化Windows"></a>被Keys化与非被Keys化Windows</h3><p>要指定的第一件事是您的流是否应该使用keyedWindow，一般都与业务逻辑有关，比如说使用一分钟的窗口进行去重。使用keyBy(…)将您的无限流分成逻辑Key化的数据流。如果keyBy(…)未调用，则表示您的流不是被Keys化的。</p><p>对于被Key化的数据流，可以将传入数据（Object）的的任何属性用作键）。拥有被Key化的数据流将允许您的窗口计算由多个任务并行执行，因为每个Key化的数据流可以独立于其余任务进行处理。引用相同Keys的所有数据将被发送到同一个并行任务进行计算。</p><p>在非被Key化的数据流的情况下，您的原始流将不会被拆分为多个逻辑流，并且所有窗口逻辑将由单个任务执行，即并行度为1。</p><h2 id="sql"><a href="#sql" class="headerlink" title="sql"></a><strong>sql</strong></h2><p>1.在flinkSql中，如果使用groupBy，尽量使用窗口，否则会认为被groupBy的数据会默认人为整个窗口内的数据还没有到达，所以会一直等待，不会产出数据</p><p>update-mode: append / update</p><p>分为 update stream 模式和 append stream 模式</p><p>window聚合为append mode stream，groupby聚合为update mode stream</p><h2 id="Flink生成-Timestamps-和-Watermarks"><a href="#Flink生成-Timestamps-和-Watermarks" class="headerlink" title="Flink生成 Timestamps 和 Watermarks"></a><strong>Flink生成 Timestamps 和 Watermarks</strong></h2><p>为了让event time工作，Flink需要知道事件的时间戳，这意味着流中的每个元素都需要分配其事件时间戳。这个通常是通过抽取或者访问事件中某些字段的时间戳来获取的。</p><p>时间戳的分配伴随着水印的生成，告诉系统事件时间中的进度。</p><p>这里有两种方式来分配时间戳和生成水印:</p><ol><li>直接在数据流源中进行。</li><li>通过timestamp assigner和watermark generator生成:在Flink中，timestamp分配器也定义了用来发射的水印。</li></ol><h3 id="数据流源生成Timestamps和Watermarks"><a href="#数据流源生成Timestamps和Watermarks" class="headerlink" title="数据流源生成Timestamps和Watermarks"></a>数据流源生成Timestamps和Watermarks</h3><p>数据流源可以直接为它们产生的数据元素分配timestamp，并且他们也能发送水印。这样做的话，就没必要再去定义timestamp分配器了，需要注意的是:如果一个timestamp分配器被使用的话，由源提供的任何timestamp和watermark都会被重写。</p><h3 id="时间戳分配器-水印生成器（Timestamp-Assigners-Watermark-Generators）"><a href="#时间戳分配器-水印生成器（Timestamp-Assigners-Watermark-Generators）" class="headerlink" title="时间戳分配器/水印生成器（Timestamp Assigners / Watermark Generators）"></a>时间戳分配器/水印生成器（Timestamp Assigners / Watermark Generators）</h3><p>Timestamp分配器获取一个流并生成一个新的带有Timestamp元素和水印的流。如果原始流已经有时间戳和/或水印，则Timestamp分配程序将覆盖它们</p><p>Timestamp分配器通常在数据源之后立即指定，但这并不是严格要求的。通常是在timestamp分配器之前先解析(MapFunction)和过滤(FilterFunction)。在任何情况下，都需要在事件时间上的第一个操作(例如第一个窗口操作)之前指定timestamp分配程序。有一个特殊情况，当使用Kafka作为流作业的数据源时，Flink允许在源内部指定timestamp分配器和watermark生成器。更多关于如何进行的信息请参考Kafka Connector的文档。</p><p>直接在FlinkKafkaConsumer010上面使用assignTimestampsAndWatermarks可以根据kafka source的partitions的特性进行设置Timestamps和Watermarks，让用户做一些特殊的处理</p><p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka<br>partition, allows users to let them exploit the per-partition characteristics.</p><h2 id="log"><a href="#log" class="headerlink" title="log"></a><strong>log</strong></h2><h3 id="Web-UI查找log"><a href="#Web-UI查找log" class="headerlink" title="Web UI查找log"></a>Web UI查找log</h3><p>JobManger log： 展示整个作业的状态变化（例如，从create 到deploy到running再到failed），通过jobManger log可以查看作业历史失败的记录和直接原因。</p><p>TaskManager log： 调度到该TaskManager上的task 的打印的相关log。</p><h2 id="shuffle"><a href="#shuffle" class="headerlink" title="shuffle"></a><strong>shuffle</strong></h2><p>被keyBy的数据流中，相同的key的数据会被发送到同一个slot中运行（partitiner决定），也就是TaskManager中slot进行shuffle的过程<br>如果有多个producer并且producer的数量和partition数量相同，则每个producer写一个partition</p><h2 id="Savepoints和Checkpoints"><a href="#Savepoints和Checkpoints" class="headerlink" title="Savepoints和Checkpoints"></a><strong>Savepoints和Checkpoints</strong></h2><p>用 Data Stream API 编写的程序可以从 savepoint 继续执行。Savepoints 允许在不丢失任何状态的情况下升级程序和 Flink 集群。</p><p>Savepoints 是手动触发的 Checkpoints，它依靠常规的 Checkpoint 机制获取程序的快照并将其写入 state backend。在执行期间，程序会定期在 worker 节点上创建快照并生成 Checkpoints。对于恢复，Flink 仅需要最后完成的 Checkpoint，而一旦完成了新的 Checkpoint，旧的就可以被丢弃。</p><p>Savepoints 类似于这些定期的 Checkpoints，除了它们是由用户触发并且在新的 Checkpoints 完成时不会自动过期。你可以通过命令行 或在取消一个 job 时通过 REST API 来创建 Savepoints。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：实时场景下的应用</title>
    <link href="https://yangyichao-mango.github.io/2019/11/03/apache-flink:study-realtime-scenario/"/>
    <id>https://yangyichao-mango.github.io/2019/11/03/apache-flink:study-realtime-scenario/</id>
    <published>2019-11-03T05:30:05.000Z</published>
    <updated>2019-11-03T08:32:58.449Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：实时场景下的应用</p><a id="more"></a><h2 id="用户需求场景"><a href="#用户需求场景" class="headerlink" title="用户需求场景"></a><strong>用户需求场景</strong></h2><p>用户想要查看 版本，机型，国家，城市 等等维度下按照分钟的时间粒度的设备活跃，新增设备数，首次活跃设备数</p><h2 id="用户需求-gt-架构方案设计"><a href="#用户需求-gt-架构方案设计" class="headerlink" title="用户需求-&gt;架构方案设计"></a><strong>用户需求-&gt;架构方案设计</strong></h2><table><thead><tr><th align="center">数据所处阶段</th><th align="center">功能描述</th></tr></thead><tbody><tr><td align="center">数据source</td><td align="center">各种各样的打到kafka的用户行为数据的日志</td></tr><tr><td align="center">数据process</td><td align="center">实时引擎消费kafka，根据数据服务化提供的接口判断当前用户是否是新增，活跃，首次活跃，将用户的相关数据打到下游kafka</td></tr><tr><td align="center">数据sink</td><td align="center">结果kafka</td></tr><tr><td align="center">olap引擎</td><td align="center">消费sink kafka</td></tr><tr><td align="center">数据产品</td><td align="center">通过BI等的产品呈现给用户</td></tr></tbody></table><h2 id="架构设计-gt-选择实时计算引擎"><a href="#架构设计-gt-选择实时计算引擎" class="headerlink" title="架构设计-&gt;选择实时计算引擎"></a><strong>架构设计-&gt;选择实时计算引擎</strong></h2><p>为什么使用flink：</p><p>A.保证消费一次：checkpoint和savepoint 容错</p><p>B.时间属性：事件，注入，处理时间</p><p>优点：事件时间的属性可以被广泛应用，比如一般的分析场景都是分析用户某个时间段的用户相关指标，而不是事件处理某个时间段的用户相关指标</p><h2 id="flink实现方案"><a href="#flink实现方案" class="headerlink" title="flink实现方案"></a><strong>flink实现方案</strong></h2><h3 id="第一种方案"><a href="#第一种方案" class="headerlink" title="第一种方案"></a>第一种方案</h3><h4 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h4><p>新增场景下，每消费一条source kafka用户数据就判断一次是否为新增，判断方式可以选择自己维护历史全量数据，或者使用数据服务化提供的接口，最后将结果写入sink kafka</p><h4 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h4><table style="text-align: center;">    <thead>        <tr>            <th style="width: 20%">数据处理阶段</th>            <th style="width: 30%">问题</th>            <th style="width: 20%">结果（仅仅指当前问题会产生的结果）</th>            <th style="width: 10%">是否可解决</th>        </tr>    </thead>    <tbody>        <tr>            <td>数据source</td>            <td>同一个用户的行为数据到达时间间隔很小，几秒内就可能会产生几十条行为日志，判断是否为新增，活跃用户时可能会被重复判断</td>            <td>最终数据结果＞真实结果</td>            <td>可部分解决</td>        </tr>        <tr>            <td>数据process</td>            <td>自己维护全量数据<br>1.每判断一条就更新历史全量数据就不存在问题<br>2.如果历史全量数据更新有问题就会产生和数据服务化一样的下面两种问题</td>            <td></td>            <td>可部分解决</td>        </tr>        <tr>            <td>数据process</td>            <td>数据服务化维护全量数据且更新不及时<br>在新增的场景下，一个新增用户使用app可能会在短时间内上报成百上千条行为日志，如果第一条数据判断出来这个用户是新增，下一条数据判断时，数据服务化提供的全量用户里还没有及时将这条新增用户数据添加进去，则这条数据也会被判断为新增，就会导致最终结果重复</td>            <td>最终数据结果＞真实结果</td>            <td>可部分解决</td>        </tr>        <tr>            <td>数据process</td>            <td>数据服务化维护全量数据且更新过快<br>数据服务化更新速度快于flink消费source kafka的速度：就会导致本来是新增的设备被判断不是新增，导致最终结果漏判</td>            <td>最终数据结果＜真实结果</td>            <td>暂时无法解决</td>        </tr>    </tbody></table><h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><p>后续解决方法只讨论上述可部分解决的问题</p><table style="text-align: center;">    <thead>        <tr>            <th style="width: 10%">数据处理阶段</th>            <th style="width: 20%">问题</th>            <th style="width: 30%">解决方案</th>        </tr>    </thead>    <tbody>        <tr>            <td>数据source</td>            <td>同一个用户的行为数据到达时间间隔很小，可能会被重复判断</td>            <td rowspan="2">使用flink窗口解决部分问题<br>使用滚动窗口解决，将一段时间内的用户行为收集，然后到达窗口结束时间处理后再进行上报。假设设置一小时的窗口，则将这一小时的用户行为数据只取一条进行判断是否为新增，则可以极大的保证当前用户判断为新增时，下一小时窗口中这个用户不太可能被判断为新增了，因为数据服务化没有那么慢</td>        </tr>        <tr>            <td>数据process</td>            <td>数据服务化维护全量数据且更新不及时，最终结果重复</td>        </tr>    </tbody></table><h3 id="第一种方案-gt-第二种方案"><a href="#第一种方案-gt-第二种方案" class="headerlink" title="第一种方案-&gt;第二种方案"></a>第一种方案-&gt;第二种方案</h3><h4 id="方案-1"><a href="#方案-1" class="headerlink" title="方案"></a>方案</h4><p>使用窗口可以部分解决在新增活跃等场景下用户行为数据重复的问题</p><p>但是使用了窗口也会引入问题，就是虽然大窗口可以保证尽可能去重，但是数据的实时性大大降低，所以窗口设置不能大也不能小，窗口大保证不了数据产出及时性，窗口小去重效果差，所以最大窗口就为一分钟，和用户期望看板中结果一致</p><h4 id="存在的问题-1"><a href="#存在的问题-1" class="headerlink" title="存在的问题"></a>存在的问题</h4><table style="text-align: center;">    <thead>        <tr>            <th style="width: 20%">数据处理阶段</th>            <th style="width: 30%">问题</th>            <th style="width: 20%">结果（仅仅指当前问题会产生的结果）</th>            <th style="width: 10%">是否可解决</th>        </tr>    </thead>    <tbody>        <tr>            <td>数据source</td>            <td rowspan="2">由于用户行为数据到达source，或者从source到达process阶段，由于网络延迟等的问题，会导致处理用户数据时有乱序情况<br>比如计算实时活跃设备，上一分钟的数据如果下一分钟才到达，则该条数据就会被上一分钟漏算</td>            <td rowspan="2">最终数据结果＜真实结果</td>            <td rowspan="2">可部分解决</td>        </tr>        <tr>            <td>数据process</td>        </tr>    </tbody></table><h4 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h4><table style="text-align: center;">    <thead>        <tr>            <th style="width: 10%">数据处理阶段</th>            <th style="width: 20%">问题</th>            <th style="width: 30%">解决方案</th>        </tr>    </thead>    <tbody>        <tr>            <td>数据source</td>            <td rowspan="2">数据乱序延迟漏算</td>            <td rowspan="2">在flink窗口计算中，通过timestamp和watermark特性来尽可能解决</td>        </tr>        <tr>            <td>数据process</td>        </tr>    </tbody></table><h3 id="第二种方案-gt-第三种方案"><a href="#第二种方案-gt-第三种方案" class="headerlink" title="第二种方案-&gt;第三种方案"></a>第二种方案-&gt;第三种方案</h3><h4 id="方案-2"><a href="#方案-2" class="headerlink" title="方案"></a>方案</h4><p>设置一分钟的窗口，然后设置一分钟的最大延迟等待时间，其语义是保证数据最多延迟一分钟到达，只要可以保证这个语义可以保证最后数据的正确性</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
      <category term="实时计算" scheme="https://yangyichao-mango.github.io/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：BoundedOutOfOrdernessTimestampExtractor</title>
    <link href="https://yangyichao-mango.github.io/2019/11/02/apache-flink:study-BoundedOutOfOrdernessTimestampExtractor/"/>
    <id>https://yangyichao-mango.github.io/2019/11/02/apache-flink:study-BoundedOutOfOrdernessTimestampExtractor/</id>
    <published>2019-11-02T08:48:43.000Z</published>
    <updated>2019-11-02T11:03:54.828Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：BoundedOutOfOrdernessTimestampExtractor</p><a id="more"></a><h2 id="源码"><a href="#源码" class="headerlink" title="源码"></a><strong>源码</strong></h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Licensed to the Apache Software Foundation (ASF) under one</span></span><br><span class="line"><span class="comment"> * or more contributor license agreements.  See the NOTICE file</span></span><br><span class="line"><span class="comment"> * distributed with this work for additional information</span></span><br><span class="line"><span class="comment"> * regarding copyright ownership.  The ASF licenses this file</span></span><br><span class="line"><span class="comment"> * to you under the Apache License, Version 2.0 (the</span></span><br><span class="line"><span class="comment"> * "License"); you may not use this file except in compliance</span></span><br><span class="line"><span class="comment"> * with the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"> * distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"> * See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"> * limitations under the License.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> org.apache.flink.streaming.api.functions.timestamps;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.watermark.Watermark;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * This is a &#123;<span class="doctag">@link</span> AssignerWithPeriodicWatermarks&#125; used to emit Watermarks that lag behind the element with</span></span><br><span class="line"><span class="comment"> * the maximum timestamp (in event time) seen so far by a fixed amount of time, &lt;code&gt;t_late&lt;/code&gt;. This can</span></span><br><span class="line"><span class="comment"> * help reduce the number of elements that are ignored due to lateness when computing the final result for a</span></span><br><span class="line"><span class="comment"> * given window, in the case where we know that elements arrive no later than &lt;code&gt;t_late&lt;/code&gt; units of time</span></span><br><span class="line"><span class="comment"> * after the watermark that signals that the system event-time has advanced past their (event-time) timestamp.</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">BoundedOutOfOrdernessTimestampExtractor</span>&lt;<span class="title">T</span>&gt; <span class="keyword">implements</span> <span class="title">AssignerWithPeriodicWatermarks</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** The current maximum timestamp seen so far. */</span></span><br><span class="line"><span class="comment">/** 数据流的最大时间戳 */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">long</span> currentMaxTimestamp;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** The timestamp of the last emitted watermark. */</span></span><br><span class="line"><span class="comment">/** 最后一次已提交的最新 [水印]（当前批次水印） */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">long</span> lastEmittedWatermark = Long.MIN_VALUE;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The (fixed) interval between the maximum seen timestamp seen in the records</span></span><br><span class="line"><span class="comment"> * and that of the watermark to be emitted.</span></span><br><span class="line"><span class="comment"> * 最大乱序时间间隔</span></span><br><span class="line"><span class="comment"> * 将要被提交的 [水印] 和 [数据流的最大时间戳] 的固定时间间隔</span></span><br><span class="line"><span class="comment"> * 如果 [数据流的最大时间戳] - [当前批次水印] &gt; [最大乱序时间间隔]</span></span><br><span class="line"><span class="comment"> * 则就会打上一个新的 [水印]</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> maxOutOfOrderness;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">BoundedOutOfOrdernessTimestampExtractor</span><span class="params">(Time maxOutOfOrderness)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (maxOutOfOrderness.toMilliseconds() &lt; <span class="number">0</span>) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Tried to set the maximum allowed "</span> +</span><br><span class="line"><span class="string">"lateness to "</span> + maxOutOfOrderness + <span class="string">". This parameter cannot be negative."</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">this</span>.maxOutOfOrderness = maxOutOfOrderness.toMilliseconds();</span><br><span class="line"><span class="keyword">this</span>.currentMaxTimestamp = Long.MIN_VALUE + <span class="keyword">this</span>.maxOutOfOrderness;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getMaxOutOfOrdernessInMillis</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> maxOutOfOrderness;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Extracts the timestamp from the given element.</span></span><br><span class="line"><span class="comment"> * 从当前数据流元素中获取 [时间戳] 字段，需要用户根据业务自定义</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> element The element that the timestamp is extracted from.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> The new timestamp.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(T element)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果 [当前数据流最大时间戳] - [最大乱序时间间隔] &gt;= [最后一次已提交的时间戳]</span></span><br><span class="line"><span class="comment"> * 则更新 [最后一次已提交的时间戳]</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> Watermark <span class="title">getCurrentWatermark</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">// this guarantees that the watermark never goes backwards.</span></span><br><span class="line"><span class="keyword">long</span> potentialWM = currentMaxTimestamp - maxOutOfOrderness;</span><br><span class="line"><span class="keyword">if</span> (potentialWM &gt;= lastEmittedWatermark) &#123;</span><br><span class="line">lastEmittedWatermark = potentialWM;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> Watermark(lastEmittedWatermark);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取数据流中当前最大时间戳</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(T element, <span class="keyword">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class="line"><span class="keyword">long</span> timestamp = extractTimestamp(element);</span><br><span class="line"><span class="keyword">if</span> (timestamp &gt; currentMaxTimestamp) &#123;</span><br><span class="line">currentMaxTimestamp = timestamp;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> timestamp;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id><a href="#" class="headerlink" title="****"></a>****</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Sets the time characteristic for all streams create from this environment, e.g., processing</span></span><br><span class="line"><span class="comment"> * time, event time, or ingestion time.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;If you set the characteristic to IngestionTime of EventTime this will set a default</span></span><br><span class="line"><span class="comment"> * watermark update interval of 200 ms. If this is not applicable for your application</span></span><br><span class="line"><span class="comment"> * you should change it using &#123;<span class="doctag">@link</span> ExecutionConfig#setAutoWatermarkInterval(long)&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> characteristic The time characteristic.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setStreamTimeCharacteristic</span><span class="params">(TimeCharacteristic characteristic)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.timeCharacteristic = Preconditions.checkNotNull(characteristic);</span><br><span class="line">    <span class="keyword">if</span> (characteristic == TimeCharacteristic.ProcessingTime) &#123;</span><br><span class="line">        getConfig().setAutoWatermarkInterval(<span class="number">0</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        getConfig().setAutoWatermarkInterval(<span class="number">200</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="window触发机制"><a href="#window触发机制" class="headerlink" title="window触发机制"></a><strong>window触发机制</strong></h2><p>window的触发机制，是先按照自然时间将window划分，如果window大小是3秒，那么1分钟内会把window划分为如下的形式:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[00:00:00,00:00:03)</span><br><span class="line">[00:00:03,00:00:06)</span><br><span class="line">...</span><br><span class="line">[00:00:57,00:01:00)</span><br></pre></td></tr></table></figure><p>如果window大小是10秒，则window会被分为如下的形式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[00:00:00,00:00:10)</span><br><span class="line">[00:00:10,00:00:20)</span><br><span class="line">...</span><br><span class="line">[00:00:50,00:01:00)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Hadoop 学习：hdfs shell 命令</title>
    <link href="https://yangyichao-mango.github.io/2019/10/30/apache-hadoop:study-hdfs-shell/"/>
    <id>https://yangyichao-mango.github.io/2019/10/30/apache-hadoop:study-hdfs-shell/</id>
    <published>2019-10-30T08:27:45.000Z</published>
    <updated>2019-10-30T08:32:07.350Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Hadoop 学习：hdfs shell 命令</p><a id="more"></a><h3 id="ls"><a href="#ls" class="headerlink" title="ls"></a><strong>ls</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -ls /</span><br></pre></td></tr></table></figure><h3 id="put"><a href="#put" class="headerlink" title="put"></a><strong>put</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put localfile /user/hadoop/hadoopfile</span><br></pre></td></tr></table></figure><h3 id="get"><a href="#get" class="headerlink" title="get"></a><strong>get</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -get /user/hadoop/hadoopfile localfile</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/categories/Apache-Hadoop/"/>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/tags/Apache-Hadoop/"/>
    
      <category term="Hdfs" scheme="https://yangyichao-mango.github.io/tags/Hdfs/"/>
    
  </entry>
  
  <entry>
    <title>Mac安装Nginx-1.17.3以及相关配置文件</title>
    <link href="https://yangyichao-mango.github.io/2019/10/28/nginx:1-17-3-mac-install-and-confs-set/"/>
    <id>https://yangyichao-mango.github.io/2019/10/28/nginx:1-17-3-mac-install-and-confs-set/</id>
    <published>2019-10-28T13:19:10.000Z</published>
    <updated>2019-10-29T02:56:24.767Z</updated>
    
    <content type="html"><![CDATA[<p>Mac安装Nginx-1.17.3以及相关配置文件</p><a id="more"></a><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a><strong>安装</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install nginx</span><br></pre></td></tr></table></figure><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a><strong>配置</strong></h2><p>nginx是一个功能非常强大的web服务器加反向代理服务器，同时又是邮件服务器等等</p><p>在项目使用中，使用最多的三个核心功能是反向代理、负载均衡和静态服务器</p><p>这三个不同的功能的使用，都跟nginx的配置密切相关，nginx服务器的配置信息主要集中在nginx.conf这个配置文件中，并且所有的可配置选项大致分为以下几个部分</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">main                                <span class="comment"># 全局配置</span></span><br><span class="line"></span><br><span class="line">events &#123;                            <span class="comment"># nginx工作模式配置</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;                                <span class="comment"># http设置</span></span><br><span class="line">    ....</span><br><span class="line"></span><br><span class="line">    server &#123;                        <span class="comment"># 服务器主机配置</span></span><br><span class="line">        ....</span><br><span class="line">        location &#123;                    <span class="comment"># 路由配置</span></span><br><span class="line">            ....</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        location path &#123;</span><br><span class="line">            ....</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        location otherpath &#123;</span><br><span class="line">            ....</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    server &#123;</span><br><span class="line">        ....</span><br><span class="line"></span><br><span class="line">        location &#123;</span><br><span class="line">            ....</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    upstream name &#123;                    <span class="comment"># 负载均衡配置</span></span><br><span class="line">        ....</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如上述配置文件所示，主要由6个部分组成：</p><p>1.main：用于进行nginx全局信息的配置<br>2.events：用于nginx工作模式的配置<br>3.http：用于进行http协议信息的一些配置<br>4.server：用于进行服务器访问信息的配置<br>5.location：用于进行访问路由的配置<br>6.upstream：用于进行负载均衡的配置</p><h2 id="main模块"><a href="#main模块" class="headerlink" title="main模块"></a><strong>main模块</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># user nobody nobody;</span></span><br><span class="line">worker_processes 2;</span><br><span class="line"><span class="comment"># error_log logs/error.log</span></span><br><span class="line"><span class="comment"># error_log logs/error.log notice</span></span><br><span class="line"><span class="comment"># error_log logs/error.log info</span></span><br><span class="line"><span class="comment"># pid logs/nginx.pid</span></span><br><span class="line">worker_rlimit_nofile 1024;</span><br></pre></td></tr></table></figure><p>上述配置都是存放在main全局配置模块中的配置项</p><p>1.user：用来指定nginx worker进程运行用户以及用户组，默认nobody账号运行<br>2.worker_processes：指定nginx要开启的子进程数量，运行过程中监控每个进程消耗内存(一般几M~几十M不等)根据实际情况进行调整，通常数量是CPU内核数量的整数倍<br>3.error_log：定义错误日志文件的位置及输出级别【debug / info / notice / warn / error / crit】<br>4.pid：用来指定进程id的存储文件的位置<br>5.worker_rlimit_nofile：用于指定一个进程可以打开最多文件数量的描述</p><h2 id="events模块"><a href="#events模块" class="headerlink" title="events模块"></a><strong>events模块</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">events &#123;</span><br><span class="line">    worker_connections 1024;</span><br><span class="line">    multi_accept on;</span><br><span class="line">    use epoll;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述配置是针对nginx服务器的工作模式的一些操作配置</p><p>1.worker_connections：指定最大可以同时接收的连接数量，这里一定要注意，最大连接数量是和worker processes共同决定的<br>2.multi_accept：配置指定nginx在收到一个新连接通知后尽可能多的接受更多的连接<br>3.use epoll：配置指定了线程轮询的方法，如果是linux2.6+，使用epoll，如果是BSD如Mac请使用Kqueue</p><h2 id="http模块"><a href="#http模块" class="headerlink" title="http模块"></a><strong>http模块</strong></h2><p>作为web服务器，http模块是nginx最核心的一个模块，配置项也是比较多的，项目中会设置到很多的实际业务场景，需要根据硬件信息进行适当的配置，常规情况下，使用默认配置即可！</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">    <span class="comment">##</span></span><br><span class="line">    <span class="comment"># 基础配置</span></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line"></span><br><span class="line">    sendfile on;</span><br><span class="line">    tcp_nopush on;</span><br><span class="line">    tcp_nodelay on;</span><br><span class="line">    keepalive_timeout 65;</span><br><span class="line">    types_hash_max_size 2048;</span><br><span class="line">    <span class="comment"># server_tokens off;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># server_names_hash_bucket_size 64;</span></span><br><span class="line">    <span class="comment"># server_name_in_redirect off;</span></span><br><span class="line"></span><br><span class="line">    include /etc/nginx/mime.types;</span><br><span class="line">    default_type application/octet-stream;</span><br><span class="line"></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line">    <span class="comment"># SSL证书配置</span></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line"></span><br><span class="line">    ssl_protocols TLSv1 TLSv1.1 TLSv1.2; <span class="comment"># Dropping SSLv3, ref: POODLE</span></span><br><span class="line">    ssl_prefer_server_ciphers on;</span><br><span class="line"></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line">    <span class="comment"># 日志配置</span></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line"></span><br><span class="line">    access_log /var/<span class="built_in">log</span>/nginx/access.log;</span><br><span class="line">    error_log /var/<span class="built_in">log</span>/nginx/error.log;</span><br><span class="line"></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line">    <span class="comment"># Gzip 压缩配置</span></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line"></span><br><span class="line">    gzip on;</span><br><span class="line">    gzip_disable <span class="string">"msie6"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># gzip_vary on;</span></span><br><span class="line">    <span class="comment"># gzip_proxied any;</span></span><br><span class="line">    <span class="comment"># gzip_comp_level 6;</span></span><br><span class="line">    <span class="comment"># gzip_buffers 16 8k;</span></span><br><span class="line">    <span class="comment"># gzip_http_version 1.1;</span></span><br><span class="line">    <span class="comment"># gzip_types text/plain text/css application/json application/javascript</span></span><br><span class="line"> text/xml application/xml application/xml+rss text/javascript;</span><br><span class="line"></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line">    <span class="comment"># 虚拟主机配置</span></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line"></span><br><span class="line">    include /etc/nginx/conf.d/*.conf;</span><br><span class="line">    include /etc/nginx/sites-enabled/*;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="基础配置"><a href="#基础配置" class="headerlink" title="基础配置"></a>基础配置</h3><p>1.sendfile on：配置on让sendfile发挥作用，将文件的回写过程交给数据缓冲去去完成，而不是放在应用中完成，这样的话在性能提升有有好处<br>2.tc_nopush on：让nginx在一个数据包中发送所有的头文件，而不是一个一个单独发<br>3.tcp_nodelay on：让nginx不要缓存数据，而是一段一段发送，如果数据的传输有实时性的要求的话可以配置它，发送完一小段数据就立刻能得到返回值，但是不要滥用哦</p><p>4.keepalive_timeout 10：给客户端分配连接超时时间，服务器会在这个时间过后关闭连接。一般设置时间较短，可以让nginx工作持续性更好<br>5.client_header_timeout 10：设置请求头的超时时间<br>6.client_body_timeout 10：设置请求体的超时时间<br>7.send_timeout 10：指定客户端响应超时时间，如果客户端两次操作间隔超过这个时间，服务器就会关闭这个链接</p><p>8.limit_conn_zone $binary_remote_addr zone=addr:5m ：设置用于保存各种key的共享内存的参数，<br>9.limit_conn addr 100: 给定的key设置最大连接数</p><p>10.server_tokens：虽然不会让nginx执行速度更快，但是可以在错误页面关闭nginx版本提示，对于网站安全性的提升有好处哦<br>11.include /etc/nginx/mime.types：指定在当前文件中包含另一个文件的指令<br>12.default_type application/octet-stream：指定默认处理的文件类型可以是二进制<br>13.type_hash_max_size 2048：混淆数据，影响三列冲突率，值越大消耗内存越多，散列key冲突率会降低，检索速度更快；值越小key，占用内存较少，冲突率越高，检索速度变慢</p><h3 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h3><p>1.access_log logs/access.log：设置存储访问记录的日志<br>2.error_log logs/error.log：设置存储记录错误发生的日志</p><h3 id="压缩配置"><a href="#压缩配置" class="headerlink" title="压缩配置"></a>压缩配置</h3><p>1.gzip：是告诉nginx采用gzip压缩的形式发送数据。这将会减少我们发送的数据量。<br>2.gzip_disable：为指定的客户端禁用gzip功能。我们设置成IE6或者更低版本以使我们的方案能够广泛兼容。<br>3.gzip_static：告诉nginx在压缩资源之前，先查找是否有预先gzip处理过的资源。这要求你预先压缩你的文件（在这个例子中被注释掉了），从而允许你使用最高压缩比，这样nginx就不用再压缩这些文件了（想要更详尽的gzip_static的信息，请点击这里）。<br>4.gzip_proxied：允许或者禁止压缩基于请求和响应的响应流。我们设置为any，意味着将会压缩所有的请求。<br>5.gzip_min_length：设置对数据启用压缩的最少字节数。如果一个请求小于1000字节，我们最好不要压缩它，因为压缩这些小的数据会降低处理此请求的所有进程的速度。<br>6.gzip_comp_level：设置数据的压缩等级。这个等级可以是1-9之间的任意数值，9是最慢但是压缩比最大的。我们设置为4，这是一个比较折中的设置。<br>7.gzip_type：设置需要压缩的数据格式。上面例子中已经有一些了，你也可以再添加更多的格式。</p><h3 id="文件缓存配置"><a href="#文件缓存配置" class="headerlink" title="文件缓存配置"></a>文件缓存配置</h3><p>1.open_file_cache：打开缓存的同时也指定了缓存最大数目，以及缓存的时间。我们可以设置一个相对高的最大时间，这样我们可以在它们不活动超过20秒后清除掉。<br>2.open_file_cache_valid：在open_file_cache中指定检测正确信息的间隔时间。<br>3.open_file_cache_min_uses：定义了open_file_cache中指令参数不活动时间期间里最小的文件数。<br>4.open_file_cache_errors：指定了当搜索一个文件时是否缓存错误信息，也包括再次给配置中添加文件。我们也包括了服务器模块，这些是在不同文件中定义的。如果你的服务器模块不在这些位置，你就得修改这一行来指定正确的位置。</p><h3 id="server模块"><a href="#server模块" class="headerlink" title="server模块"></a>server模块</h3><p>server模块配置是http模块中的一个子模块，用来定义一个虚拟访问主机，也就是一个虚拟服务器的配置信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen        80;</span><br><span class="line">    server_name localhost    192.168.1.100;</span><br><span class="line">    root        /nginx/www;</span><br><span class="line">    index        index.php index.html index.html;</span><br><span class="line">    charset        utf-8;</span><br><span class="line">    access_log    logs/access.log;</span><br><span class="line">    error_log    logs/error.log;</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>核心配置信息如下：</p><p>1.server：一个虚拟主机的配置，一个http中可以配置多个server</p><p>2.server_name：用力啊指定ip地址或者域名，多个配置之间用空格分隔</p><p>3.root：表示整个server虚拟主机内的根目录，所有当前主机中web项目的根目录</p><p>4.index：用户访问web网站时的全局首页</p><p>5.charset：用于设置www/路径中配置的网页的默认编码格式</p><p>6.access_log：用于指定该虚拟主机服务器中的访问记录日志存放路径</p><p>7.error_log：用于指定该虚拟主机服务器中访问错误日志的存放路径</p><h3 id="location模块"><a href="#location模块" class="headerlink" title="location模块"></a>location模块</h3><p>location模块是nginx配置中出现最多的一个配置，主要用于配置路由访问信息</p><p>在路由访问信息配置中关联到反向代理、负载均衡等等各项功能，所以location模块也是一个非常重要的配置模块</p><h4 id="基本配置"><a href="#基本配置" class="headerlink" title="基本配置"></a>基本配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">location / &#123;</span><br><span class="line">    root    /nginx/www;</span><br><span class="line">    index    index.php index.html index.htm;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>location /：表示匹配访问根目录</p><p>root：用于指定访问根目录时，访问虚拟主机的web目录</p><p>index：在不指定访问具体资源时，默认展示的资源文件列表</p><h4 id="反向代理配置方式"><a href="#反向代理配置方式" class="headerlink" title="反向代理配置方式"></a>反向代理配置方式</h4><p>通过反向代理代理服务器访问模式，通过proxy_set配置让客户端访问透明化</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">location / &#123;</span><br><span class="line">    proxy_pass http://localhost:8888;</span><br><span class="line">    proxy_set_header X-real-ip <span class="variable">$remote_addr</span>;</span><br><span class="line">    proxy_set_header Host <span class="variable">$http_host</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="uwsgi配置"><a href="#uwsgi配置" class="headerlink" title="uwsgi配置"></a>uwsgi配置</h4><p>wsgi模式下的服务器配置访问方式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">location / &#123;</span><br><span class="line">    include uwsgi_params;</span><br><span class="line">    uwsgi_pass localhost:8888</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="upstream模块"><a href="#upstream模块" class="headerlink" title="upstream模块"></a>upstream模块</h3><p>upstream模块主要负责负载均衡的配置，通过默认的轮询调度方式来分发请求到后端服务器</p><p>简单的配置方式如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">upstream name &#123;</span><br><span class="line">    ip_hash;</span><br><span class="line">    server 192.168.1.100:8000;</span><br><span class="line">    server 192.168.1.100:8001 down;</span><br><span class="line">    server 192.168.1.100:8002 max_fails=3;</span><br><span class="line">    server 192.168.1.100:8003 fail_timeout=20s;</span><br><span class="line">    server 192.168.1.100:8004 max_fails=3 fail_timeout=20s;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>核心配置信息如下</p><p>1.ip_hash：指定请求调度算法，默认是weight权重轮询调度，可以指定</p><p>2.server host:port：分发服务器的列表配置</p><p>– down：表示该主机暂停服务</p><p>– max_fails：表示失败最大次数，超过失败最大次数暂停服务</p><p>– fail_timeout：表示如果请求受理失败，暂停指定的时间之后重新发起请求</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Nginx" scheme="https://yangyichao-mango.github.io/categories/Nginx/"/>
    
    
      <category term="Nginx" scheme="https://yangyichao-mango.github.io/tags/Nginx/"/>
    
  </entry>
  
  <entry>
    <title>Elastic-Job学习：分布式任务调度框架</title>
    <link href="https://yangyichao-mango.github.io/2019/10/26/elastic-job:study-distributed-scheduled-job-framework/"/>
    <id>https://yangyichao-mango.github.io/2019/10/26/elastic-job:study-distributed-scheduled-job-framework/</id>
    <published>2019-10-26T14:08:15.000Z</published>
    <updated>2019-10-28T03:23:31.499Z</updated>
    
    <content type="html"><![CDATA[<p>分布式任务调度框架学习</p><a id="more"></a><p><a href="http://elasticjob.io/docs/elastic-job-lite/00-overview/intro/" target="_blank" rel="noopener">官方文档</a></p><p><a href="https://github.com/elasticjob" target="_blank" rel="noopener">elastic-job github</a></p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="分布式调度框架" scheme="https://yangyichao-mango.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E8%B0%83%E5%BA%A6%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Apache Zookeeper" scheme="https://yangyichao-mango.github.io/tags/Apache-Zookeeper/"/>
    
      <category term="分布式调度框架" scheme="https://yangyichao-mango.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%B0%83%E5%BA%A6%E6%A1%86%E6%9E%B6/"/>
    
      <category term="SpringBoot" scheme="https://yangyichao-mango.github.io/tags/SpringBoot/"/>
    
  </entry>
  
  <entry>
    <title>Vue学习：Vue前端项目部署到SpringBoot工程下</title>
    <link href="https://yangyichao-mango.github.io/2019/10/26/vue:study-vue-project-deploy-in-springboot-project/"/>
    <id>https://yangyichao-mango.github.io/2019/10/26/vue:study-vue-project-deploy-in-springboot-project/</id>
    <published>2019-10-26T07:29:37.000Z</published>
    <updated>2019-10-26T07:52:06.519Z</updated>
    
    <content type="html"><![CDATA[<p>Vue项目部署到SpringBoot工程下</p><a id="more"></a><h2 id="Vue前端项目"><a href="#Vue前端项目" class="headerlink" title="Vue前端项目"></a><strong>Vue前端项目</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm run build</span><br></pre></td></tr></table></figure><p>运行上述命令，将前端Vue项目打包，命令运行完成之后会在项目根目录下生成一个生成一个dist文件夹, 编译好的静态文件就在这里面</p><h2 id="部署在SpringBoot项目下"><a href="#部署在SpringBoot项目下" class="headerlink" title="部署在SpringBoot项目下"></a><strong>部署在SpringBoot项目下</strong></h2><p>将前端项目中dist文件夹下的所有文件拷贝到SpringBoot工程的<strong>src/main/resources/static</strong>文件夹下</p><p>运行后端项目，可以看到控制台会有这样的输出，证明将静态页面加入了容器中，现在就可以访问到前端页面了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2019-10-26 15:47:09.607  INFO 1967 --- [           main] o.s.b.a.w.s.WelcomePageHandlerMapping    : Adding welcome page: class path resource [static/index.html]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Vue" scheme="https://yangyichao-mango.github.io/categories/Vue/"/>
    
    
      <category term="SpringBoot" scheme="https://yangyichao-mango.github.io/tags/SpringBoot/"/>
    
      <category term="Vue" scheme="https://yangyichao-mango.github.io/tags/Vue/"/>
    
  </entry>
  
  <entry>
    <title>Maven学习：使用maven-shade-plugin解决依赖包冲突</title>
    <link href="https://yangyichao-mango.github.io/2019/10/25/apache-maven:study-maven-shade-resolve-jar-conflicts/"/>
    <id>https://yangyichao-mango.github.io/2019/10/25/apache-maven:study-maven-shade-resolve-jar-conflicts/</id>
    <published>2019-10-25T06:23:20.000Z</published>
    <updated>2019-10-25T08:21:33.406Z</updated>
    
    <content type="html"><![CDATA[<p>java 依赖包冲突，使用maven-shade-plugin解决</p><a id="more"></a><h3 id="错误场景详情"><a href="#错误场景详情" class="headerlink" title="错误场景详情"></a><strong>错误场景详情</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoSuchMethodError: com.google.common.util.concurrent.MoreExecutors.directExecutor()Ljava/util/concurrent/Executor;</span><br></pre></td></tr></table></figure><h3 id="错误原因"><a href="#错误原因" class="headerlink" title="错误原因"></a><strong>错误原因</strong></h3><p>出现这样的错误详情一般是由于有下面这样的包依赖情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A - B - C(guava version 18)</span><br><span class="line">  \ D - C(guava version 23.6-jre)</span><br></pre></td></tr></table></figure><p><em>A</em>：代表我们所开发的当前项目<br><em>B和D</em>：代表当前项目所依赖的项目<br><em>C</em>：代表当前项目依赖的项目所依赖的项目</p><p>由于我们当前所开发的项目A依赖了B和D，B和D又依赖了项目C<br>我们打包运行项目时，maven只会将一个版本C(guava)打进包内（maven打包遇到相同依赖，最短路径优先，在路径相同时先在pom中声明优先）<br>比如此时打进包的版本是C(guava version 23.6-jre)，那么很有可能在运行B中的一个方法时，调用C的一个方法，这个方法是C(guava version 18)中的一个方法，在C(guava version 23.6-jre)中并不存在，这时候就会报出java.lang.NoSuchMethodError</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a><strong>解决方案</strong></h3><p>使用maven-shade-plugin将所有B项目依赖的包全部打进B.jar中，并且给guava包的路径重命名为我们的自定义路径\</p><h4 id="maven-shade-plugin基本功能"><a href="#maven-shade-plugin基本功能" class="headerlink" title="maven-shade-plugin基本功能"></a>maven-shade-plugin基本功能</h4><p>maven-shade-plugin提供了两大基本功能：</p><p>1、将依赖的jar包打包到当前jar包（常规打包是不会将所依赖jar包打进来的）<br>2、对依赖的jar包进行重命名（用于类的隔离，解决包冲突就是使用了这个功能）</p><h4 id="解决示例"><a href="#解决示例" class="headerlink" title="解决示例"></a>解决示例</h4><p>如下例，就可以在B项目中使用C(guava version 18)，只不过import路径变成了我们自定的路径</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span> <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>RpcModule<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>RpcModule<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">packaging</span>&gt;</span>jar<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>RpcModule<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">guava.version</span>&gt;</span>18<span class="tag">&lt;/<span class="name">guava.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>guava<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;guava.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">source</span>&gt;</span>8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">target</span>&gt;</span>8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">createDependencyReducedPom</span>&gt;</span>false<span class="tag">&lt;/<span class="name">createDependencyReducedPom</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="comment">&lt;!-- 给guava包的路径重命名为我们的自定义路径 --&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">relocations</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>shade.com.google.guava<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>org.joda<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>shade.com.google.joda<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>com.google.common<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>shade.com.google.common<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">relocations</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">transformer</span></span></span><br><span class="line"><span class="tag">                                        <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>/&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><p>使用maven-shade-plugin之前</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> Entity.Request;</span><br><span class="line"><span class="keyword">import</span> Entity.Response;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.*;</span><br><span class="line"><span class="keyword">import</span> java.net.Socket;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.google.common.collect.ImmutableMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SocketClient</span> </span>&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用maven-shade-plugin之后编译后反编译结果</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> Entity.Request;</span><br><span class="line"><span class="keyword">import</span> Entity.Response;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.*;</span><br><span class="line"><span class="keyword">import</span> java.net.Socket;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> shade.com.google.common.collect.ImmutableMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SocketClient</span> </span>&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后将打好的包上传至我们的maven仓库，然后再在当前项目A中依赖，就没有依赖冲突了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A - B - C(guava version 18, shade.com.google.common.collect.ImmutableMap)</span><br><span class="line">  \ D - C(guava version 23.6-jre, com.google.common.collect.ImmutableMap)</span><br></pre></td></tr></table></figure><p>这样就可以做到将两个不同版本的包都引入使用，由于引入包路径不同，因此也没有冲突</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Maven" scheme="https://yangyichao-mango.github.io/categories/Apache-Maven/"/>
    
    
      <category term="Apache Maven" scheme="https://yangyichao-mango.github.io/tags/Apache-Maven/"/>
    
  </entry>
  
  <entry>
    <title>Apache Zookeeper 学习：Zookeeper实现统一配置管理中心</title>
    <link href="https://yangyichao-mango.github.io/2019/10/24/apache-zookeeper:study-zookeeper-implement-unified-configuration-management-center/"/>
    <id>https://yangyichao-mango.github.io/2019/10/24/apache-zookeeper:study-zookeeper-implement-unified-configuration-management-center/</id>
    <published>2019-10-24T10:40:24.000Z</published>
    <updated>2019-11-06T13:07:48.984Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Zookeeper 学习：模拟三台节点组成的Zookeeper集群实现的统一配置管理中心</p><a id="more"></a><h2 id="模拟Zookeeper集群"><a href="#模拟Zookeeper集群" class="headerlink" title="模拟Zookeeper集群"></a><strong>模拟Zookeeper集群</strong></h2><h3 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-zookeeper:study-zookeeper-implement-unified-configuration-management-center/zookeeper集群架构图.png" alt="Zookeeper集群架构" title>                </div>                <div class="image-caption">Zookeeper集群架构</div>            </figure><h3 id="创建zookeeper集群节点"><a href="#创建zookeeper集群节点" class="headerlink" title="创建zookeeper集群节点"></a>创建zookeeper集群节点</h3><p>模拟三台节点组成的zookeeper集群，需要在本机zookeeper目录下<br>创建三个zookeeper集群节点配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> conf/</span><br><span class="line">$ cp zoo_sample.cfg zoo1.cfg</span><br><span class="line">$ cp zoo_sample.cfg zoo2.cfg</span><br><span class="line">$ cp zoo_sample.cfg zoo3.cfg</span><br></pre></td></tr></table></figure><h3 id="创建所配置的各个文件夹"><a href="#创建所配置的各个文件夹" class="headerlink" title="创建所配置的各个文件夹"></a>创建所配置的各个文件夹</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir /tmp/zookeeper1</span><br><span class="line">$ mkdir /tmp/zookeeper1/data</span><br><span class="line">$ mkdir /tmp/zookeeper1/dataLog</span><br><span class="line">$ mkdir /tmp/zookeeper2</span><br><span class="line">$ mkdir /tmp/zookeeper2/data</span><br><span class="line">$ mkdir /tmp/zookeeper2/dataLog</span><br><span class="line">$ mkdir /tmp/zookeeper3</span><br><span class="line">$ mkdir /tmp/zookeeper3/data</span><br><span class="line">$ mkdir /tmp/zookeeper3/dataLog</span><br></pre></td></tr></table></figure><h3 id="tmp-zookeeperX-data文件夹下创建myid文件"><a href="#tmp-zookeeperX-data文件夹下创建myid文件" class="headerlink" title="/tmp/zookeeperX/data文件夹下创建myid文件"></a>/tmp/zookeeperX/data文件夹下创建myid文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> 1 &gt; /tmp/zookeeper1/data/myid</span><br><span class="line">$ <span class="built_in">echo</span> 2 &gt; /tmp/zookeeper2/data/myid</span><br><span class="line">$ <span class="built_in">echo</span> 3 &gt; /tmp/zookeeper3/data/myid</span><br></pre></td></tr></table></figure><h3 id="配置zookeeper节点信息"><a href="#配置zookeeper节点信息" class="headerlink" title="配置zookeeper节点信息"></a>配置zookeeper节点信息</h3><p><strong>zoo1.cfg</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The number of milliseconds of each tick</span></span><br><span class="line">tickTime=2000</span><br><span class="line"><span class="comment"># The number of ticks that the initial</span></span><br><span class="line"><span class="comment"># synchronization phase can take</span></span><br><span class="line">initLimit=10</span><br><span class="line"><span class="comment"># The number of ticks that can pass between</span></span><br><span class="line"><span class="comment"># sending a request and getting an acknowledgement</span></span><br><span class="line">syncLimit=5</span><br><span class="line"><span class="comment"># the directory where the snapshot is stored.</span></span><br><span class="line"><span class="comment"># do not use /tmp for storage, /tmp here is just</span></span><br><span class="line"><span class="comment"># example sakes.</span></span><br><span class="line">dataDir=/tmp/zookeeper1/data</span><br><span class="line">dataLogDir=/tmp/zookeeper1/dataLog</span><br><span class="line"><span class="comment"># the port at which the clients will connect</span></span><br><span class="line">clientPort=2181</span><br><span class="line"><span class="comment"># the maximum number of client connections.</span></span><br><span class="line"><span class="comment"># increase this if you need to handle more clients</span></span><br><span class="line"><span class="comment">#maxClientCnxns=60</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Be sure to read the maintenance section of the</span></span><br><span class="line"><span class="comment"># administrator guide before turning on autopurge.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The number of snapshots to retain in dataDir</span></span><br><span class="line"><span class="comment">#autopurge.snapRetainCount=3</span></span><br><span class="line"><span class="comment"># Purge task interval in hours</span></span><br><span class="line"><span class="comment"># Set to "0" to disable auto purge feature</span></span><br><span class="line"><span class="comment">#autopurge.purgeInterval=1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 格式:server.num=xxxx:port1:port2</span></span><br><span class="line"><span class="comment"># num对应myid中的内容，port1是zookeeper集群中各服务间的通信端口，port2是zookeeper集群选举leader的端口</span></span><br><span class="line">server.1=localhost:2888:3888</span><br><span class="line">server.2=localhost:2899:3899</span><br><span class="line">server.3=localhost:2877:3877</span><br></pre></td></tr></table></figure><p><strong>zoo2.cfg</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the directory where the snapshot is stored.</span></span><br><span class="line"><span class="comment"># do not use /tmp for storage, /tmp here is just</span></span><br><span class="line"><span class="comment"># example sakes.</span></span><br><span class="line">dataDir=/tmp/zookeeper2/data</span><br><span class="line">dataLogDir=/tmp/zookeeper2/dataLog</span><br><span class="line"><span class="comment"># the port at which the clients will connect</span></span><br><span class="line">clientPort=2182</span><br><span class="line">...</span><br><span class="line">server.1=localhost:2888:3888</span><br><span class="line">server.2=localhost:2899:3899</span><br><span class="line">server.3=localhost:2877:3877</span><br></pre></td></tr></table></figure><p><strong>zoo3.cfg</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the directory where the snapshot is stored.</span></span><br><span class="line"><span class="comment"># do not use /tmp for storage, /tmp here is just</span></span><br><span class="line"><span class="comment"># example sakes.</span></span><br><span class="line">dataDir=/tmp/zookeeper3/data</span><br><span class="line">dataLogDir=/tmp/zookeeper3/dataLog</span><br><span class="line"><span class="comment"># the port at which the clients will connect</span></span><br><span class="line">clientPort=2183</span><br><span class="line">...</span><br><span class="line">server.1=localhost:2888:3888</span><br><span class="line">server.2=localhost:2899:3899</span><br><span class="line">server.3=localhost:2877:3877</span><br></pre></td></tr></table></figure><p>配置文件中dataDir，dataLogDir，clientPort，三个zookeeper节点配置信息都不同<br>搭建zookeeper集群，需要在每个zookeeper安装目录下的data文件中创建名为myid的文件，修改zooX.cfg内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">server.1=xxx:2888:3888</span><br><span class="line">server.2=xxx:2899:3899</span><br><span class="line">server.3=xxx:2877:3877</span><br></pre></td></tr></table></figure><p>格式:server.num=xxxx:port1:port2<br>num对应myid中的内容，port1是zookeeper集群中各服务间的通信端口，port2是zookeeper集群选举leader的端口</p><h3 id="启动模拟集群节点"><a href="#启动模拟集群节点" class="headerlink" title="启动模拟集群节点"></a>启动模拟集群节点</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ ./zkServer.sh start ../conf/zoo1.cfg</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: ../conf/zoo1.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">$ ./zkServer.sh start ../conf/zoo2.cfg</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: ../conf/zoo2.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">$ ./zkServer.sh start ../conf/zoo3.cfg</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: ../conf/zoo3.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure><p>查看集群状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ ./zkServer.sh status ../conf/zoo1.cfg</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: ../conf/zoo1.cfg</span><br><span class="line">Mode: follower</span><br><span class="line">$ ./zkServer.sh status ../conf/zoo2.cfg</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: ../conf/zoo2.cfg</span><br><span class="line">Mode: leader</span><br><span class="line">$ ./zkServer.sh status ../conf/zoo3.cfg</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: ../conf/zoo3.cfg</span><br><span class="line">Mode: follower</span><br></pre></td></tr></table></figure><h2 id="创建zookeeper集群client"><a href="#创建zookeeper集群client" class="headerlink" title="创建zookeeper集群client"></a><strong>创建zookeeper集群client</strong></h2><h3 id="创建监听节点变化的server（zookeeper集群client）"><a href="#创建监听节点变化的server（zookeeper集群client）" class="headerlink" title="创建监听节点变化的server（zookeeper集群client）"></a>创建监听节点变化的server（zookeeper集群client）</h3><p>模拟监听节点变化server，启动两个BaseWatcher程序作为监听server（对zookeeper集群来说是client）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BaseWatcher</span> <span class="keyword">implements</span> <span class="title">Watcher</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> ZooKeeper zookeeper;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 超时时间</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> SESSION_TIME_OUT = <span class="number">2000</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> CountDownLatch defaultCountDownLatch = <span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> CountDownLatch childrenCountDownLatch = <span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> CountDownLatch dataCountDownLatch = <span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (event.getState() == KeeperState.SyncConnected) &#123;</span><br><span class="line">            LOGGER.info(<span class="string">"Watch received event"</span>);</span><br><span class="line">            defaultCountDownLatch.countDown();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (event.getType() == EventType.NodeCreated) &#123;</span><br><span class="line">            LOGGER.info(<span class="string">"创建节点"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (event.getType() == EventType.NodeDataChanged) &#123;</span><br><span class="line">            LOGGER.info(<span class="string">"节点改变"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (event.getType() == EventType.NodeChildrenChanged) &#123;</span><br><span class="line">            LOGGER.info(<span class="string">"子节点节点改变"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (event.getType() == EventType.NodeDeleted) &#123;</span><br><span class="line">            LOGGER.info(<span class="string">"节点删除"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 连接zookeeper</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> host</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception </span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">connectZookeeper</span><span class="params">(String host, Watcher defaultWatcher)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        zookeeper = <span class="keyword">new</span> ZooKeeper(host, SESSION_TIME_OUT, defaultWatcher);</span><br><span class="line">        defaultCountDownLatch.await();</span><br><span class="line">        LOGGER.info(<span class="string">"zookeeper connection success"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取路径下所有子节点</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> path</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> KeeperException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;String&gt; <span class="title">getChildren</span><span class="params">(String path, Watcher childrenWatcher)</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> zookeeper.getChildren(path, childrenWatcher);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取节点上面的数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> path  路径</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> KeeperException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException </span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">getData</span><span class="params">(String path, Watcher dataWatcher)</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">byte</span>[] data = zookeeper.getData(path, dataWatcher, <span class="keyword">null</span>);</span><br><span class="line">        <span class="keyword">if</span> (data == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> String(data);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 关闭连接</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">closeConnection</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (zookeeper != <span class="keyword">null</span>) &#123;</span><br><span class="line">            zookeeper.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String HOST = <span class="string">"localhost:2181,localhost:2182,localhost:2183"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = LoggerFactory.getLogger(BaseWatcher.class);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 运行程序之前需要启动zookeeper服务端，&#123;<span class="doctag">@link</span> BaseWatcher.HOST&#125; 根据zookeeper服务端具体配置去修改</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * 除了默认watcher外其他watcher一旦触发就会失效，需要充新注册，本示例中因为</span></span><br><span class="line"><span class="comment">         * 还未想到比较好的重新注册watcher方式(考虑到如果在Watcher中持有一个zk客户端的</span></span><br><span class="line"><span class="comment">         * 实例可能存在循环引用的问题)，因此暂不实现watcher失效后重新注册watcher的问题，</span></span><br><span class="line"><span class="comment">         * 后续可以查阅curator重新注册watcher的实现方法。</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        BaseWatcher defaultWatcher = <span class="keyword">new</span> BaseWatcher();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 连接zookeeper并设置一个默认的watcher监听zookeeper文件节点的变化</span></span><br><span class="line">        connectZookeeper(HOST, defaultWatcher);</span><br><span class="line">        TimeUnit.SECONDS.sleep(<span class="number">1000000</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="创建改变节点数据的server（zookeeper集群client）"><a href="#创建改变节点数据的server（zookeeper集群client）" class="headerlink" title="创建改变节点数据的server（zookeeper集群client）"></a>创建改变节点数据的server（zookeeper集群client）</h3><p>如果一个节点向被监听节点中写数据，其他节点就会接受到zookeeper的 <strong>NodeDataChanged</strong> event</p><p>模拟改变数据server，启动一个BaseWatcher程序作为改变数据server（对zookeeper集群来说是client）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 运行程序之前需要启动zookeeper服务端，&#123;<span class="doctag">@link</span> BaseWatcher.HOST&#125; 根据zookeeper服务端具体配置去修改</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 除了默认watcher外其他watcher一旦触发就会失效，需要充新注册，本示例中因为</span></span><br><span class="line"><span class="comment">     * 还未想到比较好的重新注册watcher方式(考虑到如果在Watcher中持有一个zk客户端的</span></span><br><span class="line"><span class="comment">     * 实例可能存在循环引用的问题)，因此暂不实现watcher失效后重新注册watcher的问题，</span></span><br><span class="line"><span class="comment">     * 后续可以查阅curator重新注册watcher的实现方法。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    BaseWatcher defaultWatcher = <span class="keyword">new</span> BaseWatcher();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 连接zookeeper并设置一个默认的watcher监听zookeeper文件节点的变化</span></span><br><span class="line">    connectZookeeper(HOST, defaultWatcher);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 向/GetChildren节点写数据之前需要先创建此文件夹</span></span><br><span class="line">    <span class="comment">// 向/GetChildren节点写数据，则监听程序就会收到zookeeper的 [NodeDataChanged] event</span></span><br><span class="line">    setData(<span class="string">"/GetChildren"</span>, <span class="string">"8"</span>);</span><br><span class="line">    TimeUnit.SECONDS.sleep(<span class="number">1000000</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h3><p>两个监听程序收到zookeeper的 <strong>NodeDataChanged</strong> event，log如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">11:24:27.295 [main-SendThread(localhost:2183)] DEBUG org.apache.zookeeper.ClientCnxn - Got notification sessionid:0x300001251b50003</span><br><span class="line">11:24:27.297 [main-SendThread(localhost:2183)] DEBUG org.apache.zookeeper.ClientCnxn - Got WatchedEvent state:SyncConnected <span class="built_in">type</span>:NodeDataChanged path:/GetChildren <span class="keyword">for</span> sessionid 0x300001251b50003</span><br><span class="line">11:24:27.297 [main-EventThread] INFO com.github.xxx.bigdata.demo.zookeeper.BaseWatcher - Watch received event</span><br><span class="line">11:24:27.297 [main-EventThread] INFO com.github.xxx.bigdata.demo.zookeeper.BaseWatcher - 节点改变</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Zookeeper" scheme="https://yangyichao-mango.github.io/categories/Apache-Zookeeper/"/>
    
    
      <category term="Apache Zookeeper" scheme="https://yangyichao-mango.github.io/tags/Apache-Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Apache Druid 学习：组件以及查询类型</title>
    <link href="https://yangyichao-mango.github.io/2019/10/22/apache-druid:study-components-and-query-types/"/>
    <id>https://yangyichao-mango.github.io/2019/10/22/apache-druid:study-components-and-query-types/</id>
    <published>2019-10-22T02:40:12.000Z</published>
    <updated>2019-10-22T13:18:53.844Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Druid 学习：组件以及查询类型</p><a id="more"></a><h2 id="OLAP"><a href="#OLAP" class="headerlink" title="OLAP"></a><strong>OLAP</strong></h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>维度(Dimension): 指的是观察数据的一个角度，是考虑问题的一类属性，这些属性的集合统称为一个维。<br>维的级别(Level): 对数据的观察还存在细节程度的不同，在druid中一般表示为时间的粒度(granularity)，比如一秒，一分钟，一小时，一天…… <br>度量(Measure): 度量是用来聚合分析计算的数字信息，在druid中称为”metrics”，它可以是存储在数据库中，也可以是通过策略计算得出的。比如一篇文章的点击数、或者是根据评论数、点击数、转发数计算出的热点值</p><h3 id="对于数据处理"><a href="#对于数据处理" class="headerlink" title="对于数据处理"></a>对于数据处理</h3><p>向下钻取(Drill-down)/上卷(Roll-up): <strong>改变维的层次和级别，变换分析的粒度</strong>。Roll-up在于提升维的级别（或者称粒度）或者减少维度来聚合数据，展现总览，Drill-down反之，降低维的级别(或者称粒度)或增加维度来查看细节<br>切片(slice)和切块(dice): 当维度为两个时，我们对获取数据(查询)的操作称之为切片，当维度的数量大于两个时，我们称之为切块<br>旋转(Pivoting): 变换维的方向，例如表格中的行列互换</p><h2 id="查询条件参数"><a href="#查询条件参数" class="headerlink" title="查询条件参数"></a><strong>查询条件参数</strong></h2><table><thead><tr><th align="center">字段名</th><th align="center">描述</th><th align="center">是否必须</th></tr></thead><tbody><tr><td align="center">queryType</td><td align="center">查询类型，对应聚合查询下的类型值：timeseries、topN、groupBy等</td><td align="center">是</td></tr><tr><td align="center">dataSource</td><td align="center">数据源，类似关系数据库中表的概念，对应数据导入时Json配置属性dataSource值</td><td align="center">是</td></tr><tr><td align="center">descending</td><td align="center">返回结果是否逆序，默认值为否（正序）</td><td align="center">否</td></tr><tr><td align="center">intervals</td><td align="center">查询时间区间</td><td align="center">是</td></tr><tr><td align="center">filter</td><td align="center">对Dimension进行过滤，可以根据情况对几个维度组合不同的filter类型(and、or、not、bound)，还可以根据需要定义javascript function进行过滤</td><td align="center">否</td></tr><tr><td align="center">aggregations</td><td align="center">指定度量在聚合时候的计算策略，例如相加、或者求平均值、又或者取最后一个值，在内置类型不满足的情况下可以使用javascript。比如某手游中我统计了我每一局击杀小怪数量，以及野怪的数量，通过聚合策略sum，我能知道我从开号以来击杀了多少小怪和野怪。</td><td align="center">否</td></tr><tr><td align="center">postAggregations</td><td align="center">后聚合策略，提供了多个度量组合生成新度量的能力，主要有利于聚合计算的抽象，避免对一些指标的重复计算。举个例子，假如我需要一个度量，是我击杀小怪和野怪的总和，那么，我只需要在后聚合阶段计算，只需要拿小怪和野怪的数量相加一次，大大地提高了计算效率。</td><td align="center">否</td></tr><tr><td align="center">granularity</td><td align="center">查询的时间粒度，最细粒度为秒，最大粒度为all，提供了时间维度级别的调整并对数据进行上卷和向下钻取的能力</td><td align="center">是</td></tr><tr><td align="center">dimensionSpec</td><td align="center">提供了维度在聚合前输出展示值定制的能力，比如在Dimension age一列中，拿到的是字符串类型的数字，我希望转成数字类型，又或者定制一个javascript function，统一以 ${age} year old的形式展现</td><td align="center">否</td></tr><tr><td align="center">limit</td><td align="center">返回结果数量限制</td><td align="center">否</td></tr><tr><td align="center">context</td><td align="center">表示对当前查询本身的一些配置，比如设置查询超时的时间，又比如是否使用缓存，在通用的配置基础上，每种查询类型还有特定的配置，详见文档</td><td align="center">否</td></tr></tbody></table><h2 id="基本组件"><a href="#基本组件" class="headerlink" title="基本组件"></a><strong>基本组件</strong></h2><h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><p>过滤器，在查询语句中是一个json对象，<strong>用来对维度进行筛选</strong>，表示满足filter的是我们需要的数据。类似于SQL中的where。</p><table><thead><tr><th align="center">类型</th><th align="center">功能</th></tr></thead><tbody><tr><td align="center">SelectorFilter</td><td align="center">功能类似于SQL中的where key=value</td></tr><tr><td align="center">AndFilter, OrFilter, NotFilter</td><td align="center">功能类似于SQL中and、or、not三种过滤器。支持递归嵌套，可以构造出丰富的逻辑表达式</td></tr><tr><td align="center">RegexFilter</td><td align="center">正则表达式，支持任意维度值的java正则</td></tr><tr><td align="center">SearchFilter</td><td align="center">通过字符串匹配维度，支持多种表达式</td></tr><tr><td align="center">InFilter</td><td align="center">功能类似于SQL中where key in (value1, value2)</td></tr><tr><td align="center">IntervalFilter</td><td align="center">针对于时间维度过滤</td></tr><tr><td align="center">BoundFilter</td><td align="center">功能类似于SQL中的大于、小于、等于三种算子</td></tr><tr><td align="center">JavaScriptFilter</td><td align="center">上述filter均不能满足可以自己写JavaScript来过滤维度</td></tr></tbody></table><h3 id="aggregator"><a href="#aggregator" class="headerlink" title="aggregator"></a>aggregator</h3><p>聚合可以在采集数据时规格部分的一种方式，汇总数据进入Druid之前提供。<br>聚合也可以被指定为在查询时多查询的部分，聚合类型如下：</p><table><thead><tr><th align="center">类型</th><th align="center">功能</th></tr></thead><tbody><tr><td align="center">CountAggregator</td><td align="center">SQL count(key)</td></tr><tr><td align="center">SumAggregator</td><td align="center">SQL sum(key)</td></tr><tr><td align="center">MaxAggregator, MinAggregator</td><td align="center">SQL max(key), min(key)</td></tr><tr><td align="center">DistinctCountAggregator</td><td align="center">SQL count(distinct key)</td></tr><tr><td align="center">JavaScriptAggregator</td><td align="center">上述aggregator均不能满足可以自己写JavaScript来定义计算</td></tr></tbody></table><h3 id="post-aggregator"><a href="#post-aggregator" class="headerlink" title="post-aggregator"></a>post-aggregator</h3><table><thead><tr><th align="center">类型</th><th align="center">功能</th></tr></thead><tbody><tr><td align="center">ArithmeticPostAggregator</td><td align="center">支持对聚合后<strong>指标</strong>进行”+ - * / quotient”计算</td></tr><tr><td align="center">FieldAccessPostAggregator</td><td align="center">直接获取聚合的字段（维度，指标）</td></tr><tr><td align="center">ConstantPostAggregator</td><td align="center">返回常数</td></tr><tr><td align="center">JavaScriptPostAggregator</td><td align="center">上述postAggregator均不能满足可以自己写JavaScript来定义计算</td></tr></tbody></table><h2 id="查询类型"><a href="#查询类型" class="headerlink" title="查询类型"></a><strong>查询类型</strong></h2><h3 id="聚合查询"><a href="#聚合查询" class="headerlink" title="聚合查询"></a>聚合查询</h3><h4 id="timeseries"><a href="#timeseries" class="headerlink" title="timeseries"></a>timeseries</h4><p>时序查询，实际上即是对数据基于时间点(timestamp)的一次上卷。适合用来看某几个度量在一个时间段内的趋势。排序可按时间降序或升序</p><table><thead><tr><th align="center">字段名</th><th align="center">描述</th><th align="center">是否必须</th></tr></thead><tbody><tr><td align="center">queryType</td><td align="center">查询类型，必须为 “timeseries”</td><td align="center">是</td></tr><tr><td align="center">dataSource</td><td align="center">数据源，比如 “wikipedia”</td><td align="center">是</td></tr><tr><td align="center">descending</td><td align="center">返回结果是否逆序，默认值为否（正序）</td><td align="center">否</td></tr><tr><td align="center">intervals</td><td align="center">查询时间区间</td><td align="center">是</td></tr><tr><td align="center">granularity</td><td align="center">聚合粒度，粒度决定如何在跨时间维度得到数据块</td><td align="center">是</td></tr><tr><td align="center">filter</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">aggregations</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">postAggregations</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">limit</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">context</td><td align="center"></td><td align="center">否</td></tr></tbody></table><p>context：</p><p>1.grandTotal</p><p>2.零填充<br>如果时间范围内没有值，则会填充0<br>时间序列查询通常用零填充空的内部时间段。例如，如果您对间隔2012-01-01 / 2012-01-04发出“天”粒度时间序列查询，而2012-01-02没有数据存在，您将收到：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"timestamp"</span>: <span class="string">"2012-01-01T00:00:00.000Z"</span>,</span><br><span class="line">    <span class="string">"result"</span>: &#123; <span class="string">"sample_name1"</span>: &lt;some_value&gt; &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">   <span class="string">"timestamp"</span>: <span class="string">"2012-01-02T00:00:00.000Z"</span>,</span><br><span class="line">   <span class="string">"result"</span>: &#123; <span class="string">"sample_name1"</span>: 0 &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"timestamp"</span>: <span class="string">"2012-01-03T00:00:00.000Z"</span>,</span><br><span class="line">    <span class="string">"result"</span>: &#123; <span class="string">"sample_name1"</span>: &lt;some_value&gt; &#125;</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h4 id="topN"><a href="#topN" class="headerlink" title="topN"></a>topN</h4><p>在时间点的基础上，又增加了一个维度(OLAP的概念算两个维度)，进而对源数据进行切片，切片之后分别上卷，最后返回一个聚合集，你可以指定某个指标作为排序的依据。官方文档称这对比单个druid dimension 的groupBy 更高效。适合看某个维度下的时间趋势，（比如美国和中国十年内GDP的增长趋势比对，在这里除了时间外国家就是另外一个维度）</p><table><thead><tr><th align="center">字段名</th><th align="center">描述</th><th align="center">是否必须</th></tr></thead><tbody><tr><td align="center">queryType</td><td align="center">查询类型，必须为 “topN”</td><td align="center">是</td></tr><tr><td align="center">dataSource</td><td align="center">数据源，比如 “wikipedia”</td><td align="center">是</td></tr><tr><td align="center">intervals</td><td align="center">查询时间区间</td><td align="center">是</td></tr><tr><td align="center">granularity</td><td align="center">聚合粒度，粒度决定如何在跨时间维度得到数据块</td><td align="center">是</td></tr><tr><td align="center">filter</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">aggregations</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">postAggregations</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">dimension</td><td align="center">除了时间之外聚合的维度，只能定义一个维度</td><td align="center">是</td></tr><tr><td align="center">threshold</td><td align="center">topN中的N，例如：希望查询到top2，则值为2</td><td align="center">是</td></tr><tr><td align="center">metric</td><td align="center">topN中用来排序的指标</td><td align="center">是</td></tr><tr><td align="center">context</td><td align="center"></td><td align="center">否</td></tr></tbody></table><h4 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h4><p>适用于两个维度以上的查询，druid会根据维度切块，并且分别上卷，最后返回聚合集。相对于topN而言，这是一个向下钻取的操作，每多一个维度意味着保留更多的细节。(比如增加一个行业的维度，就可以知道美国和中国十年内，每一年不同行业贡献GDP的占比)<br><strong>注意：如果要使用时间作为唯一分组进行聚合，或者在单个维度上使用有序groupBy进行聚合，请优先考虑使用Timeseries和TopN查询。在某些情况下，它们的性能可能会更好。有关更多详细信息，请参见下面的替代方法。</strong></p><table><thead><tr><th align="center">字段名</th><th align="center">描述</th><th align="center">是否必须</th></tr></thead><tbody><tr><td align="center">queryType</td><td align="center">查询类型，必须为 “groupBy”</td><td align="center">是</td></tr><tr><td align="center">dataSource</td><td align="center">数据源，比如 “wikipedia”</td><td align="center">是</td></tr><tr><td align="center">dimensions</td><td align="center">需要聚合的所有维度</td><td align="center">是</td></tr><tr><td align="center">limitSpec</td><td align="center">同关系型数据库中的limit</td><td align="center">否</td></tr><tr><td align="center">having</td><td align="center">同关系型数据库中的having</td><td align="center">否</td></tr><tr><td align="center">granularity</td><td align="center">聚合粒度，粒度决定如何在跨时间维度得到数据块</td><td align="center">是</td></tr><tr><td align="center">filter</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">aggregations</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">postAggregations</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">intervals</td><td align="center">查询时间区间</td><td align="center">是</td></tr><tr><td align="center">subtotalsSpec</td><td align="center">类似于的grouping sets</td><td align="center">否</td></tr><tr><td align="center">context</td><td align="center"></td><td align="center">否</td></tr></tbody></table><h3 id="普通查询"><a href="#普通查询" class="headerlink" title="普通查询"></a>普通查询</h3><h4 id="select"><a href="#select" class="headerlink" title="select"></a>select</h4><p>类似SQL中的select操作，select用来查看Druid中存储的数据，并支持按照指定过滤器和时间查看指定维度和指标。不支持aggregations和post aggregations</p><p><strong>注意：建议您尽可能使用scan查询类型而不是select。在涉及大量segment的情况下，select查询可能具有很高的内存和性能开销，但是scan查询没有此问题。<br>两者之间的主要区别是“扫描”查询不支持分页。但是，即使没有分页，scan查询类型也能够返回几乎无限数量的结果，使得分页在许多情况下是不必要的。</strong></p><table><thead><tr><th align="center">字段名</th><th align="center">描述</th><th align="center">是否必须</th></tr></thead><tbody><tr><td align="center">queryType</td><td align="center">查询类型，必须为 “select”</td><td align="center">是</td></tr><tr><td align="center">dataSource</td><td align="center">数据源，比如 “wikipedia”</td><td align="center">是</td></tr><tr><td align="center">intervals</td><td align="center">查询时间区间</td><td align="center">是</td></tr><tr><td align="center">descending</td><td align="center">返回结果是否逆序，默认值为否（正序）</td><td align="center">否</td></tr><tr><td align="center">filter</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">dimensions</td><td align="center">需要查询的维度列表</td><td align="center">否</td></tr><tr><td align="center">metrics</td><td align="center">需要查询的指标列表</td><td align="center">否</td></tr><tr><td align="center">granularity</td><td align="center">聚合粒度，粒度决定如何在跨时间维度得到数据块，默认是Granularity.ALL</td><td align="center">否</td></tr><tr><td align="center">pagingSpec</td><td align="center">分页</td><td align="center">是</td></tr><tr><td align="center">context</td><td align="center"></td><td align="center">否</td></tr></tbody></table><h4 id="scan"><a href="#scan" class="headerlink" title="scan"></a>scan</h4><p>扫描查询以流模式返回行，Select查询和Scan查询之间的最大区别是，Scan查询子返回给客户端数据之前不会将所有行数据保留在内存中<br>而select查询将把行保留在内存中，如果返回太多行，则会导致内存压力。扫描查询可以返回所有行，而无需发出另一个分页查询。</p><p>除了将scan查询发送给server的用法外，还可以直接向historical历史记录进程或streaming ingestion流式提取任务发出扫描查询。如果要并行检索大量数据，这将很有用。</p><table><thead><tr><th align="center">字段名</th><th align="center">描述</th><th align="center">是否必须</th></tr></thead><tbody><tr><td align="center">queryType</td><td align="center">查询类型，必须为 “scan”</td><td align="center">是</td></tr><tr><td align="center">dataSource</td><td align="center">数据源，比如 “wikipedia”</td><td align="center">是</td></tr><tr><td align="center">intervals</td><td align="center">查询时间区间</td><td align="center">是</td></tr><tr><td align="center">resultFormat</td><td align="center">返回结果类型：list，compactedList或valueVector。目前仅list和compactedList受支持。默认是list</td><td align="center">否</td></tr><tr><td align="center">filter</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">columns</td><td align="center">需要scan的维度和指标，默认为所有</td><td align="center">否</td></tr><tr><td align="center">batchSize</td><td align="center">返回数据之前默认缓存最多多少行</td><td align="center">否</td></tr><tr><td align="center">limit</td><td align="center">查询返回最大的数据条目，如果不指定，则返回所有的数据</td><td align="center">否</td></tr><tr><td align="center">order</td><td align="center">返回数据的order，基于timestamp，并且只有__time被包含在columns中才生效</td><td align="center">否</td></tr><tr><td align="center">legacy</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">context</td><td align="center"></td><td align="center">否</td></tr></tbody></table><h4 id="search"><a href="#search" class="headerlink" title="search"></a>search</h4><p>类似SQL中的Like操作，但是支持更多的匹配操作</p><table><thead><tr><th align="center">字段名</th><th align="center">描述</th><th align="center">是否必须</th></tr></thead><tbody><tr><td align="center">queryType</td><td align="center">查询类型，必须为 “search”</td><td align="center">是</td></tr><tr><td align="center">dataSource</td><td align="center">数据源，比如 “wikipedia”</td><td align="center">是</td></tr><tr><td align="center">granularity</td><td align="center">聚合粒度，粒度决定如何在跨时间维度得到数据块</td><td align="center">是</td></tr><tr><td align="center">filter</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">limit</td><td align="center">每个历史进程的最大查询返回数据条目（默认是1000）</td><td align="center">否</td></tr><tr><td align="center">intervals</td><td align="center">查询时间区间</td><td align="center">是</td></tr><tr><td align="center">searchDimensions</td><td align="center">需要search的维度（默认是所有维度），key like value中的key</td><td align="center">否</td></tr><tr><td align="center">query</td><td align="center">search维度需要匹配的value，key like value中的value</td><td align="center">是</td></tr><tr><td align="center">sort</td><td align="center">指定应如何对搜索结果进行排序，包括字典编排（默认排序），字母数字，strlen和数字排序</td><td align="center">否</td></tr><tr><td align="center">context</td><td align="center"></td><td align="center">否</td></tr></tbody></table><h3 id="元数据查询"><a href="#元数据查询" class="headerlink" title="元数据查询"></a>元数据查询</h3><h4 id="time-bounding"><a href="#time-bounding" class="headerlink" title="time bounding"></a>time bounding</h4><h4 id="segment-metadata"><a href="#segment-metadata" class="headerlink" title="segment metadata"></a>segment metadata</h4><h4 id="dataSource-metadata"><a href="#dataSource-metadata" class="headerlink" title="dataSource metadata"></a>dataSource metadata</h4>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/categories/Apache-Druid/"/>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/tags/Apache-Druid/"/>
    
  </entry>
  
  <entry>
    <title>Apache Druid 学习：kafka to druid</title>
    <link href="https://yangyichao-mango.github.io/2019/10/21/apache-druid:study-kafka-to-druid/"/>
    <id>https://yangyichao-mango.github.io/2019/10/21/apache-druid:study-kafka-to-druid/</id>
    <published>2019-10-21T07:08:45.000Z</published>
    <updated>2019-10-21T07:26:54.920Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Druid 学习：kafka to druid demo</p><a id="more"></a><h3 id="Druid-Web操作"><a href="#Druid-Web操作" class="headerlink" title="Druid Web操作"></a><strong>Druid Web操作</strong></h3><p><a href="http://druid.apache.org/docs/latest/tutorials/tutorial-kafka.html" target="_blank" rel="noopener">官网教程</a></p><h3 id="Java-Demo"><a href="#Java-Demo" class="headerlink" title="Java Demo"></a><strong>Java Demo</strong></h3>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/categories/Apache-Druid/"/>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/tags/Apache-Druid/"/>
    
      <category term="Apache Kafka" scheme="https://yangyichao-mango.github.io/tags/Apache-Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Mac安装Apache Druid-0.16.0-incubating</title>
    <link href="https://yangyichao-mango.github.io/2019/10/21/apache-druid:0.16.0-incubating-mac-install/"/>
    <id>https://yangyichao-mango.github.io/2019/10/21/apache-druid:0.16.0-incubating-mac-install/</id>
    <published>2019-10-21T06:21:53.000Z</published>
    <updated>2019-10-22T02:39:59.582Z</updated>
    
    <content type="html"><![CDATA[<p>Mac安装Apache Druid-0.16.0-incubating教程</p><a id="more"></a><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a><strong>安装</strong></h3><p>参考<a href="http://druid.apache.org/docs/latest/tutorials/index.html" target="_blank" rel="noopener">官网教程</a></p><h4 id="brew安装"><a href="#brew安装" class="headerlink" title="brew安装"></a>brew安装</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install druid</span><br></pre></td></tr></table></figure><p>quer</p><h4 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ curl https://www-us.apache.org/dist/incubator/druid/0.16.0-incubating/apache-druid-0.16.0-incubating-bin.tar.gz</span><br><span class="line">$ tar -xzf apache-druid-0.16.0-incubating-bin.tar.gz</span><br><span class="line">$ <span class="built_in">cd</span> apache-druid-0.16.0-incubating</span><br></pre></td></tr></table></figure><h3 id="配置启动"><a href="#配置启动" class="headerlink" title="配置启动"></a><strong>配置启动</strong></h3><p>启动druid服务之前需要先启动zookeeper，下面有两种方式启动和使用zookeeper</p><h4 id="使用集成zookeeper"><a href="#使用集成zookeeper" class="headerlink" title="使用集成zookeeper"></a>使用集成zookeeper</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> <span class="variable">$DRUID_HOME</span> // 需要在~/.bash_profile中进行配置</span><br><span class="line">$ curl https://archive.apache.org/dist/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz -o zookeeper-3.4.14.tar.gz</span><br><span class="line">$ tar -xzf zookeeper-3.4.14.tar.gz</span><br><span class="line">$ mv zookeeper-3.4.14 zk</span><br></pre></td></tr></table></figure><h4 id="使用外部zookeeper"><a href="#使用外部zookeeper" class="headerlink" title="使用外部zookeeper"></a>使用外部zookeeper</h4><h5 id="修改-DRUID-HOME-conf-supervise-single-server-micro-quickstart-conf-中的配置"><a href="#修改-DRUID-HOME-conf-supervise-single-server-micro-quickstart-conf-中的配置" class="headerlink" title="修改 $DRUID_HOME/conf/supervise/single-server/micro-quickstart.conf 中的配置"></a>修改 <strong>$DRUID_HOME/conf/supervise/single-server/micro-quickstart.conf</strong> 中的配置</h5><p>将 <strong>!p10 zk bin/run-zk conf</strong> 注释掉</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ vi <span class="variable">$DRUID_HOME</span>/conf/supervise/single-server/micro-quickstart.conf</span><br><span class="line"></span><br><span class="line">:verify bin/verify-java</span><br><span class="line">:verify bin/verify-default-ports</span><br><span class="line">:<span class="built_in">kill</span>-timeout 10</span><br><span class="line"></span><br><span class="line"><span class="comment"># !p10 zk bin/run-zk conf // 这里是运行集成zookeeper的代码，所以要注释掉</span></span><br><span class="line">coordinator-overlord bin/run-druid coordinator-overlord conf/druid/single-server/micro-quickstart</span><br><span class="line">broker bin/run-druid broker conf/druid/single-server/micro-quickstart</span><br><span class="line">router bin/run-druid router conf/druid/single-server/micro-quickstart</span><br><span class="line">historical bin/run-druid historical conf/druid/single-server/micro-quickstart</span><br><span class="line">!p90 middleManager bin/run-druid middleManager conf/druid/single-server/micro-quickstart</span><br><span class="line"></span><br><span class="line"><span class="comment"># Uncomment to use Tranquility Server</span></span><br><span class="line"><span class="comment">#!p95 tranquility-server tranquility/bin/tranquility server -configFile conf/tranquility/wikipedia-server.json -Ddruid.extensions.loadList=[]</span></span><br></pre></td></tr></table></figure><h5 id="修改-DRUID-HOME-conf-druid-single-server-micro-quickstart-common-common-runtime-properties-中的配置"><a href="#修改-DRUID-HOME-conf-druid-single-server-micro-quickstart-common-common-runtime-properties-中的配置" class="headerlink" title="修改 $DRUID_HOME/conf/druid/single-server/micro-quickstart/_common/common.runtime.properties 中的配置"></a>修改 <strong>$DRUID_HOME/conf/druid/single-server/micro-quickstart/_common/common.runtime.properties</strong> 中的配置</h5><p>修改zookeeper的client配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ vim <span class="variable">$DRUID_HOME</span>/conf/druid/single-server/micro-quickstart/_common/common.runtime.properties</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置外部zookeeper的信息</span></span><br><span class="line"><span class="comment"># zookeeper，大概在46~55行中间，对zk进行配置</span></span><br><span class="line"><span class="comment"># zookeeper的server运行在2181端口上</span></span><br><span class="line">druid.zk.service.host=127.0.0.1:2181</span><br><span class="line">druid.zk.paths.base=/druid</span><br></pre></td></tr></table></figure><h5 id="修改-DRUID-HOME-bin-verify-default-ports-中的配置"><a href="#修改-DRUID-HOME-bin-verify-default-ports-中的配置" class="headerlink" title="修改 $DRUID_HOME/bin/verify-default-ports 中的配置"></a>修改 <strong>$DRUID_HOME/bin/verify-default-ports</strong> 中的配置</h5><p>因为使用了外部zookeeper，并且外部zookeeper的ip:port为127.0.0.1:2181<br>所以需要将zookeeper的2181端口删除，否则会校验本机2181端口是否被占用，因为本机zookeeper已经将其占用，则会报错，服务不能启动<br>如果使用的zookeeper的不在本机部署，则可以不注释2181</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ vi <span class="variable">$DRUID_HOME</span>/bin/verify-default-ports</span><br><span class="line"></span><br><span class="line"><span class="comment"># my @ports = (1527, 2181, 8081, 8082, 8083, 8090, 8091, 8200, 9095);</span></span><br><span class="line"></span><br><span class="line">my @ports = (1527, 8081, 8082, 8083, 8090, 8091, 8200, 9095);</span><br></pre></td></tr></table></figure><h4 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/start-micro-quickstart</span><br></pre></td></tr></table></figure><p>可以到<a href="http://localhost:8888" target="_blank" rel="noopener">http://localhost:8888</a>查看是否启动成功</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/categories/Apache-Druid/"/>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/tags/Apache-Druid/"/>
    
      <category term="Apache Zookeeper" scheme="https://yangyichao-mango.github.io/tags/Apache-Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：hbase作为sink</title>
    <link href="https://yangyichao-mango.github.io/2019/10/20/apache-flink:study-hbase-sink/"/>
    <id>https://yangyichao-mango.github.io/2019/10/20/apache-flink:study-hbase-sink/</id>
    <published>2019-10-20T06:27:49.000Z</published>
    <updated>2019-10-25T03:40:04.204Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：hbase作为sink的demo</p><a id="more"></a><h3 id="依赖项"><a href="#依赖项" class="headerlink" title="依赖项"></a><strong>依赖项</strong></h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hbase.version</span>&gt;</span>2.0.5<span class="tag">&lt;/<span class="name">hbase.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hbase.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="demo代码"><a href="#demo代码" class="headerlink" title="demo代码"></a><strong>demo代码</strong></h3><p>使用了hbase作为source，hbase作为sink</p><p><strong>运行之前需要运行hadoop集群（zookeeper集群），hbase集群</strong><br><strong>flink根据部署的集群信息（比如zookeeper的ip:port为127.0.0.1:2181等的信息）去连接hbase</strong></p><h4 id="hbase-site-xml"><a href="#hbase-site-xml" class="headerlink" title="hbase-site.xml"></a>hbase-site.xml</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--Autogenerated by Cloudera Manager--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- zk configuration --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>zookeeper.session.timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>60000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>zookeeper.znode.parent<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>127.0.0.1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.clientPort<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="HBaseReader"><a href="#HBaseReader" class="headerlink" title="HBaseReader"></a>HBaseReader</h4><p>HBase作为source</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseReader</span> <span class="keyword">extends</span> <span class="title">RichSourceFunction</span>&lt;<span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = LoggerFactory.getLogger(HBaseReader.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> HBaseClient hBaseClient;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_HBASE_SOURCE_TABLE_NAME = <span class="string">"student"</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_HBASE_SOURCE_START_ROW = <span class="string">"row1"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_HBASE_SOURCE_STOP_ROW = <span class="string">"row1"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">                    hBaseClient = <span class="keyword">new</span> HBaseClient();</span><br><span class="line">                    hBaseClient.initialize();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;String&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        List&lt;<span class="keyword">byte</span>[]&gt; results = hBaseClient.scan(</span><br><span class="line">                DEFAULT_HBASE_SOURCE_TABLE_NAME</span><br><span class="line">                , DEFAULT_HBASE_SOURCE_START_ROW</span><br><span class="line">                , DEFAULT_HBASE_SOURCE_STOP_ROW);</span><br><span class="line">        results.forEach(result -&gt; ctx.collect(<span class="keyword">new</span> String(result)));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        hBaseClient.destroy();</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                        LOGGER.error(<span class="string">""</span>, e);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="HBaseWriter"><a href="#HBaseWriter" class="headerlink" title="HBaseWriter"></a>HBaseWriter</h4><p>HBase作为sink</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseWriter</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = LoggerFactory.getLogger(HBaseWriter.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> HBaseClient hBaseClient;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">                    hBaseClient = <span class="keyword">new</span> HBaseClient();</span><br><span class="line">                    hBaseClient.initialize();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(Tuple2&lt;String, Integer&gt; value, Context context)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        String result = value.toString();</span><br><span class="line">        Put put = hBaseClient.createPut(<span class="string">"row2"</span>);</span><br><span class="line">        hBaseClient.addValueOnPut(put, <span class="string">"description"</span>, <span class="string">"age"</span>, <span class="string">"19"</span>);</span><br><span class="line">        hBaseClient.put(<span class="string">"student"</span>, put);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.close();</span><br><span class="line">        <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        hBaseClient.destroy();</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                        LOGGER.error(<span class="string">""</span>, e);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="定义dag"><a href="#定义dag" class="headerlink" title="定义dag"></a>定义dag</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> SourceFunction&lt;String&gt; source;</span><br><span class="line">    <span class="keyword">final</span> ParameterTool params = ParameterTool.fromArgs(args);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/******************************* hbase source *******************************/</span></span><br><span class="line">    source = <span class="keyword">new</span> HBaseReader();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/******************************* define dag *******************************/</span></span><br><span class="line">    <span class="comment">// create the environment to create streams and configure execution</span></span><br><span class="line">    <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    <span class="comment">// make parameters available in the web interface</span></span><br><span class="line">    env.getConfig().setGlobalJobParameters(params);</span><br><span class="line">    DataStream&lt;String&gt; sentenceStream = env.addSource(source);</span><br><span class="line">    DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; wordCountStream = sentenceStream</span><br><span class="line">            .flatMap(<span class="keyword">new</span> LineSplitter())</span><br><span class="line">            .keyBy(<span class="number">0</span>)</span><br><span class="line">            .sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/******************************* hbase sink *******************************/</span></span><br><span class="line">    wordCountStream.addSink(<span class="keyword">new</span> HBaseWriter());</span><br><span class="line">    wordCountStream.print();</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"Java hbase Word Count"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="查看hbase文件"><a href="#查看hbase文件" class="headerlink" title="查看hbase文件"></a><strong>查看hbase文件</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; scan <span class="string">'student'</span></span><br><span class="line">ROW                   COLUMN+CELL</span><br><span class="line"> row1                 column=description:age, timestamp=1571460125600, value=18</span><br><span class="line"> row1                 column=description:name, timestamp=1571460129987, value=li</span><br><span class="line">                      u</span><br><span class="line"> <span class="comment"># 记录以及被写入hbase</span></span><br><span class="line"> row2                 column=description:age, timestamp=1571576517072, value=19</span><br><span class="line">2 row(s) <span class="keyword">in</span> 0.2010 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):002:0&gt;</span><br></pre></td></tr></table></figure><p>发现columnFamilyName为<strong>description</strong>，columnName为<strong>age</strong>，rowkey为<strong>row2</strong>，value为<strong>19</strong>的记录已经被写入hbase</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
      <category term="Apache HBase" scheme="https://yangyichao-mango.github.io/tags/Apache-HBase/"/>
    
  </entry>
  
  <entry>
    <title>Mac安装Apache Zookeeper-3.4.12</title>
    <link href="https://yangyichao-mango.github.io/2019/10/20/apache-zookeeper:3.4.12-mac-install/"/>
    <id>https://yangyichao-mango.github.io/2019/10/20/apache-zookeeper:3.4.12-mac-install/</id>
    <published>2019-10-20T03:23:17.000Z</published>
    <updated>2019-10-25T03:41:16.346Z</updated>
    
    <content type="html"><![CDATA[<p>Mac安装Apache Zookeeper-3.4.12教程</p><a id="more"></a><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a><strong>安装</strong></h3><h4 id="安装方式1-brew安装"><a href="#安装方式1-brew安装" class="headerlink" title="安装方式1-brew安装"></a>安装方式1-brew安装</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew install zookeeper</span><br></pre></td></tr></table></figure><h4 id="安装方式2-下载压缩包"><a href="#安装方式2-下载压缩包" class="headerlink" title="安装方式2-下载压缩包"></a>安装方式2-下载压缩包</h4><p>从此地址下载<strong><a href="http://mirrors.hust.edu.cn/apache/zookeeper/stable/" target="_blank" rel="noopener">http://mirrors.hust.edu.cn/apache/zookeeper/stable/</a></strong></p><p>解压配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ tar -zxvf zookeeper-3.4.12.tar.gz // 解压</span><br><span class="line">$ <span class="built_in">cd</span> zookeeper-3.4.12/conf // 切换到配置目录下</span><br><span class="line">$ mv zoo_sample.cfg zoo.cfg // 更改默认配置文件名称</span><br><span class="line">$ vi zoo.cfg // 编辑配置文件，自定义dataDir</span><br></pre></td></tr></table></figure><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a><strong>启动</strong></h3><h4 id="启动sever端"><a href="#启动sever端" class="headerlink" title="启动sever端"></a>启动sever端</h4><p>切换到bin目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">pwd</span></span><br><span class="line">/user/<span class="built_in">local</span>/Celler/zookeeper/3.4.12/bin</span><br><span class="line"></span><br><span class="line">$ ls</span><br><span class="line">README.txtzkCli.cmdzkEnv.cmdzkServer.cmdzookeeper.out</span><br><span class="line">zkCleanup.shzkCli.shzkEnv.shzkServer.sh</span><br><span class="line"></span><br><span class="line">$ ./zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /user/<span class="built_in">local</span>/Celler/zookeeper/3.4.12/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure><h4 id="启动client端"><a href="#启动client端" class="headerlink" title="启动client端"></a>启动client端</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">./zkCli.sh -server 127.0.0.1:2181</span><br><span class="line">Connecting to 127.0.0.1:2181</span><br><span class="line">2019-10-20 12:17:25,861 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.12-e5259e437540f349646870ea94dc2658c4e44b3b, built on 03/27/2018 03:55 GMT</span><br><span class="line">2019-10-20 12:17:25,864 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=localhost</span><br><span class="line">2019-10-20 12:17:25,864 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.8.0_191</span><br><span class="line">2019-10-20 12:17:25,866 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation</span><br><span class="line">2019-10-20 12:17:25,866 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_191.jdk/Contents/Home/jre</span><br><span class="line">2019-10-20 12:17:25,868 [myid:] - INFO  [main:ZooKeeper@441] - Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain<span class="variable">$MyWatcher</span>@799f7e29</span><br><span class="line">Welcome to ZooKeeper!</span><br><span class="line">2019-10-20 12:17:25,896 [myid:] - INFO  [main-SendThread(127.0.0.1:2181):ClientCnxn<span class="variable">$SendThread</span>@1028] - Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)</span><br><span class="line">JLine support is enabled</span><br><span class="line">2019-10-20 12:17:25,959 [myid:] - INFO  [main-SendThread(127.0.0.1:2181):ClientCnxn<span class="variable">$SendThread</span>@878] - Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session</span><br><span class="line">2019-10-20 12:17:25,966 [myid:] - INFO  [main-SendThread(127.0.0.1:2181):ClientCnxn<span class="variable">$SendThread</span>@1302] - Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x10000112e160008, negotiated timeout = 30000</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected <span class="built_in">type</span>:None path:null</span><br><span class="line">[zk: 127.0.0.1:2181(CONNECTED) 0] ls /</span><br><span class="line">[zookeeper, hbase]</span><br></pre></td></tr></table></figure><h4 id="停止server端"><a href="#停止server端" class="headerlink" title="停止server端"></a>停止server端</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; ./zkServer.sh stop //停止后，如果CLi没有关闭，将报错</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: zookeeper-3.4.12/bin/../conf/zoo.cfg</span><br><span class="line">Stopping zookeeper ... STOPPED</span><br></pre></td></tr></table></figure><h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a><strong>配置文件</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The number of milliseconds of each tick</span></span><br><span class="line">tickTime=2000</span><br><span class="line"><span class="comment"># The number of ticks that the initial</span></span><br><span class="line"><span class="comment"># synchronization phase can take</span></span><br><span class="line">initLimit=10</span><br><span class="line"><span class="comment"># The number of ticks that can pass between</span></span><br><span class="line"><span class="comment"># sending a request and getting an acknowledgement</span></span><br><span class="line">syncLimit=5</span><br><span class="line"><span class="comment"># the directory where the snapshot is stored.</span></span><br><span class="line"><span class="comment"># do not use /tmp for storage, /tmp here is just</span></span><br><span class="line"><span class="comment"># example sakes. </span></span><br><span class="line"><span class="comment"># 内存数据快照的保存目录；如果没有自定义Log也使用该目录</span></span><br><span class="line">dataDir=/tmp/zookeeper</span><br><span class="line"><span class="comment"># the port at which the clients will connect</span></span><br><span class="line"><span class="comment"># zookeeper服务端的端口，客户端启动时需要连接的端口</span></span><br><span class="line">clientPort=2181</span><br><span class="line"><span class="comment"># the maximum number of client connections.</span></span><br><span class="line"><span class="comment"># increase this if you need to handle more clients</span></span><br><span class="line"><span class="comment">#maxClientCnxns=60</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Be sure to read the maintenance section of the</span></span><br><span class="line"><span class="comment"># administrator guide before turning on autopurge.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The number of snapshots to retain in dataDir</span></span><br><span class="line"><span class="comment">#autopurge.snapRetainCount=3</span></span><br><span class="line"><span class="comment"># Purge task interval in hours</span></span><br><span class="line"><span class="comment"># Set to "0" to disable auto purge feature</span></span><br><span class="line"><span class="comment">#autopurge.purgeInterval=1</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Zookeeper" scheme="https://yangyichao-mango.github.io/categories/Apache-Zookeeper/"/>
    
    
      <category term="Apache Zookeeper" scheme="https://yangyichao-mango.github.io/tags/Apache-Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：hdfs作为sink</title>
    <link href="https://yangyichao-mango.github.io/2019/10/19/apache-flink:study-hdfs-sink/"/>
    <id>https://yangyichao-mango.github.io/2019/10/19/apache-flink:study-hdfs-sink/</id>
    <published>2019-10-19T11:33:38.000Z</published>
    <updated>2019-10-25T03:40:04.196Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：hdfs作为source和sink的demo</p><a id="more"></a><h3 id="依赖项"><a href="#依赖项" class="headerlink" title="依赖项"></a><strong>依赖项</strong></h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">flink.version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">flink.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-filesystem_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="demo代码"><a href="#demo代码" class="headerlink" title="demo代码"></a><strong>demo代码</strong></h3><p>使用了kafka作为source，hdfs作为sink<br><strong>运行之前需要运行kafka集群，hadoop集群（zookeeper集群）</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/******************************* define dag *******************************/</span></span><br><span class="line"><span class="comment">// create the environment to create streams and configure execution</span></span><br><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// make parameters available in the web interface</span></span><br><span class="line">env.getConfig().setGlobalJobParameters(params);</span><br><span class="line">DataStream&lt;String&gt; sentenceStream = env.addSource(source);</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; wordCountStream = sentenceStream</span><br><span class="line">        .flatMap(<span class="keyword">new</span> LineSplitter())</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        .sum(<span class="number">1</span>);</span><br><span class="line">wordCountStream.print();</span><br><span class="line"></span><br><span class="line">DataStream&lt;String&gt; kafkaSinkStream = wordCountStream</span><br><span class="line">        .map(<span class="keyword">new</span> WordBuilder());</span><br><span class="line"></span><br><span class="line"><span class="comment">/******************************* hdfs sink *******************************/</span></span><br><span class="line"></span><br><span class="line">BucketingSink&lt;String&gt; bucketingSink = <span class="keyword">new</span> BucketingSink&lt;&gt;(<span class="string">"/user/xxx/flink/from-kafka"</span>); <span class="comment">//hdfs上的路径</span></span><br><span class="line">bucketingSink.setWriter(<span class="keyword">new</span> StringWriter&lt;&gt;())</span><br><span class="line">        .setBatchSize(<span class="number">1024</span> * <span class="number">1024L</span>)</span><br><span class="line">        .setBatchRolloverInterval(<span class="number">2000</span>)</span><br><span class="line">        .setInactiveBucketThreshold(<span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">kafkaSinkStream.addSink(bucketingSink);</span><br></pre></td></tr></table></figure><p>上面例子将创建一个 Sink，写入遵循下面格式的分桶文件中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/base/path/&#123;date-time&#125;/_part-&#123;parallel-task&#125;-&#123;count&#125;</span><br></pre></td></tr></table></figure><p><strong>date-time</strong>：<br>是从setBucketer()自定义的日期/时间格式的字符串，如果不进行设置，默认Bucketer是DateTimeBucketer，默认值是yyyy-MM-dd–HH（DateTimeBucketer.DEFAULT_FORMAT_STRING）</p><p><strong>_part-{parallel-task}-{count}：</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_VALID_PREFIX = <span class="string">"_"</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_PART_PREFIX = <span class="string">"part"</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_PENDING_SUFFIX = <span class="string">".pending"</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">openNewPartFile</span><span class="params">(Path bucketPath, BucketState&lt;T&gt; bucketState)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Path partPath = assemblePartPath(bucketPath, subtaskIndex, bucketState.partCounter);</span><br><span class="line">    Path inProgressPath = getInProgressPathFor(partPath);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> Path <span class="title">assemblePartPath</span><span class="params">(Path bucket, <span class="keyword">int</span> subtaskIndex, <span class="keyword">int</span> partIndex)</span> </span>&#123;</span><br><span class="line">    String localPartSuffix = partSuffix != <span class="keyword">null</span> ? partSuffix : <span class="string">""</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Path(bucket, String.format(<span class="string">"%s-%s-%s%s"</span>, partPrefix, subtaskIndex, partIndex, localPartSuffix));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> Path <span class="title">getInProgressPathFor</span><span class="params">(Path path)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Path(path.getParent(), inProgressPrefix + path.getName()).suffix(inProgressSuffix);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="查看hdfs文件"><a href="#查看hdfs文件" class="headerlink" title="查看hdfs文件"></a><strong>查看hdfs文件</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ ./hdfs dfs -ls /user/xxx/flink/from-kafka/2019-10-19--19</span><br><span class="line">2019-10-19 20:08:31,244 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using <span class="built_in">builtin</span>-java classes <span class="built_in">where</span> applicable</span><br><span class="line">Found 15 items</span><br><span class="line">-rw-r--r--   1 xxx supergroup          4 2019-10-19 19:45 /user/xxx/flink/from-kafka/2019-10-19--19/_part-0-0.pending</span><br><span class="line">-rw-r--r--   1 xxx supergroup          2 2019-10-19 19:45 /user/xxx/flink/from-kafka/2019-10-19--19/_part-1-0.pending</span><br><span class="line">-rw-r--r--   1 xxx supergroup          5 2019-10-19 19:44 /user/xxx/flink/from-kafka/2019-10-19--19/_part-2-0.pending</span><br><span class="line">-rw-r--r--   1 xxx supergroup         11 2019-10-19 19:45 /user/xxx/flink/from-kafka/2019-10-19--19/_part-2-1.pending</span><br><span class="line">-rw-r--r--   1 xxx supergroup         10 2019-10-19 19:44 /user/xxx/flink/from-kafka/2019-10-19--19/_part-3-0.pending</span><br><span class="line">-rw-r--r--   1 xxx supergroup         13 2019-10-19 19:45 /user/xxx/flink/from-kafka/2019-10-19--19/_part-3-1.pending</span><br><span class="line">-rw-r--r--   1 xxx supergroup          9 2019-10-19 19:45 /user/xxx/flink/from-kafka/2019-10-19--19/_part-4-0.pending</span><br><span class="line">-rw-r--r--   1 xxx supergroup         10 2019-10-19 19:44 /user/xxx/flink/from-kafka/2019-10-19--19/_part-5-0.pending</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/tags/Apache-Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Mac安装Apache HBase-1.3.5</title>
    <link href="https://yangyichao-mango.github.io/2019/10/18/apache-hbase:1.3.5-mac-install/"/>
    <id>https://yangyichao-mango.github.io/2019/10/18/apache-hbase:1.3.5-mac-install/</id>
    <published>2019-10-18T15:14:59.000Z</published>
    <updated>2019-10-25T03:40:04.199Z</updated>
    
    <content type="html"><![CDATA[<p>Mac安装Apache HBase-1.3.5教程</p><a id="more"></a><h3 id="HBase安装"><a href="#HBase安装" class="headerlink" title="HBase安装"></a><strong>HBase安装</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew install hbase</span><br></pre></td></tr></table></figure><p>安装在<font color="red"><strong>/usr/local/Cellar/hbase/1.3.5</strong></font></p><h3 id="HBase配置"><a href="#HBase配置" class="headerlink" title="HBase配置"></a><strong>HBase配置</strong></h3><h4 id="hbase-env-sh"><a href="#hbase-env-sh" class="headerlink" title="hbase-env.sh"></a>hbase-env.sh</h4><p>在<font color="red">conf/hbase-env.sh设置JAVA_HOME</font></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/Cellar/hbase/1.3.5/libexec/conf</span><br><span class="line">$ vim hbase-env.sh</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=<span class="string">"<span class="variable">$(/usr/libexec/java_home --version 1.8)</span>"</span></span><br></pre></td></tr></table></figure><p>Apache HBase-1.3.5中JAVA_HOME已经默认被配置好了<br>如果JAVA_HOME没有配置好，则需要设置JAVA_HOME，可以通过下面的命令查看JAVA_HOME</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/libexec/java_home</span><br><span class="line">/Library/Java/JavaVirtualMachines/jdk1.8.0_191.jdk/Contents/Home</span><br></pre></td></tr></table></figure><h4 id="hbase-site-xml"><a href="#hbase-site-xml" class="headerlink" title="hbase-site.xml"></a>hbase-site.xml</h4><p>在<font color="red">conf/hbase-site.xml</font>设置HBase的核心配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ vim hbase-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">    // 这里设置让HBase存储文件的地方</span><br><span class="line">    &lt;value&gt;file:///usr/<span class="built_in">local</span>/Cellar/hbase/tmp/hbase&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</span><br><span class="line">      // 这里设置让HBase存储内建zookeeper文件的地方</span><br><span class="line">      &lt;value&gt;/usr/<span class="built_in">local</span>/Cellar/hbase/tmp/zookeeper&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="启动HBase"><a href="#启动HBase" class="headerlink" title="启动HBase"></a>启动HBase</h4><p><font color="red">/usr/local/Cellar/hbase/1.3.5/bin/start-hbase.sh</font>提供HBase的启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ./start-hbase.sh</span><br><span class="line">starting master, logging to /usr/<span class="built_in">local</span>/var/<span class="built_in">log</span>/hbase/hbase-xxx-master-xxx.local.out</span><br></pre></td></tr></table></figure><h4 id="验证是否安装成功"><a href="#验证是否安装成功" class="headerlink" title="验证是否安装成功"></a>验证是否安装成功</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ jps</span><br><span class="line">722 Launcher</span><br><span class="line">1142 HMaster</span><br><span class="line">726 Launcher</span><br><span class="line">1256 Jps</span><br></pre></td></tr></table></figure><h4 id="启动HBase-Shell"><a href="#启动HBase-Shell" class="headerlink" title="启动HBase Shell"></a>启动HBase Shell</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ ./hbase shell</span><br><span class="line">2019-10-19 11:58:34,879 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using <span class="built_in">builtin</span>-java classes <span class="built_in">where</span> applicable</span><br><span class="line">HBase Shell; enter <span class="string">'help&lt;RETURN&gt;'</span> <span class="keyword">for</span> list of supported commands.</span><br><span class="line">Type <span class="string">"exit&lt;RETURN&gt;"</span> to leave the HBase Shell</span><br><span class="line">Version 1.3.5, rb59afe7b1dc650ff3a86034477b563734e8799a9, Wed Jun  5 15:57:14 PDT 2019</span><br><span class="line"></span><br><span class="line">hbase(main):001:0&gt;</span><br></pre></td></tr></table></figure><h4 id="停止HBase运行"><a href="#停止HBase运行" class="headerlink" title="停止HBase运行"></a>停止HBase运行</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ./stop-hbase.sh</span><br><span class="line">stopping hbase...............</span><br></pre></td></tr></table></figure><h3 id="伪分布式模式"><a href="#伪分布式模式" class="headerlink" title="伪分布式模式"></a><strong>伪分布式模式</strong></h3><p>必须先关闭HBase</p><h4 id="修改hbase-env-sh"><a href="#修改hbase-env-sh" class="headerlink" title="修改hbase-env.sh"></a>修改hbase-env.sh</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HBASE_MANAGE_ZK = <span class="literal">true</span></span><br></pre></td></tr></table></figure><h4 id="修改hbase-site-xml"><a href="#修改hbase-site-xml" class="headerlink" title="修改hbase-site.xml"></a>修改hbase-site.xml</h4><p>设置HBase使用分布式模式运行</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    // Here you have to set the path where you want HBase to store its files.</span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><font color="red"><strong>hbase.rootdir路径一定要跟hadoop中core-site.xml中fs.default.name相同</strong></font></p><p>change the hbase.rootdir from the local filesystem to the address of your HDFS instance —offical quick start</p><p>如何两处设置不同会引起ERROR: <font color="red">Can’t get master address from ZooKeeper; znode data == null错误错误</font></p><p>在启动HBase之前, 请先启动Hadoop, 使之运行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ ./start-hbase.sh</span><br><span class="line">localhost: starting zookeeper, logging to /usr/<span class="built_in">local</span>/var/<span class="built_in">log</span>/hbase/hbase-xxx-zookeeper-xxx.local.out</span><br><span class="line">starting master, logging to /usr/<span class="built_in">local</span>/var/<span class="built_in">log</span>/hbase/hbase-xxx-master-xxx.local.out</span><br><span class="line">starting regionserver, logging to /usr/<span class="built_in">local</span>/var/<span class="built_in">log</span>/hbase/hbase-xxx-1-regionserver-xxx.local.out</span><br><span class="line"></span><br><span class="line">$ jps  <span class="comment">#验证是否启动成功, 包含HMaster和HRegionServer说明启动成功</span></span><br><span class="line">5614 HRegionServer</span><br><span class="line">2222 NameNode</span><br><span class="line">722 Launcher</span><br><span class="line">2323 DataNode</span><br><span class="line">5461 HMaster</span><br><span class="line">726 Launcher</span><br><span class="line">2650 ResourceManager</span><br><span class="line">2747 NodeManager</span><br><span class="line">2459 SecondaryNameNode</span><br><span class="line">5405 HQuorumPeer</span><br><span class="line">285</span><br><span class="line">5726 Jps</span><br></pre></td></tr></table></figure><p>查看hdfs中文件夹</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ./hdfs dfs -ls /</span><br><span class="line">2019-10-19 12:39:03,895 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using <span class="built_in">builtin</span>-java classes <span class="built_in">where</span> applicable</span><br><span class="line">Found 3 items</span><br><span class="line">drwxr-xr-x   - xxx supergroup          0 2019-10-19 12:38 /hbase</span><br><span class="line">drwxrwxr-x   - xxx supergroup          0 2019-10-17 14:41 /tmp</span><br><span class="line">drwxr-xr-x   - xxx supergroup          0 2019-10-17 11:44 /user</span><br></pre></td></tr></table></figure><h3 id="HBase-Shell"><a href="#HBase-Shell" class="headerlink" title="HBase Shell"></a><strong>HBase Shell</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$ hbase shell  <span class="comment">#启动HBase Shell</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建表</span></span><br><span class="line">&gt; create <span class="string">'student'</span>, <span class="string">'description'</span>, <span class="string">'course'</span>  <span class="comment">#创建表名为student的表, 指明两个列名, 分别为description和course</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 信息明细</span></span><br><span class="line">&gt; list <span class="string">'student'</span>  <span class="comment">#列出list表信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 插入数据</span></span><br><span class="line"><span class="comment"># 意思为在student表row1处插入description:age的数据为18</span></span><br><span class="line"><span class="comment"># rowKey为row1，columnFamilyName为description，columnName为age</span></span><br><span class="line">&gt; put <span class="string">'student'</span>, <span class="string">'row1'</span>, <span class="string">'description:age'</span>, <span class="string">'18'</span></span><br><span class="line">&gt; put <span class="string">'student'</span>, <span class="string">'row1'</span>, <span class="string">'description:name'</span>, <span class="string">'liu'</span></span><br><span class="line">&gt; put <span class="string">'student'</span>, <span class="string">'row1'</span>, <span class="string">'course:chinese'</span>, <span class="string">'100'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一次扫描所有数据</span></span><br><span class="line">&gt; scan <span class="string">'student'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使表失效 / 有效</span></span><br><span class="line">&gt; <span class="built_in">disable</span> <span class="string">'student'</span></span><br><span class="line">&gt; <span class="built_in">enable</span> <span class="string">'student'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除表(要先disable)</span></span><br><span class="line">&gt;  drop <span class="string">'student'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 退出shell</span></span><br><span class="line">&gt; quit</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache HBase" scheme="https://yangyichao-mango.github.io/categories/Apache-HBase/"/>
    
    
      <category term="Apache HBase" scheme="https://yangyichao-mango.github.io/tags/Apache-HBase/"/>
    
      <category term="Mac安装" scheme="https://yangyichao-mango.github.io/tags/Mac%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>IntelliJ IDEA 中如何查看一个类的所有继承关系</title>
    <link href="https://yangyichao-mango.github.io/2019/10/18/idea-mac:show-class-hierarchy/"/>
    <id>https://yangyichao-mango.github.io/2019/10/18/idea-mac:show-class-hierarchy/</id>
    <published>2019-10-18T06:09:17.000Z</published>
    <updated>2019-10-25T08:40:51.878Z</updated>
    
    <content type="html"><![CDATA[<p>IntelliJ IDEA 中如何查看一个类的所有继承关系，包括父类与子类</p><a id="more"></a><h3 id="查看方式"><a href="#查看方式" class="headerlink" title="查看方式"></a><strong>查看方式</strong></h3><p>IntelliJ IDEA 中最上端的Navigate，下拉选择Type Hierarchy，就会出现层级关系列表</p><h4 id="关于该类的父类和子类继承关系"><a href="#关于该类的父类和子类继承关系" class="headerlink" title="关于该类的父类和子类继承关系"></a>关于该类的父类和子类继承关系</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/idea-mac:show-class-hierarchy/父类和子类继承关系.png" alt="父类和子类继承关系" title>                </div>                <div class="image-caption">父类和子类继承关系</div>            </figure><h4 id="关于该类的父类继承关系"><a href="#关于该类的父类继承关系" class="headerlink" title="关于该类的父类继承关系"></a>关于该类的父类继承关系</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/idea-mac:show-class-hierarchy/父类继承关系.png" alt="父类继承关系" title>                </div>                <div class="image-caption">父类继承关系</div>            </figure><h4 id="关于该类的子类继承关系"><a href="#关于该类的子类继承关系" class="headerlink" title="关于该类的子类继承关系"></a>关于该类的子类继承关系</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/idea-mac:show-class-hierarchy/子类继承关系.png" alt="子类继承关系" title>                </div>                <div class="image-caption">子类继承关系</div>            </figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="IntelliJ IDEA" scheme="https://yangyichao-mango.github.io/categories/IntelliJ-IDEA/"/>
    
    
      <category term="IntelliJ IDEA" scheme="https://yangyichao-mango.github.io/tags/IntelliJ-IDEA/"/>
    
  </entry>
  
  <entry>
    <title>Apache Hive环境搭建错误：java.lang.IllegalArgumentException: java.net.URISyntaxException:...</title>
    <link href="https://yangyichao-mango.github.io/2019/10/17/apache-hive:error-URISyntaxException:Relative-path-in-absolute-URI:%7Bsystem:java.io.tmpdir%7D%7Bsystem-user-name%7D/"/>
    <id>https://yangyichao-mango.github.io/2019/10/17/apache-hive:error-URISyntaxException:Relative-path-in-absolute-URI:{system:java.io.tmpdir}{system-user-name}/</id>
    <published>2019-10-17T07:44:01.000Z</published>
    <updated>2019-10-25T03:40:04.210Z</updated>
    
    <content type="html"><![CDATA[<p>出现错误：Exception in thread “main” java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D</p><a id="more"></a><h3 id="错误场景详情"><a href="#错误场景详情" class="headerlink" title="错误场景详情"></a><strong>错误场景详情</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HIVE_HOME</span>/bin/hive</span><br><span class="line">Hive Session ID = 41e2ad09-81b3-4700-9b87-f42b25a29731</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration <span class="keyword">in</span> jar:file:/usr/<span class="built_in">local</span>/Cellar/hive/3.1.2/libexec/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: <span class="literal">true</span></span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path <span class="keyword">in</span> absolute URI: $&#123;system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D</span><br><span class="line">at org.apache.hadoop.fs.Path.initialize(Path.java:263)</span><br><span class="line">at org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:221)</span><br><span class="line">at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:710)</span><br><span class="line">at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:627)</span><br><span class="line">at org.apache.hadoop.hive.ql.session.SessionState.beginStart(SessionState.java:591)</span><br><span class="line">at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:747)</span><br><span class="line">at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class="line">at org.apache.hadoop.util.RunJar.run(RunJar.java:323)</span><br><span class="line">at org.apache.hadoop.util.RunJar.main(RunJar.java:236)</span><br><span class="line">Caused by: java.net.URISyntaxException: Relative path <span class="keyword">in</span> absolute URI: $&#123;system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D</span><br><span class="line">at java.net.URI.checkPath(URI.java:1823)</span><br><span class="line">at java.net.URI.&lt;init&gt;(URI.java:745)</span><br><span class="line">at org.apache.hadoop.fs.Path.initialize(Path.java:260)</span><br><span class="line">... 12 more</span><br></pre></td></tr></table></figure><h3 id="错误原因"><a href="#错误原因" class="headerlink" title="错误原因"></a><strong>错误原因</strong></h3><p>hive-site.xml里的临时目录没有设置好，一共有三个</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>Hive.exec.local.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;system:Java.io.tmpdir&#125;/$&#123;system:user.name&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Local scratch space for Hive jobs<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.downloaded.resources.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;system:java.io.tmpdir&#125;/$&#123;hive.session.id&#125;_resources<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Temporary local directory for added resources in the remote file system.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.logging.operation.log.location<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;system:Java.io.tmpdir&#125;/$&#123;system:user.name&#125;/operation_logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Top level directory where operation logs are stored if logging functionality is enabled<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a><strong>解决方案</strong></h3><p>将hive-site.xml文件中的${system:java.io.tmpdir}替换为hive的临时目录<br>例如我替换为<font color="red">/usr/local/Cellar/hive/tmp</font>，该目录如果不存在则要自己手工创建，并且赋予读写权限</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>Hive.exec.local.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/Cellar/hive/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Local scratch space for Hive jobs<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.downloaded.resources.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/Cellar/hive/tmp/$&#123;hive.session.id&#125;_resources<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Temporary local directory for added resources in the remote file system.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.logging.operation.log.location<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/Cellar/hive/tmp/root/operation_logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Top level directory where operation logs are stored if logging functionality is enabled<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/categories/Apache-Hive/"/>
    
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/tags/Apache-Hive/"/>
    
  </entry>
  
  <entry>
    <title>Mac安装Apache Hive-3.2.1</title>
    <link href="https://yangyichao-mango.github.io/2019/10/17/apache-hive:3.1.2-mac-install/"/>
    <id>https://yangyichao-mango.github.io/2019/10/17/apache-hive:3.1.2-mac-install/</id>
    <published>2019-10-17T06:58:34.000Z</published>
    <updated>2019-10-25T03:40:04.207Z</updated>
    
    <content type="html"><![CDATA[<p>Mac安装Apache Hive-3.2.1教程</p><a id="more"></a><h2 id="安装Hadoop"><a href="#安装Hadoop" class="headerlink" title="安装Hadoop"></a><strong>安装Hadoop</strong></h2><p>下载包进行安装，则hadoop需要独立安装</p><h2 id="安装Hive"><a href="#安装Hive" class="headerlink" title="安装Hive"></a><strong>安装Hive</strong></h2><h3 id="brew安装"><a href="#brew安装" class="headerlink" title="brew安装"></a>brew安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew install hive</span><br></pre></td></tr></table></figure><p>此命令会把hive依赖的hadoop安装，所以就不需要单独进行安装hadoop<br>该命令默认安装的版本较新，我的hive是3.1.2，hadoop是3.2.1，安装位置：<font color="red">/usr/local/Cellar/hive/</font></p><h3 id="环境变量修改"><a href="#环境变量修改" class="headerlink" title="环境变量修改"></a>环境变量修改</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ vim ~/.bash_profile</span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/usr/<span class="built_in">local</span>/Cellar/hive/3.1.2</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="string">"<span class="variable">$HIVE_HOME</span>/bin:<span class="variable">$PATH</span>"</span></span><br><span class="line"></span><br><span class="line">$ <span class="built_in">source</span> ~/.bash_profile</span><br></pre></td></tr></table></figure><h3 id="使用mysql作为hive元数据存储"><a href="#使用mysql作为hive元数据存储" class="headerlink" title="使用mysql作为hive元数据存储"></a>使用mysql作为hive元数据存储</h3><p>在mysql中为hive 创建用户，及初始化数据库<br>以下在mysql 中操作，注意：这里创建的用户名是 hadoop， 密码 mysql<br>第一行：创建数据库<br>第二、三行 创建用户，赋予权限<br>第四行 权限生效</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create database hive;</span><br><span class="line">CREATE USER  <span class="string">'hadoop'</span>@<span class="string">'%'</span>  IDENTIFIED BY <span class="string">'mysql'</span>;</span><br><span class="line">GRANT ALL PRIVILEGES ON  *.* TO <span class="string">'hadoop'</span>@<span class="string">'%'</span> WITH GRANT OPTION;</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure><p>查看权限是否已经存储</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM mysql.user;</span><br></pre></td></tr></table></figure><h3 id="修改配置文件hive-site-xml"><a href="#修改配置文件hive-site-xml" class="headerlink" title="修改配置文件hive-site.xml"></a>修改配置文件hive-site.xml</h3><p>修改hive配置文件，我的配置文件位置在 <font color="red">/usr/local/Cellar/hive/3.1.2/libexec/conf</font><br>如果不存在hive-site.xml文件，则使用下面这个命令创建一个默认的hive-site.xml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cp hive-default.xml.template hive-site.xml</span><br></pre></td></tr></table></figure><p>修改配置文件</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span>mysql</span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://localhost:3306/hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>javax.jdo.option.ConnectionUserName – 连接mysql的账号，hadoop<br>javax.jdo.option.ConnectionPassword – 连接mysql的密码，mysql<br>javax.jdo.option.ConnectionURL – 对应上一步创建的数据库，localhost:3306/hive</p><h3 id="hadoop中创建hive所需仓库"><a href="#hadoop中创建hive所需仓库" class="headerlink" title="hadoop中创建hive所需仓库"></a>hadoop中创建hive所需仓库</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -mkdir       /tmp</span><br><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -mkdir   -p  /user/hive/warehouse</span><br><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -chmod g+w   /tmp</span><br><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -chmod g+w   /user/hive/warehouse</span><br></pre></td></tr></table></figure><p>$HADOOP_HOME – 代表您的hadoop工作目录</p><h3 id="hive初始化mysql中的数据库hive"><a href="#hive初始化mysql中的数据库hive" class="headerlink" title="hive初始化mysql中的数据库hive"></a>hive初始化mysql中的数据库hive</h3><h4 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HIVE_HOME</span>/bin/schematool -dbType msyql -initSchema</span><br></pre></td></tr></table></figure><h4 id="可能出现错误1：java-lang-NoSuchMethodError-com-google-common…"><a href="#可能出现错误1：java-lang-NoSuchMethodError-com-google-common…" class="headerlink" title="可能出现错误1：java.lang.NoSuchMethodError: com.google.common…"></a>可能出现错误1：java.lang.NoSuchMethodError: com.google.common…</h4><p>解决方案：<a href="https://yangyichao-mango.github.io/2019/10/17/apache-hive-error-NoSuchMethodError:com.google.common.base.Preconditions.checkArgument/">java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument</a></p><h4 id="可能出现错误2：java-lang-ClassNotFoundException-com-mysql…"><a href="#可能出现错误2：java-lang-ClassNotFoundException-com-mysql…" class="headerlink" title="可能出现错误2：java.lang.ClassNotFoundException: com.mysql…"></a>可能出现错误2：java.lang.ClassNotFoundException: com.mysql…</h4><p>解决方案：<a href="https://yangyichao-mango.github.io/2019/10/17/apache-hive-error-ClassNotFoundException:com.mysql.jdbc.driver/">java.lang.ClassNotFoundException: com.mysql.jdbc.Driver</a></p><h3 id="启动Hive的Metastore-Server服务进程"><a href="#启动Hive的Metastore-Server服务进程" class="headerlink" title="启动Hive的Metastore Server服务进程"></a>启动Hive的Metastore Server服务进程</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HIVE_HOME</span>/bin/hive --service metastore &amp;</span><br></pre></td></tr></table></figure><h3 id="登录hive客户端"><a href="#登录hive客户端" class="headerlink" title="登录hive客户端"></a>登录hive客户端</h3><h4 id="命令-1"><a href="#命令-1" class="headerlink" title="命令"></a>命令</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HIVE_HOME</span>/bin/ hive</span><br></pre></td></tr></table></figure><h4 id="可能出现的错误1：java-lang-IllegalArgumentException-java-net-URISyntaxException-…"><a href="#可能出现的错误1：java-lang-IllegalArgumentException-java-net-URISyntaxException-…" class="headerlink" title="可能出现的错误1：java.lang.IllegalArgumentException: java.net.URISyntaxException:…"></a>可能出现的错误1：java.lang.IllegalArgumentException: java.net.URISyntaxException:…</h4><p>解决方案：<a href="https://yangyichao-mango.github.io/2019/10/17/apache-hive-error-URISyntaxException:Relative-path-in-absolute-URI:%7Bsystem:java.io.tmpdir%7D%7Bsystem-user-name%7D/">Exception in thread “main” java.lang.IllegalArgumentException: java.net.URISyntaxException:</a></p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/categories/Apache-Hive/"/>
    
    
      <category term="Mac安装" scheme="https://yangyichao-mango.github.io/tags/Mac%E5%AE%89%E8%A3%85/"/>
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/tags/Apache-Hive/"/>
    
  </entry>
  
  <entry>
    <title>Apache Hive环境搭建错误：com.mysql.jdbc.Driver was not found in the CLASSPATH</title>
    <link href="https://yangyichao-mango.github.io/2019/10/17/apache-hive:error-ClassNotFoundException:com.mysql.jdbc.driver/"/>
    <id>https://yangyichao-mango.github.io/2019/10/17/apache-hive:error-ClassNotFoundException:com.mysql.jdbc.driver/</id>
    <published>2019-10-17T06:35:11.000Z</published>
    <updated>2019-10-19T04:52:31.852Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Hive环境搭建错误：com.mysql.jdbc.Driver was not found in the CLASSPATH</p><a id="more"></a><h3 id="错误场景详情"><a href="#错误场景详情" class="headerlink" title="错误场景详情"></a><strong>错误场景详情</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HIVE_HOME</span>/bin/schematool -dbType mysql -initSchema</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/usr/<span class="built_in">local</span>/Cellar/hive/3.1.2/libexec/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/usr/<span class="built_in">local</span>/Cellar/hadoop/3.2.1/libexec/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html<span class="comment">#multiple_bindings for an explanation.</span></span><br><span class="line">SLF4J: Actual binding is of <span class="built_in">type</span> [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Metastore connection URL: jdbc:mysql://localhost:3306/hive?characterEncoding=UTF-8</span><br><span class="line">Metastore Connection Driver : com.mysql.jdbc.Driver</span><br><span class="line">Metastore connection User: hadoop</span><br><span class="line">org.apache.hadoop.hive.metastore.HiveMetaException: Failed to load driver</span><br><span class="line">Underlying cause: java.lang.ClassNotFoundException : com.mysql.jdbc.Driver</span><br><span class="line">Use --verbose <span class="keyword">for</span> detailed stacktrace.</span><br><span class="line">*** schemaTool failed ***</span><br></pre></td></tr></table></figure><h3 id="错误原因"><a href="#错误原因" class="headerlink" title="错误原因"></a><strong>错误原因</strong></h3><p>在配置hive-site.xml文件时配置了mysql驱动，而hive/lib目录下没有mysql驱动包。</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a><strong>解决方案</strong></h3><p>官网下载mysql驱动下载地址 (<a href="https://dev.mysql.com/downloads/connector/j/" target="_blank" rel="noopener">https://dev.mysql.com/downloads/connector/j/</a>)<br>把下载好的压缩包（mysql-connector-java-8.0.18.zip）进行解压<br>unzip mysql-connector-java-8.0.18.zip<br>复制到hive/lib下<br>cp mysql-connector-java-8.0.18/mysql-connector-java-8.0.18.jar hive/lib</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/categories/Apache-Hive/"/>
    
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/tags/Apache-Hive/"/>
    
  </entry>
  
  <entry>
    <title>Apache Hive环境搭建错误：java.lang.NoSuchMethodError: com.google.common...</title>
    <link href="https://yangyichao-mango.github.io/2019/10/17/apache-hive:error-NoSuchMethodError:com.google.common.base.Preconditions.checkArgument/"/>
    <id>https://yangyichao-mango.github.io/2019/10/17/apache-hive:error-NoSuchMethodError:com.google.common.base.Preconditions.checkArgument/</id>
    <published>2019-10-17T06:14:27.000Z</published>
    <updated>2019-10-19T04:52:31.858Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Hive环境搭建错误：java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument</p><a id="more"></a><h3 id="错误场景详情"><a href="#错误场景详情" class="headerlink" title="错误场景详情"></a><strong>错误场景详情</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HIVE_HOME</span>/bin/schematool -dbType mysql -initSchema</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/usr/<span class="built_in">local</span>/Cellar/hive/3.1.2/libexec/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/usr/<span class="built_in">local</span>/Cellar/hadoop/3.2.1/libexec/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html<span class="comment">#multiple_bindings for an explanation.</span></span><br><span class="line">SLF4J: Actual binding is of <span class="built_in">type</span> [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V</span><br><span class="line">at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)</span><br><span class="line">at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)</span><br><span class="line">at org.apache.hadoop.mapred.JobConf.setJar(JobConf.java:536)</span><br><span class="line">at org.apache.hadoop.mapred.JobConf.setJarByClass(JobConf.java:554)</span><br><span class="line">at org.apache.hadoop.mapred.JobConf.&lt;init&gt;(JobConf.java:448)</span><br><span class="line">at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:5141)</span><br><span class="line">at org.apache.hadoop.hive.conf.HiveConf.&lt;init&gt;(HiveConf.java:5104)</span><br><span class="line">at org.apache.hive.beeline.HiveSchemaTool.&lt;init&gt;(HiveSchemaTool.java:96)</span><br><span class="line">at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:1473)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class="line">at org.apache.hadoop.util.RunJar.run(RunJar.java:323)</span><br><span class="line">at org.apache.hadoop.util.RunJar.main(RunJar.java:236)</span><br></pre></td></tr></table></figure><h3 id="错误原因"><a href="#错误原因" class="headerlink" title="错误原因"></a><strong>错误原因</strong></h3><p>这是因为hive内依赖的guava.jar和hadoop内的版本不一致造成的。</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a><strong>解决方案</strong></h3><p>查看hadoop安装目录下share/hadoop/common/lib内guava.jar版本<br>查看hive安装目录下lib内guava.jar的版本，如果两者不一致，删除版本低的，并拷贝高版本的，问题解决！</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/categories/Apache-Hive/"/>
    
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/tags/Apache-Hive/"/>
    
  </entry>
  
  <entry>
    <title>Apache Hadoop错误：Unable to load native-hadoop library for your platform</title>
    <link href="https://yangyichao-mango.github.io/2019/10/17/apache-hadoop:error-unable-to-load-native-hadoop-library-from-you-platform/"/>
    <id>https://yangyichao-mango.github.io/2019/10/17/apache-hadoop:error-unable-to-load-native-hadoop-library-from-you-platform/</id>
    <published>2019-10-17T03:34:57.000Z</published>
    <updated>2019-10-25T03:40:04.184Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Hadoop错误：Unable to load native-hadoop library for your platform</p><a id="more"></a><h3 id="错误场景详情"><a href="#错误场景详情" class="headerlink" title="错误场景详情"></a><strong>错误场景详情</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -ls /</span><br><span class="line">WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using <span class="built_in">builtin</span>-java classes <span class="built_in">where</span> applicable</span><br></pre></td></tr></table></figure><h3 id="错误原因"><a href="#错误原因" class="headerlink" title="错误原因"></a><strong>错误原因</strong></h3><p>Hadoop是使用Java语言开发的,但是有一些需求和操作并不适合使用java所以会引入了本地库（Native Libraries）的概念，通过本地库，Hadoop可以更加高效地执行某一些操作.<br>当我们在linux 输入 hdoop fs -ls / 去查看 hdfs 文件系统上的资源时会出现下面错误</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a><strong>解决方案</strong></h3><h4 id="解决方案1"><a href="#解决方案1" class="headerlink" title="解决方案1"></a>解决方案1</h4><p>在Hadoop的配置文件core-site.xml中可以设置是否使用本地库：（Hadoop默认的配置为启用本地库）</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.native.lib<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Should native hadoop libraries, if present, be used.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="解决方案2"><a href="#解决方案2" class="headerlink" title="解决方案2"></a>解决方案2</h4><p>有博客说可以直接下载编译好的位包，替换原来的native包<br>由于在我本地安装的Apache Hadoop 3.2.1版本中没有找到lib文件夹，所以在3.2.1版本中暂时不能使用此种方法</p><p>执行查看文件命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -ls /</span><br><span class="line">2019-10-17 11:33:09,369 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using <span class="built_in">builtin</span>-java classes <span class="built_in">where</span> applicable</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - xxx supergroup          0 2019-10-17 11:23 /tmp</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/categories/Apache-Hadoop/"/>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/tags/Apache-Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Mac安装Apache Hadoop-3.2.1</title>
    <link href="https://yangyichao-mango.github.io/2019/10/17/apache-hadoop:3.2.1-mac-install/"/>
    <id>https://yangyichao-mango.github.io/2019/10/17/apache-hadoop:3.2.1-mac-install/</id>
    <published>2019-10-17T02:12:31.000Z</published>
    <updated>2019-10-25T08:40:28.142Z</updated>
    
    <content type="html"><![CDATA[<p>Mac安装Apache hadoop-3.2.1教程</p><a id="more"></a><h2 id="Java环境配置"><a href="#Java环境配置" class="headerlink" title="Java环境配置"></a><strong>Java环境配置</strong></h2><h3 id="安装Java，查看Java版本以测试是否安装成功"><a href="#安装Java，查看Java版本以测试是否安装成功" class="headerlink" title="安装Java，查看Java版本以测试是否安装成功"></a>安装Java，查看Java版本以测试是否安装成功</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ java -version</span><br><span class="line">java version <span class="string">"1.8.0_191"</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_191-b12)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode)</span><br></pre></td></tr></table></figure><h3 id="查看Java安装位置信息，之后配置Hadoop运行环境需要使用"><a href="#查看Java安装位置信息，之后配置Hadoop运行环境需要使用" class="headerlink" title="查看Java安装位置信息，之后配置Hadoop运行环境需要使用"></a>查看Java安装位置信息，之后配置Hadoop运行环境需要使用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/libexec/java_home</span><br><span class="line">/Library/Java/JavaVirtualMachines/jdk1.8.0_191.jdk/Contents/Home</span><br></pre></td></tr></table></figure><h2 id="ssh配置"><a href="#ssh配置" class="headerlink" title="ssh配置"></a><strong>ssh配置</strong></h2><h3 id="配置ssh"><a href="#配置ssh" class="headerlink" title="配置ssh"></a>配置ssh</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line">$ chmod 0600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><h3 id="创建ssh公钥"><a href="#创建ssh公钥" class="headerlink" title="创建ssh公钥"></a>创建ssh公钥</h3><p>如果没有ssh公钥，执行以下命令创建</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa</span><br></pre></td></tr></table></figure><h3 id="开启远程登录"><a href="#开启远程登录" class="headerlink" title="开启远程登录"></a>开启远程登录</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/blog-img/apache-hadoop:3.2.1-mac-install/remote-login.png" alt="系统偏好设置->共享" title>                </div>                <div class="image-caption">系统偏好设置->共享</div>            </figure><h3 id="测试远程登录是否开启"><a href="#测试远程登录是否开启" class="headerlink" title="测试远程登录是否开启"></a>测试远程登录是否开启</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh localhost</span><br></pre></td></tr></table></figure><h2 id="安装hadoop"><a href="#安装hadoop" class="headerlink" title="安装hadoop"></a><strong>安装hadoop</strong></h2><h3 id="brew安装hadoop"><a href="#brew安装hadoop" class="headerlink" title="brew安装hadoop"></a>brew安装hadoop</h3><p>brew安装的一般都是最新的hadoop，我这里是hadoop 3.2.1 <br>如果需要安装其他版本的hadoop，通过<a href="https://www.jianshu.com/p/aadb54eac0a8" target="_blank" rel="noopener">brew安装指定版本的软件</a>进行安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ brew install hadoop</span><br><span class="line">Updating Homebrew...</span><br><span class="line">==&gt; Downloading https://www.apache.org/dyn/closer.cgi?path=hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz</span><br><span class="line">==&gt; Downloading from http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz</span><br><span class="line"><span class="comment">######################################################################## 100.0%</span></span><br><span class="line">🍺  /usr/<span class="built_in">local</span>/Cellar/hadoop/3.2.1: 21,686 files, 774.1MB, built <span class="keyword">in</span> 10 minutes 1 second</span><br></pre></td></tr></table></figure><p>注意上面的下载信息中 <br>默认brew是会从apache官方的镜像中下载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">==&gt; Downloading https://www.apache.org/dyn/closer.cgi?path=hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz</span><br></pre></td></tr></table></figure><p>如果下载很慢，可以配置国内镜像进行下载(<a href="https://mirror.tuna.tsinghua.edu.cn/help/homebrew/" target="_blank" rel="noopener">清华大学开源软件镜像站</a>)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">==&gt; Downloading from http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz</span><br></pre></td></tr></table></figure><p>安装完之后查看hadoop安装位置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ brew info hadoop</span><br><span class="line">hadoop: stable 3.2.1</span><br><span class="line">Framework <span class="keyword">for</span> distributed processing of large data sets</span><br><span class="line">https://hadoop.apache.org/</span><br><span class="line">Conflicts with:</span><br><span class="line">  yarn (because both install `yarn` binaries)</span><br><span class="line">/usr/<span class="built_in">local</span>/Cellar/hadoop/hdfs (20 files, 1MB)</span><br><span class="line">  Built from <span class="built_in">source</span></span><br><span class="line">/usr/<span class="built_in">local</span>/Cellar/hadoop/3.2.1 (22,408 files, 815.8MB)</span><br><span class="line">  Built from <span class="built_in">source</span> on 2019-10-17 at 09:46:37</span><br><span class="line">From: https://github.com/Homebrew/homebrew-core/blob/master/Formula/hadoop.rb</span><br><span class="line">==&gt; Requirements</span><br><span class="line">Required: java &gt;= 1.8 ✔</span><br><span class="line">==&gt; Analytics</span><br><span class="line">install: 4,572 (30 days), 10,774 (90 days), 44,762 (365 days)</span><br><span class="line">install_on_request: 3,822 (30 days), 9,128 (90 days), 38,206 (365 days)</span><br><span class="line">build_error: 0 (30 days)</span><br></pre></td></tr></table></figure><h3 id="配置hadoop"><a href="#配置hadoop" class="headerlink" title="配置hadoop"></a>配置hadoop</h3><p>需要修改的配置文件都在<font color="red">/usr/local/Cellar/hadoop/3.2.1/libexec/etc/hadoop</font>这个目录下</p><h4 id="hadoop-env-sh"><a href="#hadoop-env-sh" class="headerlink" title="hadoop-env.sh"></a>hadoop-env.sh</h4><p>配置 export JAVA_HOME</p><p>将/usr/libexec/java_home查到的 Java 路径配置进去，记得去掉注释 #。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_191.jdk/Contents/Home</span><br></pre></td></tr></table></figure><h4 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h4><p>修改core-site.xml 文件参数,配置NameNode的主机名和端口号</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/Cellar/hadoop/hdfs/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>A base for other temporary directories<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h4><p>变量dfs.replication指定了每个HDFS数据库的复制次数。 通常为3, 由于我们只有一台主机和一个伪分布式模式的DataNode，将此值修改为1</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="格式化"><a href="#格式化" class="headerlink" title="格式化"></a>格式化</h4><p>格式化hdfs操作只要第一次才使用，否则会造成数据全部丢失</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hdfs namenode -format</span><br></pre></td></tr></table></figure><h3 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h3><p>启动服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./start-all.sh</span><br></pre></td></tr></table></figure><p>启动成功后，可以在<br><a href="http://localhost:9870/" target="_blank" rel="noopener">http://localhost:9870/</a><br><a href="http://localhost:8088/cluster" target="_blank" rel="noopener">http://localhost:8088/cluster</a><br>进行查看</p><p>关闭服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./stop-all.sh</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/categories/Apache-Hadoop/"/>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/tags/Apache-Hadoop/"/>
    
      <category term="Mac安装" scheme="https://yangyichao-mango.github.io/tags/Mac%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>useful-api-for-java</title>
    <link href="https://yangyichao-mango.github.io/2019/10/16/java-api:useful-api/"/>
    <id>https://yangyichao-mango.github.io/2019/10/16/java-api:useful-api/</id>
    <published>2019-10-16T02:16:10.000Z</published>
    <updated>2019-10-19T04:52:48.324Z</updated>
    
    <content type="html"><![CDATA[<h2 id="API"><a href="#API" class="headerlink" title="API"></a><strong>API</strong></h2><ul><li><a href="https://www.jianshu.com/p/865e9ae667a0" target="_blank" rel="noopener">Retrofit（http请求工具包）</a></li><li><a href="https://www.jianshu.com/p/4271ebc40be8" target="_blank" rel="noopener">Resilience4j（接口重试，限流，熔断器工具）</a></li></ul>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Java Api" scheme="https://yangyichao-mango.github.io/categories/Java-Api/"/>
    
    
      <category term="Java Api" scheme="https://yangyichao-mango.github.io/tags/Java-Api/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 零基础入门（四）：DataStream API 编程 学习心得</title>
    <link href="https://yangyichao-mango.github.io/2019/10/15/apache-flink:study-4-datastream-api/"/>
    <id>https://yangyichao-mango.github.io/2019/10/15/apache-flink:study-4-datastream-api/</id>
    <published>2019-10-15T07:57:16.000Z</published>
    <updated>2019-11-06T13:07:48.987Z</updated>
    
    <content type="html"><![CDATA[<p>学习心得</p><a id="more"></a><h2 id="DataStream"><a href="#DataStream" class="headerlink" title="DataStream"></a><strong>DataStream</strong></h2><h3 id="RichParallelSourceFunction"><a href="#RichParallelSourceFunction" class="headerlink" title="RichParallelSourceFunction"></a>RichParallelSourceFunction</h3><p>用户通过实现SourceFunction自定义DataSource</p><p>如果设置了并行度，则会产生指定并行度个数的DataSource消费客户端去消费DataSource</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment.setParallelism(int)</span><br></pre></td></tr></table></figure><p>举例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">GroupedProcessingTimeWindow</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = LoggerFactory.getLogger(GroupedProcessingTimeWindow.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">DataSource</span> <span class="keyword">extends</span> <span class="title">RichParallelSourceFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isRunning = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Tuple2&lt;String, Integer&gt;&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            Random random = <span class="keyword">new</span> Random();</span><br><span class="line">            <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">                TimeUnit.MILLISECONDS.sleep((getRuntimeContext().getIndexOfThisSubtask() + <span class="number">1</span>) * <span class="number">1000</span> * <span class="number">5</span>);</span><br><span class="line">                String key = <span class="string">"类别"</span> + (<span class="keyword">char</span>) (<span class="string">'A'</span> + random.nextInt(<span class="number">3</span>));</span><br><span class="line">                <span class="keyword">int</span> value = random.nextInt(<span class="number">10</span>) + <span class="number">1</span>;</span><br><span class="line">                LOGGER.info(<span class="string">"Thread: &#123;&#125;, key: &#123;&#125;, value: &#123;&#125;, dataSource object: &#123;&#125;)"</span></span><br><span class="line">                        , Thread.currentThread().getName()</span><br><span class="line">                        , key</span><br><span class="line">                        , value</span><br><span class="line">                        , <span class="keyword">this</span>);</span><br><span class="line">                ctx.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(key, value));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            isRunning = <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        DataSource dataSource = <span class="keyword">new</span> DataSource();</span><br><span class="line">        LOGGER.info(<span class="string">"dataSource object: &#123;&#125;"</span>, dataSource);</span><br><span class="line"></span><br><span class="line">        DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; ds = env.addSource(dataSource);</span><br><span class="line">        KeyedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple&gt; keyedStream = ds.keyBy(<span class="number">0</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// KeyedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple&gt; keyedStream = ds.keyBy("f0"); 通过指定字段名 f0</span></span><br><span class="line"></span><br><span class="line">        keyedStream</span><br><span class="line">            .sum(<span class="number">1</span>)</span><br><span class="line">            <span class="comment">// .sum("f1") 通过制定字段名 f1</span></span><br><span class="line">            .keyBy((KeySelector&lt;Tuple2&lt;String, Integer&gt;, Object&gt;) stringIntegerTuple2 -&gt; StringUtils.EMPTY)</span><br><span class="line">            .fold(<span class="keyword">new</span> HashMap&lt;String, Integer&gt;(),</span><br><span class="line">                    <span class="keyword">new</span> FoldFunction&lt;Tuple2&lt;String, Integer&gt;, HashMap&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> HashMap&lt;String, Integer&gt; <span class="title">fold</span><span class="params">(HashMap&lt;String, Integer&gt; accumulator,</span></span></span><br><span class="line"><span class="function"><span class="params">                                Tuple2&lt;String, Integer&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                            accumulator.put(value.f0, value.f1);</span><br><span class="line">                            <span class="keyword">return</span> accumulator;</span><br><span class="line">                        &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            .addSink(<span class="keyword">new</span> SinkFunction&lt;HashMap&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(HashMap&lt;String, Integer&gt; value, Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    <span class="comment">// 每个类型的商品成交量</span></span><br><span class="line">                    LOGGER.info(<span class="string">"&#123;&#125;"</span></span><br><span class="line">                            , value);</span><br><span class="line">                    <span class="comment">// 商品成交总量</span></span><br><span class="line">                    LOGGER.info(<span class="string">"&#123;&#125;"</span></span><br><span class="line">                            , value.values().stream().mapToInt(v -&gt; v).sum());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过查看dataSource object:的log就会发现上面这个例子中国产生了3个DataSource实例。</p><h3 id="Evictor"><a href="#Evictor" class="headerlink" title="Evictor"></a>Evictor</h3><p>CountEvictor：保持窗口内元素数量符合用户指定数量，如果多于用户指定的数量，从窗口缓冲区的开头丢弃剩余的元素。<br>DeltaEvictor：使用 DeltaFunction和 一个阈值，计算窗口缓冲区中的最后一个元素与其余每个元素之间的 delta 值，并删除 delta 值大于或等于阈值的元素。<br>TimeEvictor：以毫秒为单位的时间间隔作为参数，对于给定的窗口，找到元素中的最大的时间戳max_ts，并删除时间戳小于max_ts - interval的所有元素。</p><h2 id="keyedStream"><a href="#keyedStream" class="headerlink" title="keyedStream"></a><strong>keyedStream</strong></h2><h3 id="KeyedStream-fold-R-initialValue-FoldFunction-lt-T-R-gt-folder"><a href="#KeyedStream-fold-R-initialValue-FoldFunction-lt-T-R-gt-folder" class="headerlink" title="KeyedStream.fold(R initialValue, FoldFunction&lt;T, R&gt; folder)"></a>KeyedStream.fold(R initialValue, FoldFunction&lt;T, R&gt; folder)</h3><p>添加一个合并key分组的算子，FoldFunction会接收到同一key的value，只有key相同的值才会被分发到同一个folder。</p><h2 id="可能出现的问题"><a href="#可能出现的问题" class="headerlink" title="可能出现的问题"></a><strong>可能出现的问题</strong></h2><h3 id="Apache-Flink-Return-type-of-function-could-not-be-determined-automatically-due-to-type-erasure"><a href="#Apache-Flink-Return-type-of-function-could-not-be-determined-automatically-due-to-type-erasure" class="headerlink" title="Apache Flink: Return type of function could not be determined automatically due to type erasure"></a><font color="red">Apache Flink: Return type of function could not be determined automatically due to type erasure</font></h3><p>错误场景：<br>在用户定义DAG图算子的时候，可能会出现不支持lambda表达式的情况</p><p>原因：<br>为了执行程序，Flink需要知道要处理的值的类型，因为它需要序列化和反序列化数据。<br>Flink的类型系统基于描述数据类型的TypeInformation进行序列化和反序列化，会将Java中的基本类型以及Object类型与TypeInformation进行映射。<br>当您指定一个函数时，Flink会尝试推断该函数的返回类型。<br>但是某些Lambda函数由于类型擦除而丢失了此信息（可以自己编译后再对编译成的.class文件进行反编译，然后查看函数签名，发现函数签名具体类型被擦除），<br>因此Flink无法通过此自动推断类型。<br><a href="https://flink.sojb.cn/dev/java_lambdas.html" target="_blank" rel="noopener">Flink Java Lambda表达式</a></p><p>因此，必须显式声明返回类型。</p><p>解决方案1：用户自己定义返回类型</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; wordDataStream = dataStream.flatMap(</span><br><span class="line">    (String sentence, Collector&lt;String&gt; out) -&gt; &#123;</span><br><span class="line">        <span class="keyword">for</span>(String word: sentence.split(<span class="string">"\\W+"</span>)) &#123;</span><br><span class="line">            out.collect(word); <span class="comment">// collect objects of type String</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">).returns(Types.STRING);</span><br></pre></td></tr></table></figure><p>解决方案2：显示声明返回类型</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; wordDataStream = dataStream.flatMap(</span><br><span class="line">    <span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String sentence, Collector&lt;String&gt; out)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// normalize and split the line</span></span><br><span class="line">            String[] words = sentence.toLowerCase().split(<span class="string">"\\W+"</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// emit the pairs</span></span><br><span class="line">            <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                <span class="keyword">if</span> (word.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                    out.collect(word);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://yangyichao-mango.github.io/2019/10/14/hello-world/"/>
    <id>https://yangyichao-mango.github.io/2019/10/14/hello-world/</id>
    <published>2019-10-14T12:54:25.000Z</published>
    <updated>2019-10-19T11:37:10.532Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><a id="more"></a><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
    
  </entry>
  
</feed>
