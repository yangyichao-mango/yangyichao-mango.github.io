<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>antigeneral&#39;s blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://yangyichao-mango.github.io/"/>
  <updated>2021-12-09T15:26:46.226Z</updated>
  <id>https://yangyichao-mango.github.io/</id>
  
  <author>
    <name>yangyichao-mango</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>在配置不同的状态后端时，到底对 flink 任务有什么影响</title>
    <link href="https://yangyichao-mango.github.io/2022/11/15/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/04_%E5%BD%93%E6%88%91%E4%BB%AC%E5%9C%A8%E9%85%8D%E7%BD%AE%E4%B8%8D%E5%90%8C%E7%9A%84%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF%E6%97%B6%EF%BC%8C%E5%88%B0%E5%BA%95%E5%BD%B1%E5%93%8D%E4%BA%86%E4%BB%80%E4%B9%88%E4%B8%9C%E8%A5%BF/"/>
    <id>https://yangyichao-mango.github.io/2022/11/15/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/04_%E5%BD%93%E6%88%91%E4%BB%AC%E5%9C%A8%E9%85%8D%E7%BD%AE%E4%B8%8D%E5%90%8C%E7%9A%84%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF%E6%97%B6%EF%BC%8C%E5%88%B0%E5%BA%95%E5%BD%B1%E5%93%8D%E4%BA%86%E4%BB%80%E4%B9%88%E4%B8%9C%E8%A5%BF/</id>
    <published>2022-11-15T06:25:58.000Z</published>
    <updated>2021-12-09T15:26:46.226Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-大家首先要知道的一些背景"><a href="#1-大家首先要知道的一些背景" class="headerlink" title="1.大家首先要知道的一些背景"></a>1.大家首先要知道的一些背景</h1><p><code>状态</code>：状态就是用户在程序中使用的数据结构。比如 flink 中的 MapState，ValueState，ListState。在一个 flink 任务中，不管我们使用了多少状态，这些状态只会分为 operator state，keyed state 两类状态。</p><p><code>状态管理</code>：为了防止 long run 的 flink 任务挂了导致状态丢失，产生数据质量问题，flink 提供了状态管理（Checkpoint，Savepoint）的能力把我们使用的状态给管理起来，定时的保存到远程。<br>然后可以在 flink 任务 failover 时，从远程把状态数据恢复到 flink 任务中，保障数据质量。</p><p><code>状态后端</code>：状态后端就是决定了以什么样数据结构，什么样的存储方式去存储和管理我们的状态。flink 目前官方提供了 memory、filesystem，rocksdb 三种状态后端来存储我们的状态。</p><h1 id="2-在配置不同的状态后端时，到底对-flink-任务有什么影响"><a href="#2-在配置不同的状态后端时，到底对-flink-任务有什么影响" class="headerlink" title="2.在配置不同的状态后端时，到底对 flink 任务有什么影响"></a>2.在配置不同的状态后端时，到底对 flink 任务有什么影响</h1><p>其实所有的内容都浓缩到了这样图中：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/03_choose_state/2.png" alt="sql 开发"></p><ol><li>横向（行）来看，分为 Operator state-backend、Keyed state-backend 来管理一个 flink 任务中的所有状态（operator state，keyed state）</li><li>纵向（列）来看，用户可以通过配置 memory，filesystem，rocksdb，在 flink 任务中生成 MemoryStateBackend，FsStateBackend，RocksdbStateBackend，其声明了整个任务的状态管理后端类型</li><li>每个格子中的内容就是用户在配置 xx 状态后端（列）时，给用户使用的状态（行）生成的状态后端实例，生成的这个实例就是用于管理用户使用的状态的。</li></ol><p>那么可以得到的结论就是：</p><ol><li>flink 任务中的 operator state。无论用户配置哪种状态后端（无论是 memory，filesystem，rocksdb），都是使用 DefaultOperatorStateBackend 来管理的，状态数据都存储在内存中。</li><li>flink 任务中的 keyed state，会有不同。用户在配置 rocksdb 时，会使用 RocksdbKeyedStateBackend 去管理状态；用户在配置 memory，filesystem 时，会使用 HeapKeyedStateBackend 去管理状态。</li><li>那么也就是说，你配置的 rocksdb 只会影响 keyed state 存储的方式和地方，operator state 不会受到影响。</li></ol>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink sql 知其所以然（十九）：Table 与 DataStream 的转转转</title>
    <link href="https://yangyichao-mango.github.io/2021/11/16/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B9%9D%EF%BC%89%EF%BC%9Aflinksqltable%E8%BD%AC%E6%8D%A2%E4%B8%BAdatastream/"/>
    <id>https://yangyichao-mango.github.io/2021/11/16/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B9%9D%EF%BC%89%EF%BC%9Aflinksqltable%E8%BD%AC%E6%8D%A2%E4%B8%BAdatastream/</id>
    <published>2021-11-16T08:26:59.000Z</published>
    <updated>2021-12-16T16:54:01.085Z</updated>
    
    <content type="html"><![CDATA[<p>看了那么多的技术文，你能明白作者想让你在读完文章后学到什么吗？</p><p>大数据羊说的文章会让你明白</p><ol><li><p>博主会阐明博主期望本文能给小伙伴们带来什么帮助，让小伙伴萌能直观明白博主的心思</p></li><li><p>博主会以实际的应用场景和案例入手，不只是知识点的简单堆砌</p></li><li><p>博主会把重要的知识点的原理进行剖析，让小伙伴萌做到深入浅出</p></li></ol><h1 id="1-序篇"><a href="#1-序篇" class="headerlink" title="1.序篇"></a>1.序篇</h1><p>源码公众号后台回复<strong>1.13.2 table datastream</strong>获取。</p><p>废话不多说，咱们先直接上本文的目录和结论，小伙伴可以先看结论快速了解博主期望本文能给小伙伴们带来什么帮助：</p><ol><li><strong>背景及应用场景介绍</strong>：博主期望你能了解到，Flink 支持了 SQL 和 Table API 中的 Table 与 DataStream 互转的接口。通过这种互转的方式，我们就可以将一些自定义的数据源（DataStream）创建为 SQL 表，也可以将 SQL 执行结果转换为 DataStream 然后后续去完成一些在 SQL 中实现不了的复杂操作。肥肠的方便。</li><li><strong>目前只有流任务支持互转，批任务不支持</strong>：在 1.13 版本中，由于流和批的 env 接口不一样，流任务为 StreamTableEnvironment，批任务为 TableEnvironment，目前只有 StreamTableEnvironment 支持了互转的接口，TableEnvironment 没有这样的接口，因此目前流任务支持互转，批任务不支持。但是 1.14 版本中流批任务的 env 都统一到了 StreamTableEnvironment 中，流批任务中就都可以进行互转了。</li><li><strong>Retract 语义 SQL 转 DataStream 需要重点注意</strong>：Append 语义的 SQL 转为 DataStream 使用的 API 为 <code>StreamTableEnvironment::toDataStream</code>，Retract 语义的 SQL 转为 DataStream 使用的 API 为 <code>StreamTableEnvironment::toRetractStream</code>，两个接口不一样，小伙伴萌一定要特别注意。</li></ol><h1 id="2-背景及应用场景介绍"><a href="#2-背景及应用场景介绍" class="headerlink" title="2.背景及应用场景介绍"></a>2.背景及应用场景介绍</h1><p>相信大家看到本文的标题时，会比较好奇，要写 SQL 就纯 SQL 呗，要写 DataStream 就纯 DataStream 呗，为啥还要把这两个接口做集成呢？</p><p>博主举一个案例：在拼多多发优惠券的场景下，为了控制成本，希望能在每日优惠券发放金额加和超过 1w 时，及时报警出来，控制预算。</p><p>优惠券表的发放数据：</p><table><thead><tr><th>id（id）</th><th>time（时间）</th><th>money（金额）</th></tr></thead><tbody><tr><td>1</td><td>2021-11-01 00:01:03</td><td>10</td></tr><tr><td>2</td><td>2021-11-01 00:03:00</td><td>20</td></tr><tr><td>3</td><td>2021-11-01 00:05:00</td><td>30</td></tr><tr><td>4</td><td>2021-11-01 00:06:00</td><td>40</td></tr><tr><td>5</td><td>2021-11-01 00:07:00</td><td>50</td></tr></tbody></table><p>最终期望的结果是：每天的 money 之和超过 1w 的时候，报警报警报警！！！</p><p>那么针对上述场景，有两种对应的解决方案：</p><ol><li><p>方案 1：可想而知，DataStream 是必然能够解决我们的问题的。</p></li><li><p>方案 2：DataStream 开发效率不高，可以使用 SQL 计算优惠券发放的结果，但是 SQL 无法做到报警。所以可以将 SQL 的查询的结果（即 Table）转为 DataStream，然后在 DataStream 后自定义报警逻辑的算子，超过阈值进行报警。</p></li></ol><p>本节就介绍方案 2 的实现思路。</p><blockquote><p>注意：</p><p>当然还有一些其他的比如模式识别监控异常然后报警的场景使用 DataStream 去实现就更加复杂了，所以我们也可以使用类似的思路，先 SQL 实现业务逻辑，然后接一个 DataStream 算子实现报警逻辑。</p></blockquote><h1 id="3-Table-与-DataStream-API-的转换具体实现"><a href="#3-Table-与-DataStream-API-的转换具体实现" class="headerlink" title="3.Table 与 DataStream API 的转换具体实现"></a>3.Table 与 DataStream API 的转换具体实现</h1><h2 id="3-1-先看一个官网的简单案例"><a href="#3-1-先看一个官网的简单案例" class="headerlink" title="3.1.先看一个官网的简单案例"></a>3.1.先看一个官网的简单案例</h2><p>官网的案例主要是让大家看看要做到 Table 与 DataStream API 的转换会涉及到使用哪些接口。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.Row;</span><br><span class="line"></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">DataStream&lt;String&gt; dataStream = env.fromElements(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;John&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. 使用 StreamTableEnvironment::fromDataStream API 将 DataStream 转为 Table</span></span><br><span class="line">Table inputTable = tableEnv.fromDataStream(dataStream);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 Table 注册为一个临时表</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">&quot;InputTable&quot;</span>, inputTable);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 然后就可以在这个临时表上做一些自定义的查询了</span></span><br><span class="line">Table resultTable = tableEnv.sqlQuery(<span class="string">&quot;SELECT UPPER(f0) FROM InputTable&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 也可以使用 StreamTableEnvironment::toDataStream 将 Table 转为 DataStream</span></span><br><span class="line"><span class="comment">// 注意：这里只能转为 DataStream&lt;Row&gt;，其中的数据类型只能为 Row</span></span><br><span class="line">DataStream&lt;Row&gt; resultStream = tableEnv.toDataStream(resultTable);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 DataStream 结果打印到控制台</span></span><br><span class="line">resultStream.print();</span><br><span class="line">env.execute();</span><br><span class="line"></span><br><span class="line"><span class="comment">// prints:</span></span><br><span class="line"><span class="comment">// +I[Alice]</span></span><br><span class="line"><span class="comment">// +I[Bob]</span></span><br><span class="line"><span class="comment">// +I[John]</span></span><br></pre></td></tr></table></figure><p>可以看到重点的接口就是：</p><ol><li>StreamTableEnvironment::toDataStream：将 Table 转为 DataStream</li><li>StreamTableEnvironment::fromDataStream：将 DataStream 转为 Table</li></ol><h2 id="3-2-实现第-2-节中的逻辑"><a href="#3-2-实现第-2-节中的逻辑" class="headerlink" title="3.2.实现第 2 节中的逻辑"></a>3.2.实现第 2 节中的逻辑</h2><p>我们使用上面介绍的两个接口对优惠券发放金额预警的案例做一个实现。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AlertExample</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        FlinkEnv flinkEnv = FlinkEnvUtils.getStreamTableEnv(args);</span><br><span class="line"></span><br><span class="line">        String createTableSql = <span class="string">&quot;CREATE TABLE source_table (\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    id BIGINT,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    money BIGINT,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    row_time AS cast(CURRENT_TIMESTAMP as timestamp_LTZ(3)),\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    WATERMARK FOR row_time AS row_time - INTERVAL &#x27;5&#x27; SECOND\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;) WITH (\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;connector&#x27; = &#x27;datagen&#x27;,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;rows-per-second&#x27; = &#x27;1&#x27;,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;fields.id.min&#x27; = &#x27;1&#x27;,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;fields.id.max&#x27; = &#x27;100000&#x27;,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;fields.money.min&#x27; = &#x27;1&#x27;,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;fields.money.max&#x27; = &#x27;100000&#x27;\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;)\n&quot;</span>;</span><br><span class="line"></span><br><span class="line">        String querySql = <span class="string">&quot;SELECT UNIX_TIMESTAMP(CAST(window_end AS STRING)) * 1000 as window_end, \n&quot;</span></span><br><span class="line">                + <span class="string">&quot;      window_start, \n&quot;</span></span><br><span class="line">                + <span class="string">&quot;      sum(money) as sum_money,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;      count(distinct id) as count_distinct_id\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;FROM TABLE(CUMULATE(\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;         TABLE source_table\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;         , DESCRIPTOR(row_time)\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;         , INTERVAL &#x27;5&#x27; SECOND\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;         , INTERVAL &#x27;1&#x27; DAY))\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;GROUP BY window_start, \n&quot;</span></span><br><span class="line">                + <span class="string">&quot;        window_end&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建数据源表，即优惠券发放明细数据</span></span><br><span class="line">        flinkEnv.streamTEnv().executeSql(createTableSql);</span><br><span class="line">        <span class="comment">// 2. 执行 query 查询，计算每日发放金额</span></span><br><span class="line">        Table resultTable = flinkEnv.streamTEnv().sqlQuery(querySql);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 报警逻辑（toDataStream 返回 Row 类型），如果 sum_money 超过 1w，报警</span></span><br><span class="line">        flinkEnv.streamTEnv()</span><br><span class="line">                .toDataStream(resultTable, Row.class)</span><br><span class="line">                .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;Row, Object&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Row value, Collector&lt;Object&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">long</span> l = Long.parseLong(String.valueOf(value.getField(<span class="string">&quot;sum_money&quot;</span>)));</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">if</span> (l &gt; <span class="number">10000L</span>) &#123;</span><br><span class="line">                            log.info(<span class="string">&quot;报警，超过 1w&quot;</span>);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        flinkEnv.env().execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行效果如下：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/22_flinksqltabletrans/1.jpg" alt="warn"></p><h2 id="3-3-Table-和-DataStream-转换注意事项"><a href="#3-3-Table-和-DataStream-转换注意事项" class="headerlink" title="3.3.Table 和 DataStream 转换注意事项"></a>3.3.Table 和 DataStream 转换注意事项</h2><h3 id="3-3-1-目前只支持流任务互转（1-13）"><a href="#3-3-1-目前只支持流任务互转（1-13）" class="headerlink" title="3.3.1.目前只支持流任务互转（1.13）"></a>3.3.1.目前只支持流任务互转（1.13）</h3><p>目前在 1.13 版本中，Flink 对于 Table 和 DataStream 的转化是有一些限制的：</p><p>目前流任务使用的 env 为 StreamTableEnvironment，批任务为 TableEnvironment，而 Table 和 DataStream 之间的转换目前只有 StreamTableEnvironment 的接口支持。</p><p>所以其实小伙伴萌可以理解为只有流任务才支持 Table 和 DataStream 之间的转换，批任务是不支持的（虽然可以使用流模式处理有界流（批数据），但效率较低，这种骚操作不建议大家搞）。</p><p>那什么时候才能支持批任务的 Table 和 DataStream 之间的转换呢？</p><p>1.14 版本支持。1.14 版本中，流和批的都统一到了 StreamTableEnvironment 中，因此就可以做 Table 和 DataStream 的互相转换了。</p><h3 id="3-3-2-Retract-语义-SQL-转-DataStream-注意事项"><a href="#3-3-2-Retract-语义-SQL-转-DataStream-注意事项" class="headerlink" title="3.3.2.Retract 语义 SQL 转 DataStream 注意事项"></a>3.3.2.Retract 语义 SQL 转 DataStream 注意事项</h3><p>Retract 语义的 SQL 使用 <code>toDataStream</code> 转换会报错不支持。具体报错截图如下。意思是不支持 update 类型的结果数据。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/22_flinksqltabletrans/2.png" alt="Retract error"></p><p>如果要把 Retract 语义的 SQL 转为 DataStream，我们需要使用 <code>toRetractStream</code>。如下案例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">@Slf4j</span><br><span class="line">public class AlertExampleRetract &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        FlinkEnv flinkEnv &#x3D; FlinkEnvUtils.getStreamTableEnv(args);</span><br><span class="line"></span><br><span class="line">        String createTableSql &#x3D; &quot;CREATE TABLE source_table (\n&quot;</span><br><span class="line">                + &quot;    id BIGINT,\n&quot;</span><br><span class="line">                + &quot;    money BIGINT,\n&quot;</span><br><span class="line">                + &quot;    &#96;time&#96; as cast(CURRENT_TIMESTAMP as bigint) * 1000\n&quot;</span><br><span class="line">                + &quot;) WITH (\n&quot;</span><br><span class="line">                + &quot;  &#39;connector&#39; &#x3D; &#39;datagen&#39;,\n&quot;</span><br><span class="line">                + &quot;  &#39;rows-per-second&#39; &#x3D; &#39;1&#39;,\n&quot;</span><br><span class="line">                + &quot;  &#39;fields.id.min&#39; &#x3D; &#39;1&#39;,\n&quot;</span><br><span class="line">                + &quot;  &#39;fields.id.max&#39; &#x3D; &#39;100000&#39;,\n&quot;</span><br><span class="line">                + &quot;  &#39;fields.money.min&#39; &#x3D; &#39;1&#39;,\n&quot;</span><br><span class="line">                + &quot;  &#39;fields.money.max&#39; &#x3D; &#39;100000&#39;\n&quot;</span><br><span class="line">                + &quot;)\n&quot;;</span><br><span class="line"></span><br><span class="line">        String querySql &#x3D; &quot;SELECT max(&#96;time&#96;), \n&quot;</span><br><span class="line">                + &quot;      sum(money) as sum_money\n&quot;</span><br><span class="line">                + &quot;FROM source_table\n&quot;</span><br><span class="line">                + &quot;GROUP BY (&#96;time&#96; + 8 * 3600 * 1000) &#x2F; (24 * 3600 * 1000)&quot;;</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 1. 创建数据源表，即优惠券发放明细数据</span><br><span class="line">        flinkEnv.streamTEnv().executeSql(createTableSql);</span><br><span class="line">        &#x2F;&#x2F; 2. 执行 query 查询，计算每日发放金额</span><br><span class="line">        Table resultTable &#x3D; flinkEnv.streamTEnv().sqlQuery(querySql);</span><br><span class="line">        &#x2F;&#x2F; 3. 报警逻辑（toRetractStream 返回 Tuple2&lt;Boolean, Row&gt; 类型），如果 sum_money 超过 1w，报警</span><br><span class="line">        &#x2F;&#x2F; Tuple2&lt;Boolean, Row&gt; f0 的 Boolean 标识是否是回撤消息</span><br><span class="line">        flinkEnv.streamTEnv()</span><br><span class="line">                .toRetractStream(resultTable, Row.class)</span><br><span class="line">                .flatMap(new FlatMapFunction&lt;Tuple2&lt;Boolean, Row&gt;, Object&gt;() &#123;</span><br><span class="line">                    @Override</span><br><span class="line">                    public void flatMap(Tuple2&lt;Boolean, Row&gt; value, Collector&lt;Object&gt; out) throws Exception &#123;</span><br><span class="line">                        long l &#x3D; Long.parseLong(String.valueOf(value.f1.getField(&quot;sum_money&quot;)));</span><br><span class="line"></span><br><span class="line">                        if (l &gt; 10000L) &#123;</span><br><span class="line">                            log.info(&quot;报警，超过 1w&quot;);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        flinkEnv.env().execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="5-总结与展望"><a href="#5-总结与展望" class="headerlink" title="5.总结与展望"></a>5.总结与展望</h1><p>源码公众号后台回复<strong>1.13.2 table datastream</strong>获取。</p><p>本文主要介绍了 flink 中 Table 和 DataStream 互转使用方式，并介绍了一些使用注意事项，总结如下：</p><ol><li><strong>背景及应用场景介绍</strong>：博主期望你能了解到，Flink 支持了 SQL 和 Table API 中的 Table 与 DataStream 互转的接口。通过这种互转的方式，我们就可以将一些自定义的数据源（DataStream）创建为 SQL 表，也可以将 SQL 执行结果转换为 DataStream 然后后续去完成一些在 SQL 中实现不了的复杂操作。肥肠的方便。</li><li><strong>目前只有流任务支持互转，批任务不支持</strong>：在 1.13 版本中，由于流和批的 env 接口不一样，流任务为 StreamTableEnvironment，批任务为 TableEnvironment，目前只有 StreamTableEnvironment 支持了互转的接口，TableEnvironment 没有这样的接口，因此目前流任务支持互转，批任务不支持。但是 1.14 版本中流批任务的 env 都统一到了 StreamTableEnvironment 中，流批任务中就都可以进行互转了。</li><li><strong>Retract 语义 SQL 转 DataStream 需要重点注意</strong>：Append 语义的 SQL 转为 DataStream 使用的 API 为 <code>StreamTableEnvironment::toDataStream</code>，Retract 语义的 SQL 转为 DataStream 使用的 API 为 <code>StreamTableEnvironment::toRetractStream</code>，两个接口不一样，小伙伴萌一定要特别注意。</li></ol>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink sql 知其所以然（十七）：flink sql 开发利器之 Zeppelin</title>
    <link href="https://yangyichao-mango.github.io/2021/11/15/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/18_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%EF%BC%9Aflink-sql-%E5%BC%80%E5%8F%91%E5%88%A9%E5%99%A8%E4%B9%8Bzeppelin/"/>
    <id>https://yangyichao-mango.github.io/2021/11/15/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/18_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%EF%BC%9Aflink-sql-%E5%BC%80%E5%8F%91%E5%88%A9%E5%99%A8%E4%B9%8Bzeppelin/</id>
    <published>2021-11-15T08:26:59.000Z</published>
    <updated>2021-12-08T15:25:15.031Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-序篇"><a href="#1-序篇" class="headerlink" title="1.序篇"></a>1.序篇</h1><p>上节介绍了 flink sql 的企业级开发利器 Dlink。本节就来介绍下 Apache Zeppelin。</p><p>本节主要介绍一下博主在本地部署 Apache Zeppelin 的过程以及感受。</p><p>先说下安装感受，Apache Zeppelin 安装起来真的是非常的方便！！！</p><p>几步就完成了。</p><h1 id="2-Apache-Zeppelin-平台效果"><a href="#2-Apache-Zeppelin-平台效果" class="headerlink" title="2.Apache Zeppelin 平台效果"></a>2.Apache Zeppelin 平台效果</h1><p>具体功能如下图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/19_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%EF%BC%9Aflinksql%E5%BC%80%E5%8F%91%E5%88%A9%E5%99%A8%E4%B9%8Bzeppelin/1.png" alt="sql 开发"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/19_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%EF%BC%9Aflinksql%E5%BC%80%E5%8F%91%E5%88%A9%E5%99%A8%E4%B9%8Bzeppelin/2.png" alt="sql 开发"></p><h1 id="3-安装部署篇"><a href="#3-安装部署篇" class="headerlink" title="3.安装部署篇"></a>3.安装部署篇</h1><p>安装真的很简单。</p><p>可以参考 jeff 大佬的安装教程，也可以参考博主本文的安装教程。</p><p>jeff 大佬安装教程：<a href="https://www.yuque.com/jeffzhangjianfeng/gldg8w/bam5y1">https://www.yuque.com/jeffzhangjianfeng/gldg8w/bam5y1</a></p><h2 id="3-1-安装-flink"><a href="#3-1-安装-flink" class="headerlink" title="3.1.安装 flink"></a>3.1.安装 flink</h2><p>这一步是为了配置 flink 的环境。</p><p><a href="https://archive.apache.org/dist/flink/flink-1.13.2/">https://archive.apache.org/dist/flink/flink-1.13.2/</a></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/19_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%EF%BC%9Aflinksql%E5%BC%80%E5%8F%91%E5%88%A9%E5%99%A8%E4%B9%8Bzeppelin/3.png" alt="flink 下载"></p><ol><li><p>点击链接之后可以直接下载。</p></li><li><p>下载完成之后直接 <code>tar -xvf jar 包</code>解压就行。</p></li></ol><p>就得到了以下的 flink 目录。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/19_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%EF%BC%9Aflinksql%E5%BC%80%E5%8F%91%E5%88%A9%E5%99%A8%E4%B9%8Bzeppelin/5.png" alt="flink dir"></p><ol start="3"><li>我们要把 <code>lib</code> 目录下的 <code>flink-python_2.11-1.13.2.jar</code> 复制到 <code>opt</code> 目录下</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/19_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%EF%BC%9Aflinksql%E5%BC%80%E5%8F%91%E5%88%A9%E5%99%A8%E4%B9%8Bzeppelin/10.png" alt="lib"></p><p>然后就完成了本地 flink 环境的配置。</p><h2 id="3-2-下载-zeppelin-的-tar-包"><a href="#3-2-下载-zeppelin-的-tar-包" class="headerlink" title="3.2.下载 zeppelin 的 tar 包"></a>3.2.下载 zeppelin 的 tar 包</h2><p>这一步是为了配置 zeppelin 的环境。</p><p><a href="https://zeppelin.apache.org/download.html">https://zeppelin.apache.org/download.html</a></p><ol><li>点击上述链接直接下载：博主这里为了使用方便直接下载包含所有 interpreter 的包，下载完成之后直接 <code>tar -xvf jar 包</code>解压就行。</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/19_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%EF%BC%9Aflinksql%E5%BC%80%E5%8F%91%E5%88%A9%E5%99%A8%E4%B9%8Bzeppelin/4.png" alt="zeppelin 下载"></p><p>得到以下目录。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/19_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%EF%BC%9Aflinksql%E5%BC%80%E5%8F%91%E5%88%A9%E5%99%A8%E4%B9%8Bzeppelin/6.png" alt="zeppelin 下载"></p><ol start="2"><li>直接 <code>bin/zeppelin-daemon.sh start</code> 就完事了。看到下面的 console，就代表启动成功了。</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/19_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%EF%BC%9Aflinksql%E5%BC%80%E5%8F%91%E5%88%A9%E5%99%A8%E4%B9%8Bzeppelin/11.png" alt="start"></p><ol start="3"><li>打开 <code>localhost:8080</code> 就可以直接看到页面了。看到下面的页面说明就没啥问题了。</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/19_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%EF%BC%9Aflinksql%E5%BC%80%E5%8F%91%E5%88%A9%E5%99%A8%E4%B9%8Bzeppelin/12.png" alt="首页"></p><h2 id="3-3-在-zeppelin-中配置-flink-环境信息"><a href="#3-3-在-zeppelin-中配置-flink-环境信息" class="headerlink" title="3.3.在 zeppelin 中配置 flink 环境信息"></a>3.3.在 zeppelin 中配置 flink 环境信息</h2><p>按照下面的步骤配置环境信息。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/19_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%EF%BC%9Aflinksql%E5%BC%80%E5%8F%91%E5%88%A9%E5%99%A8%E4%B9%8Bzeppelin/7.png" alt="环境"></p><p>然后我们就可以直接在 Zeppelin 中编辑 sql 了。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/19_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%EF%BC%9Aflinksql%E5%BC%80%E5%8F%91%E5%88%A9%E5%99%A8%E4%B9%8Bzeppelin/8.png" alt="sql 脚本"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/19_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%EF%BC%9Aflinksql%E5%BC%80%E5%8F%91%E5%88%A9%E5%99%A8%E4%B9%8Bzeppelin/9.png" alt="预制案例"></p><p>可以看到上述流程是非常简单的。</p><p>几乎没有什么学习和复杂配置的成本。</p><p>推荐大家可以尝试安装。</p><p>真的是肥肠的方便。</p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink sql 知其所以然（十八）：在 flink 中怎么使用 hive udf？</title>
    <link href="https://yangyichao-mango.github.io/2021/11/15/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/19_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89%EF%BC%9Aflinksqludf/"/>
    <id>https://yangyichao-mango.github.io/2021/11/15/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/19_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89%EF%BC%9Aflinksqludf/</id>
    <published>2021-11-15T08:26:59.000Z</published>
    <updated>2021-12-11T12:27:48.081Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-序篇"><a href="#1-序篇" class="headerlink" title="1.序篇"></a>1.序篇</h1><p>源码公众号后台回复<strong>1.13.2 sql hive udf</strong>获取。</p><p>废话不多说，咱们先直接上本文的目录和结论，小伙伴可以先看结论快速了解博主期望本文能给小伙伴们带来什么帮助：</p><ol><li><strong>背景及应用场景介绍</strong>：博主期望你能了解到，其实很多场景下实时数仓的建设都是随着离线数仓而建设的（相同的逻辑在实时数仓中重新实现一遍），因此能够在 flink sql 中复用 hive udf 是能够大大提高人效的。</li><li><strong>flink 扩展支持 hive 内置 udf</strong>：flink sql 提供了扩展 udf 的能力，即 module，并且 flink sql 也内置了 HiveModule（需要你主动加载进环境），来支持一些 hive 内置的 udf （比如 get_json_object）给小伙伴们使用。</li><li><strong>flink 扩展支持用户自定义的 hive udf</strong>：主要介绍 flink sql 流任务中，不能使用 create temporary function 去引入一个用户自定义的 hive udf。因此博主只能通过 flink sql 提供的 module 插件能力，自定义了 module，来支持引入用户自定义的 hive udf。</li></ol><h1 id="2-背景及应用场景介绍"><a href="#2-背景及应用场景介绍" class="headerlink" title="2.背景及应用场景介绍"></a>2.背景及应用场景介绍</h1><p>其实大多数公司都是从离线数仓开始建设的。相信大家必然在自己的生产环境中开发了非常多的 hive udf。<br>随着需求对于时效性要求的增高，越来越多的公司也开始建设起实时数仓。很多场景下实时数仓的建设都是随着离线数仓而建设的。<br>实时数据使用 flink 产出，离线数据使用 hive\spark 产出。</p><p>那么回到我们文章标题的问题：为什么需要 flink 支持 hive udf 呢？</p><p>博主分析了下，结论如下：</p><p>站在数据需求的角度来说，一般会有以下两种情况：</p><ol><li>以前已经有了离线数据链路，需求方也想要实时数据。如果直接能用已经开发好的 hive udf，则不用将相同的逻辑迁移到 flink udf 中，并且后续无需费时费力维护两个 udf 的逻辑一致性。</li><li>实时和离线的需求都是新的，需要新开发。如果只开发一套 udf，则事半功倍。</li></ol><p>因此在 flink 中支持 hive udf 这件事对开发人员提效来说是非常有好处的。</p><h1 id="3-在扩展前，你需要知道一些基本概念"><a href="#3-在扩展前，你需要知道一些基本概念" class="headerlink" title="3.在扩展前，你需要知道一些基本概念"></a>3.在扩展前，你需要知道一些基本概念</h1><p>flink 支持 hive udf 这件事分为两个部分。</p><ol><li>flink 扩展支持 hive 内置 udf</li><li>flink 扩展支持用户自定义 hive udf</li></ol><p>第一部分：flink 扩展支持 hive 内置 udf，比如 <code>get_json_object</code>，<code>rlike</code> 等等。</p><p>有同学问了，这么基本的 udf，flink 都没有吗？</p><p>确实没有。关于 flink sql 内置的 udf 见如下链接，大家可以看看 flink 支持了哪些 udf：<br><a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/functions/systemfunctions/">https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/functions/systemfunctions/</a></p><p>那么如果我如果强行使用 get_json_object 这个 udf，会发生啥呢？结果如下图。</p><p>直接报错找不到 udf。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/20_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89%EF%BC%9Aflinksqludf/2.png" alt="error"></p><p>第二部分：flink 扩展支持用户自定义 hive udf。</p><p>内置函数解决不了用户的复杂需求，用户就需要自己写 hive udf，并且这部分自定义 udf 也想在 flink sql 中使用。</p><p>下面看看怎么在 flink sql 中进行这两种扩展。</p><h1 id="4-hive-udf-扩展支持"><a href="#4-hive-udf-扩展支持" class="headerlink" title="4.hive udf 扩展支持"></a>4.hive udf 扩展支持</h1><h2 id="4-1-flink-sql-module"><a href="#4-1-flink-sql-module" class="headerlink" title="4.1.flink sql module"></a>4.1.flink sql module</h2><p>涉及到扩展 udf 就不得不提到 flink 提供的 module。见官网下图。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/20_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89%EF%BC%9Aflinksqludf/1.png" alt="module"></p><p>从第一句话就可以看到，module 的作用就是让用户去扩展 udf 的。</p><p>flink 本身已经内置了一个 module，名字叫 CoreModule，其中已经包含了一些 udf。</p><p>那我们要怎么使用 module 这玩意去扩展我们的 hive udf 呢？</p><h2 id="4-2-flink-扩展支持-hive-内置-udf"><a href="#4-2-flink-扩展支持-hive-内置-udf" class="headerlink" title="4.2.flink 扩展支持 hive 内置 udf"></a>4.2.flink 扩展支持 hive 内置 udf</h2><p>步骤如下：</p><ol><li>引入 hive 的 connector。其中包含了 flink 官方提供的一个 <code>HiveModule</code>。在 <code>HiveModule</code> 中包含了 hive 内置的 udf。</li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-hive_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><ol start="2"><li>在 <code>StreamTableEnvironment</code> 中加载 <code>HiveModule</code>。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">String name = <span class="string">&quot;default&quot;</span>;</span><br><span class="line">String version = <span class="string">&quot;3.1.2&quot;</span>;</span><br><span class="line">tEnv.loadModule(name, <span class="keyword">new</span> HiveModule(version));</span><br></pre></td></tr></table></figure><p>然后在控制台打印一下目前有的 module。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">String[] modules = tEnv.listModules();</span><br><span class="line">Arrays.stream(modules).forEach(System.out::println);</span><br></pre></td></tr></table></figure><p>然后可以看到除了 <code>core</code> module，还有我们刚刚加载进去的 <code>default</code> module。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">default</span><br><span class="line">core</span><br></pre></td></tr></table></figure><ol start="3"><li>查看所有 module 的所有 udf。在控制台打印一下。<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">String[] functions = tEnv.listFunctions();</span><br><span class="line">Arrays.stream(functions).forEach(System.out::println);</span><br></pre></td></tr></table></figure></li></ol><p>就会将 default 和 core module 中的所有包含的 udf 给列举出来，当然也就包含了 hive module 中的 get_json_object。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/20_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89%EF%BC%9Aflinksqludf/3.png" alt="get_json_object"></p><p>然后我们再去在 flink sql 中使用 get_json_object 这个 udf，就没有报错，能正常输出结果了。</p><p>使用 flink hive connector 自带的 <code>HiveModule</code>，已经能够解决很大一部分常见 udf 使用的问题了。</p><h2 id="4-2-flink-扩展支持用户自定义-hive-udf"><a href="#4-2-flink-扩展支持用户自定义-hive-udf" class="headerlink" title="4.2.flink 扩展支持用户自定义 hive udf"></a>4.2.flink 扩展支持用户自定义 hive udf</h2><p>原本博主是直接想要使用 flink sql 中的 <code>create temporary function</code> 去执行引入自定义 hive udf 的。</p><p>举例如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> TEMPORARY <span class="keyword">FUNCTION</span> test_hive_udf <span class="keyword">as</span> <span class="string">&#x27;flink.examples.sql._09.udf._02_stream_hive_udf.TestGenericUDF&#x27;</span>;</span><br></pre></td></tr></table></figure><p>发现在执行这句 sql 时，是可以执行成功，将 udf 注册进去的。</p><p>但是在后续 udf 初始化时就报错了。具体错误如下图。直接报错 ClassCastException。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/20_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89%EF%BC%9Aflinksqludf/4.png" alt="ddl hive udf error"></p><p>看了下源码，flink 流环境下（未连接 hive catalog 时）在创建 udf 时会认为这个 udf 是 flink 生态体系中的 udf。</p><p>所以在初始化我们引入的 <code>TestGenericUDF</code> 时，默认会按照 flink 的 <code>UserDefinedFunction</code> 强转，因此才会报强转错误。</p><p>那么我们就不能使用 hive udf 了吗？</p><p>错误，小伙伴萌岂敢有这种想法。博主都把这个标题列出来了（牛逼都吹出去了），还能给不出解决方案嘛。</p><p>思路见下一章节。</p><h2 id="4-3-flink-扩展支持用户自定义-hive-udf-的增强-module"><a href="#4-3-flink-扩展支持用户自定义-hive-udf-的增强-module" class="headerlink" title="4.3.flink 扩展支持用户自定义 hive udf 的增强 module"></a>4.3.flink 扩展支持用户自定义 hive udf 的增强 module</h2><p>其实思路很简单。</p><p>使用 flink sql 中的 <code>create temporary function</code> 虽然不能执行，但是 flink 提供了插件化的自定义 module。</p><p>我们可以扩展一个支持用户自定义 hive udf 的 module，使用这个 module 来支持自定义的 hive udf。</p><p>实现的代码也非常简单。简单的把 flink hive connector 提供的 <code>HiveModule</code> 做一个增强即可，即下图中的 <code>HiveModuleV2</code>。</p><p>使用方式如下图所示：</p><p>源码公众号后台回复<strong>1.13.2 sql hive udf</strong>获取。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/20_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89%EF%BC%9Aflinksqludf/5.png" alt="hive module enhance"></p><p>然后程序就正常跑起来了。</p><p>肥肠滴好用！</p><h1 id="5-总结与展望"><a href="#5-总结与展望" class="headerlink" title="5.总结与展望"></a>5.总结与展望</h1><p>源码公众号后台回复<strong>1.13.2 sql hive udf</strong>获取。</p><p>本文主要介绍了如果在 flink sql 使用 hive 内置 udf 及用户自定义 hive udf，总结如下：</p><ol><li><strong>背景及应用场景介绍</strong>：博主期望你能了解到，其实很多场景下实时数仓的建设都是随着离线数仓而建设的（相同的逻辑在实时数仓中重新实现一遍），因此能够在 flink sql 中复用 hive udf 是能够大大提高人效的。</li><li><strong>flink 扩展支持 hive 内置 udf</strong>：flink sql 提供了扩展 udf 的能力，即 module，并且 flink sql 也内置了 HiveModule（需要你主动加载进环境），来支持一些 hive 内置的 udf （比如 get_json_object）给小伙伴们使用。</li><li><strong>flink 扩展支持用户自定义的 hive udf</strong>：主要介绍 flink sql 流任务中，不能使用 create temporary function 去引入一个用户自定义的 hive udf。因此博主只能通过 flink sql 提供的 module 插件能力，自定义了 module，来支持引入用户自定义的 hive udf。</li></ol>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>史上最全干货！Flink SQL 成神之路（全文 18 万字、138 个案例、42 张图）</title>
    <link href="https://yangyichao-mango.github.io/2021/11/15/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/20_%E5%8F%B2%E4%B8%8A%E6%9C%80%E5%85%A8%E5%B9%B2%E8%B4%A7%EF%BC%81FlinkSQL%E6%88%90%E7%A5%9E%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%85%A8%E6%96%876%E4%B8%87%E5%AD%97%E3%80%81110%E4%B8%AA%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%81160%E5%BC%A0%E5%9B%BE%EF%BC%89/"/>
    <id>https://yangyichao-mango.github.io/2021/11/15/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/20_%E5%8F%B2%E4%B8%8A%E6%9C%80%E5%85%A8%E5%B9%B2%E8%B4%A7%EF%BC%81FlinkSQL%E6%88%90%E7%A5%9E%E4%B9%8B%E8%B7%AF%EF%BC%88%E5%85%A8%E6%96%876%E4%B8%87%E5%AD%97%E3%80%81110%E4%B8%AA%E7%9F%A5%E8%AF%86%E7%82%B9%E3%80%81160%E5%BC%A0%E5%9B%BE%EF%BC%89/</id>
    <published>2021-11-15T08:26:59.000Z</published>
    <updated>2022-02-04T03:37:40.240Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1.前言"></a>1.前言</h1><p>提前说明，如有抄袭，版权必究。</p><p>呕心沥血，Flink SQL 成神之路出品。小伙伴萌可以先体验一下下图大纲。由于微信公众号限制上传图片像素，所以博主分隔成了 5 张图片。。。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/FlinkSQLnb.png" alt="NB"></p><h1 id="2-基础概念篇"><a href="#2-基础概念篇" class="headerlink" title="2.基础概念篇"></a>2.基础概念篇</h1><h2 id="2-1-SQL-amp-Table-简介及运行环境"><a href="#2-1-SQL-amp-Table-简介及运行环境" class="headerlink" title="2.1.SQL &amp; Table 简介及运行环境"></a>2.1.SQL &amp; Table 简介及运行环境</h2><h3 id="2-1-1-简介"><a href="#2-1-1-简介" class="headerlink" title="2.1.1.简介"></a>2.1.1.简介</h3><p>Apache Flink 提供了两种关系型 API 用于统一流和批处理，Table 和 SQL API。</p><ol><li>⭐ Table API 是一种集成在 Java、Scala 和 Python 语言中的查询 API，简单理解就是用 Java、Scala、Python 按照 SQL 的查询接口封装了一层 lambda 表达式的查询 API，它允许以强类型接口的方式组合各种关系运算符（如选择、筛选和联接）的查询操作，然后生成一个 Flink 任务运行。如下案例所示：</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.flink.table.api.Expressions.*;</span><br><span class="line"></span><br><span class="line">EnvironmentSettings settings = EnvironmentSettings</span><br><span class="line">    .newInstance()</span><br><span class="line">    .inStreamingMode()</span><br><span class="line">    .build();</span><br><span class="line"></span><br><span class="line">TableEnvironment tEnv = TableEnvironment.create(settings);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 下面就是 Table API 的案例，其语义等同于</span></span><br><span class="line"><span class="comment">// select a, count(b) as cnt </span></span><br><span class="line"><span class="comment">// from Orders</span></span><br><span class="line"><span class="comment">// group by a</span></span><br><span class="line">DataSet&lt;Row&gt; result = tEnv</span><br><span class="line">        .from(<span class="string">&quot;Orders&quot;</span>)</span><br><span class="line">        .groupBy($(<span class="string">&quot;a&quot;</span>))</span><br><span class="line">        .select($(<span class="string">&quot;a&quot;</span>), $(<span class="string">&quot;b&quot;</span>).count().as(<span class="string">&quot;cnt&quot;</span>))</span><br><span class="line">        .toDataSet(counts, Row.class);</span><br><span class="line"></span><br><span class="line">result.print();</span><br></pre></td></tr></table></figure><ol start="2"><li>⭐ SQL API 是基于 SQL 标准的 Apache Calcite 框架实现的，我们可以使用纯 SQL 来开发和运行一个 Flink 任务。如下案例所示：</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> target</span><br><span class="line"><span class="keyword">select</span> a, <span class="built_in">count</span>(b) <span class="keyword">as</span> cnt</span><br><span class="line"><span class="keyword">from</span> Orders</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> a</span><br></pre></td></tr></table></figure><blockquote><p>注意：<br>无论输入是连续（流处理）还是有界（批处理），在 Table 和 SQL 任一 API 中同一条查询语句是具有相同的语义并且会产出相同的结果的。<br>这就是说为什么 Flink SQL 和 Table API 可以做到在用户接口层面的流批统一。xdm，用一套 SQL 既能跑流任务，也能跑批任务，它不香嘛？</p></blockquote><p>Table API 和 SQL API 也与 DataStream API 做到了无缝集成。可以轻松地在三种 API 之间灵活切换。例如，可以使用 SQL 的 MATCH_RECOGNIZE 子句匹配出异常的数据，然后使用再转为 DataStream API 去灵活的构建针对于异常数据的自定义报警机制。</p><p>在 xdm 大体了解了这两个 API 是干啥的之后，我们就可以直接来看看，怎么使用这两个 API 了。</p><h3 id="2-1-2-SQL-和-Table-API-运行环境依赖"><a href="#2-1-2-SQL-和-Table-API-运行环境依赖" class="headerlink" title="2.1.2.SQL 和 Table API 运行环境依赖"></a>2.1.2.SQL 和 Table API 运行环境依赖</h3><p>根据小伙伴们使用的编程语言的不同（Java 或 Scala），需要将对应的依赖包添加到项目中。</p><ol><li>⭐ Java 依赖如下</li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-api-java-bridge_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.13.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner-blink_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.13.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.13.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.13.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><ol start="2"><li>⭐ Scala 依赖如下</li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-api-scala-bridge_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.13.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner-blink_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.13.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-scala_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.13.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.13.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>引入上述依赖之后，小伙伴萌就可以开始使用 Table\SQL API 了。具体案例如下文所示。</p><h2 id="2-2-SQL-amp-Table-的基本概念及常用-API"><a href="#2-2-SQL-amp-Table-的基本概念及常用-API" class="headerlink" title="2.2.SQL &amp; Table 的基本概念及常用 API"></a>2.2.SQL &amp; Table 的基本概念及常用 API</h2><p>在小伙伴萌看下文之前，先看一下 2.2 节整体的思路，跟着博主思路走，会更清晰：</p><ol><li>⭐ 先通过一个 SQL\Table API 任务看一下我们在实际开发时的代码结构应该长啥样，让大家能有直观的感受</li><li>⭐ 重点介绍 SQL\Table API 中核心 API - TableEnvironment。SQL\Table 所有能用的接口都在 TableEnvironment 中</li><li>⭐ 通过两个角度（外部表\视图、临时\非临时）认识 Flink SQL 体系中的表的概念</li><li>⭐ 举几个创建外部表、视图的实际应用案例</li></ol><h3 id="2-2-1-一个-SQL-Table-API-任务的代码结构"><a href="#2-2-1-一个-SQL-Table-API-任务的代码结构" class="headerlink" title="2.2.1.一个 SQL\Table API 任务的代码结构"></a>2.2.1.一个 SQL\Table API 任务的代码结构</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建一个 TableEnvironment，为后续使用 SQL 或者 Table API 提供上线</span></span><br><span class="line">EnvironmentSettings settings = EnvironmentSettings</span><br><span class="line">    .newInstance()</span><br><span class="line">    .inStreamingMode() <span class="comment">// 声明为流任务</span></span><br><span class="line">    <span class="comment">//.inBatchMode() // 声明为批任务</span></span><br><span class="line">    .build();</span><br><span class="line"></span><br><span class="line">TableEnvironment tEnv = TableEnvironment.create(settings);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一个输入表</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;CREATE TEMPORARY TABLE table1 ... WITH ( &#x27;connector&#x27; = ... )&quot;</span>);</span><br><span class="line"><span class="comment">// 创建一个输出表</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;CREATE TEMPORARY TABLE outputTable ... WITH ( &#x27;connector&#x27; = ... )&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. 使用 Table API 做一个查询并返回 Table</span></span><br><span class="line">Table table2 = tableEnv.from(<span class="string">&quot;table1&quot;</span>).select(...);</span><br><span class="line"><span class="comment">// 2. 使用 SQl API 做一个查询并返回 Table</span></span><br><span class="line">Table table3 = tableEnv.sqlQuery(<span class="string">&quot;SELECT ... FROM table1 ... &quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 table2 的结果使用 Table API 写入 outputTable 中，并返回结果</span></span><br><span class="line">TableResult tableResult = table2.executeInsert(<span class="string">&quot;outputTable&quot;</span>);</span><br><span class="line">tableResult...</span><br></pre></td></tr></table></figure><p>总结一下上面案例使用到的一些 API，让大家先对 Table\SQL API 的能力有一个大概了解：</p><ol><li>⭐ TableEnvironment：Table API 和 SQL API 的都集成在一个统一上下文（即 TableEnvironment）中，其地位等同于 DataStream API 中的 StreamExecutionEnvironment 的地位</li><li>⭐ TableEnvironment::executeSql：用于 SQL API 中，可以执行一段完整 DDL，DML SQL。举例，方法入参可以是 <code>CREATE TABLE xxx</code>，<code>INSERT INTO xxx SELECT xxx FROM xxx</code>。</li><li>⭐ TableEnvironment::from(xxx)：用于 Table API 中，可以以强类型接口的方式运行。方法入参是一个表名称。</li><li>⭐ TableEnvironment::sqlQuery：用于 SQL API 中，可以执行一段查询 SQL，并把结果以 Table 的形式返回。举例，方法的入参是 <code>SELECT xxx FROM xxx</code> </li><li>⭐ Table::executeInsert：用于将 Table 的结果插入到结果表中。方法入参是写入的目标表。</li></ol><p><strong>无论是对于 SQL API 来说还是对于 Table API 来说，都是使用 TableEnvironment 接口承载我们的业务查询逻辑的。只是在用户的使用接口的方式上有区别，以上述的 Java 代码为例，Table API 其实就是模拟 SQL 的查询方式封装了 Java 语言的 lambda 强类型 API，SQL 就是纯 SQL 查询。Table 和 SQL 很多时候都是掺杂在一起的，大家理解的时候就可以直接将 Table 和 SQL API 直接按照 SQL 进行理解，不用强行做特殊的区分。</strong></p><p><strong>而且博主推荐的话，直接上 SQL API 就行，其实 Table API 在企业实战中用的不是特别多。你说 Table API 方便吧，它确实比 DataStream API 方便，但是又比 SQL 复杂。一般生产使用不多。</strong></p><p><code>注意：由于 Table 和 SQL API 基本上属于一回事，后续如果没有特别介绍的话，博主就直接按照 SQL API 进行介绍了。</code></p><p>如果 xdm 想直接上手运行一段 Flink SQL 的代码。</p><p>可以直接在公众号后台回复<strong>1.13.2 最全 flink sql</strong>获取源代码。所有的源码都开源到 github 上面了。里面包含了非常多的案例。可以直接拿来在本地运行的！！！肥肠的方便。</p><h3 id="2-2-2-SQL-上下文：TableEnvironment"><a href="#2-2-2-SQL-上下文：TableEnvironment" class="headerlink" title="2.2.2.SQL 上下文：TableEnvironment"></a>2.2.2.SQL 上下文：TableEnvironment</h3><p>TableEnvironment 是使用 SQL API 永远都离不开的一个接口。其是 SQL API 使用的入口（上下文），就像是你要使用 Java DataStream API 去写一个 Flink 任务需要使用到 StreamExecutionEnvironment 一样。</p><p>可以认为 TableEnvironment 在 SQL API 中的地位和 StreamExecutionEnvironment 在 DataStream 中的地位是一样的，都是包含了一个 Flink 任务运行时的所有上下文环境信息。大家这样对比学习会比较好理解。</p><p>TableEnvironment 包含的功能如下：</p><ol><li><p>⭐ ️Catalog 管理：Catalog 可以理解为 Flink 的 MetaStore，类似 Hive MetaStore 对在 Hive 中的地位，关于 Flink Catalog 的详细内容后续进行介绍</p></li><li><p>⭐ ️表管理：在 Catalog 中注册表</p></li><li><p>⭐️ SQL 查询：（这 TMD 还用说，最基本的功能啊），就像 DataStream 中提供了 addSource、map、flatmap 等接口</p></li><li><p>⭐ UDF 管理：注册用户定义（标量函数：一进一出、表函数：一进多出、聚合函数：多进一出）函数</p></li><li><p>⭐️ UDF 扩展：加载可插拔 Module（Module 可以理解为 Flink 管理 UDF 的模块，是可插拔的，可以让小伙伴萌自定义 Module，去支持奇奇怪怪的 UDF 功能）</p></li><li><p>⭐ DataStream 和 Table（Table\SQL API 的查询结果）之间进行转换：目前 1.13 版本的只有流任务支持，批任务不支持。1.14 支持批任务。</p></li></ol><p>接下来介绍如何创建一个 TableEnvironment。案例为 Java。easy game。</p><ol><li>⭐ 方法 1：通过 EnvironmentSettings 创建 TableEnvironment</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableEnvironment;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. 就是设置一些环境信息</span></span><br><span class="line">EnvironmentSettings settings = EnvironmentSettings</span><br><span class="line">    .newInstance()</span><br><span class="line">    .inStreamingMode() <span class="comment">// 声明为流任务</span></span><br><span class="line">    <span class="comment">//.inBatchMode() // 声明为批任务</span></span><br><span class="line">    .build();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 创建 TableEnvironment</span></span><br><span class="line">TableEnvironment tEnv = TableEnvironment.create(settings);</span><br></pre></td></tr></table></figure><p>在 1.13 版本中。</p><p>如果你是 <code>inStreamingMode</code>，则最终创建出来的 <code>TableEnvironment</code> 实例为 <code>StreamTableEnvironmentImpl</code>。</p><p>如果你是 <code>inBatchMode</code>，则最终创建出来的 <code>TableEnvironment</code> 实例为 <code>TableEnvironmentImpl</code>。</p><p>它两虽然都继承了 <code>TableEnvironment</code> 接口，但是 <code>StreamTableEnvironmentImpl</code> 支持的功能更多一些。大家可以直接去看看接口实验一下，这里就不进行详细介绍。</p><ol start="2"><li>⭐ 方法 2：通过已有的 StreamExecutionEnvironment 创建 TableEnvironment</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);</span><br></pre></td></tr></table></figure><h3 id="2-2-3-SQL-中表的概念"><a href="#2-2-3-SQL-中表的概念" class="headerlink" title="2.2.3.SQL 中表的概念"></a>2.2.3.SQL 中表的概念</h3><p>一个表的全名（标识）会由三个部分组成：<code>Catalog 名称.数据库名称.表名称</code>。如果 <code>Catalog 名称</code>或者<code>数据库名称</code>没有指明，就会使用当前默认值 default。</p><p>举个例子，下面这个 SQL 创建的 Table 的全名为 <code>default.default.table1</code>。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.executeSql(<span class="string">&quot;CREATE TEMPORARY TABLE table1 ... WITH ( &#x27;connector&#x27; = ... )&quot;</span>);</span><br></pre></td></tr></table></figure><p>下面这个 SQL 创建的 Table 的全名为 <code>default.mydatabase.table1</code>。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.executeSql(<span class="string">&quot;CREATE TEMPORARY TABLE mydatabase.table1 ... WITH ( &#x27;connector&#x27; = ... )&quot;</span>);</span><br></pre></td></tr></table></figure><p>表可以是常规的（外部表 TABLE），也可以是虚拟的（视图 VIEW）。</p><ol><li>⭐ 外部表 TABLE：描述的是外部数据，例如文件（HDFS）、消息队列（Kafka）等。依然拿离线 Hive SQL 举个例子，离线中一个表指的是 Hive 表，也就是所说的外部数据。</li><li>⭐ 视图 VIEW：从已经存在的表中创建，视图一般是一个 SQL 逻辑的查询结果。对比到离线的 Hive SQL 中，在离线的场景（Hive 表）中 VIEW 也都是从已有的表中去创建的。其实 Flink 也是一样的。</li></ol><blockquote><p>注意：</p><p>这里有不同的地方就是，离线 Hive MetaStore 中不会有 Catalog 这个概念，其标识都是 <code>数据库.数据表</code>。</p></blockquote><h3 id="2-2-4-SQL-临时表、永久表"><a href="#2-2-4-SQL-临时表、永久表" class="headerlink" title="2.2.4.SQL 临时表、永久表"></a>2.2.4.SQL 临时表、永久表</h3><p>表（视图、外部表）可以是临时的，并与单个 Flink session（可以理解为 Flink 任务运行一次就是一个 session）的生命周期绑定。</p><p>表（视图、外部表）也可以是永久的，并且对多个 Flink session 都生效。</p><ol><li>⭐ 临时表：通常保存于内存中并且仅在创建它们的 Flink session（可以理解为一次 Flink 任务的运行）持续期间存在。这些表对于其它 session（即其他 Flink 任务或非此次运行的 Flink 任务）是不可见的。因为这个表的元数据没有被持久化。如下案例：</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 临时外部表</span></span><br><span class="line"><span class="keyword">CREATE</span> TEMPORARY <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    user_id <span class="type">BIGINT</span>,</span><br><span class="line">    `name` STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;user_defined&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;class.name&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;flink.examples.sql._03.source_sink.table.user_defined.UserDefinedSource&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 临时视图</span></span><br><span class="line"><span class="keyword">CREATE</span> TEMPORARY <span class="keyword">VIEW</span> query_view <span class="keyword">as</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> source_table</span><br><span class="line">;</span><br></pre></td></tr></table></figure><ol start="2"><li>⭐ 永久表：需要外部 Catalog（例如 Hive Metastore）来持久化表的元数据。一旦永久表被创建，它将对任何连接到这个 Catalog 的 Flink session 可见且持续存在，直至从 Catalog 中被明确删除。如下案例：</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 永久外部表。需要外部 Catalog 持久化！！！</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    user_id <span class="type">BIGINT</span>,</span><br><span class="line">    `name` STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;user_defined&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;class.name&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;flink.examples.sql._03.source_sink.table.user_defined.UserDefinedSource&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 永久视图。需要外部 Catalog 持久化！！！</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> query_view <span class="keyword">as</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> source_table</span><br><span class="line">;</span><br></pre></td></tr></table></figure><ol start="3"><li>⭐ 如果临时表和永久表使用了相同的名称（Catalog名.数据库名.表名）。那么在这个 Flink session 中，你的任务访问到这个表时，访问到的永远是临时表（即相同名称的表，临时表会屏蔽永久表）。</li></ol><h3 id="2-2-5-SQL-外部数据表"><a href="#2-2-5-SQL-外部数据表" class="headerlink" title="2.2.5.SQL 外部数据表"></a>2.2.5.SQL 外部数据表</h3><p>由于目前在实时数据的场景中多以消息队列作为数据表。此处就以 Kafka 为例创建一个外部数据表。</p><ol><li>⭐ Table API 创建外部数据表</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    StreamExecutionEnvironment env =</span><br><span class="line">            StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(<span class="keyword">new</span> Configuration());</span><br><span class="line">    </span><br><span class="line">    EnvironmentSettings settings = EnvironmentSettings</span><br><span class="line">            .newInstance()</span><br><span class="line">            .useBlinkPlanner()</span><br><span class="line">            .inStreamingMode()</span><br><span class="line">            .build();</span><br><span class="line"></span><br><span class="line">    StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, settings);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// kafka 数据源</span></span><br><span class="line">    DataStream&lt;Row&gt; r = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer&lt;Row&gt;(xxx));</span><br><span class="line">    <span class="comment">// 将 DataStream 转为一个 Table API 中的 Table 对象进行使用</span></span><br><span class="line">    Table sourceTable = tEnv.fromDataStream(r</span><br><span class="line">            , Schema</span><br><span class="line">                    .newBuilder()</span><br><span class="line">                    .column(<span class="string">&quot;f0&quot;</span>, <span class="string">&quot;string&quot;</span>)</span><br><span class="line">                    .column(<span class="string">&quot;f1&quot;</span>, <span class="string">&quot;string&quot;</span>)</span><br><span class="line">                    .column(<span class="string">&quot;f2&quot;</span>, <span class="string">&quot;bigint&quot;</span>)</span><br><span class="line">                    .columnByExpression(<span class="string">&quot;proctime&quot;</span>, <span class="string">&quot;PROCTIME()&quot;</span>)</span><br><span class="line">                    .build());</span><br><span class="line"></span><br><span class="line">    tEnv.createTemporaryView(<span class="string">&quot;source_table&quot;</span>, sourceTable);</span><br><span class="line"></span><br><span class="line">    String selectWhereSql = <span class="string">&quot;select f0 from source_table where f1 = &#x27;b&#x27;&quot;</span>;</span><br><span class="line"></span><br><span class="line">    Table resultTable = tEnv.sqlQuery(selectWhereSql);</span><br><span class="line"></span><br><span class="line">    tEnv.toRetractStream(resultTable, Row.class).print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述案例中，Table API 将一个 DataStream 的结果集通过 <code>StreamTableEnvironment::fromDataStream</code> 转为一个 Table 对象来使用。</p><ol start="2"><li>⭐ SQL API 创建外部数据表</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">EnvironmentSettings settings = EnvironmentSettings</span><br><span class="line">        .newInstance()</span><br><span class="line">        .useBlinkPlanner()</span><br><span class="line">        .inStreamingMode()</span><br><span class="line">        .build();</span><br><span class="line"></span><br><span class="line">StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, settings);</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL API 执行 create table 创建表</span></span><br><span class="line">tEnv.executeSql(</span><br><span class="line">        <span class="string">&quot;CREATE TABLE KafkaSourceTable (\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  `f0` STRING,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  `f1` STRING\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;) WITH (\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;connector&#x27; = &#x27;kafka&#x27;,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;topic&#x27; = &#x27;topic&#x27;,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;properties.bootstrap.servers&#x27; = &#x27;localhost:9092&#x27;,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;properties.group.id&#x27; = &#x27;testGroup&#x27;,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;format&#x27; = &#x27;json&#x27;\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;)&quot;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">Table t = tEnv.sqlQuery(<span class="string">&quot;SELECT * FROM KafkaSourceTable&quot;</span>);</span><br></pre></td></tr></table></figure><p>具体的创建方式就是使用 <code>Create Table xxx</code> DDL 定义一个 Kafka 数据源（输入）表（也可以是 Kafka 数据汇（输出）表）。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/9.jpg" alt="9"></p><p>xdm，是不是又和 Hive 一样？惊不惊喜意不意外。对比学习 +1。</p><h3 id="2-2-6-SQL-视图-VIEW"><a href="#2-2-6-SQL-视图-VIEW" class="headerlink" title="2.2.6.SQL 视图 VIEW"></a>2.2.6.SQL 视图 VIEW</h3><p>上文已经说了，一个 VIEW 其实就是一段 SQL 逻辑的查询结果。</p><p>视图 VIEW 在 Table API 中的体现就是：一个 Table 的 Java 对象，其封装了一段查询逻辑。如下案例所示：</p><ol><li>⭐ Table API 创建 VIEW</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableEnvironment;</span><br><span class="line"></span><br><span class="line">EnvironmentSettings settings = EnvironmentSettings</span><br><span class="line">    .newInstance()</span><br><span class="line">    .inStreamingMode() <span class="comment">// 声明为流任务</span></span><br><span class="line">    .build();</span><br><span class="line"></span><br><span class="line">TableEnvironment tEnv = TableEnvironment.create(settings);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Table API 中的一个 Table 对象</span></span><br><span class="line">Table projTable = tEnv.from(<span class="string">&quot;X&quot;</span>).select(...);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 projTable 创建为一个叫做 projectedTable 的 VIEW</span></span><br><span class="line">tEnv.createTemporaryView(<span class="string">&quot;projectedTable&quot;</span>, projTable);</span><br></pre></td></tr></table></figure><p>Table API 是使用了 <code>TableEnvironment::createTemporaryView</code> 接口将一个 Table 对象创建为一个 VIEW。</p><ol start="2"><li>⭐ SQL API 创建 VIEW</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableEnvironment;</span><br><span class="line"></span><br><span class="line">EnvironmentSettings settings = EnvironmentSettings</span><br><span class="line">    .newInstance()</span><br><span class="line">    .inStreamingMode() <span class="comment">// 声明为流任务</span></span><br><span class="line">    .build();</span><br><span class="line"></span><br><span class="line">TableEnvironment tEnv = TableEnvironment.create(settings);</span><br><span class="line"></span><br><span class="line">String sql = <span class="string">&quot;CREATE TABLE source_table (\n&quot;</span></span><br><span class="line">    + <span class="string">&quot;    user_id BIGINT,\n&quot;</span></span><br><span class="line">    + <span class="string">&quot;    `name` STRING\n&quot;</span></span><br><span class="line">    + <span class="string">&quot;) WITH (\n&quot;</span></span><br><span class="line">    + <span class="string">&quot;  &#x27;connector&#x27; = &#x27;user_defined&#x27;,\n&quot;</span></span><br><span class="line">    + <span class="string">&quot;  &#x27;format&#x27; = &#x27;json&#x27;,\n&quot;</span></span><br><span class="line">    + <span class="string">&quot;  &#x27;class.name&#x27; = &#x27;flink.examples.sql._03.source_sink.table.user_defined.UserDefinedSource&#x27;\n&quot;</span></span><br><span class="line">    + <span class="string">&quot;);\n&quot;</span></span><br><span class="line">    + <span class="string">&quot;\n&quot;</span></span><br><span class="line">    + <span class="string">&quot;CREATE TABLE sink_table (\n&quot;</span></span><br><span class="line">    + <span class="string">&quot;    user_id BIGINT,\n&quot;</span></span><br><span class="line">    + <span class="string">&quot;    name STRING\n&quot;</span></span><br><span class="line">    + <span class="string">&quot;) WITH (\n&quot;</span></span><br><span class="line">    + <span class="string">&quot;  &#x27;connector&#x27; = &#x27;print&#x27;\n&quot;</span></span><br><span class="line">    + <span class="string">&quot;);\n&quot;</span></span><br><span class="line">    + <span class="string">&quot;CREATE VIEW query_view as\n&quot;</span> <span class="comment">// 创建 VIEW</span></span><br><span class="line">    + <span class="string">&quot;SELECT\n&quot;</span></span><br><span class="line">    + <span class="string">&quot;    *\n&quot;</span></span><br><span class="line">    + <span class="string">&quot;FROM source_table\n&quot;</span></span><br><span class="line">    + <span class="string">&quot;;\n&quot;</span></span><br><span class="line">    + <span class="string">&quot;INSERT INTO sink_table\n&quot;</span></span><br><span class="line">    + <span class="string">&quot;SELECT\n&quot;</span></span><br><span class="line">    + <span class="string">&quot;    *\n&quot;</span></span><br><span class="line">    + <span class="string">&quot;FROM query_view;&quot;</span>;</span><br><span class="line"></span><br><span class="line">Arrays.stream(sql.split(<span class="string">&quot;;&quot;</span>))</span><br><span class="line">      .forEach(tEnv::executeSql);</span><br></pre></td></tr></table></figure><p>SQL API 是直接通过一段 <code>CREATE VIEW query_view as select * from source_table</code> 来创建的 VIEW，是纯 SQL 写法。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/9.jpg" alt="9"></p><p>这种创建方式是不是贼熟悉，和离线 Hive 一样 +1~</p><blockquote><p>注意：</p><p>在 Table API 中的一个 Table 对象被后续的多个查询使用的场景下：<br>Table 对象不会真的产生一个中间表供下游多个查询去引用，即多个查询不共享这个 Table 的结果，小伙伴萌可以理解为是一种中间表的简化写法，不会先产出一个中间表结果，然后将这个结果在下游多个查询中复用，后续的多个查询会将这个 Table 的逻辑执行多次。类似于 with tmp as (DML) 的语法</p></blockquote><h3 id="2-2-7-一个-SQL-查询案例"><a href="#2-2-7-一个-SQL-查询案例" class="headerlink" title="2.2.7.一个 SQL 查询案例"></a>2.2.7.一个 SQL 查询案例</h3><p>首先，如果 xdm 想直接上手运行一段 Flink SQL 的代码。</p><p>可以直接在公众号后台回复<strong>1.13.2 最全 flink sql</strong>获取源代码。所有的源码都开源到 github 上面了。里面包含了非常多的案例。可以直接拿来在本地运行的！！！肥肠的方便。</p><p>来看看一个 SQL 查询案例。</p><ol><li><p>⭐ 案例场景：计算每一种商品（sku_id 唯一标识）的售出个数、总销售额、平均销售额、最低价、最高价</p></li><li><p>⭐ 数据准备：数据源为商品的销售流水（sku_id：商品，price：销售价格），然后写入到 Kafka 的指定 topic（sku_id：商品，count_result：售出个数、sum_result：总销售额、avg_result：平均销售额、min_result：最低价、max_result：最高价）当中</p></li><li><p>⭐ 任务代码：</p></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">EnvironmentSettings settings = EnvironmentSettings</span><br><span class="line">    .newInstance()</span><br><span class="line">    .inStreamingMode() <span class="comment">// 声明为流任务</span></span><br><span class="line">    <span class="comment">//.inBatchMode() // 声明为批任务</span></span><br><span class="line">    .build();</span><br><span class="line"></span><br><span class="line">TableEnvironment tEnv = TableEnvironment.create(settings);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. 创建一个数据源（输入）表，这里的数据源是 flink 自带的一个随机 mock 数据的数据源。</span></span><br><span class="line">String sourceSql = <span class="string">&quot;CREATE TABLE source_table (\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;    sku_id STRING,\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;    price BIGINT\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;) WITH (\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;  &#x27;connector&#x27; = &#x27;datagen&#x27;,\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;  &#x27;rows-per-second&#x27; = &#x27;1&#x27;,\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;  &#x27;fields.sku_id.length&#x27; = &#x27;1&#x27;,\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;  &#x27;fields.price.min&#x27; = &#x27;1&#x27;,\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;  &#x27;fields.price.max&#x27; = &#x27;1000000&#x27;\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;)&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 创建一个数据汇（输出）表，输出到 kafka 中</span></span><br><span class="line">String sinkSql = <span class="string">&quot;CREATE TABLE sink_table (\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;    sku_id STRING,\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;    count_result BIGINT,\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;    sum_result BIGINT,\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;    avg_result DOUBLE,\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;    min_result BIGINT,\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;    max_result BIGINT,\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;    PRIMARY KEY (`sku_id`) NOT ENFORCED\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;) WITH (\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;  &#x27;connector&#x27; = &#x27;upsert-kafka&#x27;,\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;  &#x27;topic&#x27; = &#x27;tuzisir&#x27;,\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;  &#x27;properties.bootstrap.servers&#x27; = &#x27;localhost:9092&#x27;,\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;  &#x27;key.format&#x27; = &#x27;json&#x27;,\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;  &#x27;value.format&#x27; = &#x27;json&#x27;\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;)&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 执行一段 group by 的聚合 SQL 查询</span></span><br><span class="line">String selectWhereSql = <span class="string">&quot;insert into sink_table\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;select sku_id,\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;       count(*) as count_result,\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;       sum(price) as sum_result,\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;       avg(price) as avg_result,\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;       min(price) as min_result,\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;       max(price) as max_result\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;from source_table\n&quot;</span></span><br><span class="line">        + <span class="string">&quot;group by sku_id&quot;</span>;</span><br><span class="line"></span><br><span class="line">tEnv.executeSql(sourceSql);</span><br><span class="line">tEnv.executeSql(sinkSql);</span><br><span class="line">tEnv.executeSql(selectWhereSql);</span><br></pre></td></tr></table></figure><h3 id="2-2-8-SQL-与-DataStream-API-的转换"><a href="#2-2-8-SQL-与-DataStream-API-的转换" class="headerlink" title="2.2.8.SQL 与 DataStream API 的转换"></a>2.2.8.SQL 与 DataStream API 的转换</h3><p>大家会比较好奇，要写 SQL 就纯 SQL 呗，要写 DataStream 就纯 DataStream 呗，为啥还要把这两类接口做集成呢？</p><p>博主举一个案例：在 pdd 这种发补贴券的场景下，希望可以在发的补贴券总金额超过 1w 元时，及时报警出来，来帮助控制预算，防止发的太多。</p><p>对应的解决方案，我们可以想到使用 SQL 计算补贴券发放的结果，但是 SQL 的问题在于无法做到报警。所以我们可以将 SQL 的查询的结果（即 Table 对象）转为 DataStream，然后就可以在 DataStream 后自定义报警逻辑的算子。</p><p>我们直接上 SQL 和 DataStream API 互相转化的案例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    FlinkEnv flinkEnv = FlinkEnvUtils.getStreamTableEnv(args);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. pdd 发补贴券流水数据</span></span><br><span class="line">    String createTableSql = <span class="string">&quot;CREATE TABLE source_table (\n&quot;</span></span><br><span class="line">            + <span class="string">&quot;    id BIGINT,\n&quot;</span> -- 补贴券的流水 id</span><br><span class="line">            + <span class="string">&quot;    money BIGINT,\n&quot;</span> -- 补贴券的金额</span><br><span class="line">            + <span class="string">&quot;    row_time AS cast(CURRENT_TIMESTAMP as timestamp_LTZ(3)),\n&quot;</span></span><br><span class="line">            + <span class="string">&quot;    WATERMARK FOR row_time AS row_time - INTERVAL &#x27;5&#x27; SECOND\n&quot;</span></span><br><span class="line">            + <span class="string">&quot;) WITH (\n&quot;</span></span><br><span class="line">            + <span class="string">&quot;  &#x27;connector&#x27; = &#x27;datagen&#x27;,\n&quot;</span></span><br><span class="line">            + <span class="string">&quot;  &#x27;rows-per-second&#x27; = &#x27;1&#x27;,\n&quot;</span></span><br><span class="line">            + <span class="string">&quot;  &#x27;fields.id.min&#x27; = &#x27;1&#x27;,\n&quot;</span></span><br><span class="line">            + <span class="string">&quot;  &#x27;fields.id.max&#x27; = &#x27;100000&#x27;,\n&quot;</span></span><br><span class="line">            + <span class="string">&quot;  &#x27;fields.money.min&#x27; = &#x27;1&#x27;,\n&quot;</span></span><br><span class="line">            + <span class="string">&quot;  &#x27;fields.money.max&#x27; = &#x27;100000&#x27;\n&quot;</span></span><br><span class="line">            + <span class="string">&quot;)\n&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 计算总计发放补贴券的金额</span></span><br><span class="line">    String querySql = <span class="string">&quot;SELECT UNIX_TIMESTAMP(CAST(window_end AS STRING)) * 1000 as window_end, \n&quot;</span></span><br><span class="line">            + <span class="string">&quot;      window_start, \n&quot;</span></span><br><span class="line">            + <span class="string">&quot;      sum(money) as sum_money,\n&quot;</span> -- 补贴券的发放总金额</span><br><span class="line">            + <span class="string">&quot;      count(distinct id) as count_distinct_id\n&quot;</span></span><br><span class="line">            + <span class="string">&quot;FROM TABLE(CUMULATE(\n&quot;</span></span><br><span class="line">            + <span class="string">&quot;         TABLE source_table\n&quot;</span></span><br><span class="line">            + <span class="string">&quot;         , DESCRIPTOR(row_time)\n&quot;</span></span><br><span class="line">            + <span class="string">&quot;         , INTERVAL &#x27;5&#x27; SECOND\n&quot;</span></span><br><span class="line">            + <span class="string">&quot;         , INTERVAL &#x27;1&#x27; DAY))\n&quot;</span></span><br><span class="line">            + <span class="string">&quot;GROUP BY window_start, \n&quot;</span></span><br><span class="line">            + <span class="string">&quot;        window_end&quot;</span>;</span><br><span class="line"></span><br><span class="line">    flinkEnv.streamTEnv().executeSql(createTableSql);</span><br><span class="line"></span><br><span class="line">    Table resultTable = flinkEnv.streamTEnv().sqlQuery(querySql);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 将金额结果转为 DataStream，然后自定义超过 1w 的报警逻辑</span></span><br><span class="line">    flinkEnv.streamTEnv()</span><br><span class="line">            .toDataStream(resultTable, Row.class)</span><br><span class="line">            .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;Row, Object&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Row value, Collector&lt;Object&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    <span class="keyword">long</span> l = Long.parseLong(String.valueOf(value.getField(<span class="string">&quot;sum_money&quot;</span>)));</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> (l &gt; <span class="number">10000L</span>) &#123;</span><br><span class="line">                        log.info(<span class="string">&quot;报警，超过 1w&quot;</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">    flinkEnv.env().execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>注意：</p><p>目前在 1.13 版本中，Flink 对于 Table 和 DataStream 的转化是有一些限制的：<br>上面的案例可以看到，Table 和 DataStream 之间的转换目前只有 <code>StreamTableEnvironment::toDataStream</code>、<code>StreamTableEnvironment::fromDataStream</code> 接口支持。</p><p>所以其实小伙伴萌可以理解为只有流任务才支持 Table 和 DataStream 之间的转换，批任务是不支持的（虽然可以使用流执行模式处理有界流 - 批数据，也就是模拟按照批执行，但效率较低，这种骚操作不建议大家搞）。</p><p>那什么时候才能支持批任务的 Table 和 DataStream 之间的转换呢？<br>1.14 版本支持。1.14 版本中，流和批的都统一到了 StreamTableEnvironment 中，因此就可以做 Table 和 DataStream 的互相转换了。</p></blockquote><h2 id="2-3-SQL-数据类型"><a href="#2-3-SQL-数据类型" class="headerlink" title="2.3.SQL 数据类型"></a>2.3.SQL 数据类型</h2><p>在介绍完一些基本概念之后，我们来认识一下，Flink SQL 中的数据类型。</p><p>Flink SQL 内置了很多常见的数据类型，并且也为用户提供了自定义数据类型的能力。</p><p>总共包含 3 部分：</p><ol><li>⭐ 原子数据类型</li><li>⭐ 复合数据类型</li><li>⭐ 用户自定义数据类型</li></ol><h3 id="2-3-1-原子数据类型"><a href="#2-3-1-原子数据类型" class="headerlink" title="2.3.1.原子数据类型"></a>2.3.1.原子数据类型</h3><ol><li>⭐ 字符串类型：</li></ol><ul><li>⭐ CHAR、CHAR(n)：定长字符串，就和 Java 中的 Char 一样，n 代表字符的定长，取值范围 [1, 2,147,483,647]。如果不指定 n，则默认为 1。</li><li>⭐ VARCHAR、VARCHAR(n)、STRING：可变长字符串，就和 Java 中的 String 一样，n 代表字符的最大长度，取值范围 [1, 2,147,483,647]。如果不指定 n，则默认为 1。STRING 等同于 VARCHAR(2147483647)。</li></ul><ol start="2"><li>⭐ 二进制字符串类型：</li></ol><ul><li>⭐ BINARY、BINARY(n)：定长二进制字符串，n 代表定长，取值范围 [1, 2,147,483,647]。如果不指定 n，则默认为 1。</li><li>⭐ VARBINARY、VARBINARY(n)、BYTES：可变长二进制字符串，n 代表字符的最大长度，取值范围 [1, 2,147,483,647]。如果不指定 n，则默认为 1。BYTES 等同于 VARBINARY(2147483647)。</li></ul><ol start="3"><li>⭐ 精确数值类型：</li></ol><ul><li>⭐ DECIMAL、DECIMAL(p)、DECIMAL(p, s)、DEC、DEC(p)、DEC(p, s)、NUMERIC、NUMERIC(p)、NUMERIC(p, s)：固定长度和精度的数值类型，就和 Java 中的 BigDecimal 一样，p 代表数值位数（长度），取值范围 [1, 38]；s 代表小数点后的位数（精度），取值范围 [0, p]。如果不指定，p 默认为 10，s 默认为 0。</li><li>⭐ TINYINT：-128 到 127 的 1 字节大小的有符号整数，就和 Java 中的 byte 一样。</li><li>⭐ SMALLINT：-32,768 to 32,767 的 2 字节大小的有符号整数，就和 Java 中的 short 一样。</li><li>⭐ INT、INTEGER：-2,147,483,648 to 2,147,483,647 的 4 字节大小的有符号整数，就和 Java 中的 int 一样。</li><li>⭐ BIGINT：-9,223,372,036,854,775,808 to 9,223,372,036,854,775,807 的 8 字节大小的有符号整数，就和 Java 中的 long 一样。</li></ul><ol start="4"><li>⭐ 有损精度数值类型：</li></ol><ul><li>⭐ FLOAT：4 字节大小的单精度浮点数值，就和 Java 中的 float 一样。</li><li>⭐ DOUBLE、DOUBLE PRECISION：8 字节大小的双精度浮点数值，就和 Java 中的 double 一样。</li><li>⭐ 关于 FLOAT 和 DOUBLE 的区别可见 <a href="https://www.runoob.com/w3cnote/float-and-double-different.html">https://www.runoob.com/w3cnote/float-and-double-different.html</a></li></ul><ol start="5"><li><p>⭐ 布尔类型：BOOLEAN</p></li><li><p>⭐ NULL 类型：NULL</p></li><li><p>⭐ Raw 类型：RAW(‘class’, ‘snapshot’) 。只会在数据发生网络传输时进行序列化，反序列化操作，可以保留其原始数据。以 Java 举例，<code>class</code> 参数代表具体对应的 Java 类型，<code>snapshot</code> 代表类型在发生网络传输时的序列化器</p></li><li><p>⭐ 日期、时间类型：</p></li></ol><ul><li>⭐ DATE：由 <code>年-月-日</code> 组成的 <code>不带时区含义</code> 的日期类型，取值范围 [0000-01-01, 9999-12-31]</li><li>⭐ TIME、TIME(p)：由 <code>小时：分钟：秒[.小数秒]</code> 组成的 <code>不带时区含义</code> 的的时间的数据类型，精度高达纳秒，取值范围 [00:00:00.000000000到23:59:59.9999999]。其中 p 代表小数秒的位数，取值范围 [0, 9]，如果不指定 p，默认为 0。</li><li>⭐ TIMESTAMP、TIMESTAMP(p)、TIMESTAMP WITHOUT TIME ZONE、TIMESTAMP(p) WITHOUT TIME ZONE：由 <code>年-月-日 小时：分钟：秒[.小数秒]</code> 组成的 <code>不带时区含义</code> 的时间类型，取值范围 [0000-01-01 00:00:00.000000000, 9999-12-31 23:59:59.999999999]。其中 p 代表小数秒的位数，取值范围 [0, 9]，如果不指定 p，默认为 6。</li><li>⭐ TIMESTAMP WITH TIME ZONE、TIMESTAMP(p) WITH TIME ZONE：由 <code>年-月-日 小时：分钟：秒[.小数秒] 时区</code> 组成的 <code>带时区含义</code> 的时间类型，取值范围 [0000-01-01 00:00:00.000000000 +14:59, 9999-12-31 23:59:59.999999999 -14:59]。其中 p 代表小数秒的位数，取值范围 [0, 9]，如果不指定 p，默认为 6。</li><li>⭐ TIMESTAMP_LTZ、TIMESTAMP_LTZ(p)：由 <code>年-月-日 小时：分钟：秒[.小数秒] 时区</code> 组成的 <code>带时区含义</code> 的时间类型，取值范围 [0000-01-01 00:00:00.000000000 +14:59, 9999-12-31 23:59:59.999999999 -14:59]。其中 p 代表小数秒的位数，取值范围 [0, 9]，如果不指定 p，默认为 6。</li><li>⭐ TIMESTAMP_LTZ 与 TIMESTAMP WITH TIME ZONE 的区别在于：TIMESTAMP WITH TIME ZONE 的时区信息是携带在数据中的，举例：其输入数据应该是 2022-01-01 00:00:00.000000000 +08:00；TIMESTAMP_LTZ 的时区信息不是携带在数据中的，而是由 Flink SQL 任务的全局配置决定的，我们可以由 <code>table.local-time-zone</code> 参数来设置时区。</li><li>⭐ INTERVAL YEAR TO MONTH、 INTERVAL DAY TO SECOND：interval 的涉及到的种类比较多。INTERVAL 主要是用于给 TIMESTAMP、TIMESTAMP_LTZ 添加偏移量的。举例，比如给 TIMESTAMP 加、减几天、几个月、几年。INTERVAL 子句总共涉及到的语法种类如下 Flink SQL 案例所示。</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    result_interval_year <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    result_interval_year_p <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    result_interval_year_p_to_month <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    result_interval_month <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    result_interval_day <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    result_interval_day_p1 <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    result_interval_day_p1_to_hour <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    result_interval_day_p1_to_minute <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    result_interval_day_p1_to_second_p2 <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    result_interval_hour <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    result_interval_hour_to_minute <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    result_interval_hour_to_second <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    result_interval_minute <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    result_interval_minute_to_second_p2 <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    result_interval_second <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    result_interval_second_p2 <span class="type">TIMESTAMP</span>(<span class="number">3</span>)</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    <span class="comment">-- Flink SQL 支持的所有 INTERVAL 子句如下，总体可以分为 `年-月`、`日-小时-秒` 两种</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">-- 1. 年-月。取值范围为 [-9999-11, +9999-11]，其中 p 是指有效位数，取值范围 [1, 4]，默认值为 2。比如如果值为 1000，但是 p = 2，则会直接报错。</span></span><br><span class="line">    <span class="comment">-- INTERVAL YEAR</span></span><br><span class="line">    f1 <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">YEAR</span> <span class="keyword">as</span> result_interval_year</span><br><span class="line">    <span class="comment">-- INTERVAL YEAR(p)</span></span><br><span class="line">    , f1 <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="string">&#x27;100&#x27;</span> <span class="keyword">YEAR</span>(<span class="number">3</span>) <span class="keyword">as</span> result_interval_year_p</span><br><span class="line">    <span class="comment">-- INTERVAL YEAR(p) TO MONTH</span></span><br><span class="line">    , f1 <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="string">&#x27;10-03&#x27;</span> <span class="keyword">YEAR</span>(<span class="number">3</span>) <span class="keyword">TO</span> <span class="keyword">MONTH</span> <span class="keyword">as</span> result_interval_year_p_to_month</span><br><span class="line">    <span class="comment">-- INTERVAL MONTH</span></span><br><span class="line">    , f1 <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="string">&#x27;13&#x27;</span> <span class="keyword">MONTH</span> <span class="keyword">as</span> result_interval_month</span><br><span class="line"></span><br><span class="line">    <span class="comment">-- 2. 日-小时-秒。取值范围为 [-999999 23:59:59.999999999, +999999 23:59:59.999999999]，其中 p1\p2 都是有效位数，p1 取值范围 [1, 6]，默认值为 2；p2 取值范围 [0, 9]，默认值为 6</span></span><br><span class="line">    <span class="comment">-- INTERVAL DAY</span></span><br><span class="line">    , f1 <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">DAY</span> <span class="keyword">as</span> result_interval_day</span><br><span class="line">    <span class="comment">-- INTERVAL DAY(p1)</span></span><br><span class="line">    , f1 <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="string">&#x27;100&#x27;</span> <span class="keyword">DAY</span>(<span class="number">3</span>) <span class="keyword">as</span> result_interval_day_p1</span><br><span class="line">    <span class="comment">-- INTERVAL DAY(p1) TO HOUR</span></span><br><span class="line">    , f1 <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="string">&#x27;10 03&#x27;</span> <span class="keyword">DAY</span>(<span class="number">3</span>) <span class="keyword">TO</span> <span class="keyword">HOUR</span> <span class="keyword">as</span> result_interval_day_p1_to_hour</span><br><span class="line">    <span class="comment">-- INTERVAL DAY(p1) TO MINUTE</span></span><br><span class="line">    , f1 <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="string">&#x27;10 03:12&#x27;</span> <span class="keyword">DAY</span>(<span class="number">3</span>) <span class="keyword">TO</span> <span class="keyword">MINUTE</span> <span class="keyword">as</span> result_interval_day_p1_to_minute</span><br><span class="line">    <span class="comment">-- INTERVAL DAY(p1) TO SECOND(p2)</span></span><br><span class="line">    , f1 <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="string">&#x27;10 00:00:00.004&#x27;</span> <span class="keyword">DAY</span> <span class="keyword">TO</span> <span class="keyword">SECOND</span>(<span class="number">3</span>) <span class="keyword">as</span> result_interval_day_p1_to_second_p2</span><br><span class="line">    <span class="comment">-- INTERVAL HOUR</span></span><br><span class="line">    , f1 <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">HOUR</span> <span class="keyword">as</span> result_interval_hour</span><br><span class="line">    <span class="comment">-- INTERVAL HOUR TO MINUTE</span></span><br><span class="line">    , f1 <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="string">&#x27;10:03&#x27;</span> <span class="keyword">HOUR</span> <span class="keyword">TO</span> <span class="keyword">MINUTE</span> <span class="keyword">as</span> result_interval_hour_to_minute</span><br><span class="line">    <span class="comment">-- INTERVAL HOUR TO SECOND(p2)</span></span><br><span class="line">    , f1 <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="string">&#x27;00:00:00.004&#x27;</span> <span class="keyword">HOUR</span> <span class="keyword">TO</span> <span class="keyword">SECOND</span>(<span class="number">3</span>) <span class="keyword">as</span> result_interval_hour_to_second</span><br><span class="line">    <span class="comment">-- INTERVAL MINUTE</span></span><br><span class="line">    , f1 <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">MINUTE</span> <span class="keyword">as</span> result_interval_minute</span><br><span class="line">    <span class="comment">-- INTERVAL MINUTE TO SECOND(p2)</span></span><br><span class="line">    , f1 <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="string">&#x27;05:05.006&#x27;</span> <span class="keyword">MINUTE</span> <span class="keyword">TO</span> <span class="keyword">SECOND</span>(<span class="number">3</span>) <span class="keyword">as</span> result_interval_minute_to_second_p2</span><br><span class="line">    <span class="comment">-- INTERVAL SECOND</span></span><br><span class="line">    , f1 <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="string">&#x27;3&#x27;</span> <span class="keyword">SECOND</span> <span class="keyword">as</span> result_interval_second</span><br><span class="line">    <span class="comment">-- INTERVAL SECOND(p2)</span></span><br><span class="line">    , f1 <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="string">&#x27;300&#x27;</span> <span class="keyword">SECOND</span>(<span class="number">3</span>) <span class="keyword">as</span> result_interval_second_p2</span><br><span class="line"><span class="keyword">FROM</span> (<span class="keyword">SELECT</span> TO_TIMESTAMP_LTZ(<span class="number">1640966476500</span>, <span class="number">3</span>) <span class="keyword">as</span> f1)</span><br></pre></td></tr></table></figure><h3 id="2-3-2-复合数据类型"><a href="#2-3-2-复合数据类型" class="headerlink" title="2.3.2.复合数据类型"></a>2.3.2.复合数据类型</h3><ol><li>⭐ 数组类型：ARRAY<t>、t ARRAY。数组最大长度为 2,147,483,647。t 代表数组内的数据类型。举例 ARRAY<INT>、ARRAY<STRING>，其等同于 INT ARRAY、STRING ARRAY</li><li>⭐ Map 类型：MAP&lt;kt, vt&gt;。Map 类型就和 Java 中的 Map 类型一样，key 是没有重复的。举例 Map&lt;STRING, INT&gt;、Map&lt;BIGINT, STRING&gt;</li><li>⭐ 集合类型：MULTISET<t>、t MULTISET。就和 Java 中的 List 类型，一样，运行重复的数据。举例 MULTISET<INT>，其等同于 INT MULTISET</li><li>⭐ 对象类型：ROW&lt;n0 t0, n1 t1, …&gt;、ROW&lt;n0 t0 ‘d0’, n1 t1 ‘d1’, …&gt;、ROW(n0 t0, n1 t1, …&gt;、ROW(n0 t0 ‘d0’, n1 t1 ‘d1’, …)。就和 Java 中的自定义对象一样。举例：ROW(myField INT, myOtherField BOOLEAN)，其等同于 ROW&lt;myField INT, myOtherField BOOLEAN&gt;</li></ol><h3 id="2-3-3-用户自定义数据类型"><a href="#2-3-3-用户自定义数据类型" class="headerlink" title="2.3.3.用户自定义数据类型"></a>2.3.3.用户自定义数据类型</h3><p>用户自定义类型就是运行用户使用 Java 等语言自定义一个数据类型出来。但是目前数据类型不支持使用 CREATE TABLE 的 DDL 进行定义，只支持作为函数的输入输出参数。如下案例：</p><ol><li>⭐ 第一步，自定义数据类型</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 基础类型，Flink 可以通过反射类型信息自动把数据类型获取到</span></span><br><span class="line">    <span class="comment">// 关于 SQL 类型和 Java 类型之间的映射见：https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/types/#data-type-extraction</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> age;</span><br><span class="line">    <span class="keyword">public</span> String name;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 复杂类型，用户可以通过 @DataTypeHint(&quot;DECIMAL(10, 2)&quot;) 注解标注此字段的数据类型</span></span><br><span class="line">    <span class="keyword">public</span> <span class="meta">@DataTypeHint(&quot;DECIMAL(10, 2)&quot;)</span> BigDecimal totalBalance;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="2"><li>⭐ 第二步，在 UDF 中使用此数据类型</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserScalarFunction</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 自定义数据类型作为输出参数</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> User <span class="title">eval</span><span class="params">(<span class="keyword">long</span> i)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (i &gt; <span class="number">0</span> &amp;&amp; i &lt;= <span class="number">5</span>) &#123;</span><br><span class="line">            User u = <span class="keyword">new</span> User();</span><br><span class="line">            u.age = (<span class="keyword">int</span>) i;</span><br><span class="line">            u.name = <span class="string">&quot;name1&quot;</span>;</span><br><span class="line">            u.totalBalance = <span class="keyword">new</span> BigDecimal(<span class="number">1.1d</span>);</span><br><span class="line">            <span class="keyword">return</span> u;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            User u = <span class="keyword">new</span> User();</span><br><span class="line">            u.age = (<span class="keyword">int</span>) i;</span><br><span class="line">            u.name = <span class="string">&quot;name2&quot;</span>;</span><br><span class="line">            u.totalBalance = <span class="keyword">new</span> BigDecimal(<span class="number">2.2d</span>);</span><br><span class="line">            <span class="keyword">return</span> u;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 2. 自定义数据类型作为输入参数</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">eval</span><span class="params">(User i)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (i.age &gt; <span class="number">0</span> &amp;&amp; i.age &lt;= <span class="number">5</span>) &#123;</span><br><span class="line">            User u = <span class="keyword">new</span> User();</span><br><span class="line">            u.age = <span class="number">1</span>;</span><br><span class="line">            u.name = <span class="string">&quot;name1&quot;</span>;</span><br><span class="line">            u.totalBalance = <span class="keyword">new</span> BigDecimal(<span class="number">1.1d</span>);</span><br><span class="line">            <span class="keyword">return</span> u.name;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            User u = <span class="keyword">new</span> User();</span><br><span class="line">            u.age = <span class="number">2</span>;</span><br><span class="line">            u.name = <span class="string">&quot;name2&quot;</span>;</span><br><span class="line">            u.totalBalance = <span class="keyword">new</span> BigDecimal(<span class="number">2.2d</span>);</span><br><span class="line">            <span class="keyword">return</span> u.name;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="3"><li>⭐ 第三步，在 Flink SQL 中使用</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 1. 创建 UDF</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">FUNCTION</span> user_scalar_func <span class="keyword">AS</span> <span class="string">&#x27;flink.examples.sql._12_data_type._02_user_defined.UserScalarFunction&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2. 创建数据源表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    user_id <span class="type">BIGINT</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span> COMMENT <span class="string">&#x27;用户 id&#x27;</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 3. 创建数据汇表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    result_row_1 <span class="type">ROW</span><span class="operator">&lt;</span>age <span class="type">INT</span>, name STRING, totalBalance <span class="type">DECIMAL</span>(<span class="number">10</span>, <span class="number">2</span>)<span class="operator">&gt;</span>,</span><br><span class="line">    result_row_2 STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 4. SQL 查询语句</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="comment">-- 4.a. 用户自定义类型作为输出</span></span><br><span class="line">    user_scalar_func(user_id) <span class="keyword">as</span> result_row_1,</span><br><span class="line">    <span class="comment">-- 4.b. 用户自定义类型作为输出及输入</span></span><br><span class="line">    user_scalar_func(user_scalar_func(user_id)) <span class="keyword">as</span> result_row_2</span><br><span class="line"><span class="keyword">from</span> source_table;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 5. 查询结果</span></span><br><span class="line"><span class="operator">+</span>I[<span class="operator">+</span>I[<span class="number">9</span>, name2, <span class="number">2.20</span>], name2]</span><br><span class="line"><span class="operator">+</span>I[<span class="operator">+</span>I[<span class="number">1</span>, name1, <span class="number">1.10</span>], name1]</span><br><span class="line"><span class="operator">+</span>I[<span class="operator">+</span>I[<span class="number">5</span>, name1, <span class="number">1.10</span>], name1]</span><br></pre></td></tr></table></figure><h2 id="2-4-SQL-动态表-amp-连续查询"><a href="#2-4-SQL-动态表-amp-连续查询" class="headerlink" title="2.4.SQL 动态表 &amp; 连续查询"></a>2.4.SQL 动态表 &amp; 连续查询</h2><p>在小伙伴萌看下文之前，先看一下 2.4 节整体的思路，跟着博主思路走，会更清晰：</p><ol><li>⭐ 先分析一下将 SQL 应用到流处理的思路</li><li>⭐ SQL 应用于批处理已经很成熟了，通过对比流批处理在输入、数据处理、输出的异同点来分析出将 SQL 应用于流处理的核心要解决的问题点</li><li>⭐ 分析如何使用 <code>SQL 动态输入表</code> 技术来将 <code>输入数据流</code> 映射到 <code>SQL 中的输入表</code></li><li>⭐ 分析如何使用 <code>SQL 连续查询</code> 技术来将 <code>计算逻辑</code> 映射到 <code>SQL 中的运算语义</code></li><li>⭐ 使用 <code>SQL 动态表 &amp; 连续查询技术</code> 两种技术方案来将 <code>流式 SQL</code> 实际应用到两个常见案例中</li><li>⭐ 分析 <code>SQL 连续查询</code> 的两种类型：更新（Update）查询 &amp; 追加（Append）查询</li><li>⭐ 分析如何使用 <code>SQL 动态输出表</code> 技术来将 <code>输出数据流</code> 映射到 <code>SQL 中的输出表</code></li></ol><p>博主认为读完本节你应该掌握：</p><ol><li>⭐ <code>SQL 动态输入表</code>、<code>SQL 动态输出表</code></li><li>⭐ <code>SQL 连续查询</code> 的两种类型分别对应的查询场景及 SQL 语义</li></ol><h3 id="2-4-1-SQL-应用于流处理的思路"><a href="#2-4-1-SQL-应用于流处理的思路" class="headerlink" title="2.4.1.SQL 应用于流处理的思路"></a>2.4.1.SQL 应用于流处理的思路</h3><p>在流式 SQL 诞生之前，所有的基于 SQL 的数据查询都是基于批数据的，没有将 SQL 应用到流数据处理这一说法。</p><p>那么如果我们想将 SQL 应用到流处理中，必然要站在巨人的肩膀（批数据处理的流程）上面进行，那么具体的分析思路如下：</p><ol><li>⭐ 步骤一：先比较 <code>批处理</code> 与 <code>流处理</code> 的异同之处：如果有相同的部分，那么可以直接复用；不同之处才是我们需要重点克服和关注的。</li><li>⭐ 步骤二：摘出 1 中说到的不同之处，分析如果要满足这个不同之处，目前有哪些技术是类似的</li><li>⭐ 步骤三：再从这些类似的技术上进一步发展，以满足将 SQL 应用于流任务中</li></ol><p>博主下文就会根据上述三个步骤来一步一步介绍 <code>动态表</code> 诞生的背景以及这个概念是如何诞生的。</p><h3 id="2-4-2-流批处理的异同点及将-SQL-应用于流处理核心解决的问题"><a href="#2-4-2-流批处理的异同点及将-SQL-应用于流处理核心解决的问题" class="headerlink" title="2.4.2.流批处理的异同点及将 SQL 应用于流处理核心解决的问题"></a>2.4.2.流批处理的异同点及将 SQL 应用于流处理核心解决的问题</h3><p>首先对比一下常见的 <code>批处理</code> 和 <code>流处理</code> 中 <code>数据源（输入表）</code>、<code>处理逻辑</code>、<code>数据汇（结果表）</code> 的异同点。</p><table><thead><tr><th>-</th><th>输入表</th><th>处理逻辑</th><th>结果表</th></tr></thead><tbody><tr><td>批处理</td><td>静态表：输入数据有限、是有界集合</td><td>批式计算：每次执行查询能够访问到完整的输入数据，然后计算，输出完整的结果数据</td><td>静态表：数据有限</td></tr><tr><td>流处理</td><td>动态表：输入数据无限，数据实时增加，并且源源不断</td><td>流式计算：执行时不能够访问到完整的输入数据，每次计算的结果都是一个中间结果</td><td>动态表：数据无限</td></tr></tbody></table><p>对比上述流批处理之后，我们得到了要将 SQL 应用于流式任务的三个要解决的核心点：</p><ol><li>⭐ SQL 输入表：分析如何将一个实时的，源源不断的输入流数据表示为 SQL 中的输入表。</li><li>⭐ SQL 处理计算：分析将 SQL 查询逻辑翻译成什么样的底层处理技术才能够实时的处理流式输入数据，然后产出流式输出数据。</li><li>⭐ SQL 输出表：分析如何将 SQL 查询输出的源源不断的流数据表示为一个 SQL 中的输出表。</li></ol><p>将上面 3 个点总结一下，也就引出了本节的 <code>动态表</code> 和 <code>连续查询</code> 两种技术方案：</p><ol><li>⭐ <code>动态表</code>：源源不断的输入、输出流数据映射到 <code>动态表</code></li><li>⭐ <code>连续查询</code>：实时处理输入数据，产出输出数据的实时处理技术</li></ol><h3 id="2-4-3-SQL-流处理的输入：输入流映射为-SQL-动态输入表"><a href="#2-4-3-SQL-流处理的输入：输入流映射为-SQL-动态输入表" class="headerlink" title="2.4.3.SQL 流处理的输入：输入流映射为 SQL 动态输入表"></a>2.4.3.SQL 流处理的输入：输入流映射为 SQL 动态输入表</h3><p><code>动态表</code>。这里的动态其实是相比于批处理的静态（有界）来说的。</p><ol><li>⭐ 静态表：应用于批处理数据中，静态表可以理解为是不随着时间<code>实时</code>进行变化的。一般都是一天、一小时的粒度新生成一个分区。</li><li>⭐ 动态表：动态表是随时间实时进行变化的。是将 SQL 体系中表的概念应用到 Flink 上面的的核心点。</li></ol><p>来看一个具体的案例，下图显示了<code>点击事件流</code>（左侧）如何转换为<code>动态表</code>（右侧）。当数据源生成更多的点击事件记录时，映射出来的动态表也会不断增长，这就是动态表的概念：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/4.png" alt="Dynamic Table"></p><h3 id="2-4-4-SQL-流处理的计算：实时处理底层技术-SQL-连续查询"><a href="#2-4-4-SQL-流处理的计算：实时处理底层技术-SQL-连续查询" class="headerlink" title="2.4.4.SQL 流处理的计算：实时处理底层技术 - SQL 连续查询"></a>2.4.4.SQL 流处理的计算：实时处理底层技术 - SQL 连续查询</h3><p><code>连续查询</code>。</p><p>部分高级关系数据库系统提供了一个称为<strong>物化视图</strong>（Materialized Views) 的特性。</p><p>物化视图其实就是一条 SQL 查询，就像常规的虚拟视图 VIEW 一样。但与虚拟视图不同的是，物化视图会缓存查询的结果，因此在请求访问视图时不需要对查询进行重新计算，可以直接获取物化视图的结果，小伙伴萌可以认为物化视图其实就是把结果缓存了下来。</p><p>举个例子：批处理中，如果以 Hive 天级别的物化视图来说，其实就是每天等数据源 ready 之后，调度物化视图的 SQL 执行然后产生新的结果提供服务。<code>那么就可以认为一条表示了输入、处理、输出的 SQL 就是一个构建物化视图的过程。</code></p><p>映射到我们的流任务中，输入、处理逻辑、输出这一套流程也是一个物化视图的概念。相比批处理来说，流处理中，我们的数据源表的数据是源源不断的。那么从输入、处理、输出的整个物化视图的维护流程也必须是实时的。</p><p>因此我们就需要引入一种<code>实时视图维护（Eager View Maintenance）</code>的技术去做到：一旦更新了物化视图的数据源表就立即更新视图的结果，从而保证输出的结果也是最新的。</p><p>这种 <code>实时视图维护（Eager View Maintenance）</code>的技术就叫做 <code>连续查询</code>。</p><blockquote><p>注意：</p><ol><li>⭐ <strong>连续查询（Continuous Query）</strong> 不断的消费动态输入表的的数据，不断的更新动态结果表的数据。</li><li>⭐ <strong>连续查询（Continuous Query）</strong> 的产出的结果 = 批处理模式在输入表的上执行的相同查询的结果。相同的 SQL，对应于同一个输入数据，虽然执行方式不同，但是流处理和批处理的结果是永远都会相同的。</li></ol></blockquote><h3 id="2-4-5-SQL-流处理实际应用：动态表-amp-连续查询技术的两个实战案例"><a href="#2-4-5-SQL-流处理实际应用：动态表-amp-连续查询技术的两个实战案例" class="headerlink" title="2.4.5.SQL 流处理实际应用：动态表 &amp; 连续查询技术的两个实战案例"></a>2.4.5.SQL 流处理实际应用：动态表 &amp; 连续查询技术的两个实战案例</h3><p>总结前两节，<code>动态表</code> &amp; <code>连续查询</code> 两项技术在一条流 SQL 中的执行流程总共包含了三个步骤，如下图及总结所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/30.png" alt="Query"></p><ol><li>⭐ 第一步：将数据输入流转换为 SQL 中的动态输入表。这里的转化其实就是指将输入流映射（绑定）为一个动态输入表。上图虽然分开画了，但是可以理解为一个东西。</li><li>⭐ 第二步：在动态输入表上执行一个连续查询，然后生成一个新的动态结果表。</li><li>⭐ 第三步：生成的动态结果表被转换回数据输出流。</li></ol><p>我们实际介绍一个案例来看看其运行方式，以上文介绍到的<strong>点击事件流</strong>为例，点击事件流数据的字段如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  user:  VARCHAR,   &#x2F;&#x2F; 用户名</span><br><span class="line">  cTime: TIMESTAMP, &#x2F;&#x2F; 访问 URL 的时间</span><br><span class="line">  url:   VARCHAR    &#x2F;&#x2F; 用户访问的 URL</span><br><span class="line">]</span><br></pre></td></tr></table></figure><ol><li>⭐ 第一步，将输入数据流映射为一个动态输入表。以下图为例，我们将点击事件流（图左）转换为动态表 (图右)。当点击数据源源不断的来到时，动态表的数据也会不断的增加。</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/4.png" alt="Dynamic Table"></p><ol start="2"><li>⭐ 第二步，在点击事件流映射的动态输入表上执行一个<strong>连续查询（Continuous Query）</strong>，并生成一个新的动态输出表。</li></ol><p>下面介绍两个查询的案例：</p><p>第一个查询：一个简单的 GROUP-BY COUNT 聚合查询，写过 SQL 的都不会陌生吧，这种应该都是最基础，最常用的对数据按照类别分组的方法。</p><p>如下图所示 group by 聚合的常用案例。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/12.png" alt="time"></p><p>那么本案例中呢，是基于 clicks 表中 user 字段对 clicks 表（点击事件流）进行分组，来统计每一个 user 的访问的 URL 的数量。下面的图展示了当 clicks 输入表来了新数据（即表更新时），<strong>连续查询（Continuous Query）</strong> 的计算逻辑。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/5.png" alt="group agg"></p><p>当查询开始，clicks 表(左侧)是空的。</p><ol><li>⭐ 当第一行数据被插入到 clicks 表时，连续查询（Continuous Query）开始计算结果数据。数据源表第一行数据 [Mary,./home] 输入后，会计算结果 [Mary, 1] <code>插入（insert）结果表</code>。</li><li>⭐ 当第二行 [Bob, ./cart] 插入到 clicks 表时，连续查询（Continuous Query）会计算结果 [Bob, 1]，并<code>插入（insert）到结果表</code>。</li><li>⭐ 第三行 [Mary, ./prod?id=1] 输出时，会计算出[Mary, 2]（user 为 Mary 的数据总共来过两条，所以为 2），并<code>更新（update）结果表</code>，[Mary, 1] 更新成 [Mary, 2]。</li><li>⭐ 最后，当第四行数据加入 clicks 表时，查询将第三行 [Liz, 1] <code>插入（insert）结果表</code>中。</li></ol><p>注意上述特殊标记出来的字体，可以看到连续查询对于结果的数据输出方式有两种：</p><ol><li>⭐ 插入（insert）结果表</li><li>⭐ 更新（update）结果表</li></ol><p>大家对于 <code>插入（insert）结果表</code> 这件事都比较好理解，因为离线数据都只有插入这个概念。</p><p>但是 <code>更新（update）结果表</code> 就是离线处理中没有概念了。这就是连续查询中中比较重要一个概念。后文会介绍。</p><p>接下来介绍第二条查询语句。</p><p><code>第二条查询与第一条类似</code>，但是 group by 中除了 user 字段之外，还 group by 了 tumble，其代表开了个滚动窗口（后面会详细说明滚动窗口的作用），然后计算 url 数量。</p><p>group by user，是按照类别（横向）给数据分组，group by tumble 滚动窗口是按时间粒度（纵向）给数据进行分组。如下图所示。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/11.png" alt="time"></p><p>图形化一解释就很好理解了，两种都是对数据进行分组，一个是按照 <code>类别</code> 分组，另一种是按照 <code>时间</code> 分组。</p><p>与前面一样，左边显示了输入表 clicks。查询每小时持续计算结果并更新结果表。clicks 表有三列，user，cTime，url。其中 cTime 代表数据的时间戳，用于给数据按照时间粒度分组。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/6.png" alt="tumble window"></p><p>我们的滚动窗口的步长为 1 小时，即时间粒度上面的分组为 1 小时。其中时间戳在 12:00:00 - 12:59:59 之间有四条数据。13:00:00 - 13:59:59 有三条数据。14:00:00 - 14:59:59 之间有四条数据。</p><ol><li>⭐ 当 12:00:00 - 12:59:59 数据输入之后，1 小时的窗口，连续查询（Continuous Query）计算的结果如右图所示，将 [Mary, 3]，[Bob, 1] <code>插入（insert）结果表</code>。</li><li>⭐ 当 13:00:00 - 13:59:59 数据输入之后，1 小时的窗口，连续查询（Continuous Query）计算的结果如右图所示，将 [Bob, 1]，[Liz, 2] <code>插入（insert）结果表</code>。</li><li>⭐ 当 14:00:00 - 14:59:59 数据输入之后，1 小时的窗口，连续查询（Continuous Query）计算的结果如右图所示，将 [Mary, 1]，[Bob, 2]，[Liz, 1] <code>插入（insert）结果表</code>。</li></ol><p>而这个查询只有 <code>插入（insert）结果表</code> 这个行为。</p><h3 id="2-4-6-SQL-连续查询的两种类型：更新（Update）查询-amp-追加（Append）查询"><a href="#2-4-6-SQL-连续查询的两种类型：更新（Update）查询-amp-追加（Append）查询" class="headerlink" title="2.4.6.SQL 连续查询的两种类型：更新（Update）查询 &amp; 追加（Append）查询"></a>2.4.6.SQL 连续查询的两种类型：更新（Update）查询 &amp; 追加（Append）查询</h3><p>虽然前一节的两个查询看起来非常相似（都计算分组进行计数聚合），但它们在一个重要方面不同：</p><ol><li><p>⭐ 第一个查询（group by user），即（Update）查询：会更新先前输出的结果，即结果表流数据中包含 INSERT 和 UPDATE 数据。<br>小伙伴萌可以理解为 group by user 这条语句当中，输入源的数据是一直有的，源源不断的，同一个 user 的数据之后可能还是会有的，因此可以认为此 SQL 的每次的输出结果都是一个中间结果，<br>当同一个 user 下一条数据到来的时候，就要用新结果把上一次的产出中间结果（旧结果）给 UPDATE 了。所以这就是 UPDATE 查询的由来（其中 INSERT 就是第一条数据到来的时候，没有之前的中间结果，所以是 INSERT）。</p></li><li><p>⭐ 第二个查询（group by user, tumble(xxx)），即（Append）查询：只追加到结果表，即结果表流数据中只包含 INSERT 的数据。<br>小伙伴萌可以理解为虽然 group by user, tumble(xxx) 上游也是一个源源不断的数据，但是这个查询本质上是对时间上的划分，而时间都是越变越大的，当前这个滚动窗口结束之后，后面来的数据的时间都会比这个滚动窗口的结束时间大，都归属于之后的窗口了，当前这个滚动窗口的结果数据就不会再改变了，因此这条查询只有 INSERT 数据，即一个 Append 查询。</p></li></ol><p>上面是 Flink SQL 连续查询处理机制上面的两类查询方式。我们可以发现连续查询的处理机制不一样，产出到结果表中的结果数据也是不一样的。针对上面两种结果表的更新方式，Flink SQL 提出了 changelog 表的概念来进行兼容。</p><p>changelog 表这个概念其实就和 MySQL binlog 是一样的。会包含 <code>INSERT</code>、<code>UPDATE</code>、<code>DELETE</code> 三种数据，通过这三种数据的处理来描述实时处理技术对于动态表的变更：</p><ol><li>⭐ changelog 表：即第一个查询的输出表，输出结果数据不但会追加，还会发生更新</li><li>⭐ changelog insert-only 表：即第二个查询的输出表，输出结果数据只会追加，不会发生更新</li></ol><h3 id="2-4-7-SQL-流处理的输出：动态输出表转化为输出数据"><a href="#2-4-7-SQL-流处理的输出：动态输出表转化为输出数据" class="headerlink" title="2.4.7.SQL 流处理的输出：动态输出表转化为输出数据"></a>2.4.7.SQL 流处理的输出：动态输出表转化为输出数据</h3><p>可以看到我们的标题都是随着一个 SQL 的生命周期的。从 <code>输入流映射为 SQL 动态输入表</code>、<code>实时处理底层技术 - SQL 连续查询</code> 到本小节的 <code>SQL 动态输出表转化为输出数据</code>。都是有逻辑关系的。</p><p>我们上面介绍到了 <strong>连续查询（Continuous Query）</strong> 的输出结果表是一个 changelog。其可以像普通数据库表一样通过 INSERT、UPDATE 和 DELETE 来不断修改。</p><p>它可能是一个只有一行、不断更新 changelog 表，也可能是一个 insert-only 的 changelog 表，没有 UPDATE 和 DELETE 修改，或者介于两者之间的其他表。</p><p>在将动态表转换为流或将其写入外部系统时，需要对这些不同状态的数据进行编码。Flink 的 Table API 和 SQL API 支持三种方式来编码一个动态表的变化:</p><ol><li><p>⭐ Append-only 流： 输出的结果只有 <code>INSERT</code> 操作的数据。</p></li><li><p>⭐ Retract 流： </p></li></ol><ul><li>⭐ Retract 流包含两种类型的 message： add messages 和 retract messages 。其将 <code>INSERT</code> 操作编码为 add message、将 <code>DELETE</code> 操作编码为 retract message、将 <code>UPDATE</code> 操作编码为更新先前行的 retract message 和更新（新）行的 add message，从而将动态表转换为 retract 流。</li><li>⭐ Retract 流写入到输出结果表的数据如下图所示，有 <code>-</code>，<code>+</code> 两种，分别 <code>-</code> 代表撤回旧数据，<code>+</code> 代表输出最新的数据。这两种数据最终都会写入到输出的数据引擎中。</li><li>⭐ 如果下游还有任务去消费这条流的话，要注意需要正确处理 <code>-</code>，<code>+</code> 两种数据，防止数据计算重复或者错误。</li></ul><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/7.png" alt="retract"></p><ol start="3"><li>⭐ Upsert 流：</li></ol><ul><li>⭐ Upsert 流包含两种类型的 message： upsert messages 和 delete messages。转换为 upsert 流的动态表需要唯一键（唯一键可以由多个字段组合而成）。其会将 <code>INSERT</code> 和 <code>UPDATE</code> 操作编码为 upsert message，将 <code>DELETE</code> 操作编码为 delete message。</li><li>⭐ Upsert 流写入到输出结果表的数据如下图所示，每次输出的结果都是当前每一个 user 的最新结果数据，不会有 Retract 中的 <code>-</code> 回撤数据。</li><li>⭐ 如果下游还有一个任务去消费这条流的话，消费流的算子需要知道唯一键（即 user），以便正确地根据唯一键（user）去拿到每一个 user 当前最新的状态。其与 retract 流的主要区别在于 UPDATE 操作是用单个 message 编码的，因此效率更高。下图显示了将动态表转换为 upsert 流的过程。</li></ul><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/8.png" alt="upsert"></p><h3 id="2-4-8-补充知识：SQL-与关系代数"><a href="#2-4-8-补充知识：SQL-与关系代数" class="headerlink" title="2.4.8.补充知识：SQL 与关系代数"></a>2.4.8.补充知识：SQL 与关系代数</h3><p>小伙伴萌会问到，关系代数是啥东西？</p><p>其实关系代数就是对于数据集（即表）的一系列的 <code>操作</code>（即查询语句）。常见关系代数有：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/1.png" alt="Relational Algebra"></p><p>⭐ 那么 SQL 和关系代数是啥关系呢？</p><p><strong>SQL 就是能够表示关系代数一种面向用户的接口：即用户能使用 SQL 表达关系代数的处理逻辑，也就是我们可以用 SQL 去在表（数据集）上执行我们的业务逻辑操作（关系代数操作）。</strong></p><h2 id="2-5-SQL-的时间属性"><a href="#2-5-SQL-的时间属性" class="headerlink" title="2.5.SQL 的时间属性"></a>2.5.SQL 的时间属性</h2><p>在小伙伴萌看下文之前，先看一下 2.5 节整体的思路，跟着博主思路走：</p><ol><li>⭐ 与离线处理中常见的时间分区字段一样，在实时处理中，时间属性也是一个核心概念。Flink 支持 <code>处理时间</code>、<code>事件时间</code>、<code>摄入时间</code> 三种时间语义。</li><li>⭐ 分别介绍三种时间语义的应用场景及案例。三种时间在生产环境的使用频次 <code>事件时间（SQL 常用）</code> &gt; <code>处理时间（SQL 几乎不用，DataStream 少用）</code> &gt; <code>摄入时间（不用）</code></li></ol><h3 id="2-5-1-Flink-三种时间属性简介"><a href="#2-5-1-Flink-三种时间属性简介" class="headerlink" title="2.5.1.Flink 三种时间属性简介"></a>2.5.1.Flink 三种时间属性简介</h3><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/10.png" alt="time"></p><ol><li>⭐ 事件时间：指的是数据本身携带的时间，这个时间是在事件产生时的时间，而且在 Flink SQL 触发计算时，也使用数据本身携带的时间。这就叫做 <code>事件时间</code>。<code>目前生产环境中用的最多</code>。</li><li>⭐ 处理时间：指的是具体算子计算数据执行时的机器时间（例如在算子中 Java 取 System.currentTimeMillis()) ），<code>在生产环境中用的次多</code>。</li><li>⭐ 摄入时间：指的是数据从数据源进入 Flink 的时间。<code>摄入时间用的最少，可以说基本不使用</code>。</li></ol><p>小伙伴萌要注意到：</p><ol><li>⭐ 上述的三种时间概念不是由于有了数据而诞生的，而是有了 Flink 之后根据实际的应用场景而诞生的。以事件时间举个例子，如果只是数据携带了时间，Flink 也消费了这个数据，但是在 Flink 中没有使用数据的这个时间作为计算的触发条件，也不能把这个 Flink 任务叫做事件时间的任务。</li><li>⭐ 其次，要认识到，一般一个 Flink 任务只会有一个时间属性，所以时间属性通常认为是一个任务粒度的。举例：我们可以说 A 任务是事件时间语义的任务，B 任务是处理时间语义的任务。当然了，一个任务也可以存在多个时间属性。</li></ol><h3 id="2-5-2-Flink-三种时间属性的应用场景"><a href="#2-5-2-Flink-三种时间属性的应用场景" class="headerlink" title="2.5.2.Flink 三种时间属性的应用场景"></a>2.5.2.Flink 三种时间属性的应用场景</h3><p>讲到这里，xdm 会问，博主上面写的 3 种时间属性到底对我们的任务有啥影响呢？3 种时间属性的应用场景是啥？</p><p>先说结论，在 Flink 中时间的作用：</p><ol><li>⭐ <code>主要体现在包含时间窗口的计算中</code>：用于标识任务的时间进度，来判断是否需要触发窗口的计算。比如常用的<code>滚动窗口</code>、<code>滑动窗口</code>等都需要时间推动触发。这些窗口的应用场景后续会详细介绍。</li><li>⭐ <code>次要体现在自定义时间语义的计算中</code>：举个例子，比如用户可以自定义每隔 10s 的本地时间，或者消费到的数据的时间戳每增大 10s，就把计算结果输出一次，时间在此类应用中也是一种标识任务进度的作用。</li></ol><p>博主以 <code>滚动窗口</code> 的聚合任务为例来介绍一下事件时间和处理时间的对比区别。</p><ol><li>⭐ 事件时间案例：还是以之前的 clicks 表拿来举例。</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/6.png" alt="tumble window"></p><p>上面这个案例的窗口大小是 1 小时，需求方需要按照用户点击时间戳 <code>cTime</code> 划分数据（划分滚动窗口），然后计算出 count 聚合结果（这样计算能反映出事件的真实发生时间），那么就需要把 <code>cTime</code> 设置为窗口的划分时间戳，即代码中 <code>tumble(cTime, interval &#39;1&#39; hour)</code>。</p><p>上面这种就叫做事件时间。即用数据中自带的时间戳进行窗口的划分（点击操作真实的发生时间）。</p><p>后续 Flink SQL 任务在运行的过程中也会实际按照 <code>cTime</code> 的当前时间作为一小时窗口结束触发条件并计算一个小时窗口内的数据。</p><ol start="2"><li>⭐ 处理时间案例：还是以之前的 clicks 表拿来举例。</li></ol><p>还是上面那个案例，但是这次需求方不需要按照数据上的时间戳划分数据（划分滚动窗口），只需要数据来了之后， 在 Flink 机器上的时间作为一小时窗口结束的书法条件并计算。</p><p>那么这种触发机制就是处理时间。</p><ol start="3"><li>⭐ 摄入时间案例：在 Flink 从外部数据源读取到数据时，给这条数据带上的当前数据源算子的本地时间戳。下游可以用这个时间戳进行窗口聚合，不过这种几乎不使用。</li></ol><h3 id="2-5-3-SQL-指定时间属性的两种方式"><a href="#2-5-3-SQL-指定时间属性的两种方式" class="headerlink" title="2.5.3.SQL 指定时间属性的两种方式"></a>2.5.3.SQL 指定时间属性的两种方式</h3><p>如果要满足 Flink SQL 时间窗口类的聚合操作，SQL 或 Table API 中的 <code>数据源表</code> 就需要提供时间属性（相当于我们把这个时间属性在 <code>数据源表</code> 上面进行声明），以及支持时间相关的操作。</p><p>那么来看看 Flink SQL 为我们提供的两种指定时间戳的方式：</p><ol><li>⭐ <code>CREATE TABLE DDL</code> 创建表的时候指定</li><li>⭐ <code>可以在 DataStream 中指定</code>，在后续的 DataStream 转的 Table 中使用</li></ol><p>一旦时间属性定义好，它就可以像普通列一样使用，也可以在时间相关的操作中使用。</p><h3 id="2-5-4-SQL-事件时间案例"><a href="#2-5-4-SQL-事件时间案例" class="headerlink" title="2.5.4.SQL 事件时间案例"></a>2.5.4.SQL 事件时间案例</h3><p>来看看 Flink 中如何指定事件时间。</p><ol><li>⭐ <code>CREATE TABLE DDL</code> 指定时间戳的方式。</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_actions (</span><br><span class="line">  user_name STRING,</span><br><span class="line">  data STRING,</span><br><span class="line">  user_action_time <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  <span class="comment">-- 使用下面这句来将 user_action_time 声明为事件时间，并且声明 watermark 的生成规则，即 user_action_time 减 5 秒</span></span><br><span class="line">  <span class="comment">-- 事件时间列的字段类型必须是 TIMESTAMP 或者 TIMESTAMP_LTZ 类型</span></span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> user_action_time <span class="keyword">AS</span> user_action_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> TUMBLE_START(user_action_time, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">MINUTE</span>), <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> user_name)</span><br><span class="line"><span class="keyword">FROM</span> user_actions</span><br><span class="line"><span class="comment">-- 然后就可以在窗口算子中使用 user_action_time</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> TUMBLE(user_action_time, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">MINUTE</span>);</span><br></pre></td></tr></table></figure><p>从上面这条语句可以看到，如果想使用事件时间，那么我们的时间戳类型必须是 TIMESTAMP 或者 TIMESTAMP_LTZ 类型。很多小伙伴会想到，我们的时间戳一般不都是秒或者是毫秒（BIGINT 类型）嘛，那这种情况怎么办？</p><p>解决方案必须要有啊。如下。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_actions (</span><br><span class="line">  user_name STRING,</span><br><span class="line">  data STRING,</span><br><span class="line">  <span class="comment">-- 1. 这个 ts 就是常见的毫秒级别时间戳</span></span><br><span class="line">  ts <span class="type">BIGINT</span>,</span><br><span class="line">  <span class="comment">-- 2. 将毫秒时间戳转换成 TIMESTAMP_LTZ 类型</span></span><br><span class="line">  time_ltz <span class="keyword">AS</span> TO_TIMESTAMP_LTZ(ts, <span class="number">3</span>),</span><br><span class="line">  <span class="comment">-- 3. 使用下面这句来将 user_action_time 声明为事件时间，并且声明 watermark 的生成规则，即 user_action_time 减 5 秒</span></span><br><span class="line">  <span class="comment">-- 事件时间列的字段类型必须是 TIMESTAMP 或者 TIMESTAMP_LTZ 类型</span></span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> time_ltz <span class="keyword">AS</span> time_ltz <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> TUMBLE_START(time_ltz, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">MINUTE</span>), <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> user_name)</span><br><span class="line"><span class="keyword">FROM</span> user_actions</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> TUMBLE(time_ltz, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">MINUTE</span>);</span><br></pre></td></tr></table></figure><ol start="2"><li>⭐ <code>DataStream</code> 中指定事件时间。</li></ol><p>之前介绍了 <code>Table</code> 和 <code>DataStream</code> 可以互转，那么 Flink 也提供了一个能力，就是在 Table 转为 DataStream 时，指定时间戳字段。如下案例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DataStreamSourceEventTimeTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment env =</span><br><span class="line">                StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(<span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">        EnvironmentSettings settings = EnvironmentSettings</span><br><span class="line">                .newInstance()</span><br><span class="line">                .useBlinkPlanner()</span><br><span class="line">                .inStreamingMode()</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, settings);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 分配 watermark</span></span><br><span class="line">        DataStream&lt;Row&gt; r = env.addSource(<span class="keyword">new</span> UserDefinedSource())</span><br><span class="line">                .assignTimestampsAndWatermarks(<span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor&lt;Row&gt;(Time.minutes(<span class="number">0L</span>)) &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Row element)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> (<span class="keyword">long</span>) element.getField(<span class="string">&quot;f2&quot;</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">        <span class="comment">// 2. 使用 f2.rowtime 的方式将 f2 字段指为事件时间时间戳</span></span><br><span class="line">        Table sourceTable = tEnv.fromDataStream(r, <span class="string">&quot;f0, f1, f2.rowtime&quot;</span>);</span><br><span class="line"></span><br><span class="line">        tEnv.createTemporaryView(<span class="string">&quot;source_table&quot;</span>, sourceTable);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 在 tumble window 中使用 f2</span></span><br><span class="line">        String tumbleWindowSql =</span><br><span class="line">                <span class="string">&quot;SELECT TUMBLE_START(f2, INTERVAL &#x27;5&#x27; SECOND), COUNT(DISTINCT f0)\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;FROM source_table\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;GROUP BY TUMBLE(f2, INTERVAL &#x27;5&#x27; SECOND)&quot;</span></span><br><span class="line">                ;</span><br><span class="line"></span><br><span class="line">        Table resultTable = tEnv.sqlQuery(tumbleWindowSql);</span><br><span class="line"></span><br><span class="line">        tEnv.toDataStream(resultTable, Row.class).print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UserDefinedSource</span> <span class="keyword">implements</span> <span class="title">SourceFunction</span>&lt;<span class="title">Row</span>&gt;, <span class="title">ResultTypeQueryable</span>&lt;<span class="title">Row</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isCancel;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Row&gt; sourceContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> (!<span class="keyword">this</span>.isCancel) &#123;</span><br><span class="line"></span><br><span class="line">                sourceContext.collect(Row.of(<span class="string">&quot;a&quot;</span> + i, <span class="string">&quot;b&quot;</span>, System.currentTimeMillis()));</span><br><span class="line"></span><br><span class="line">                Thread.sleep(<span class="number">10L</span>);</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.isCancel = <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> TypeInformation&lt;Row&gt; <span class="title">getProducedType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> RowTypeInfo(TypeInformation.of(String.class), TypeInformation.of(String.class),</span><br><span class="line">                    TypeInformation.of(Long.class));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-5-5-SQL-处理时间案例"><a href="#2-5-5-SQL-处理时间案例" class="headerlink" title="2.5.5.SQL 处理时间案例"></a>2.5.5.SQL 处理时间案例</h3><p>来看看 Flink SQL 中如何指定处理时间。</p><ol><li>⭐ <code>CREATE TABLE DDL</code> 指定时间戳的方式。</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_actions (</span><br><span class="line">  user_name STRING,</span><br><span class="line">  data STRING,</span><br><span class="line">  <span class="comment">-- 使用下面这句来将 user_action_time 声明为处理时间</span></span><br><span class="line">  user_action_time <span class="keyword">AS</span> PROCTIME()</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> TUMBLE_START(user_action_time, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">MINUTE</span>), <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> user_name)</span><br><span class="line"><span class="keyword">FROM</span> user_actions</span><br><span class="line"><span class="comment">-- 然后就可以在窗口算子中使用 user_action_time</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> TUMBLE(user_action_time, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">MINUTE</span>);</span><br></pre></td></tr></table></figure><ol start="2"><li>⭐ <code>DataStream</code> 中指定处理时间。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DataStreamSourceProcessingTimeTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment env =</span><br><span class="line">                StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(<span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">        EnvironmentSettings settings = EnvironmentSettings</span><br><span class="line">                .newInstance()</span><br><span class="line">                .useBlinkPlanner()</span><br><span class="line">                .inStreamingMode()</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, settings);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 分配 watermark</span></span><br><span class="line">        DataStream&lt;Row&gt; r = env.addSource(<span class="keyword">new</span> UserDefinedSource());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 使用 proctime.proctime 的方式将 f2 字段指为处理时间时间戳</span></span><br><span class="line">        Table sourceTable = tEnv.fromDataStream(r, <span class="string">&quot;f0, f1, f2, proctime.proctime&quot;</span>);</span><br><span class="line"></span><br><span class="line">        tEnv.createTemporaryView(<span class="string">&quot;source_table&quot;</span>, sourceTable);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 在 tumble window 中使用 f2</span></span><br><span class="line">        String tumbleWindowSql =</span><br><span class="line">                <span class="string">&quot;SELECT TUMBLE_START(proctime, INTERVAL &#x27;5&#x27; SECOND), COUNT(DISTINCT f0)\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;FROM source_table\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;GROUP BY TUMBLE(proctime, INTERVAL &#x27;5&#x27; SECOND)&quot;</span></span><br><span class="line">                ;</span><br><span class="line"></span><br><span class="line">        Table resultTable = tEnv.sqlQuery(tumbleWindowSql);</span><br><span class="line"></span><br><span class="line">        tEnv.toDataStream(resultTable, Row.class).print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UserDefinedSource</span> <span class="keyword">implements</span> <span class="title">SourceFunction</span>&lt;<span class="title">Row</span>&gt;, <span class="title">ResultTypeQueryable</span>&lt;<span class="title">Row</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isCancel;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Row&gt; sourceContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> (!<span class="keyword">this</span>.isCancel) &#123;</span><br><span class="line"></span><br><span class="line">                sourceContext.collect(Row.of(<span class="string">&quot;a&quot;</span> + i, <span class="string">&quot;b&quot;</span>, System.currentTimeMillis()));</span><br><span class="line"></span><br><span class="line">                Thread.sleep(<span class="number">10L</span>);</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.isCancel = <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> TypeInformation&lt;Row&gt; <span class="title">getProducedType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> RowTypeInfo(TypeInformation.of(String.class), TypeInformation.of(String.class),</span><br><span class="line">                    TypeInformation.of(Long.class));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-6-SQL-时区问题"><a href="#2-6-SQL-时区问题" class="headerlink" title="2.6.SQL 时区问题"></a>2.6.SQL 时区问题</h2><h3 id="2-6-1-SQL-时区解决的问题"><a href="#2-6-1-SQL-时区解决的问题" class="headerlink" title="2.6.1.SQL 时区解决的问题"></a>2.6.1.SQL 时区解决的问题</h3><p>首先说一下这个问题的背景：</p><p>大家想一下离线 Hive 环境中，有遇到过时区时区相关的问题吗？</p><p>至少博主目前没有碰到过，因为这个问题在底层的数据集成系统都已经给解决了，小伙伴萌拿到手的 ODS 层表都是已经按照所在地区的时区给格式化好的了。</p><p>举个例子：小伙伴萌看到日期分区为 2022-01-01 的 Hive 表时，可以默认认为该分区中的数据就对应到你所在地区的时区的 2022-01-01 日的数据。</p><p>但是 Flink 中时区问题要特别引起关注，不加小心就会误用。</p><p>而本节 SQL 时区旨在帮助大家了解到以下两个场景的问题：</p><ol><li>⭐ 在 1.13 之前，DDL create table 中使用 <code>PROCTIME()</code> 指定处理时间列时，返回值类型为 TIMESTAMP(3) 类型，而 TIMESTAMP(3) 是不带任何时区信息的，默认为 UTC 时间（0 时区）。</li><li>⭐ 使用 <code>StreamTableEnvironment::createTemporaryView</code> 将 DataStream 转为 Table 时，注册处理时间（<code>proctime.proctime</code>）、事件时间列（<code>rowtime.rowtime</code>）时，两列时间类型也为 TIMESTAMP(3) 类型，不带时区信息。</li></ol><p>而以上两个场景就会导致：</p><ol><li>⭐ 在北京时区的用户使用 TIMESTAMP(3) 类型的时间列开最常用的 1 天的窗口时，划分出来的窗口范围是北京时间的 [2022-01-01 08:00:00, 2022-01-02 08:00:00]，而不是北京时间的 [2022-01-01 00:00:00, 2022-01-02 00:00:00]。因为 TIMESTAMP(3) 是默认的 UTC 时间，即 0 时区。</li><li>⭐ 北京时区的用户将 TIMESTAMP(3) 类型时间属性列转为 STRING 类型的数据展示时，也是 UTC 时区的，而不是北京时间的。</li></ol><p>因此充分了解本节的知识内容可以很好的帮你避免时区问题错误。</p><h3 id="2-6-1-SQL-时间类型"><a href="#2-6-1-SQL-时间类型" class="headerlink" title="2.6.1.SQL 时间类型"></a>2.6.1.SQL 时间类型</h3><ol><li>⭐ Flink SQL 支持 TIMESTAMP（不带时区信息的时间）、TIMESTAMP_LTZ（带时区信息的时间）</li><li>⭐ TIMESTAMP（不带时区信息的时间）：是通过一个 <code>年， 月， 日， 小时， 分钟， 秒 和 小数秒</code> 的字符串来指定。举例：1970-01-01 00:00:04.001。</li></ol><ul><li>⭐ 为什么要使用字符串来指定呢？因为此种类型不带时区信息，所以直接用一个字符串指定就好了</li><li>⭐ 那 TIMESTAMP 字符串的时间代表的是什么时区的时间呢？UTC 时区，也就是默认 0 时区，对应中国北京是东八区</li></ul><ol start="3"><li>⭐ TIMESTAMP_LTZ（带时区信息的时间）：没有字符串来指定，而是通过 java 标准 epoch 时间 1970-01-01T00:00:00Z 开始计算的毫秒数。举例：1640966400000</li></ol><ul><li>⭐ 其时区信息是怎么指定的呢？是通过本次任务中的时区配置参数 <code>table.local-time-zone</code> 设置的</li><li>⭐ 时间戳本身也不带有时区信息，为什么要使用时间戳来指定呢？就是因为时间戳不带有时区信息，所以我们通过配置 <code>table.local-time-zone</code> 时区参数之后，就能将一个不带有时区信息的时间戳转换为带有时区信息的字符串了。举例：<code>table.local-time-zone</code> 为 <code>Asia/Shanghai</code> 时，4001 时间戳转化为字符串的效果是 <code>1970-01-01 08:00:04.001</code>。</li></ul><p>如果你还对时区问题有疑惑，可以参考博主写的一篇时区相关的文章。</p><p><a href="https://mp.weixin.qq.com/s/PSwHs18ZhKsBUaTsppkp9Q">https://mp.weixin.qq.com/s/PSwHs18ZhKsBUaTsppkp9Q</a></p><h3 id="2-6-2-时区参数生效的-SQL-时间函数"><a href="#2-6-2-时区参数生效的-SQL-时间函数" class="headerlink" title="2.6.2.时区参数生效的 SQL 时间函数"></a>2.6.2.时区参数生效的 SQL 时间函数</h3><p>以下 SQL 中的时间函数都会受到时区参数的影响，从而做到最后显示给用户的时间、窗口的划分都按照用户设置时区之内的时间。</p><ol><li>⭐ LOCALTIME</li><li>⭐ LOCALTIMESTAMP</li><li>⭐ CURRENT_DATE</li><li>⭐ CURRENT_TIME</li><li>⭐ CURRENT_TIMESTAMP</li><li>⭐ CURRENT_ROW_TIMESTAMP()</li><li>⭐ NOW()</li><li>⭐ PROCTIME()：其中 PROCTIME() 在 1.13 版本及之后版本，返回值类型是 TIMESTAMP_LTZ(3)</li></ol><p>在 Flink SQL client 中执行结果如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SET</span> <span class="keyword">sql</span><span class="operator">-</span>client.execution.result<span class="operator">-</span>mode<span class="operator">=</span>tableau;</span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">CREATE</span> <span class="keyword">VIEW</span> MyView1 <span class="keyword">AS</span> <span class="keyword">SELECT</span> <span class="built_in">LOCALTIME</span>, <span class="built_in">LOCALTIMESTAMP</span>, <span class="built_in">CURRENT_DATE</span>, <span class="built_in">CURRENT_TIME</span>, <span class="built_in">CURRENT_TIMESTAMP</span>, CURRENT_ROW_TIMESTAMP(), NOW(), PROCTIME();</span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">DESC</span> MyView1;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------+-----------------------------+-------+-----+--------+-----------+</span></span><br><span class="line"><span class="operator">|</span>                   name <span class="operator">|</span>                        type <span class="operator">|</span>  <span class="keyword">null</span> <span class="operator">|</span> key <span class="operator">|</span> extras <span class="operator">|</span> watermark <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------+-----------------------------+-------+-----+--------+-----------+</span></span><br><span class="line"><span class="operator">|</span>              <span class="built_in">LOCALTIME</span> <span class="operator">|</span>                     <span class="type">TIME</span>(<span class="number">0</span>) <span class="operator">|</span> <span class="literal">false</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>         <span class="built_in">LOCALTIMESTAMP</span> <span class="operator">|</span>                <span class="type">TIMESTAMP</span>(<span class="number">3</span>) <span class="operator">|</span> <span class="literal">false</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           <span class="built_in">CURRENT_DATE</span> <span class="operator">|</span>                        <span class="type">DATE</span> <span class="operator">|</span> <span class="literal">false</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           <span class="built_in">CURRENT_TIME</span> <span class="operator">|</span>                     <span class="type">TIME</span>(<span class="number">0</span>) <span class="operator">|</span> <span class="literal">false</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>      <span class="built_in">CURRENT_TIMESTAMP</span> <span class="operator">|</span>            TIMESTAMP_LTZ(<span class="number">3</span>) <span class="operator">|</span> <span class="literal">false</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>CURRENT_ROW_TIMESTAMP() <span class="operator">|</span>            TIMESTAMP_LTZ(<span class="number">3</span>) <span class="operator">|</span> <span class="literal">false</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                  NOW() <span class="operator">|</span>            TIMESTAMP_LTZ(<span class="number">3</span>) <span class="operator">|</span> <span class="literal">false</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             PROCTIME() <span class="operator">|</span> TIMESTAMP_LTZ(<span class="number">3</span>) <span class="operator">*</span>PROCTIME<span class="operator">*</span> <span class="operator">|</span> <span class="literal">false</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------+-----------------------------+-------+-----+--------+-----------+</span></span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SET</span> table.local<span class="operator">-</span><span class="type">time</span><span class="operator">-</span>zone<span class="operator">=</span>UTC;</span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> MyView1;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+-------------------------+--------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="built_in">LOCALTIME</span> <span class="operator">|</span>          <span class="built_in">LOCALTIMESTAMP</span> <span class="operator">|</span> <span class="built_in">CURRENT_DATE</span> <span class="operator">|</span> <span class="built_in">CURRENT_TIME</span> <span class="operator">|</span>       <span class="built_in">CURRENT_TIMESTAMP</span> <span class="operator">|</span> CURRENT_ROW_TIMESTAMP() <span class="operator">|</span>                   NOW() <span class="operator">|</span>              PROCTIME() <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+-------------------------+--------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+</span></span><br><span class="line"><span class="operator">|</span>  <span class="number">15</span>:<span class="number">18</span>:<span class="number">36</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">15</span>:<span class="number">18</span>:<span class="number">36.384</span> <span class="operator">|</span>   <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="operator">|</span>     <span class="number">15</span>:<span class="number">18</span>:<span class="number">36</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">15</span>:<span class="number">18</span>:<span class="number">36.384</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">15</span>:<span class="number">18</span>:<span class="number">36.384</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">15</span>:<span class="number">18</span>:<span class="number">36.384</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">15</span>:<span class="number">18</span>:<span class="number">36.384</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+-------------------------+--------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+</span></span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SET</span> table.local<span class="operator">-</span><span class="type">time</span><span class="operator">-</span>zone<span class="operator">=</span>Asia<span class="operator">/</span>Shanghai;</span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> MyView1;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+-------------------------+--------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="built_in">LOCALTIME</span> <span class="operator">|</span>          <span class="built_in">LOCALTIMESTAMP</span> <span class="operator">|</span> <span class="built_in">CURRENT_DATE</span> <span class="operator">|</span> <span class="built_in">CURRENT_TIME</span> <span class="operator">|</span>       <span class="built_in">CURRENT_TIMESTAMP</span> <span class="operator">|</span> CURRENT_ROW_TIMESTAMP() <span class="operator">|</span>                   NOW() <span class="operator">|</span>              PROCTIME() <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+-------------------------+--------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+</span></span><br><span class="line"><span class="operator">|</span>  <span class="number">23</span>:<span class="number">18</span>:<span class="number">36</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">23</span>:<span class="number">18</span>:<span class="number">36.384</span> <span class="operator">|</span>   <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="operator">|</span>     <span class="number">23</span>:<span class="number">18</span>:<span class="number">36</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">23</span>:<span class="number">18</span>:<span class="number">36.384</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">23</span>:<span class="number">18</span>:<span class="number">36.384</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">23</span>:<span class="number">18</span>:<span class="number">36.384</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">23</span>:<span class="number">18</span>:<span class="number">36.384</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+-------------------------+--------------+--------------+-------------------------+-------------------------+-------------------------+-------------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">CREATE</span> <span class="keyword">VIEW</span> MyView2 <span class="keyword">AS</span> <span class="keyword">SELECT</span> TO_TIMESTAMP_LTZ(<span class="number">4001</span>, <span class="number">3</span>) <span class="keyword">AS</span> ltz, <span class="type">TIMESTAMP</span> <span class="string">&#x27;1970-01-01 00:00:01.001&#x27;</span>  <span class="keyword">AS</span> ntz;</span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">DESC</span> MyView2;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">------+------------------+-------+-----+--------+-----------+</span></span><br><span class="line"><span class="operator">|</span> name <span class="operator">|</span>             type <span class="operator">|</span>  <span class="keyword">null</span> <span class="operator">|</span> key <span class="operator">|</span> extras <span class="operator">|</span> watermark <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+------------------+-------+-----+--------+-----------+</span></span><br><span class="line"><span class="operator">|</span>  ltz <span class="operator">|</span> TIMESTAMP_LTZ(<span class="number">3</span>) <span class="operator">|</span>  <span class="literal">true</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>  ntz <span class="operator">|</span>     <span class="type">TIMESTAMP</span>(<span class="number">3</span>) <span class="operator">|</span> <span class="literal">false</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+------------------+-------+-----+--------+-----------+</span></span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SET</span> table.local<span class="operator">-</span><span class="type">time</span><span class="operator">-</span>zone<span class="operator">=</span>UTC;</span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> MyView2;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     ltz <span class="operator">|</span>                     ntz <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1970</span><span class="number">-01</span><span class="number">-01</span> <span class="number">00</span>:<span class="number">00</span>:<span class="number">04.001</span> <span class="operator">|</span> <span class="number">1970</span><span class="number">-01</span><span class="number">-01</span> <span class="number">00</span>:<span class="number">00</span>:<span class="number">01.001</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+</span></span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SET</span> table.local<span class="operator">-</span><span class="type">time</span><span class="operator">-</span>zone<span class="operator">=</span>Asia<span class="operator">/</span>Shanghai;</span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> MyView2;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     ltz <span class="operator">|</span>                     ntz <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1970</span><span class="number">-01</span><span class="number">-01</span> <span class="number">08</span>:<span class="number">00</span>:<span class="number">04.001</span> <span class="operator">|</span> <span class="number">1970</span><span class="number">-01</span><span class="number">-01</span> <span class="number">00</span>:<span class="number">00</span>:<span class="number">01.001</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+</span></span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">CREATE</span> <span class="keyword">VIEW</span> MyView3 <span class="keyword">AS</span> <span class="keyword">SELECT</span> ltz, <span class="built_in">CAST</span>(ltz <span class="keyword">AS</span> <span class="type">TIMESTAMP</span>(<span class="number">3</span>)), <span class="built_in">CAST</span>(ltz <span class="keyword">AS</span> STRING), ntz, <span class="built_in">CAST</span>(ntz <span class="keyword">AS</span> TIMESTAMP_LTZ(<span class="number">3</span>)) <span class="keyword">FROM</span> MyView2;</span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">DESC</span> MyView3;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------------+------------------+-------+-----+--------+-----------+</span></span><br><span class="line"><span class="operator">|</span>                          name <span class="operator">|</span>             type <span class="operator">|</span>  <span class="keyword">null</span> <span class="operator">|</span> key <span class="operator">|</span> extras <span class="operator">|</span> watermark <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------------+------------------+-------+-----+--------+-----------+</span></span><br><span class="line"><span class="operator">|</span>                           ltz <span class="operator">|</span> TIMESTAMP_LTZ(<span class="number">3</span>) <span class="operator">|</span>  <span class="literal">true</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>     <span class="built_in">CAST</span>(ltz <span class="keyword">AS</span> <span class="type">TIMESTAMP</span>(<span class="number">3</span>)) <span class="operator">|</span>     <span class="type">TIMESTAMP</span>(<span class="number">3</span>) <span class="operator">|</span>  <span class="literal">true</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           <span class="built_in">CAST</span>(ltz <span class="keyword">AS</span> STRING) <span class="operator">|</span>           STRING <span class="operator">|</span>  <span class="literal">true</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                           ntz <span class="operator">|</span>     <span class="type">TIMESTAMP</span>(<span class="number">3</span>) <span class="operator">|</span> <span class="literal">false</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="built_in">CAST</span>(ntz <span class="keyword">AS</span> TIMESTAMP_LTZ(<span class="number">3</span>)) <span class="operator">|</span> TIMESTAMP_LTZ(<span class="number">3</span>) <span class="operator">|</span> <span class="literal">false</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------------+------------------+-------+-----+--------+-----------+</span></span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> MyView3;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+---------------------------+-------------------------+-------------------------+-------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     ltz <span class="operator">|</span> <span class="built_in">CAST</span>(ltz <span class="keyword">AS</span> <span class="type">TIMESTAMP</span>(<span class="number">3</span>)) <span class="operator">|</span>     <span class="built_in">CAST</span>(ltz <span class="keyword">AS</span> STRING) <span class="operator">|</span>                     ntz <span class="operator">|</span> <span class="built_in">CAST</span>(ntz <span class="keyword">AS</span> TIMESTAMP_LTZ(<span class="number">3</span>)) <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+---------------------------+-------------------------+-------------------------+-------------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1970</span><span class="number">-01</span><span class="number">-01</span> <span class="number">08</span>:<span class="number">00</span>:<span class="number">04.001</span> <span class="operator">|</span>   <span class="number">1970</span><span class="number">-01</span><span class="number">-01</span> <span class="number">08</span>:<span class="number">00</span>:<span class="number">04.001</span> <span class="operator">|</span> <span class="number">1970</span><span class="number">-01</span><span class="number">-01</span> <span class="number">08</span>:<span class="number">00</span>:<span class="number">04.001</span> <span class="operator">|</span> <span class="number">1970</span><span class="number">-01</span><span class="number">-01</span> <span class="number">00</span>:<span class="number">00</span>:<span class="number">01.001</span> <span class="operator">|</span>       <span class="number">1970</span><span class="number">-01</span><span class="number">-01</span> <span class="number">00</span>:<span class="number">00</span>:<span class="number">01.001</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+---------------------------+-------------------------+-------------------------+-------------------------------+</span></span><br></pre></td></tr></table></figure><h3 id="2-6-3-事件时间和时区应用案例"><a href="#2-6-3-事件时间和时区应用案例" class="headerlink" title="2.6.3.事件时间和时区应用案例"></a>2.6.3.事件时间和时区应用案例</h3><p>这里分两类，分别是 TIMESTAMP（不带时区信息的时间）、TIMESTAMP_LTZ（带时区信息的时间） 的事件时间 Flink SQL 任务</p><ol><li>⭐ TIMESTAMP（不带时区信息的时间）</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable2 (</span><br><span class="line">                  item STRING,</span><br><span class="line">                  price <span class="keyword">DOUBLE</span>,</span><br><span class="line">                  ts <span class="type">TIMESTAMP</span>(<span class="number">3</span>), <span class="comment">-- TIMESTAMP 类型的时间戳</span></span><br><span class="line">                  WATERMARK <span class="keyword">FOR</span> ts <span class="keyword">AS</span> ts <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">            ) <span class="keyword">WITH</span> (</span><br><span class="line">                <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;socket&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;hostname&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;127.0.0.1&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;port&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;9999&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;csv&#x27;</span></span><br><span class="line">           );</span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">CREATE</span> <span class="keyword">VIEW</span> MyView4 <span class="keyword">AS</span></span><br><span class="line">            <span class="keyword">SELECT</span></span><br><span class="line">                TUMBLE_START(ts, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> MINUTES) <span class="keyword">AS</span> window_start,</span><br><span class="line">                TUMBLE_END(ts, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> MINUTES) <span class="keyword">AS</span> window_end,</span><br><span class="line">                TUMBLE_ROWTIME(ts, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> MINUTES) <span class="keyword">as</span> window_rowtime,</span><br><span class="line">                item,</span><br><span class="line">                <span class="built_in">MAX</span>(price) <span class="keyword">as</span> max_price</span><br><span class="line">            <span class="keyword">FROM</span> MyTable2</span><br><span class="line">                <span class="keyword">GROUP</span> <span class="keyword">BY</span> TUMBLE(ts, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> MINUTES), item;</span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">DESC</span> MyView4;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+------------------------+------+-----+--------+-----------+</span></span><br><span class="line"><span class="operator">|</span>           name <span class="operator">|</span>                   type <span class="operator">|</span> <span class="keyword">null</span> <span class="operator">|</span> key <span class="operator">|</span> extras <span class="operator">|</span> watermark <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+------------------------+------+-----+--------+-----------+</span></span><br><span class="line"><span class="operator">|</span>   window_start <span class="operator">|</span>           <span class="type">TIMESTAMP</span>(<span class="number">3</span>) <span class="operator">|</span> <span class="literal">true</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>     window_end <span class="operator">|</span>           <span class="type">TIMESTAMP</span>(<span class="number">3</span>) <span class="operator">|</span> <span class="literal">true</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> window_rowtime <span class="operator">|</span> <span class="type">TIMESTAMP</span>(<span class="number">3</span>) <span class="operator">*</span>ROWTIME<span class="operator">*</span> <span class="operator">|</span> <span class="literal">true</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           item <span class="operator">|</span>                 STRING <span class="operator">|</span> <span class="literal">true</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>      max_price <span class="operator">|</span>                 <span class="keyword">DOUBLE</span> <span class="operator">|</span> <span class="literal">true</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+------------------------+------+-----+--------+-----------+</span></span><br></pre></td></tr></table></figure><p>将数据写入到 MyTable2 中：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> nc -lk 9999</span></span><br><span class="line">A,1.1,2021-04-15 14:01:00</span><br><span class="line">B,1.2,2021-04-15 14:02:00</span><br><span class="line">A,1.8,2021-04-15 14:03:00 </span><br><span class="line">B,2.5,2021-04-15 14:04:00</span><br><span class="line">C,3.8,2021-04-15 14:05:00       </span><br><span class="line">C,3.8,2021-04-15 14:11:00</span><br></pre></td></tr></table></figure><p>最终结果如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SET</span> table.local<span class="operator">-</span><span class="type">time</span><span class="operator">-</span>zone<span class="operator">=</span>UTC; </span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> MyView4;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+-------------------------+------+-----------+</span></span><br><span class="line"><span class="operator">|</span>            window_start <span class="operator">|</span>              window_end <span class="operator">|</span>          window_rowtime <span class="operator">|</span> item <span class="operator">|</span> max_price <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+-------------------------+------+-----------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">00</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">10</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">09</span>:<span class="number">59.999</span> <span class="operator">|</span>    A <span class="operator">|</span>       <span class="number">1.8</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">00</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">10</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">09</span>:<span class="number">59.999</span> <span class="operator">|</span>    B <span class="operator">|</span>       <span class="number">2.5</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">00</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">10</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">09</span>:<span class="number">59.999</span> <span class="operator">|</span>    C <span class="operator">|</span>       <span class="number">3.8</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+-------------------------+------+-----------+</span></span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SET</span> table.local<span class="operator">-</span><span class="type">time</span><span class="operator">-</span>zone<span class="operator">=</span>Asia<span class="operator">/</span>Shanghai; </span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> MyView4;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+-------------------------+------+-----------+</span></span><br><span class="line"><span class="operator">|</span>            window_start <span class="operator">|</span>              window_end <span class="operator">|</span>          window_rowtime <span class="operator">|</span> item <span class="operator">|</span> max_price <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+-------------------------+------+-----------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">00</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">10</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">09</span>:<span class="number">59.999</span> <span class="operator">|</span>    A <span class="operator">|</span>       <span class="number">1.8</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">00</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">10</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">09</span>:<span class="number">59.999</span> <span class="operator">|</span>    B <span class="operator">|</span>       <span class="number">2.5</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">00</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">10</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">09</span>:<span class="number">59.999</span> <span class="operator">|</span>    C <span class="operator">|</span>       <span class="number">3.8</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+-------------------------+------+-----------+</span></span><br></pre></td></tr></table></figure><p>通过上述结果可见，使用 TIMESTAMP（不带时区信息的时间） 进开窗，在 UTC 时区下的计算结果与在 Asia/Shanghai 时区下计算的窗口开始时间，窗口结束时间和窗口的时间是相同的。</p><ol start="2"><li>⭐ TIMESTAMP_LTZ（带时区信息的时间）</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable3 (</span><br><span class="line">                  item STRING,</span><br><span class="line">                  price <span class="keyword">DOUBLE</span>,</span><br><span class="line">                  ts <span class="type">BIGINT</span>, <span class="comment">-- long 类型的时间戳</span></span><br><span class="line">                  ts_ltz <span class="keyword">AS</span> TO_TIMESTAMP_LTZ(ts, <span class="number">3</span>), <span class="comment">-- 转为 TIMESTAMP_LTZ 类型的时间戳</span></span><br><span class="line">                  WATERMARK <span class="keyword">FOR</span> ts_ltz <span class="keyword">AS</span> ts_ltz <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">            ) <span class="keyword">WITH</span> (</span><br><span class="line">                <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;socket&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;hostname&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;127.0.0.1&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;port&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;9999&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;csv&#x27;</span></span><br><span class="line">           );</span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">CREATE</span> <span class="keyword">VIEW</span> MyView5 <span class="keyword">AS</span> </span><br><span class="line">            <span class="keyword">SELECT</span> </span><br><span class="line">                TUMBLE_START(ts_ltz, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> MINUTES) <span class="keyword">AS</span> window_start,        </span><br><span class="line">                TUMBLE_END(ts_ltz, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> MINUTES) <span class="keyword">AS</span> window_end,</span><br><span class="line">                TUMBLE_ROWTIME(ts_ltz, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> MINUTES) <span class="keyword">as</span> window_rowtime,</span><br><span class="line">                item,</span><br><span class="line">                <span class="built_in">MAX</span>(price) <span class="keyword">as</span> max_price</span><br><span class="line">            <span class="keyword">FROM</span> MyTable3</span><br><span class="line">                <span class="keyword">GROUP</span> <span class="keyword">BY</span> TUMBLE(ts_ltz, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> MINUTES), item;</span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">DESC</span> MyView5;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+----------------------------+-------+-----+--------+-----------+</span></span><br><span class="line"><span class="operator">|</span>           name <span class="operator">|</span>                       type <span class="operator">|</span>  <span class="keyword">null</span> <span class="operator">|</span> key <span class="operator">|</span> extras <span class="operator">|</span> watermark <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+----------------------------+-------+-----+--------+-----------+</span></span><br><span class="line"><span class="operator">|</span>   window_start <span class="operator">|</span>               <span class="type">TIMESTAMP</span>(<span class="number">3</span>) <span class="operator">|</span> <span class="literal">false</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>     window_end <span class="operator">|</span>               <span class="type">TIMESTAMP</span>(<span class="number">3</span>) <span class="operator">|</span> <span class="literal">false</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> window_rowtime <span class="operator">|</span> TIMESTAMP_LTZ(<span class="number">3</span>) <span class="operator">*</span>ROWTIME<span class="operator">*</span> <span class="operator">|</span>  <span class="literal">true</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           item <span class="operator">|</span>                     STRING <span class="operator">|</span>  <span class="literal">true</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>      max_price <span class="operator">|</span>                     <span class="keyword">DOUBLE</span> <span class="operator">|</span>  <span class="literal">true</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+----------------------------+-------+-----+--------+-----------+</span></span><br></pre></td></tr></table></figure><p>将数据写入 MyTable3：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">A,1.1,1618495260000  # 对应到 UTC 时区的时间为 2021-04-15 14:01:00</span><br><span class="line">B,1.2,1618495320000  # 对应到 UTC 时区的时间为 2021-04-15 14:02:00</span><br><span class="line">A,1.8,1618495380000  # 对应到 UTC 时区的时间为 2021-04-15 14:03:00</span><br><span class="line">B,2.5,1618495440000  # 对应到 UTC 时区的时间为 2021-04-15 14:04:00</span><br><span class="line">C,3.8,1618495500000  # 对应到 UTC 时区的时间为 2021-04-15 14:05:00       </span><br><span class="line">C,3.8,1618495860000  # 对应到 UTC 时区的时间为 2021-04-15 14:11:00</span><br></pre></td></tr></table></figure><p>最终结果如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SET</span> table.local<span class="operator">-</span><span class="type">time</span><span class="operator">-</span>zone<span class="operator">=</span>UTC; </span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> MyView5;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+-------------------------+------+-----------+</span></span><br><span class="line"><span class="operator">|</span>            window_start <span class="operator">|</span>              window_end <span class="operator">|</span>          window_rowtime <span class="operator">|</span> item <span class="operator">|</span> max_price <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+-------------------------+------+-----------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">00</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">10</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">09</span>:<span class="number">59.999</span> <span class="operator">|</span>    A <span class="operator">|</span>       <span class="number">1.8</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">00</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">10</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">09</span>:<span class="number">59.999</span> <span class="operator">|</span>    B <span class="operator">|</span>       <span class="number">2.5</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">00</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">10</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">09</span>:<span class="number">59.999</span> <span class="operator">|</span>    C <span class="operator">|</span>       <span class="number">3.8</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+-------------------------+------+-----------+</span></span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SET</span> table.local<span class="operator">-</span><span class="type">time</span><span class="operator">-</span>zone<span class="operator">=</span>Asia<span class="operator">/</span>Shanghai; </span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> MyView5;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+-------------------------+------+-----------+</span></span><br><span class="line"><span class="operator">|</span>            window_start <span class="operator">|</span>              window_end <span class="operator">|</span>          window_rowtime <span class="operator">|</span> item <span class="operator">|</span> max_price <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+-------------------------+------+-----------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">22</span>:<span class="number">00</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">22</span>:<span class="number">10</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">22</span>:<span class="number">09</span>:<span class="number">59.999</span> <span class="operator">|</span>    A <span class="operator">|</span>       <span class="number">1.8</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">22</span>:<span class="number">00</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">22</span>:<span class="number">10</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">22</span>:<span class="number">09</span>:<span class="number">59.999</span> <span class="operator">|</span>    B <span class="operator">|</span>       <span class="number">2.5</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">22</span>:<span class="number">00</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">22</span>:<span class="number">10</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">22</span>:<span class="number">09</span>:<span class="number">59.999</span> <span class="operator">|</span>    C <span class="operator">|</span>       <span class="number">3.8</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+-------------------------+------+-----------+</span></span><br></pre></td></tr></table></figure><p>通过上述结果可见，使用 TIMESTAMP_LTZ（带时区信息的时间） 进开窗，在 UTC 时区下的计算结果与在 Asia/Shanghai 时区下计算的窗口开始时间，窗口结束时间和窗口的时间是不同的，都是按照时区进行格式化的。</p><h3 id="2-6-4-处理时间和时区应用案例"><a href="#2-6-4-处理时间和时区应用案例" class="headerlink" title="2.6.4.处理时间和时区应用案例"></a>2.6.4.处理时间和时区应用案例</h3><p>Flink SQL 定义处理时间属性列是通过 <code>PROCTIME()</code> 函数来指定的，其返回值类型是 TIMESTAMP_LTZ。</p><blockquote><p>注意：</p><p>在 Flink 1.13 之前，<code>PROCTIME()</code> 函数返回类型是 TIMESTAMP，返回值是 UTC 时区的时间戳，例如，上海时间显示为 2021-03-01 12:00:00 时，PROCTIME() 返回值显示 2021-03-01 04:00:00，我们进行使用是错误的。Flink 1.13 修复了这个问题，使用 TIMESTAMP_LTZ 作为 PROCTIME() 的返回类型，这样 Flink 就会自动获取当前时区信息，然后进行处理，不需要用户再进行时区的格式化处理了。</p></blockquote><p>如下案例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SET</span> table.local<span class="operator">-</span><span class="type">time</span><span class="operator">-</span>zone<span class="operator">=</span>UTC;</span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SELECT</span> PROCTIME();</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+</span></span><br><span class="line"><span class="operator">|</span>              PROCTIME() <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">48</span>:<span class="number">31.387</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+</span></span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SET</span> table.local<span class="operator">-</span><span class="type">time</span><span class="operator">-</span>zone<span class="operator">=</span>Asia<span class="operator">/</span>Shanghai;</span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SELECT</span> PROCTIME();</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+</span></span><br><span class="line"><span class="operator">|</span>              PROCTIME() <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">22</span>:<span class="number">48</span>:<span class="number">31.387</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+</span></span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable1 (</span><br><span class="line">                  item STRING,</span><br><span class="line">                  price <span class="keyword">DOUBLE</span>,</span><br><span class="line">                  proctime <span class="keyword">as</span> PROCTIME()</span><br><span class="line">            ) <span class="keyword">WITH</span> (</span><br><span class="line">                <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;socket&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;hostname&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;127.0.0.1&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;port&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;9999&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;csv&#x27;</span></span><br><span class="line">           );</span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">CREATE</span> <span class="keyword">VIEW</span> MyView3 <span class="keyword">AS</span></span><br><span class="line">            <span class="keyword">SELECT</span></span><br><span class="line">                TUMBLE_START(proctime, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> MINUTES) <span class="keyword">AS</span> window_start,</span><br><span class="line">                TUMBLE_END(proctime, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> MINUTES) <span class="keyword">AS</span> window_end,</span><br><span class="line">                TUMBLE_PROCTIME(proctime, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> MINUTES) <span class="keyword">as</span> window_proctime,</span><br><span class="line">                item,</span><br><span class="line">                <span class="built_in">MAX</span>(price) <span class="keyword">as</span> max_price</span><br><span class="line">            <span class="keyword">FROM</span> MyTable1</span><br><span class="line">                <span class="keyword">GROUP</span> <span class="keyword">BY</span> TUMBLE(proctime, <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> MINUTES), item;</span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">DESC</span> MyView3;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------+-----------------------------+-------+-----+--------+-----------+</span></span><br><span class="line"><span class="operator">|</span>           name  <span class="operator">|</span>                        type <span class="operator">|</span>  <span class="keyword">null</span> <span class="operator">|</span> key <span class="operator">|</span> extras <span class="operator">|</span> watermark <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------+-----------------------------+-------+-----+--------+-----------+</span></span><br><span class="line"><span class="operator">|</span>    window_start <span class="operator">|</span>                <span class="type">TIMESTAMP</span>(<span class="number">3</span>) <span class="operator">|</span> <span class="literal">false</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>      window_end <span class="operator">|</span>                <span class="type">TIMESTAMP</span>(<span class="number">3</span>) <span class="operator">|</span> <span class="literal">false</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> window_proctime <span class="operator">|</span> TIMESTAMP_LTZ(<span class="number">3</span>) <span class="operator">*</span>PROCTIME<span class="operator">*</span> <span class="operator">|</span> <span class="literal">false</span> <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>            item <span class="operator">|</span>                      STRING <span class="operator">|</span> <span class="literal">true</span>  <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       max_price <span class="operator">|</span>                      <span class="keyword">DOUBLE</span> <span class="operator">|</span> <span class="literal">true</span>  <span class="operator">|</span>     <span class="operator">|</span>        <span class="operator">|</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------+-----------------------------+-------+-----+--------+-----------+</span></span><br></pre></td></tr></table></figure><p>将数据写入到 MyTable1 中：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> nc -lk 9999</span></span><br><span class="line">A,1.1</span><br><span class="line">B,1.2</span><br><span class="line">A,1.8</span><br><span class="line">B,2.5</span><br><span class="line">C,3.8</span><br></pre></td></tr></table></figure><p>其输出结果如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SET</span> table.local<span class="operator">-</span><span class="type">time</span><span class="operator">-</span>zone<span class="operator">=</span>UTC;</span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> MyView3;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+-------------------------+------+-----------+</span></span><br><span class="line"><span class="operator">|</span>            window_start <span class="operator">|</span>              window_end <span class="operator">|</span>          window_procime <span class="operator">|</span> item <span class="operator">|</span> max_price <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+-------------------------+------+-----------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">00</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">10</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">10</span>:<span class="number">00.005</span> <span class="operator">|</span>    A <span class="operator">|</span>       <span class="number">1.8</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">00</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">10</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">10</span>:<span class="number">00.007</span> <span class="operator">|</span>    B <span class="operator">|</span>       <span class="number">2.5</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">00</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">10</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">14</span>:<span class="number">10</span>:<span class="number">00.007</span> <span class="operator">|</span>    C <span class="operator">|</span>       <span class="number">3.8</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+-------------------------+------+-----------+</span></span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SET</span> table.local<span class="operator">-</span><span class="type">time</span><span class="operator">-</span>zone<span class="operator">=</span>Asia<span class="operator">/</span>Shanghai;</span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> MyView3;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+-------------------------+------+-----------+</span></span><br><span class="line"><span class="operator">|</span>            window_start <span class="operator">|</span>              window_end <span class="operator">|</span>          window_procime <span class="operator">|</span> item <span class="operator">|</span> max_price <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+-------------------------+------+-----------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">22</span>:<span class="number">00</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">22</span>:<span class="number">10</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">22</span>:<span class="number">10</span>:<span class="number">00.005</span> <span class="operator">|</span>    A <span class="operator">|</span>       <span class="number">1.8</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">22</span>:<span class="number">00</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">22</span>:<span class="number">10</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">22</span>:<span class="number">10</span>:<span class="number">00.007</span> <span class="operator">|</span>    B <span class="operator">|</span>       <span class="number">2.5</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">22</span>:<span class="number">00</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">22</span>:<span class="number">10</span>:<span class="number">00.000</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-04</span><span class="number">-15</span> <span class="number">22</span>:<span class="number">10</span>:<span class="number">00.007</span> <span class="operator">|</span>    C <span class="operator">|</span>       <span class="number">3.8</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+-------------------------+-------------------------+------+-----------+</span></span><br></pre></td></tr></table></figure><p>通过上述结果可见，使用处理时间进行开窗，在 UTC 时区下的计算结果与在 Asia/Shanghai 时区下计算的窗口开始时间，窗口结束时间和窗口的时间是不同的，都是按照时区进行格式化的。</p><h3 id="2-6-5-SQL-时间函数返回在流批任务中的异同"><a href="#2-6-5-SQL-时间函数返回在流批任务中的异同" class="headerlink" title="2.6.5.SQL 时间函数返回在流批任务中的异同"></a>2.6.5.SQL 时间函数返回在流批任务中的异同</h3><p>以下函数：</p><ol><li>⭐ LOCALTIME</li><li>⭐ LOCALTIMESTAMP</li><li>⭐ CURRENT_DATE</li><li>⭐ CURRENT_TIME</li><li>⭐ CURRENT_TIMESTAMP</li><li>⭐ NOW()</li></ol><p>在 Streaming 模式下这些函数是每条记录都会计算一次，但在 Batch 模式下，只会在 query 开始时计算一次，所有记录都使用相同的时间结果。</p><p>以下时间函数无论是在 Streaming 模式还是 Batch 模式下，都会为每条记录计算一次结果：</p><ol><li>⭐ CURRENT_ROW_TIMESTAMP()</li><li>⭐ PROCTIME()</li></ol><h1 id="3-SQL-语法篇"><a href="#3-SQL-语法篇" class="headerlink" title="3.SQL 语法篇"></a>3.SQL 语法篇</h1><h2 id="3-1-DDL：Create-子句"><a href="#3-1-DDL：Create-子句" class="headerlink" title="3.1.DDL：Create 子句"></a>3.1.DDL：Create 子句</h2><p>CREATE 语句用于向当前或指定的 Catalog 中注册库、表、视图或函数。注册后的库、表、视图和函数可以在 SQL 查询中使用。</p><p>目前 Flink SQL 支持下列 CREATE 语句：</p><ol><li>⭐ CREATE TABLE</li><li>⭐ CREATE DATABASE</li><li>⭐ CREATE VIEW</li><li>⭐ CREATE FUNCTION</li></ol><p>此节重点介绍建表，建数据库、视图和 UDF 会在后面的扩展章节进行介绍。</p><h3 id="3-1-1-建表语句"><a href="#3-1-1-建表语句" class="headerlink" title="3.1.1.建表语句"></a>3.1.1.建表语句</h3><p>下面的 SQL 语句就是建表语句的定义，根据指定的表名创建一个表，如果同名表已经在 catalog 中存在了，则无法注册。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [catalog_name.][db_name.]table_name</span><br><span class="line">  (</span><br><span class="line">    &#123; &lt;physical_column_definition&gt; | &lt;metadata_column_definition&gt; | &lt;computed_column_definition&gt; &#125;[ , ...n]</span><br><span class="line">    [ <span class="operator">&lt;</span>watermark_definition<span class="operator">&gt;</span> ]</span><br><span class="line">    [ <span class="operator">&lt;</span>table_constraint<span class="operator">&gt;</span> ][ , ...n]</span><br><span class="line">  )</span><br><span class="line">  [COMMENT table_comment]</span><br><span class="line">  [PARTITIONED <span class="keyword">BY</span> (partition_column_name1, partition_column_name2, ...)]</span><br><span class="line">  <span class="keyword">WITH</span> (key1<span class="operator">=</span>val1, key2<span class="operator">=</span>val2, ...)</span><br><span class="line">  [ <span class="keyword">LIKE</span> source_table [( <span class="operator">&lt;</span>like_options<span class="operator">&gt;</span> )] ]</span><br><span class="line">   </span><br><span class="line"><span class="operator">&lt;</span>physical_column_definition<span class="operator">&gt;</span>:</span><br><span class="line">  column_name column_type [ <span class="operator">&lt;</span>column_constraint<span class="operator">&gt;</span> ] [COMMENT column_comment]</span><br><span class="line">  </span><br><span class="line"><span class="operator">&lt;</span>column_constraint<span class="operator">&gt;</span>:</span><br><span class="line">  [<span class="keyword">CONSTRAINT</span> constraint_name] <span class="keyword">PRIMARY</span> KEY <span class="keyword">NOT</span> ENFORCED</span><br><span class="line"></span><br><span class="line"><span class="operator">&lt;</span>table_constraint<span class="operator">&gt;</span>:</span><br><span class="line">  [<span class="keyword">CONSTRAINT</span> constraint_name] <span class="keyword">PRIMARY</span> KEY (column_name, ...) <span class="keyword">NOT</span> ENFORCED</span><br><span class="line"></span><br><span class="line"><span class="operator">&lt;</span>metadata_column_definition<span class="operator">&gt;</span>:</span><br><span class="line">  column_name column_type METADATA [ <span class="keyword">FROM</span> metadata_key ] [ VIRTUAL ]</span><br><span class="line"></span><br><span class="line"><span class="operator">&lt;</span>computed_column_definition<span class="operator">&gt;</span>:</span><br><span class="line">  column_name <span class="keyword">AS</span> computed_column_expression [COMMENT column_comment]</span><br><span class="line"></span><br><span class="line"><span class="operator">&lt;</span>watermark_definition<span class="operator">&gt;</span>:</span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> rowtime_column_name <span class="keyword">AS</span> watermark_strategy_expression</span><br><span class="line"></span><br><span class="line"><span class="operator">&lt;</span>source_table<span class="operator">&gt;</span>:</span><br><span class="line">  [catalog_name.][db_name.]table_name</span><br><span class="line"></span><br><span class="line"><span class="operator">&lt;</span>like_options<span class="operator">&gt;</span>:</span><br><span class="line">&#123;</span><br><span class="line">   &#123; INCLUDING | EXCLUDING &#125; &#123; ALL | CONSTRAINTS | PARTITIONS &#125;</span><br><span class="line"> | &#123; INCLUDING | EXCLUDING | OVERWRITING &#125; &#123; GENERATED | OPTIONS | WATERMARKS &#125; </span><br><span class="line">&#125;[, ...]</span><br></pre></td></tr></table></figure><h3 id="3-1-2-表中的列"><a href="#3-1-2-表中的列" class="headerlink" title="3.1.2.表中的列"></a>3.1.2.表中的列</h3><ol><li>⭐ 常规列（即物理列）</li></ol><p>物理列是数据库中所说的常规列。其定义了物理介质中存储的数据中字段的名称、类型和顺序。</p><p>其他类型的列可以在物理列之间声明，但不会影响最终的物理列的读取。</p><p>举一个仅包含常规列的表的案例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line">  `user_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `name` STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">);</span><br></pre></td></tr></table></figure><ol start="2"><li>⭐ 元数据列</li></ol><p>元数据列是 SQL 标准的扩展，允许访问数据源本身具有的一些元数据。元数据列由 <code>METADATA</code> 关键字标识。</p><p>例如，我们可以使用元数据列从 Kafka 数据中读取 Kafka 数据自带的时间戳（这个时间戳不是数据中的某个时间戳字段，而是数据写入 Kafka 时，Kafka 引擎给这条数据打上的时间戳标记），然后我们可以在 Flink SQL 中使用这个时间戳，比如进行基于时间的窗口操作。</p><p>举例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line">  `user_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `name` STRING,</span><br><span class="line">  <span class="comment">-- 读取 kafka 本身自带的时间戳</span></span><br><span class="line">  `record_time` TIMESTAMP_LTZ(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">&#x27;timestamp&#x27;</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span></span><br><span class="line">  ...</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>元数据列可以用于后续数据的处理，或者写入到目标表中。</p><p>举例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> MyTable </span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    user_id</span><br><span class="line">    , name</span><br><span class="line">    , record_time <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">SECOND</span> </span><br><span class="line"><span class="keyword">FROM</span> MyTable;</span><br></pre></td></tr></table></figure><p>如果自定义的列名称和 Connector 中定义 metadata 字段的名称一样的话，<code>FROM xxx</code> 子句是可以被省略的。</p><p>举例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line">  `user_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `name` STRING,</span><br><span class="line">  <span class="comment">-- 读取 kafka 本身自带的时间戳</span></span><br><span class="line">  `<span class="type">timestamp</span>` TIMESTAMP_LTZ(<span class="number">3</span>) METADATA</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span></span><br><span class="line">  ...</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>关于 Flink SQL 的每种 Connector 都提供了哪些 metadata 字段，详细可见官网文档 <a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/connectors/table/overview/">https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/connectors/table/overview/</a></p><p>如果自定义列的数据类型和 Connector 中定义的 metadata 字段的数据类型不一致的话，程序运行时会自动 cast 强转。但是这要求两种数据类型是可以强转的。举例如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line">  `user_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `name` STRING,</span><br><span class="line">  <span class="comment">-- 将时间戳强转为 BIGINT</span></span><br><span class="line">  `<span class="type">timestamp</span>` <span class="type">BIGINT</span> METADATA</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span></span><br><span class="line">  ...</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>默认情况下，Flink SQL planner 认为 metadata 列是可以 <code>读取</code> 也可以 <code>写入</code> 的。但是有些外部存储系统的元数据信息是只能用于读取，不能写入的。</p><p>那么在往一个表写入的场景下，我们就可以使用 <code>VIRTUAL</code> 关键字来标识某个元数据列不写入到外部存储中（不持久化）。</p><p>以 Kafka 举例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line">  <span class="comment">-- sink 时会写入</span></span><br><span class="line">  `<span class="type">timestamp</span>` <span class="type">BIGINT</span> METADATA,</span><br><span class="line">  <span class="comment">-- sink 时不写入</span></span><br><span class="line">  `<span class="keyword">offset</span>` <span class="type">BIGINT</span> METADATA VIRTUAL,</span><br><span class="line">  `user_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `name` STRING,</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span></span><br><span class="line">  ...</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>在上面这个案例中，Kafka 引擎的 <code>offset</code> 是只读的。所以我们在把 <code>MyTable</code> 作为数据源（输入）表时，schema 中是包含 <code>offset</code> 的。在把 <code>MyTable</code> 作为数据汇（输出）表时，schema 中是不包含 <code>offset</code> 的。如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-- 当做数据源（输入）的 schema</span><br><span class="line">MyTable(&#96;timestamp&#96; BIGINT, &#96;offset&#96; BIGINT, &#96;user_id&#96; BIGINT, &#96;name&#96; STRING)</span><br><span class="line"></span><br><span class="line">-- 当做数据汇（输出）的 schema</span><br><span class="line">MyTable(&#96;timestamp&#96; BIGINT, &#96;user_id&#96; BIGINT, &#96;name&#96; STRING)</span><br></pre></td></tr></table></figure><p>所以这里在写入时需要注意，不要在 SQL 的 INSERT INTO 语句中写入 <code>offset</code> 列，否则 Flink SQL 任务会直接报错。</p><ol start="3"><li>⭐ 计算列</li></ol><p>计算列其实就是在写建表的 DDL 时，可以拿已有的一些列经过一些自定义的运算生成的新列。这些列本身是没有以物理形式存储到数据源中的。</p><p>举例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line">  `user_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `price` <span class="keyword">DOUBLE</span>,</span><br><span class="line">  `quantity` <span class="keyword">DOUBLE</span>,</span><br><span class="line">  <span class="comment">-- cost 就是使用 price 和 quanitity 生成的计算列，计算方式为 price * quanitity</span></span><br><span class="line">  `cost` <span class="keyword">AS</span> price <span class="operator">*</span> quanitity,</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span></span><br><span class="line">  ...</span><br><span class="line">);</span><br></pre></td></tr></table></figure><blockquote><p>注意！！！</p><p>计算列可以包含其他列、常量或者函数，但是不能写一个子查询进去。</p></blockquote><p>小伙伴萌这时会问到一个问题，既然只能包含列、常量或者函数计算，我就直接在 DML query 代码中写就完事了呗，为啥还要专门在 DDL 中定义呢？</p><p>结论：没错，如果只是简单的四则运算的话直接写在 DML 中就可以，但是计算列一般是用于定义时间属性的（因为在 SQL 任务中时间属性只能在 DDL 中定义，不能在 DML 语句中定义）。比如要把输入数据的时间格式标准化。处理时间、事件时间分别举例如下：</p><ul><li><p>⭐ 处理时间：使用 <code>PROCTIME()</code> 函数来定义处理时间列</p></li><li><p>⭐ 事件时间：事件时间的时间戳可以在声明 Watermark 之前进行预处理。比如如果字段不是 TIMESTAMP(3) 类型或者时间戳是嵌套在 JSON 字符串中的，则可以使用计算列进行预处理。</p></li></ul><p>注意！！!和虚拟 metadata 列是类似的，计算列也是只能读不能写的。</p><p>也就是说，我们在把 <code>MyTable</code> 作为数据源（输入）表时，schema 中是包含 <code>cost</code> 的。</p><p>在把 <code>MyTable</code> 作为数据汇（输出）表时，schema 中是不包含 <code>cost</code> 的。举例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 当做数据源（输入）的 schema</span></span><br><span class="line">MyTable(`user_id` <span class="type">BIGINT</span>, `price` <span class="keyword">DOUBLE</span>, `quantity` <span class="keyword">DOUBLE</span>, `cost` <span class="keyword">DOUBLE</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 当做数据汇（输出）的 schema</span></span><br><span class="line">MyTable(`user_id` <span class="type">BIGINT</span>, `price` <span class="keyword">DOUBLE</span>, `quantity` <span class="keyword">DOUBLE</span>)</span><br></pre></td></tr></table></figure><h3 id="3-1-3-定义-Watermark"><a href="#3-1-3-定义-Watermark" class="headerlink" title="3.1.3.定义 Watermark"></a>3.1.3.定义 Watermark</h3><p>Watermark 是在 <code>Create Table</code> 中进行定义的。具体 SQL 语法标准是 <code>WATERMARK FOR rowtime_column_name AS watermark_strategy_expression</code>。</p><p>其中：</p><ol><li>⭐ <code>rowtime_column_name</code>：表的事件时间属性字段。该列必须是 <code>TIMESTAMP(3)</code>、<code>TIMESTAMP_LTZ(3)</code> 类，这个时间可以是一个计算列。</li><li>⭐ <code>watermark_strategy_expression</code>：定义 Watermark 的生成策略。Watermark 的一般都是由 <code>rowtime_column_name</code> 列减掉一段固定时间间隔。SQL 中 Watermark 的生产策略是：当前 Watermark 大于上次发出的 Watermark 时发出当前 Watermark。</li></ol><blockquote><p>注意：</p><ol><li>如果你使用的是事件时间语义，那么必须要设设置事件时间属性和 WATERMARK 生成策略。</li><li>Watermark 的发出频率：Watermark 发出一般是间隔一定时间的，Watermark 的发出间隔时间可以由 <code>pipeline.auto-watermark-interval</code> 进行配置，如果设置为 200ms 则每 200ms 会计算一次 Watermark，然如果比之前发出的 Watermark 大，则发出。如果间隔设为 0ms，则 Watermark 只要满足触发条件就会发出，不会受到间隔时间控制。</li></ol></blockquote><p>Flink SQL 提供了几种 WATERMARK 生产策略：</p><ol><li>⭐ 有界无序：设置方式为 <code>WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL &#39;string&#39; timeUnit</code>。此类策略就可以用于设置最大乱序时间，假如设置为 <code>WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL &#39;5&#39; SECOND</code>，则生成的是运行 5s 延迟的 Watermark。。<code>一般都用这种 Watermark 生成策略</code>，此类 Watermark 生成策略通常用于有数据乱序的场景中，而对应到实际的场景中，数据都是会存在乱序的，所以基本都使用此类策略。</li><li>⭐ 严格升序：设置方式为 <code>WATERMARK FOR rowtime_column AS rowtime_column</code>。<code>一般基本不用这种方式</code>。如果你能保证你的数据源的时间戳是严格升序的，那就可以使用这种方式。严格升序代表 Flink 任务认为时间戳只会越来越大，也不存在相等的情况，只要相等或者小于之前的，就认为是迟到的数据。</li><li>⭐ 递增：设置方式为 <code>WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL &#39;0.001&#39; SECOND</code>。<code>一般基本不用这种方式</code>。如果设置此类，则允许有相同的时间戳出现。</li></ol><h3 id="3-1-4-Create-Table-With-子句"><a href="#3-1-4-Create-Table-With-子句" class="headerlink" title="3.1.4.Create Table With 子句"></a>3.1.4.Create Table With 子句</h3><p>先看一个案例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> KafkaTable (</span><br><span class="line">  `user_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `item_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `behavior` STRING,</span><br><span class="line">  `ts` <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">&#x27;timestamp&#x27;</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;user_behavior&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;scan.startup.mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;earliest-offset&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;csv&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>可以看到 DDL 中 With 子句就是在建表时，描述数据源、数据汇的具体外部存储的元数据信息的。</p><p>一般 With 中的配置项由 Flink SQL 的 Connector（链接外部存储的连接器） 来定义，每种 Connector 提供的 With 配置项都是不同的。</p><blockquote><p>注意：</p><ol><li>Flink SQL 中 Connector 其实就是 Flink 用于链接外部数据源的接口。举一个类似的例子，在 Java 中想连接到 MySQL，需要使用 mysql-connector-java 包提供的 Java API 去链接。映射到 Flink SQL 中，在 Flink SQL 中要连接到 Kafka，需要使用 kafka connector</li><li>Flink SQL 已经提供了一系列的内置 Connector，具体可见 <a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/connectors/table/overview/">https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/connectors/table/overview/</a></li></ol></blockquote><p>回到上述案例中，With 声明了以下几项信息：</p><ol><li>⭐ <code>&#39;connector&#39; = &#39;kafka&#39;</code>：声明外部存储是 Kafka</li><li>⭐ <code>&#39;topic&#39; = &#39;user_behavior&#39;</code>：声明 Flink SQL 任务要连接的 Kafka 表的 topic 是 user_behavior</li><li>⭐ <code>&#39;properties.bootstrap.servers&#39; = &#39;localhost:9092&#39;</code>：声明 Kafka 的 server ip 是 localhost:9092</li><li>⭐ <code>&#39;properties.group.id&#39; = &#39;testGroup&#39;</code>：声明 Flink SQL 任务消费这个 Kafka topic，会使用 testGroup 的 group id 去消费</li><li>⭐ <code>&#39;scan.startup.mode&#39; = &#39;earliest-offset&#39;</code>：声明 Flink SQL 任务消费这个 Kafka topic 会从最早位点开始消费</li><li>⭐ <code>&#39;format&#39; = &#39;csv&#39;</code>：声明 Flink SQL 任务读入或者写出时对于 Kafka 消息的序列化方式是 csv 格式</li></ol><p>从这里也可以看出来 With 中具体要配置哪些配置项都是和每种 Connector 决定的。</p><h3 id="3-1-4-Create-Table-Like-子句"><a href="#3-1-4-Create-Table-Like-子句" class="headerlink" title="3.1.4.Create Table Like 子句"></a>3.1.4.Create Table Like 子句</h3><p>Like 子句是 Create Table 子句的一个延伸。举例：</p><p>下面定义了一张 <code>Orders</code> 表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> Orders (</span><br><span class="line">    `<span class="keyword">user</span>` <span class="type">BIGINT</span>,</span><br><span class="line">    product STRING,</span><br><span class="line">    order_time <span class="type">TIMESTAMP</span>(<span class="number">3</span>)</span><br><span class="line">) <span class="keyword">WITH</span> ( </span><br><span class="line">    <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;scan.startup.mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;earliest-offset&#x27;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>但是忘记定义 Watermark 了，那如果想加上 Watermark，就可以用 <code>Like</code> 子句定义一张带 Watermark 的新表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> Orders_with_watermark (</span><br><span class="line">    <span class="comment">-- 1. 添加了 WATERMARK 定义</span></span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> order_time <span class="keyword">AS</span> order_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span> </span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="comment">-- 2. 覆盖了原 Orders 表中 scan.startup.mode 参数</span></span><br><span class="line">    <span class="string">&#x27;scan.startup.mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;latest-offset&#x27;</span></span><br><span class="line">)</span><br><span class="line"><span class="comment">-- 3. Like 子句声明是在原来的 Orders 表的基础上定义 Orders_with_watermark 表</span></span><br><span class="line"><span class="keyword">LIKE</span> Orders;</span><br></pre></td></tr></table></figure><p>上面这个语句的效果就等同于：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> Orders_with_watermark (</span><br><span class="line">    `<span class="keyword">user</span>` <span class="type">BIGINT</span>,</span><br><span class="line">    product STRING,</span><br><span class="line">    order_time <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> order_time <span class="keyword">AS</span> order_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span> </span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;scan.startup.mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;latest-offset&#x27;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>不过这种不常使用。就不过多介绍了。如果小伙伴萌感兴趣，直接去官网参考具体注意事项：</p><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/sql/create/#like">https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/sql/create/#like</a></p><h2 id="3-2-DML：With-子句"><a href="#3-2-DML：With-子句" class="headerlink" title="3.2.DML：With 子句"></a>3.2.DML：With 子句</h2><ol><li><p>⭐ 应用场景（支持 Batch\Streaming）：With 语句和离线 Hive SQL With 语句一样的，xdm，语法糖 +1，使用它可以让你的代码逻辑更加清晰。</p></li><li><p>⭐ 直接上案例：</p></li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 语法糖+1</span></span><br><span class="line"><span class="keyword">WITH</span> orders_with_total <span class="keyword">AS</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> </span><br><span class="line">        order_id</span><br><span class="line">        , price <span class="operator">+</span> tax <span class="keyword">AS</span> total</span><br><span class="line">    <span class="keyword">FROM</span> Orders</span><br><span class="line">)</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    order_id</span><br><span class="line">    , <span class="built_in">SUM</span>(total)</span><br><span class="line"><span class="keyword">FROM</span> orders_with_total</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> </span><br><span class="line">    order_id;</span><br></pre></td></tr></table></figure><h2 id="3-3-DML：SELECT-amp-WHERE-子句"><a href="#3-3-DML：SELECT-amp-WHERE-子句" class="headerlink" title="3.3.DML：SELECT &amp; WHERE 子句"></a>3.3.DML：SELECT &amp; WHERE 子句</h2><ol><li><p>⭐ 应用场景（支持 Batch\Streaming）：SELECT &amp; WHERE 语句和离线 Hive SQL 语句一样的，xdm，常用作 ETL，过滤，字段清洗标准化</p></li><li><p>⭐ 直接上案例：</p></li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> target_table</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> Orders</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> target_table</span><br><span class="line"><span class="keyword">SELECT</span> order_id, price <span class="operator">+</span> tax <span class="keyword">FROM</span> Orders</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> target_table</span><br><span class="line"><span class="comment">-- 自定义 Source 的数据</span></span><br><span class="line"><span class="keyword">SELECT</span> order_id, price <span class="keyword">FROM</span> (<span class="keyword">VALUES</span> (<span class="number">1</span>, <span class="number">2.0</span>), (<span class="number">2</span>, <span class="number">3.1</span>))  <span class="keyword">AS</span> t (order_id, price)</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> target_table</span><br><span class="line"><span class="keyword">SELECT</span> price <span class="operator">+</span> tax <span class="keyword">FROM</span> Orders <span class="keyword">WHERE</span> id <span class="operator">=</span> <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 使用 UDF 做字段标准化处理</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> target_table</span><br><span class="line"><span class="keyword">SELECT</span> PRETTY_PRINT(order_id) <span class="keyword">FROM</span> Orders</span><br><span class="line"><span class="comment">-- 过滤条件</span></span><br><span class="line"><span class="keyword">Where</span> id <span class="operator">&gt;</span> <span class="number">3</span></span><br></pre></td></tr></table></figure><ol start="3"><li>⭐ <code>SQL 语义</code>：</li></ol><p>其实理解一个 SQL 最后生成的任务是怎样执行的，最好的方式就是理解其语义。</p><p>以下面的 SQL 为例，我们来介绍下其在离线中和在实时中执行的区别，对比学习一下，大家就比较清楚了</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> target_table</span><br><span class="line"><span class="keyword">SELECT</span> PRETTY_PRINT(order_id) <span class="keyword">FROM</span> Orders</span><br><span class="line"><span class="keyword">Where</span> id <span class="operator">&gt;</span> <span class="number">3</span></span><br></pre></td></tr></table></figure><p>这个 SQL 对应的实时任务，假设 Orders 为 kafka，target_table 也为 Kafka，在执行时，会生成三个算子：</p><ul><li>⭐ <code>数据源算子</code>（From Order）：连接到 Kafka topic，数据源算子一直运行，实时的从 Order Kafka 中一条一条的读取数据，然后一条一条发送给下游的 <code>过滤和字段标准化算子</code></li><li>⭐ <code>过滤和字段标准化算子</code>（Where id &gt; 3 和 PRETTY_PRINT(order_id)）：接收到上游算子发的一条一条的数据，然后判断 id &gt; 3？将判断结果为 true 的数据执行 PRETTY_PRINT UDF 后，一条一条将计算结果数据发给下游 <code>数据汇算子</code></li><li>⭐ <code>数据汇算子</code>（INSERT INTO target_table）：接收到上游发的一条一条的数据，写入到 target_table Kafka 中</li></ul><p>可以看到这个实时任务的所有算子是以一种 pipeline 模式运行的，所有的算子在同一时刻都是处于 running 状态的，24 小时一直在运行，实时任务中也没有离线中常见的分区概念。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/35.png" alt="select &amp; where"></p><blockquote><p>关于看如何看一段 Flink SQL 最终的执行计划：</p><p>最好的方法就如上图，看 Flink web ui 的算子图，算子图上详细的标记清楚了每一个算子做的事情。以上图来说，我们可以看到主要有三个算子：</p><ol><li>⭐ Source 算子：Source: TableSourceScan(table=[[default_catalog, default_database, Orders]], fields=[order_id, name]) -&gt; Calc(select=[order_id, name, CAST(CURRENT_TIMESTAMP()) AS row_time]) -&gt; WatermarkAssigner(rowtime=[row_time], watermark=[(row_time - 5000:INTERVAL SECOND)]) ，其中 Source 表名称为 <code>table=[[default_catalog, default_database, Orders]</code>，字段为 <code>select=[order_id, name, CAST(CURRENT_TIMESTAMP()) AS row_time]</code>，Watermark 策略为 <code>rowtime=[row_time], watermark=[(row_time - 5000:INTERVAL SECOND)]</code>。</li><li>⭐ 过滤算子：Calc(select=[order_id, name, row_time], where=[(order_id &gt; 3)]) -&gt; NotNullEnforcer(fields=[order_id])，其中过滤条件为 <code>where=[(order_id &gt; 3)]</code>，结果字段为 <code>select=[order_id, name, row_time]</code></li><li>⭐ Sink 算子：Sink: Sink(table=[default_catalog.default_database.target_table], fields=[order_id, name, row_time])，其中最终产出的表名称为 <code>table=[default_catalog.default_database.target_table]</code>，表字段为 <code>fields=[order_id, name, row_time]</code></li></ol><p>可以看到 Flink SQL 具体执行了哪些操作是非常详细的标记在算子图上。所以小伙伴萌一定要学会看算子图，这是掌握 debug、调优前最基础的一个技巧。</p></blockquote><p>那么如果这个 SQL 放在 Hive 中执行时，假设其中 Orders 为 Hive 表，target_table 也为 Hive 表，其也会生成三个类似的算子（虽然实际可能会被优化为一个算子，这里为了方便对比，划分为三个进行介绍），离线和实时任务的执行方式完全不同：</p><ul><li>⭐ <code>数据源算子</code>（From Order）：数据源从 Order Hive 表（通常都是读一天、一小时的分区数据）中一次性读取所有的数据，然后将读到的数据全部发给下游 <code>过滤和字段标准化算子</code>，然后 <code>数据源算子</code> 就运行结束了，释放资源了</li><li>⭐ <code>过滤和字段标准化算子</code>（Where id &gt; 3 和 PRETTY_PRINT(order_id)）：接收到上游算子的所有数据，然后遍历所有数据判断 id &gt; 3？将判断结果为 true 的数据执行 PRETTY_PRINT UDF 后，将所有数据发给下游 <code>数据汇算子</code>，然后 <code>过滤和字段标准化算子</code> 就运行结束了，释放资源了</li><li>⭐ <code>数据汇算子</code>（INSERT INTO target_table）：接收到上游的所有数据，将所有数据都写到 target_table Hive 表中，然后整个任务就运行结束了，整个任务的资源也就都释放了</li></ul><p>可以看到离线任务的算子是分阶段（stage）进行运行的，每一个 stage 运行结束之后，然后下一个 stage 开始运行，全部的 stage 运行完成之后，这个离线任务就跑结束了。</p><blockquote><p>注意：</p><p>很多小伙伴都是之前做过离线数仓的，熟悉了离线的分区、计算任务定时调度运行这两个概念，所以在最初接触 Flink SQL 时，会以为 Flink SQL 实时任务也会存在这两个概念，这里博主做一下解释</p><ol><li>分区概念：离线由于能力限制问题，通常都是进行一批一批的数据计算，每一批数据的数据量都是有限的集合，这一批一批的数据自然的划分方式就是时间，比如按小时、天进行划分分区。但是 <code>在实时任务中，是没有分区的概念的</code>，实时任务的上游、下游都是无限的数据流。</li><li>计算任务定时调度概念：同上，离线就是由于计算能力限制，数据要一批一批算，一批一批输入、产出，所以要按照小时、天定时的调度和计算。但是 <code>在实时任务中，是没有定时调度的概念的</code>，实时任务一旦运行起来就是 24 小时不间断，不间断的处理上游无限的数据，不简单的产出数据给到下游。</li></ol></blockquote><p>详细可参考：<a href="https://mp.weixin.qq.com/s/VAhodnMetqFEXB33zH8lCg">https://mp.weixin.qq.com/s/VAhodnMetqFEXB33zH8lCg</a></p><h2 id="3-4-DML：SELECT-DISTINCT-子句"><a href="#3-4-DML：SELECT-DISTINCT-子句" class="headerlink" title="3.4.DML：SELECT DISTINCT 子句"></a>3.4.DML：SELECT DISTINCT 子句</h2><ol><li><p>⭐ 应用场景（支持 Batch\Streaming）：语句和离线 Hive SQL SELECT DISTINCT 语句一样的，xdm，用作根据 key 进行数据去重</p></li><li><p>⭐ 直接上案例：</p></li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">into</span> target_table</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    <span class="keyword">DISTINCT</span> id </span><br><span class="line"><span class="keyword">FROM</span> Orders</span><br></pre></td></tr></table></figure><ol start="3"><li>⭐ <code>SQL 语义</code>：</li></ol><p>也是拿离线和实时做对比。</p><p>这个 SQL 对应的实时任务，假设 Orders 为 kafka，target_table 也为 Kafka，在执行时，会生成三个算子：</p><ul><li>⭐ <code>数据源算子</code>（From Order）：连接到 Kafka topic，数据源算子一直运行，实时的从 Order Kafka 中一条一条的读取数据，然后一条一条发送给下游的 <code>去重算子</code></li><li>⭐ <code>去重算子</code>（DISTINCT id）：接收到上游算子发的一条一条的数据，然后判断这个 id 之前是否已经来过了，判断方式就是使用 Flink 中的 state 状态，如果状态中已经有这个 id 了，则说明已经来过了，不往下游算子发，如果状态中没有这个 id，则说明没来过，则往下游算子发，也是一条一条发给下游 <code>数据汇算子</code></li><li>⭐ <code>数据汇算子</code>（INSERT INTO target_table）：接收到上游发的一条一条的数据，写入到 target_table Kafka 中</li></ul><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/36.png" alt="select distinct"></p><blockquote><p>注意：</p><p>对于实时任务，计算时的状态可能会无限增长。</p><p>状态大小取决于不同 key（上述案例为 id 字段）的数量。为了防止状态无限变大，我们可以设置状态的 TTL。但是这可能会影响查询结果的正确性，比如某个 key 的数据过期从状态中删除了，那么下次再来这么一个 key，由于在状态中找不到，就又会输出一遍。</p></blockquote><p>那么如果这个 SQL 放在 Hive 中执行时，假设其中 Orders 为 Hive 表，target_table 也为 Hive 表，其也会生成三个相同的算子（虽然可能会被优化为一个算子，这里为了方便对比，划分为三个进行介绍），但是其和实时任务的执行方式完全不同：</p><ul><li>⭐ <code>数据源算子</code>（From Order）：数据源从 Order Hive 表（通常都有天、小时分区限制）中一次性读取所有的数据，然后将读到的数据全部发给下游 <code>去重算子</code>，然后 <code>数据源算子</code> 就运行结束了，释放资源了</li><li>⭐ <code>去重算子</code>（DISTINCT id）：接收到上游算子的所有数据，然后遍历所有数据进行去重，将去重完的所有结果数据发给下游 <code>数据汇算子</code>，然后 <code>去重算子</code> 就运行结束了，释放资源了</li><li>⭐ <code>数据汇算子</code>（INSERT INTO target_table）：接收到上游的所有数据，将所有数据都写到 target_table Hive 中，然后整个任务就运行结束了，整个任务的资源也就都释放了</li></ul><h2 id="3-5-DML：窗口聚合"><a href="#3-5-DML：窗口聚合" class="headerlink" title="3.5.DML：窗口聚合"></a>3.5.DML：窗口聚合</h2><p>由于窗口涉及到的知识内容比较多，所以博主先为大家说明介绍下面内容时的思路，大家跟着思路走。思路如下：</p><ol><li>⭐ 先介绍 Flink SQL 支持的 4 种时间窗口</li><li>⭐ 分别详细介绍上述的 4 种时间窗口的功能及 SQL 语法</li><li>⭐ 结合实际案例介绍 4 种时间窗口</li></ol><p>首先来看看 Flink SQL 中支持的 4 种窗口的运算。</p><ol><li>⭐ 滚动窗口（TUMBLE）</li><li>⭐ 滑动窗口（HOP）</li><li>⭐ Session 窗口（SESSION）</li><li>⭐ 渐进式窗口（CUMULATE）</li></ol><h3 id="3-5-1-滚动窗口（TUMBLE）"><a href="#3-5-1-滚动窗口（TUMBLE）" class="headerlink" title="3.5.1.滚动窗口（TUMBLE）"></a>3.5.1.滚动窗口（TUMBLE）</h3><ol><li>⭐ 滚动窗口定义：滚动窗口将每个元素指定给指定窗口大小的窗口。滚动窗口具有固定大小，且不重叠。例如，指定一个大小为 5 分钟的滚动窗口。在这种情况下，Flink 将每隔 5 分钟开启一个新的窗口，其中每一条数都会划分到唯一一个 5 分钟的窗口中，如下图所示。</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/14.jpg" alt="tumble window"></p><ol start="2"><li><p>⭐ 应用场景：常见的按照一分钟对数据进行聚合，计算一分钟内 PV，UV 数据。</p></li><li><p>⭐ 实际案例：简单且常见的分维度分钟级别同时在线用户数、总销售额</p></li></ol><p>那么上面这个案例的 SQL 要咋写呢？</p><p>关于滚动窗口，在 1.13 版本之前和 1.13 及之后版本有两种 Flink SQL 实现方式，分别是：</p><ul><li>⭐ Group Window Aggregation（1.13 之前只有此类方案，此方案在 1.13 及之后版本已经标记为废弃，不推荐小伙伴萌使用）</li><li>⭐ Windowing TVF（1.13 及之后建议使用 Windowing TVF）</li></ul><p>博主这里两种方法都会介绍：</p><ul><li>⭐ Group Window Aggregation 方案（支持 Batch\Streaming 任务）：</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 数据源表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    <span class="comment">-- 维度数据</span></span><br><span class="line">    dim STRING,</span><br><span class="line">    <span class="comment">-- 用户 id</span></span><br><span class="line">    user_id <span class="type">BIGINT</span>,</span><br><span class="line">    <span class="comment">-- 用户</span></span><br><span class="line">    price <span class="type">BIGINT</span>,</span><br><span class="line">    <span class="comment">-- 事件时间戳</span></span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    <span class="comment">-- watermark 设置</span></span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.dim.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 数据汇表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    dim STRING,</span><br><span class="line">    pv <span class="type">BIGINT</span>,</span><br><span class="line">    sum_price <span class="type">BIGINT</span>,</span><br><span class="line">    max_price <span class="type">BIGINT</span>,</span><br><span class="line">    min_price <span class="type">BIGINT</span>,</span><br><span class="line">    uv <span class="type">BIGINT</span>,</span><br><span class="line">    window_start <span class="type">bigint</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 数据处理逻辑</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> sink_table</span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    dim,</span><br><span class="line">    <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">as</span> pv,</span><br><span class="line">    <span class="built_in">sum</span>(price) <span class="keyword">as</span> sum_price,</span><br><span class="line">    <span class="built_in">max</span>(price) <span class="keyword">as</span> max_price,</span><br><span class="line">    <span class="built_in">min</span>(price) <span class="keyword">as</span> min_price,</span><br><span class="line">    <span class="comment">-- 计算 uv 数</span></span><br><span class="line">    <span class="built_in">count</span>(<span class="keyword">distinct</span> user_id) <span class="keyword">as</span> uv,</span><br><span class="line">    UNIX_TIMESTAMP(<span class="built_in">CAST</span>(tumble_start(row_time, <span class="type">interval</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">minute</span>) <span class="keyword">AS</span> STRING)) <span class="operator">*</span> <span class="number">1000</span>  <span class="keyword">as</span> window_start</span><br><span class="line"><span class="keyword">from</span> source_table</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    dim,</span><br><span class="line">    tumble(row_time, <span class="type">interval</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">minute</span>)</span><br></pre></td></tr></table></figure><p>可以看到 Group Window Aggregation 滚动窗口的 SQL 语法就是把 tumble window 的声明写在了 group by 子句中，即 <code>tumble(row_time, interval &#39;1&#39; minute)</code>，第一个参数为事件时间的时间戳；第二个参数为滚动窗口大小。</p><ul><li>⭐ Window TVF 方案（1.13 只支持 Streaming 任务）：</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 数据源表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    <span class="comment">-- 维度数据</span></span><br><span class="line">    dim STRING,</span><br><span class="line">    <span class="comment">-- 用户 id</span></span><br><span class="line">    user_id <span class="type">BIGINT</span>,</span><br><span class="line">    <span class="comment">-- 用户</span></span><br><span class="line">    price <span class="type">BIGINT</span>,</span><br><span class="line">    <span class="comment">-- 事件时间戳</span></span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    <span class="comment">-- watermark 设置</span></span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.dim.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 数据汇表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    dim STRING,</span><br><span class="line">    pv <span class="type">BIGINT</span>,</span><br><span class="line">    sum_price <span class="type">BIGINT</span>,</span><br><span class="line">    max_price <span class="type">BIGINT</span>,</span><br><span class="line">    min_price <span class="type">BIGINT</span>,</span><br><span class="line">    uv <span class="type">BIGINT</span>,</span><br><span class="line">    window_start <span class="type">bigint</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 数据处理逻辑</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    dim,</span><br><span class="line">    UNIX_TIMESTAMP(<span class="built_in">CAST</span>(window_start <span class="keyword">AS</span> STRING)) <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">as</span> window_start,</span><br><span class="line">    <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">as</span> pv,</span><br><span class="line">    <span class="built_in">sum</span>(price) <span class="keyword">as</span> sum_price,</span><br><span class="line">    <span class="built_in">max</span>(price) <span class="keyword">as</span> max_price,</span><br><span class="line">    <span class="built_in">min</span>(price) <span class="keyword">as</span> min_price,</span><br><span class="line">    <span class="built_in">count</span>(<span class="keyword">distinct</span> user_id) <span class="keyword">as</span> uv</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">TABLE</span>(TUMBLE(</span><br><span class="line">        <span class="keyword">TABLE</span> source_table</span><br><span class="line">        , DESCRIPTOR(row_time)</span><br><span class="line">        , <span class="type">INTERVAL</span> <span class="string">&#x27;60&#x27;</span> <span class="keyword">SECOND</span>))</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> window_start, </span><br><span class="line">      window_end,</span><br><span class="line">      dim</span><br></pre></td></tr></table></figure><p>可以看到 Windowing TVF 滚动窗口的写法就是把 tumble window 的声明写在了数据源的 Table 子句中，即 <code>TABLE(TUMBLE(TABLE source_table, DESCRIPTOR(row_time), INTERVAL &#39;60&#39; SECOND))</code>，包含三部分参数。</p><p>第一个参数 <code>TABLE source_table</code> 声明数据源表；<br>第二个参数 <code>DESCRIPTOR(row_time)</code> 声明数据源的时间戳；<br>第三个参数 <code>INTERVAL &#39;60&#39; SECOND</code> 声明滚动窗口大小为 1 min。</p><p>可以直接在公众号后台回复<strong>1.13.2 最全 flink sql</strong>获取源代码。所有的源码都开源到 github 上面了。里面包含了非常多的案例。可以直接拿来在本地运行的！！！肥肠的方便。</p><ol start="4"><li>⭐ <code>SQL 语义</code>：</li></ol><p>由于离线没有相同的时间窗口聚合概念，这里就直接说实时场景 SQL 语义，假设 Orders 为 kafka，target_table 也为 Kafka，这个 SQL 生成的实时任务，在执行时，会生成三个算子：</p><ul><li>⭐ <code>数据源算子</code>（From Order）：连接到 Kafka topic，数据源算子一直运行，实时的从 Order Kafka 中一条一条的读取数据，然后一条一条发送给下游的 <code>窗口聚合算子</code></li><li>⭐ <code>窗口聚合算子</code>（TUMBLE 算子）：接收到上游算子发的一条一条的数据，然后将每一条数据按照时间戳划分到对应的窗口中（根据事件时间、处理时间的不同语义进行划分），上述案例为事件时间，事件时间中，滚动窗口算子接收到上游的 Watermark 大于窗口的结束时间时，则说明当前这一分钟的滚动窗口已经结束了，将窗口计算完的结果发往下游算子（一条一条发给下游 <code>数据汇算子</code>）</li><li>⭐ <code>数据汇算子</code>（INSERT INTO target_table）：接收到上游发的一条一条的数据，写入到 target_table Kafka 中</li></ul><p>这个实时任务也是 24 小时一直在运行的，所有的算子在同一时刻都是处于 running 状态的。</p><blockquote><p>注意：</p><p>事件时间中滚动窗口的窗口计算触发是由 Watermark 推动的。</p></blockquote><h3 id="3-5-2-滑动窗口（HOP）"><a href="#3-5-2-滑动窗口（HOP）" class="headerlink" title="3.5.2.滑动窗口（HOP）"></a>3.5.2.滑动窗口（HOP）</h3><ol><li>⭐ 滑动窗口定义：滑动窗口也是将元素指定给固定长度的窗口。与滚动窗口功能一样，也有窗口大小的概念。不一样的地方在于，滑动窗口有另一个参数控制窗口计算的频率（滑动窗口滑动的步长）。因此，如果滑动的步长小于窗口大小，则滑动窗口之间每个窗口是可以重叠。在这种情况下，一条数据就会分配到多个窗口当中。举例，有 10 分钟大小的窗口，滑动步长为 5 分钟。这样，每 5 分钟会划分一次窗口，这个窗口包含的数据是过去 10 分钟内的数据，如下图所示。</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/15.png" alt="hop window"></p><ol start="2"><li><p>⭐ 应用场景：比如计算同时在线的数据，要求结果的输出频率是 1 分钟一次，每次计算的数据是过去 5 分钟的数据（有的场景下用户可能在线，但是可能会 2 分钟不活跃，但是这也要算在同时在线数据中，所以取最近 5 分钟的数据就能计算进去了）</p></li><li><p>⭐ 实际案例：简单且常见的分维度分钟级别同时在线用户数，1 分钟输出一次，计算最近 5 分钟的数据</p></li></ol><p>依然是 Group Window Aggregation、Windowing TVF 两种方案：</p><ul><li>⭐ Group Window Aggregation 方案（支持 Batch\Streaming 任务）：</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 数据源表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    <span class="comment">-- 维度数据</span></span><br><span class="line">    dim STRING,</span><br><span class="line">    <span class="comment">-- 用户 id</span></span><br><span class="line">    user_id <span class="type">BIGINT</span>,</span><br><span class="line">    <span class="comment">-- 用户</span></span><br><span class="line">    price <span class="type">BIGINT</span>,</span><br><span class="line">    <span class="comment">-- 事件时间戳</span></span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    <span class="comment">-- watermark 设置</span></span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.dim.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 数据汇表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    dim STRING,</span><br><span class="line">    uv <span class="type">BIGINT</span>,</span><br><span class="line">    window_start <span class="type">bigint</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 数据处理逻辑</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span> dim,</span><br><span class="line">    UNIX_TIMESTAMP(<span class="built_in">CAST</span>(hop_start(row_time, <span class="type">interval</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">minute</span>, <span class="type">interval</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">minute</span>) <span class="keyword">AS</span> STRING)) <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">as</span> window_start, </span><br><span class="line">    <span class="built_in">count</span>(<span class="keyword">distinct</span> user_id) <span class="keyword">as</span> uv</span><br><span class="line"><span class="keyword">FROM</span> source_table</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> dim</span><br><span class="line">    , hop(row_time, <span class="type">interval</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">minute</span>, <span class="type">interval</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">minute</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以看到 Group Window Aggregation 滚动窗口的写法就是把 hop window 的声明写在了 group by 子句中，即 <code>hop(row_time, interval &#39;1&#39; minute, interval &#39;5&#39; minute)</code>。其中：</p><p>第一个参数为事件时间的时间戳；<br>第二个参数为滑动窗口的滑动步长；<br>第三个参数为滑动窗口大小。</p><ul><li>⭐ Windowing TVF 方案（1.13 只支持 Streaming 任务）：</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 数据源表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    <span class="comment">-- 维度数据</span></span><br><span class="line">    dim STRING,</span><br><span class="line">    <span class="comment">-- 用户 id</span></span><br><span class="line">    user_id <span class="type">BIGINT</span>,</span><br><span class="line">    <span class="comment">-- 用户</span></span><br><span class="line">    price <span class="type">BIGINT</span>,</span><br><span class="line">    <span class="comment">-- 事件时间戳</span></span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    <span class="comment">-- watermark 设置</span></span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.dim.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 数据汇表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    dim STRING,</span><br><span class="line">    uv <span class="type">BIGINT</span>,</span><br><span class="line">    window_start <span class="type">bigint</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 数据处理逻辑</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    dim,</span><br><span class="line">    UNIX_TIMESTAMP(<span class="built_in">CAST</span>(window_start <span class="keyword">AS</span> STRING)) <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">as</span> window_start, </span><br><span class="line">    <span class="built_in">count</span>(<span class="keyword">distinct</span> user_id) <span class="keyword">as</span> bucket_uv</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">TABLE</span>(HOP(</span><br><span class="line">        <span class="keyword">TABLE</span> source_table</span><br><span class="line">        , DESCRIPTOR(row_time)</span><br><span class="line">        , <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> MINUTES, <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> MINUTES))</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> window_start, </span><br><span class="line">      window_end,</span><br><span class="line">      dim</span><br></pre></td></tr></table></figure><p>可以看到 Windowing TVF 滚动窗口的写法就是把 hop window 的声明写在了数据源的 Table 子句中，即 <code>TABLE(HOP(TABLE source_table, DESCRIPTOR(row_time), INTERVAL &#39;1&#39; MINUTES, INTERVAL &#39;5&#39; MINUTES))</code>，包含四部分参数：</p><p>第一个参数 <code>TABLE source_table</code> 声明数据源表；<br>第二个参数 <code>DESCRIPTOR(row_time)</code> 声明数据源的时间戳；<br>第三个参数 <code>INTERVAL &#39;1&#39; MINUTES</code> 声明滚动窗口滑动步长大小为 1 min。<br>第四个参数 <code>INTERVAL &#39;5&#39; MINUTES</code> 声明滚动窗口大小为 5 min。</p><ol start="4"><li>⭐ <code>SQL 语义</code>：</li></ol><p>滑动窗口语义和滚动窗口类似，这里不再赘述。</p><h3 id="3-5-3-Session-窗口（SESSION）"><a href="#3-5-3-Session-窗口（SESSION）" class="headerlink" title="3.5.3.Session 窗口（SESSION）"></a>3.5.3.Session 窗口（SESSION）</h3><ol><li>⭐ Session 窗口定义：Session 时间窗口和滚动、滑动窗口不一样，其没有固定的持续时间，如果在定义的间隔期（Session Gap）内没有新的数据出现，则 Session 就会窗口关闭。如下图对比所示：</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/16.jpg" alt="session window"></p><ol start="2"><li>⭐ 实际案例：计算每个用户在活跃期间（一个 Session）总共购买的商品数量，如果用户 5 分钟没有活动则视为 Session 断开</li></ol><p>目前 1.13 版本中 Flink SQL 不支持 Session 窗口的 Window TVF，所以这里就只介绍 Group Window Aggregation 方案：</p><ul><li>⭐ Group Window Aggregation 方案（支持 Batch\Streaming 任务）：</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 数据源表，用户购买行为记录表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    <span class="comment">-- 维度数据</span></span><br><span class="line">    dim STRING,</span><br><span class="line">    <span class="comment">-- 用户 id</span></span><br><span class="line">    user_id <span class="type">BIGINT</span>,</span><br><span class="line">    <span class="comment">-- 用户</span></span><br><span class="line">    price <span class="type">BIGINT</span>,</span><br><span class="line">    <span class="comment">-- 事件时间戳</span></span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    <span class="comment">-- watermark 设置</span></span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.dim.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 数据汇表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    dim STRING,</span><br><span class="line">    pv <span class="type">BIGINT</span>, <span class="comment">-- 购买商品数量</span></span><br><span class="line">    window_start <span class="type">bigint</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 数据处理逻辑</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    dim,</span><br><span class="line">    UNIX_TIMESTAMP(<span class="built_in">CAST</span>(session_start(row_time, <span class="type">interval</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">minute</span>) <span class="keyword">AS</span> STRING)) <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">as</span> window_start, </span><br><span class="line">    <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">as</span> pv</span><br><span class="line"><span class="keyword">FROM</span> source_table</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> dim</span><br><span class="line">      , session(row_time, <span class="type">interval</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">minute</span>)</span><br></pre></td></tr></table></figure><blockquote><p>注意：</p><p>上述 SQL 任务是在整个 Session 窗口结束之后才会把数据输出。Session 窗口即支持 <code>处理时间</code> 也支持 <code>事件时间</code>。但是处理时间只支持在 Streaming 任务中运行，Batch 任务不支持。</p></blockquote><p>可以看到 Group Window Aggregation 中 Session 窗口的写法就是把 session window 的声明写在了 group by 子句中，即 <code>session(row_time, interval &#39;5&#39; minute)</code>。其中：</p><p>第一个参数为事件时间的时间戳；<br>第二个参数为 Session gap 间隔。</p><ol start="4"><li>⭐ <code>SQL 语义</code>：</li></ol><p>Session 窗口语义和滚动窗口类似，这里不再赘述。</p><p>可以直接在公众号后台回复<strong>1.13.2 最全 flink sql</strong>获取源代码。所有的源码都开源到 github 上面了。里面包含了非常多的案例。可以直接拿来在本地运行的！！！肥肠的方便。</p><h3 id="3-5-4-渐进式窗口（CUMULATE）"><a href="#3-5-4-渐进式窗口（CUMULATE）" class="headerlink" title="3.5.4.渐进式窗口（CUMULATE）"></a>3.5.4.渐进式窗口（CUMULATE）</h3><ol><li>⭐ 渐进式窗口定义（1.13 只支持 Streaming 任务）：渐进式窗口在其实就是 <code>固定窗口间隔内提前触发的的滚动窗口</code>，其实就是 <code>Tumble Window + early-fire</code> 的一个事件时间的版本。例如，从每日零点到当前这一分钟绘制累积 UV，其中 10:00 时的 UV 表示从 00:00 到 10:00 的 UV 总数。<br>渐进式窗口可以认为是首先开一个最大窗口大小的滚动窗口，然后根据用户设置的触发的时间间隔将这个滚动窗口拆分为多个窗口，这些窗口具有相同的窗口起点和不同的窗口终点。如下图所示：</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/17.jpg" alt="cumulate window"></p><ol start="2"><li><p>⭐ 应用场景：周期内累计 PV，UV 指标（如每天累计到当前这一分钟的 PV，UV）。这类指标是一段周期内的累计状态，对分析师来说更具统计分析价值，而且几乎所有的复合指标都是基于此类指标的统计（不然离线为啥都要累计一天的数据，而不要一分钟累计的数据呢）。</p></li><li><p>⭐ 实际案例：每天的截止当前分钟的累计 money（sum(money)），去重 id 数（count(distinct id)）。每天代表渐进式窗口大小为 1 天，分钟代表渐进式窗口移动步长为分钟级别。举例如下：</p></li></ol><p>明细输入数据：</p><table><thead><tr><th>time</th><th>id</th><th>money</th></tr></thead><tbody><tr><td>2021-11-01 00:01:00</td><td>A</td><td>3</td></tr><tr><td>2021-11-01 00:01:00</td><td>B</td><td>5</td></tr><tr><td>2021-11-01 00:01:00</td><td>A</td><td>7</td></tr><tr><td>2021-11-01 00:02:00</td><td>C</td><td>3</td></tr><tr><td>2021-11-01 00:03:00</td><td>C</td><td>10</td></tr></tbody></table><p>预期经过渐进式窗口计算的输出数据：</p><table><thead><tr><th>time</th><th>count distinct id</th><th>sum money</th></tr></thead><tbody><tr><td>2021-11-01 00:01:00</td><td>2</td><td>15</td></tr><tr><td>2021-11-01 00:02:00</td><td>3</td><td>18</td></tr><tr><td>2021-11-01 00:03:00</td><td>3</td><td>28</td></tr></tbody></table><p>转化为折线图长这样：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/12_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%EF%BC%89%EF%BC%9A1.13cumulatewindow/1.png" alt="当日累计"></p><p>可以看到，其特点就在于，每一分钟的输出结果都是当天零点累计到当前的结果。</p><p>渐进式窗口目前只有 Windowing TVF 方案支持：</p><ul><li>⭐ Windowing TVF 方案（1.13 只支持 Streaming 任务）：</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 数据源表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    <span class="comment">-- 用户 id</span></span><br><span class="line">    user_id <span class="type">BIGINT</span>,</span><br><span class="line">    <span class="comment">-- 用户</span></span><br><span class="line">    money <span class="type">BIGINT</span>,</span><br><span class="line">    <span class="comment">-- 事件时间戳</span></span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    <span class="comment">-- watermark 设置</span></span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 数据汇表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    window_end <span class="type">bigint</span>,</span><br><span class="line">    window_start <span class="type">bigint</span>,</span><br><span class="line">    sum_money <span class="type">BIGINT</span>,</span><br><span class="line">    count_distinct_id <span class="type">bigint</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 数据处理逻辑</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    UNIX_TIMESTAMP(<span class="built_in">CAST</span>(window_end <span class="keyword">AS</span> STRING)) <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">as</span> window_end, </span><br><span class="line">    window_start, </span><br><span class="line">    <span class="built_in">sum</span>(money) <span class="keyword">as</span> sum_money,</span><br><span class="line">    <span class="built_in">count</span>(<span class="keyword">distinct</span> id) <span class="keyword">as</span> count_distinct_id</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">TABLE</span>(CUMULATE(</span><br><span class="line">       <span class="keyword">TABLE</span> source_table</span><br><span class="line">       , DESCRIPTOR(row_time)</span><br><span class="line">       , <span class="type">INTERVAL</span> <span class="string">&#x27;60&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">       , <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>))</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">    window_start, </span><br><span class="line">    window_end</span><br></pre></td></tr></table></figure><p>可以看到 Windowing TVF 滚动窗口的写法就是把 cumulate window 的声明写在了数据源的 Table 子句中，即 <code>TABLE(CUMULATE(TABLE source_table, DESCRIPTOR(row_time), INTERVAL &#39;60&#39; SECOND, INTERVAL &#39;1&#39; DAY))</code>，其中包含四部分参数：</p><p>第一个参数 <code>TABLE source_table</code> 声明数据源表；<br>第二个参数 <code>DESCRIPTOR(row_time)</code> 声明数据源的时间戳；<br>第三个参数 <code>INTERVAL &#39;60&#39; SECOND</code> 声明渐进式窗口触发的渐进步长为 1 min。<br>第四个参数 <code>INTERVAL &#39;1&#39; DAY</code> 声明整个渐进式窗口的大小为 1 天，到了第二天新开一个窗口重新累计。</p><ol start="4"><li>⭐ <code>SQL 语义</code>：</li></ol><p>渐进式窗口语义和滚动窗口类似，这里不再赘述。</p><h3 id="3-5-5-Window-TVF-支持-Grouping-Sets、Rollup、Cube"><a href="#3-5-5-Window-TVF-支持-Grouping-Sets、Rollup、Cube" class="headerlink" title="3.5.5.Window TVF 支持 Grouping Sets、Rollup、Cube"></a>3.5.5.Window TVF 支持 Grouping Sets、Rollup、Cube</h3><p>具体应用场景：实际的案例场景中，经常会有多个维度进行组合（cube）计算指标的场景。如果把每个维度组合的代码写一遍，然后 union all 起来，这样写起来非常麻烦，而且会导致一个数据源读取多遍。</p><p>这时，有离线 Hive SQL 使用经验的小伙伴萌就会想到，如果有了 Grouping Sets，我们就可以直接用 Grouping Sets 将维度组合写在一条 SQL 中，写起来方便并且执行效率也高。当然，Flink 支持这个功能。</p><p><code>但是目前 Grouping Sets 只在 Window TVF 中支持，不支持 Group Window Aggregation。</code></p><p>来一个实际案例感受一下，计算每日零点累计到当前这一分钟的分汇总、age、sex、age+sex 维度的用户数。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 用户访问明细表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    age STRING,</span><br><span class="line">    sex STRING,</span><br><span class="line">    user_id <span class="type">BIGINT</span>,</span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.age.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.sex.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    age STRING,</span><br><span class="line">    sex STRING,</span><br><span class="line">    uv <span class="type">BIGINT</span>,</span><br><span class="line">    window_end <span class="type">bigint</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    UNIX_TIMESTAMP(<span class="built_in">CAST</span>(window_end <span class="keyword">AS</span> STRING)) <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">as</span> window_end, </span><br><span class="line">    if (age <span class="keyword">is</span> <span class="keyword">null</span>, <span class="string">&#x27;ALL&#x27;</span>, age) <span class="keyword">as</span> age,</span><br><span class="line">    if (sex <span class="keyword">is</span> <span class="keyword">null</span>, <span class="string">&#x27;ALL&#x27;</span>, sex) <span class="keyword">as</span> sex,</span><br><span class="line">    <span class="built_in">count</span>(<span class="keyword">distinct</span> user_id) <span class="keyword">as</span> bucket_uv</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">TABLE</span>(CUMULATE(</span><br><span class="line">       <span class="keyword">TABLE</span> source_table</span><br><span class="line">       , DESCRIPTOR(row_time)</span><br><span class="line">       , <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">       , <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>))</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> </span><br><span class="line">    window_start, </span><br><span class="line">    window_end,</span><br><span class="line">    <span class="comment">-- grouping sets 写法</span></span><br><span class="line">    <span class="keyword">GROUPING</span> SETS (</span><br><span class="line">        ()</span><br><span class="line">        , (age)</span><br><span class="line">        , (sex)</span><br><span class="line">        , (age, sex)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>小伙伴萌这里需要注意下！！！</p><p>Flink SQL 中 Grouping Sets 的语法和 Hive SQL 的语法有一些不同，如果我们使用 Hive SQL 实现上述 SQL 的语义，其实现如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    UNIX_TIMESTAMP(<span class="built_in">CAST</span>(window_end <span class="keyword">AS</span> STRING)) <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">as</span> window_end, </span><br><span class="line">    if (age <span class="keyword">is</span> <span class="keyword">null</span>, <span class="string">&#x27;ALL&#x27;</span>, age) <span class="keyword">as</span> age,</span><br><span class="line">    if (sex <span class="keyword">is</span> <span class="keyword">null</span>, <span class="string">&#x27;ALL&#x27;</span>, sex) <span class="keyword">as</span> sex,</span><br><span class="line">    <span class="built_in">count</span>(<span class="keyword">distinct</span> user_id) <span class="keyword">as</span> bucket_uv</span><br><span class="line"><span class="keyword">FROM</span> source_table</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">    age</span><br><span class="line">    , sex</span><br><span class="line"><span class="comment">-- hive sql grouping sets 写法</span></span><br><span class="line"><span class="keyword">GROUPING</span> SETS (</span><br><span class="line">    ()</span><br><span class="line">    , (age)</span><br><span class="line">    , (sex)</span><br><span class="line">    , (age, sex)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="3-6-DML：Group-聚合"><a href="#3-6-DML：Group-聚合" class="headerlink" title="3.6.DML：Group 聚合"></a>3.6.DML：Group 聚合</h2><ol><li>⭐ Group 聚合定义（支持 Batch\Streaming 任务）：Flink 也支持 Group 聚合。Group 聚合和上面介绍到的窗口聚合的不同之处，就在于 Group 聚合是按照数据的类别进行分组，比如年龄、性别，是横向的；而窗口聚合是在时间粒度上对数据进行分组，是纵向的。如下图所示，就展示出了其区别。其中 <code>按颜色分 key（横向）</code> 就是 Group 聚合，<code>按窗口划分（纵向）</code> 就是窗口聚合。</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/13.jpg" alt="tumble window + key"></p><ol start="2"><li>⭐ 应用场景：一般用于对数据进行分组，然后后续使用聚合函数进行 count、sum 等聚合操作。</li></ol><p>那么这时候，小伙伴萌就会问到，我其实可以把窗口聚合的写法也转换为 Group 聚合，只需要把 Group 聚合的 Group By key 换成时间就行，那这两个聚合的区别到底在哪？</p><p>首先来举一个例子看看怎么将窗口聚合转换为 Group 聚合。假如一个窗口聚合是按照 1 分钟的粒度进行聚合，如下 SQL：</p><ul><li>⭐ 滚动窗口（TUMBLE）</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 数据源表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    <span class="comment">-- 维度数据</span></span><br><span class="line">    dim STRING,</span><br><span class="line">    <span class="comment">-- 用户 id</span></span><br><span class="line">    user_id <span class="type">BIGINT</span>,</span><br><span class="line">    <span class="comment">-- 用户</span></span><br><span class="line">    price <span class="type">BIGINT</span>,</span><br><span class="line">    <span class="comment">-- 事件时间戳</span></span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    <span class="comment">-- watermark 设置</span></span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.dim.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 数据汇表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    dim STRING,</span><br><span class="line">    pv <span class="type">BIGINT</span>,</span><br><span class="line">    sum_price <span class="type">BIGINT</span>,</span><br><span class="line">    max_price <span class="type">BIGINT</span>,</span><br><span class="line">    min_price <span class="type">BIGINT</span>,</span><br><span class="line">    uv <span class="type">BIGINT</span>,</span><br><span class="line">    window_start <span class="type">bigint</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 数据处理逻辑</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> sink_table</span><br><span class="line"><span class="keyword">select</span> dim,</span><br><span class="line">    <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">as</span> pv,</span><br><span class="line">    <span class="built_in">sum</span>(price) <span class="keyword">as</span> sum_price,</span><br><span class="line">    <span class="built_in">max</span>(price) <span class="keyword">as</span> max_price,</span><br><span class="line">    <span class="built_in">min</span>(price) <span class="keyword">as</span> min_price,</span><br><span class="line">    <span class="comment">-- 计算 uv 数</span></span><br><span class="line">    <span class="built_in">count</span>(<span class="keyword">distinct</span> user_id) <span class="keyword">as</span> uv,</span><br><span class="line">    UNIX_TIMESTAMP(<span class="built_in">CAST</span>(tumble_start(row_time, <span class="type">interval</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">minute</span>) <span class="keyword">AS</span> STRING)) <span class="operator">*</span> <span class="number">1000</span>  <span class="keyword">as</span> window_start</span><br><span class="line"><span class="keyword">from</span> source_table</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    dim,</span><br><span class="line">    <span class="comment">-- 按照 Flink SQL tumble 窗口写法划分窗口</span></span><br><span class="line">    tumble(row_time, <span class="type">interval</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">minute</span>)</span><br></pre></td></tr></table></figure><p>转换为 Group 聚合的写法如下：</p><ul><li>⭐ Group 聚合</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 数据源表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    <span class="comment">-- 维度数据</span></span><br><span class="line">    dim STRING,</span><br><span class="line">    <span class="comment">-- 用户 id</span></span><br><span class="line">    user_id <span class="type">BIGINT</span>,</span><br><span class="line">    <span class="comment">-- 用户</span></span><br><span class="line">    price <span class="type">BIGINT</span>,</span><br><span class="line">    <span class="comment">-- 事件时间戳</span></span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    <span class="comment">-- watermark 设置</span></span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.dim.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 数据汇表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    dim STRING,</span><br><span class="line">    pv <span class="type">BIGINT</span>,</span><br><span class="line">    sum_price <span class="type">BIGINT</span>,</span><br><span class="line">    max_price <span class="type">BIGINT</span>,</span><br><span class="line">    min_price <span class="type">BIGINT</span>,</span><br><span class="line">    uv <span class="type">BIGINT</span>,</span><br><span class="line">    window_start <span class="type">bigint</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 数据处理逻辑</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> sink_table</span><br><span class="line"><span class="keyword">select</span> dim,</span><br><span class="line">    <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">as</span> pv,</span><br><span class="line">    <span class="built_in">sum</span>(price) <span class="keyword">as</span> sum_price,</span><br><span class="line">    <span class="built_in">max</span>(price) <span class="keyword">as</span> max_price,</span><br><span class="line">    <span class="built_in">min</span>(price) <span class="keyword">as</span> min_price,</span><br><span class="line">    <span class="comment">-- 计算 uv 数</span></span><br><span class="line">    <span class="built_in">count</span>(<span class="keyword">distinct</span> user_id) <span class="keyword">as</span> uv,</span><br><span class="line">    <span class="built_in">cast</span>((UNIX_TIMESTAMP(<span class="built_in">CAST</span>(row_time <span class="keyword">AS</span> STRING))) <span class="operator">/</span> <span class="number">60</span> <span class="keyword">as</span> <span class="type">bigint</span>) <span class="keyword">as</span> window_start</span><br><span class="line"><span class="keyword">from</span> source_table</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    dim,</span><br><span class="line">    <span class="comment">-- 将秒级别时间戳 / 60 转化为 1min</span></span><br><span class="line">    <span class="built_in">cast</span>((UNIX_TIMESTAMP(<span class="built_in">CAST</span>(row_time <span class="keyword">AS</span> STRING))) <span class="operator">/</span> <span class="number">60</span> <span class="keyword">as</span> <span class="type">bigint</span>)</span><br></pre></td></tr></table></figure><p>确实没错，上面这个转换是一点问题都没有的。</p><p>但是窗口聚合和 Group by 聚合的差异在于：</p><ul><li><p>⭐ 本质区别：窗口聚合是具有时间语义的，其本质是想实现窗口结束输出结果之后，后续有迟到的数据也不会对原有的结果发生更改了，即输出结果值是定值（不考虑 allowLateness）。而 Group by 聚合是没有时间语义的，不管数据迟到多长时间，只要数据来了，就把上一次的输出的结果数据撤回，然后把计算好的新的结果数据发出</p></li><li><p>⭐ 运行层面：窗口聚合是和 <code>时间</code> 绑定的，窗口聚合其中窗口的计算结果触发都是由时间（Watermark）推动的。Group by 聚合完全由数据推动触发计算，新来一条数据去根据这条数据进行计算出结果发出；由此可见两者的实现方式也大为不同。</p></li></ul><ol start="3"><li>⭐ <code>SQL 语义</code></li></ol><p>也是拿离线和实时做对比，Orders 为 kafka，target_table 为 Kafka，这个 SQL 生成的实时任务，在执行时，会生成三个算子：</p><ul><li>⭐ <code>数据源算子</code>（From Order）：数据源算子一直运行，实时的从 Order Kafka 中一条一条的读取数据，然后一条一条发送给下游的 <code>Group 聚合算子</code>，向下游发送数据的 shuffle 策略是根据 group by 中的 key 进行发送，相同的 key 发到同一个 SubTask（并发） 中</li><li>⭐ <code>Group 聚合算子</code>（group by key + sum\count\max\min）：接收到上游算子发的一条一条的数据，去状态 state 中找这个 key 之前的 sum\count\max\min 结果。如果有结果 <code>oldResult</code>，拿出来和当前的数据进行 <code>sum\count\max\min</code> 计算出这个 key 的新结果 <code>newResult</code>，并将新结果 <code>[key, newResult]</code> 更新到 state 中，在向下游发送新计算的结果之前，先发一条撤回上次结果的消息 <code>-[key, oldResult]</code>，然后再将新结果发往下游 <code>+[key, newResult]</code>；如果 state 中没有当前 key 的结果，则直接使用当前这条数据计算 sum\max\min 结果 <code>newResult</code>，并将新结果 <code>[key, newResult]</code> 更新到 state 中，当前是第一次往下游发，则不需要先发回撤消息，直接发送 <code>+[key, newResult]</code>。</li><li>⭐ <code>数据汇算子</code>（INSERT INTO target_table）：接收到上游发的一条一条的数据，写入到 target_table Kafka 中</li></ul><p>这个实时任务也是 24 小时一直在运行的，所有的算子在同一时刻都是处于 running 状态的。</p><blockquote><p>特别注意：</p><ol><li>Group by 聚合涉及到了回撤流（也叫 retract 流），会产生回撤流是因为从整个 SQL 的语义来看，上游的 Kafka 数据是源源不断的，无穷无尽的，那么每次这个 SQL 任务产出的结果都是一个中间结果，所以每次结果发生更新时，都需要将上一次发出的中间结果给撤回，然后将最新的结果发下去。</li><li>Group by 聚合涉及到了状态：状态大小也取决于不同 key 的数量。为了防止状态无限变大，我们可以设置状态的 TTL。以上面的 SQL 为例，上面 SQL 是按照分钟进行聚合的，理论上到了今天，通常我们就可以不用关心昨天的数据了，那么我们可以设置状态过期时间为一天。关于状态过期时间的设置参数可以参考下文 <code>运行时参数</code> 小节。</li></ol></blockquote><p>如果这个 SQL 放在 Hive 中执行时，其中 Orders 为 Hive，target_table 也为 Hive，其也会生成三个相同的算子，但是其和实时任务的执行方式完全不同：</p><ul><li>⭐ <code>数据源算子</code>（From Order）：数据源算子从 Order Hive 中读取到所有的数据，然后所有数据发送给下游的 <code>Group 聚合算子</code>，向下游发送数据的 shuffle 策略是根据 group by 中的 key 进行发送，相同的 key 发到同一个算子中，然后这个算子就运行结束了，释放资源了</li><li>⭐ <code>Group 聚合算子</code>（group by + sum\count\max\min）：接收到上游算子发的所有数据，然后遍历计算 sum\count\max\min 结果，批量发给下游 <code>数据汇算子</code>，这个算子也就运行结束了，释放资源了</li><li>⭐ <code>数据汇算子</code>（INSERT INTO target_table）：接收到上游发的一条一条的数据，写入到 target_table Hive 中，整个任务也就运行结束了，整个任务的资源也就都释放了</li></ul><h3 id="3-6-1-Group-聚合支持-Grouping-sets、Rollup、Cube"><a href="#3-6-1-Group-聚合支持-Grouping-sets、Rollup、Cube" class="headerlink" title="3.6.1.Group 聚合支持 Grouping sets、Rollup、Cube"></a>3.6.1.Group 聚合支持 Grouping sets、Rollup、Cube</h3><p>Group 聚合也支持 <code>Grouping sets</code>、<code>Rollup</code>、<code>Cube</code></p><p>举一个 <code>Grouping sets</code> 的案例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    supplier_id</span><br><span class="line">    , rating</span><br><span class="line">    , product_id</span><br><span class="line">    , <span class="built_in">COUNT</span>(<span class="operator">*</span>)</span><br><span class="line"><span class="keyword">FROM</span> (<span class="keyword">VALUES</span></span><br><span class="line">    (<span class="string">&#x27;supplier1&#x27;</span>, <span class="string">&#x27;product1&#x27;</span>, <span class="number">4</span>),</span><br><span class="line">    (<span class="string">&#x27;supplier1&#x27;</span>, <span class="string">&#x27;product2&#x27;</span>, <span class="number">3</span>),</span><br><span class="line">    (<span class="string">&#x27;supplier2&#x27;</span>, <span class="string">&#x27;product3&#x27;</span>, <span class="number">3</span>),</span><br><span class="line">    (<span class="string">&#x27;supplier2&#x27;</span>, <span class="string">&#x27;product4&#x27;</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">AS</span> Products(supplier_id, product_id, rating)</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">GROUPING</span> <span class="keyword">SET</span> (</span><br><span class="line">    ( supplier_id, product_id, rating ),</span><br><span class="line">    ( supplier_id, product_id         ),</span><br><span class="line">    ( supplier_id,             rating ),</span><br><span class="line">    ( supplier_id                     ),</span><br><span class="line">    (              product_id, rating ),</span><br><span class="line">    (              product_id         ),</span><br><span class="line">    (                          rating ),</span><br><span class="line">    (                                 )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="3-7-DML：Over-聚合"><a href="#3-7-DML：Over-聚合" class="headerlink" title="3.7.DML：Over 聚合"></a>3.7.DML：Over 聚合</h2><ol><li>⭐ Over 聚合定义（支持 Batch\Streaming）：可以理解为是一种特殊的滑动窗口聚合函数。</li></ol><p>那这里我们拿 <code>Over 聚合</code> 与 <code>窗口聚合</code> 做一个对比，其之间的最大不同之处在于：</p><ul><li>⭐ 窗口聚合：不在 group by 中的字段，不能直接在 select 中拿到</li><li>⭐ Over 聚合：能够保留原始字段</li></ul><blockquote><p>注意：</p><p>其实在生产环境中，Over 聚合的使用场景还是比较少的。在 Hive 中也有相同的聚合，但是小伙伴萌可以想想你在离线数仓经常使用嘛？</p></blockquote><ol start="2"><li><p>⭐ 应用场景：计算最近一段滑动窗口的聚合结果数据。</p></li><li><p>⭐ 实际案例：查询每个产品最近一小时订单的金额总和：</p></li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> order_id, order_time, amount,</span><br><span class="line">  <span class="built_in">SUM</span>(amount) <span class="keyword">OVER</span> (</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">BY</span> product</span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> order_time</span><br><span class="line">    <span class="keyword">RANGE</span> <span class="keyword">BETWEEN</span> <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">HOUR</span> PRECEDING <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="type">ROW</span></span><br><span class="line">  ) <span class="keyword">AS</span> one_hour_prod_amount_sum</span><br><span class="line"><span class="keyword">FROM</span> Orders</span><br></pre></td></tr></table></figure><p>Over 聚合的语法总结如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  agg_func(agg_col) <span class="keyword">OVER</span> (</span><br><span class="line">    [<span class="keyword">PARTITION</span> <span class="keyword">BY</span> col1[, col2, ...]]</span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> time_col</span><br><span class="line">    range_definition),</span><br><span class="line">  ...</span><br><span class="line"><span class="keyword">FROM</span> ...</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li>⭐ ORDER BY：必须是时间戳列（事件时间、处理时间）</li><li>⭐ PARTITION BY：标识了聚合窗口的聚合粒度，如上述案例是按照 product 进行聚合</li><li>⭐ range_definition：这个标识聚合窗口的聚合数据范围，在 Flink 中有两种指定数据范围的方式。第一种为 <code>按照行数聚合</code>，第二种为 <code>按照时间区间聚合</code>。如下案例所示：</li></ul><p>a. ⭐ 时间区间聚合：</p><p>按照时间区间聚合就是时间区间的一个滑动窗口，比如下面案例 1 小时的区间，最新输出的一条数据的 sum 聚合结果就是最近一小时数据的 amount 之和。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    order_id <span class="type">BIGINT</span>,</span><br><span class="line">    product <span class="type">BIGINT</span>,</span><br><span class="line">    amount <span class="type">BIGINT</span>,</span><br><span class="line">    order_time <span class="keyword">as</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">TIMESTAMP</span>(<span class="number">3</span>)),</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> order_time <span class="keyword">AS</span> order_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;0.001&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.order_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.order_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;2&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.amount.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.amount.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.product.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.product.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;2&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    product <span class="type">BIGINT</span>,</span><br><span class="line">    order_time <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    amount <span class="type">BIGINT</span>,</span><br><span class="line">    one_hour_prod_amount_sum <span class="type">BIGINT</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span> product, order_time, amount,</span><br><span class="line">  <span class="built_in">SUM</span>(amount) <span class="keyword">OVER</span> (</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">BY</span> product</span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> order_time</span><br><span class="line">    <span class="comment">-- 标识统计范围是一个 product 的最近 1 小时的数据</span></span><br><span class="line">    <span class="keyword">RANGE</span> <span class="keyword">BETWEEN</span> <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">HOUR</span> PRECEDING <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="type">ROW</span></span><br><span class="line">  ) <span class="keyword">AS</span> one_hour_prod_amount_sum</span><br><span class="line"><span class="keyword">FROM</span> source_table</span><br></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">+I[2, 2021-12-24T22:08:26.583, 7, 73]</span><br><span class="line">+I[2, 2021-12-24T22:08:27.583, 7, 80]</span><br><span class="line">+I[2, 2021-12-24T22:08:28.583, 4, 84]</span><br><span class="line">+I[2, 2021-12-24T22:08:29.584, 7, 91]</span><br><span class="line">+I[2, 2021-12-24T22:08:30.583, 8, 99]</span><br><span class="line">+I[1, 2021-12-24T22:08:31.583, 9, 138]</span><br><span class="line">+I[2, 2021-12-24T22:08:32.584, 6, 105]</span><br><span class="line">+I[1, 2021-12-24T22:08:33.584, 7, 145]</span><br></pre></td></tr></table></figure><p>b. ⭐ 行数聚合：</p><p>按照行数聚合就是数据行数的一个滑动窗口，比如下面案例，最新输出的一条数据的 sum 聚合结果就是最近 5 行数据的 amount 之和。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    order_id <span class="type">BIGINT</span>,</span><br><span class="line">    product <span class="type">BIGINT</span>,</span><br><span class="line">    amount <span class="type">BIGINT</span>,</span><br><span class="line">    order_time <span class="keyword">as</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">TIMESTAMP</span>(<span class="number">3</span>)),</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> order_time <span class="keyword">AS</span> order_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;0.001&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.order_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.order_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;2&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.amount.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.amount.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;2&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.product.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.product.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;2&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    product <span class="type">BIGINT</span>,</span><br><span class="line">    order_time <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    amount <span class="type">BIGINT</span>,</span><br><span class="line">    one_hour_prod_amount_sum <span class="type">BIGINT</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span> product, order_time, amount,</span><br><span class="line">  <span class="built_in">SUM</span>(amount) <span class="keyword">OVER</span> (</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">BY</span> product</span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> order_time</span><br><span class="line">    <span class="comment">-- 标识统计范围是一个 product 的最近 5 行数据</span></span><br><span class="line">    <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">5</span> PRECEDING <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="type">ROW</span></span><br><span class="line">  ) <span class="keyword">AS</span> one_hour_prod_amount_sum</span><br><span class="line"><span class="keyword">FROM</span> source_table</span><br></pre></td></tr></table></figure><p>预跑结果如下：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">+I[2, 2021-12-24T22:18:19.147, 1, 9]</span><br><span class="line">+I[1, 2021-12-24T22:18:20.147, 2, 11]</span><br><span class="line">+I[1, 2021-12-24T22:18:21.147, 2, 12]</span><br><span class="line">+I[1, 2021-12-24T22:18:22.147, 2, 12]</span><br><span class="line">+I[1, 2021-12-24T22:18:23.148, 2, 12]</span><br><span class="line">+I[1, 2021-12-24T22:18:24.147, 1, 11]</span><br><span class="line">+I[1, 2021-12-24T22:18:25.146, 1, 10]</span><br><span class="line">+I[1, 2021-12-24T22:18:26.147, 1, 9]</span><br><span class="line">+I[2, 2021-12-24T22:18:27.145, 2, 11]</span><br><span class="line">+I[2, 2021-12-24T22:18:28.148, 1, 10]</span><br><span class="line">+I[2, 2021-12-24T22:18:29.145, 2, 10]</span><br></pre></td></tr></table></figure><p>当然，如果你在一个 SELECT 中有多个聚合窗口的聚合方式，Flink SQL 支持了一种简化写法，如下案例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> order_id, order_time, amount,</span><br><span class="line">  <span class="built_in">SUM</span>(amount) <span class="keyword">OVER</span> w <span class="keyword">AS</span> sum_amount,</span><br><span class="line">  <span class="built_in">AVG</span>(amount) <span class="keyword">OVER</span> w <span class="keyword">AS</span> avg_amount</span><br><span class="line"><span class="keyword">FROM</span> Orders</span><br><span class="line"><span class="comment">-- 使用下面子句，定义 Over Window</span></span><br><span class="line"><span class="keyword">WINDOW</span> w <span class="keyword">AS</span> (</span><br><span class="line">  <span class="keyword">PARTITION</span> <span class="keyword">BY</span> product</span><br><span class="line">  <span class="keyword">ORDER</span> <span class="keyword">BY</span> order_time</span><br><span class="line">  <span class="keyword">RANGE</span> <span class="keyword">BETWEEN</span> <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">HOUR</span> PRECEDING <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="type">ROW</span>)</span><br></pre></td></tr></table></figure><h2 id="3-8-DML：Joins"><a href="#3-8-DML：Joins" class="headerlink" title="3.8.DML：Joins"></a>3.8.DML：Joins</h2><p>Flink 也支持了非常多的数据 Join 方式，主要包括以下三种：</p><ol><li>⭐ 动态表（流）与动态表（流）的 Join</li><li>⭐ 动态表（流）与外部维表（比如 Redis）的 Join</li><li>⭐ 动态表字段的列转行（一种特殊的 Join）</li></ol><p>细分 Flink SQL 支持的 Join：</p><ol><li>⭐ Regular Join：流与流的 Join，包括 Inner Equal Join、Outer Equal Join</li><li>⭐ Interval Join：流与流的 Join，两条流一段时间区间内的 Join</li><li>⭐ Temporal Join：流与流的 Join，包括事件时间，处理时间的 Temporal Join，类似于离线中的快照 Join</li><li>⭐ Lookup Join：流与外部维表的 Join</li><li>⭐ Array Expansion：表字段的列转行，类似于 Hive 的 explode 数据炸开的列转行</li><li>⭐ Table Function：自定义函数的表字段的列转行，支持 Inner Join 和 Left Outer Join</li></ol><h3 id="3-8-1-Regular-Join"><a href="#3-8-1-Regular-Join" class="headerlink" title="3.8.1.Regular Join"></a>3.8.1.Regular Join</h3><ol><li><p>⭐ Regular Join 定义（支持 Batch\Streaming）：Regular Join 其实就是和离线 Hive SQL 一样的 Regular Join，通过条件关联两条流数据输出。</p></li><li><p>⭐ 应用场景：Join 其实在我们的数仓建设过程中应用是非常广泛的。离线数仓可以说基本上是离不开 Join 的。那么实时数仓的建设也必然离不开 Join，比如日志关联扩充维度数据，构建宽表；日志通过 ID 关联计算 CTR。</p></li><li><p>⭐ Regular Join 包含以下几种（以 <code>L</code> 作为左流中的数据标识，<code>R</code> 作为右流中的数据标识）：</p></li></ol><ul><li>⭐ Inner Join（Inner Equal Join）：流任务中，只有两条流 Join 到才输出，输出 <code>+[L, R]</code></li><li>⭐ Left Join（Outer Equal Join）：流任务中，左流数据到达之后，无论有没有 Join 到右流的数据，都会输出（Join 到输出 <code>+[L, R]</code>，没 Join 到输出 <code>+[L, null]</code>），如果右流之后数据到达之后，发现左流之前输出过没有 Join 到的数据，则会发起回撤流，先输出 <code>-[L, null]</code>，然后输出 <code>+[L, R]</code></li><li>⭐ Right Join（Outer Equal Join）：有 Left Join 一样，左表和右表的执行逻辑完全相反</li><li>⭐ Full Join（Outer Equal Join）：流任务中，左流或者右流的数据到达之后，无论有没有 Join 到另外一条流的数据，都会输出（对右流来说：Join 到输出 <code>+[L, R]</code>，没 Join 到输出 <code>+[null, R]</code>；对左流来说：Join 到输出 <code>+[L, R]</code>，没 Join 到输出 <code>+[L, null]</code>）。如果一条流的数据到达之后，发现之前另一条流之前输出过没有 Join 到的数据，则会发起回撤流（左流数据到达为例：回撤 <code>-[null, R]</code>，输出 <code>+[L, R]</code>，右流数据到达为例：回撤 <code>-[L, null]</code>，输出 <code>+[L, R]</code>）。</li></ul><ol start="4"><li>⭐ 实际案例：案例为曝光日志关联点击日志筛选既有曝光又有点击的数据，并且补充点击的扩展参数（show inner click）：</li></ol><p>下面这个案例为 <code>Inner Join 案例</code>：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 曝光日志数据</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> show_log_table (</span><br><span class="line">    log_id <span class="type">BIGINT</span>,</span><br><span class="line">    show_params STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;2&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.show_params.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 点击日志数据</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> click_log_table (</span><br><span class="line">  log_id <span class="type">BIGINT</span>,</span><br><span class="line">  click_params     STRING</span><br><span class="line">)</span><br><span class="line"><span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;2&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.click_params.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    s_id <span class="type">BIGINT</span>,</span><br><span class="line">    s_params STRING,</span><br><span class="line">    c_id <span class="type">BIGINT</span>,</span><br><span class="line">    c_params STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 流的 INNER JOIN，条件为 log_id</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    show_log_table.log_id <span class="keyword">as</span> s_id,</span><br><span class="line">    show_log_table.show_params <span class="keyword">as</span> s_params,</span><br><span class="line">    click_log_table.log_id <span class="keyword">as</span> c_id,</span><br><span class="line">    click_log_table.click_params <span class="keyword">as</span> c_params</span><br><span class="line"><span class="keyword">FROM</span> show_log_table</span><br><span class="line"><span class="keyword">INNER</span> <span class="keyword">JOIN</span> click_log_table <span class="keyword">ON</span> show_log_table.log_id <span class="operator">=</span> click_log_table.log_id;</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">+I[5, d, 5, f]</span><br><span class="line">+I[5, d, 5, 8]</span><br><span class="line">+I[5, d, 5, 2]</span><br><span class="line">+I[3, 4, 3, 0]</span><br><span class="line">+I[3, 4, 3, 3]</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>如果为 <code>Left Join</code> 案例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> show_log_table (</span><br><span class="line">    log_id <span class="type">BIGINT</span>,</span><br><span class="line">    show_params STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.show_params.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;3&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> click_log_table (</span><br><span class="line">  log_id <span class="type">BIGINT</span>,</span><br><span class="line">  click_params     STRING</span><br><span class="line">)</span><br><span class="line"><span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.click_params.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;3&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    s_id <span class="type">BIGINT</span>,</span><br><span class="line">    s_params STRING,</span><br><span class="line">    c_id <span class="type">BIGINT</span>,</span><br><span class="line">    c_params STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    show_log_table.log_id <span class="keyword">as</span> s_id,</span><br><span class="line">    show_log_table.show_params <span class="keyword">as</span> s_params,</span><br><span class="line">    click_log_table.log_id <span class="keyword">as</span> c_id,</span><br><span class="line">    click_log_table.click_params <span class="keyword">as</span> c_params</span><br><span class="line"><span class="keyword">FROM</span> show_log_table</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> click_log_table <span class="keyword">ON</span> show_log_table.log_id <span class="operator">=</span> click_log_table.log_id;</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">+I[5, f3c, 5, c05]</span><br><span class="line">+I[5, 6e2, 5, 1f6]</span><br><span class="line">+I[5, 86b, 5, 1f6]</span><br><span class="line">+I[5, f3c, 5, 1f6]</span><br><span class="line">-D[3, 4ab, null, null]</span><br><span class="line">-D[3, 6f2, null, null]</span><br><span class="line">+I[3, 4ab, 3, 765]</span><br><span class="line">+I[3, 6f2, 3, 765]</span><br><span class="line">+I[2, 3c4, null, null]</span><br><span class="line">+I[3, 4ab, 3, a8b]</span><br><span class="line">+I[3, 6f2, 3, a8b]</span><br><span class="line">+I[2, c03, null, null]</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>如果为 <code>Full Join</code> 案例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> show_log_table (</span><br><span class="line">    log_id <span class="type">BIGINT</span>,</span><br><span class="line">    show_params STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;2&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.show_params.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> click_log_table (</span><br><span class="line">  log_id <span class="type">BIGINT</span>,</span><br><span class="line">  click_params     STRING</span><br><span class="line">)</span><br><span class="line"><span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;2&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.click_params.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    s_id <span class="type">BIGINT</span>,</span><br><span class="line">    s_params STRING,</span><br><span class="line">    c_id <span class="type">BIGINT</span>,</span><br><span class="line">    c_params STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    show_log_table.log_id <span class="keyword">as</span> s_id,</span><br><span class="line">    show_log_table.show_params <span class="keyword">as</span> s_params,</span><br><span class="line">    click_log_table.log_id <span class="keyword">as</span> c_id,</span><br><span class="line">    click_log_table.click_params <span class="keyword">as</span> c_params</span><br><span class="line"><span class="keyword">FROM</span> show_log_table</span><br><span class="line"><span class="keyword">FULL</span> <span class="keyword">JOIN</span> click_log_table <span class="keyword">ON</span> show_log_table.log_id <span class="operator">=</span> click_log_table.log_id;</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">+I[null, null, 7, 6]</span><br><span class="line">+I[6, 5, null, null]</span><br><span class="line">-D[1, c, null, null]</span><br><span class="line">+I[1, c, 1, 2]</span><br><span class="line">+I[3, 1, null, null]</span><br><span class="line">+I[null, null, 7, d]</span><br><span class="line">+I[10, 0, null, null]</span><br><span class="line">+I[null, null, 2, 6]</span><br><span class="line">-D[null, null, 7, 6]</span><br><span class="line">-D[null, null, 7, d]</span><br><span class="line">...</span><br></pre></td></tr></table></figure><blockquote><p>关于 Regular Join 的注意事项：</p><ul><li><p>⭐ 实时 Regular Join 可以不是 <code>等值 join</code>。<code>等值 join</code> 和 <code>非等值 join</code> 区别在于，<code>等值 join</code> 数据 shuffle 策略是 Hash，会按照 Join on 中的等值条件作为 id 发往对应的下游；<code>非等值 join</code> 数据 shuffle 策略是 Global，所有数据发往一个并发，按照非等值条件进行关联</p></li><li><p>⭐ Join 的流程是左流新来一条数据之后，会和右流中符合条件的所有数据做 Join，然后输出。</p></li><li><p>⭐ 流的上游是无限的数据，所以要做到关联的话，Flink 会将两条流的所有数据都存储在 State 中，所以 Flink 任务的 State 会无限增大，因此你需要为 State 配置合适的 TTL，以防止 State 过大。</p></li></ul></blockquote><ol start="5"><li>⭐ <code>SQL 语义</code>：</li></ol><p>详细的 SQL 语义案例可以参考：<a href="https://mp.weixin.qq.com/s/Z8QfKfhrX5KEnR-s7gRtsA">https://mp.weixin.qq.com/s/Z8QfKfhrX5KEnR-s7gRtsA</a></p><h3 id="3-8-2-Interval-Join（时间区间-Join）"><a href="#3-8-2-Interval-Join（时间区间-Join）" class="headerlink" title="3.8.2.Interval Join（时间区间 Join）"></a>3.8.2.Interval Join（时间区间 Join）</h3><ol><li><p>⭐ Interval Join 定义（支持 Batch\Streaming）：Interval Join 在离线的概念中是没有的。Interval Join 可以让一条流去 Join 另一条流中前后一段时间内的数据。</p></li><li><p>⭐ 应用场景：为什么有 Regular Join 还要 Interval Join 呢？刚刚的案例也讲了，Regular Join 会产生回撤流，但是在实时数仓中一般写入的 sink 都是类似于 Kafka 这样的消息队列，然后后面接 clickhouse 等引擎，这些引擎又不具备处理回撤流的能力。所以博主理解 Interval Join 就是用于消灭回撤流的。</p></li><li><p>⭐ Interval Join 包含以下几种（以 <code>L</code> 作为左流中的数据标识，<code>R</code> 作为右流中的数据标识）：</p></li></ol><ul><li>⭐ Inner Interval Join：流任务中，只有两条流 Join 到（满足 Join on 中的条件：两条流的数据在时间区间 + 满足其他等值条件）才输出，输出 <code>+[L, R]</code></li><li>⭐ Left Interval Join：流任务中，左流数据到达之后，如果没有 Join 到右流的数据，就会等待（放在 State 中等），如果之后右流之后数据到达之后，发现能和刚刚那条左流数据 Join 到，则会输出 <code>+[L, R]</code>。事件时间中随着 Watermark 的推进（也支持处理时间）。如果发现发现左流 State 中的数据过期了，就把左流中过期的数据从 State 中删除，然后输出 <code>+[L, null]</code>，如果右流 State 中的数据过期了，就直接从 State 中删除。</li><li>⭐ Right Interval Join：和 Left Interval Join 执行逻辑一样，只不过左表和右表的执行逻辑完全相反</li><li>⭐ Full Interval Join：流任务中，左流或者右流的数据到达之后，如果没有 Join 到另外一条流的数据，就会等待（左流放在左流对应的 State 中等，右流放在右流对应的 State 中等），如果之后另一条流数据到达之后，发现能和刚刚那条数据 Join 到，则会输出 <code>+[L, R]</code>。事件时间中随着 Watermark 的推进（也支持处理时间），发现 State 中的数据能够过期了，就将这些数据从 State 中删除并且输出（左流过期输出 <code>+[L, null]</code>，右流过期输出 <code>-[null, R]</code>）</li></ul><p>可以发现 Inner Interval Join 和其他三种 Outer Interval Join 的区别在于，Outer 在随着时间推移的过程中，如果有数据过期了之后，会根据是否是 Outer 将没有 Join 到的数据也给输出。</p><ol start="4"><li>⭐ 实际案例：还是刚刚的案例，曝光日志关联点击日志筛选既有曝光又有点击的数据，条件是曝光关联之后发生 4 小时之内的点击，并且补充点击的扩展参数（show inner interval click）：</li></ol><p>下面为 <code>Inner Interval Join</code>：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> show_log_table (</span><br><span class="line">    log_id <span class="type">BIGINT</span>,</span><br><span class="line">    show_params STRING,</span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.show_params.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> click_log_table (</span><br><span class="line">    log_id <span class="type">BIGINT</span>,</span><br><span class="line">    click_params STRING,</span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time</span><br><span class="line">)</span><br><span class="line"><span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.click_params.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    s_id <span class="type">BIGINT</span>,</span><br><span class="line">    s_params STRING,</span><br><span class="line">    c_id <span class="type">BIGINT</span>,</span><br><span class="line">    c_params STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    show_log_table.log_id <span class="keyword">as</span> s_id,</span><br><span class="line">    show_log_table.show_params <span class="keyword">as</span> s_params,</span><br><span class="line">    click_log_table.log_id <span class="keyword">as</span> c_id,</span><br><span class="line">    click_log_table.click_params <span class="keyword">as</span> c_params</span><br><span class="line"><span class="keyword">FROM</span> show_log_table <span class="keyword">INNER</span> <span class="keyword">JOIN</span> click_log_table <span class="keyword">ON</span> show_log_table.log_id <span class="operator">=</span> click_log_table.log_id</span><br><span class="line"><span class="keyword">AND</span> show_log_table.row_time <span class="keyword">BETWEEN</span> click_log_table.row_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;4&#x27;</span> <span class="keyword">HOUR</span> <span class="keyword">AND</span> click_log_table.row_time;</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">6&gt;</span><span class="bash"> +I[2, a, 2, 6]</span></span><br><span class="line"><span class="meta">6&gt;</span><span class="bash"> +I[2, 6, 2, 6]</span></span><br><span class="line"><span class="meta">2&gt;</span><span class="bash"> +I[4, 1, 4, 5]</span></span><br><span class="line"><span class="meta">2&gt;</span><span class="bash"> +I[10, 8, 10, d]</span></span><br><span class="line"><span class="meta">2&gt;</span><span class="bash"> +I[10, 7, 10, d]</span></span><br><span class="line"><span class="meta">2&gt;</span><span class="bash"> +I[10, d, 10, d]</span></span><br><span class="line"><span class="meta">2&gt;</span><span class="bash"> +I[5, b, 5, d]</span></span><br><span class="line"><span class="meta">6&gt;</span><span class="bash"> +I[1, a, 1, 7]</span></span><br></pre></td></tr></table></figure><p>如果是 <code>Left Interval Join</code>：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> show_log (</span><br><span class="line">    log_id <span class="type">BIGINT</span>,</span><br><span class="line">    show_params STRING,</span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.show_params.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> click_log (</span><br><span class="line">    log_id <span class="type">BIGINT</span>,</span><br><span class="line">    click_params STRING,</span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time</span><br><span class="line">)</span><br><span class="line"><span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.click_params.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    s_id <span class="type">BIGINT</span>,</span><br><span class="line">    s_params STRING,</span><br><span class="line">    c_id <span class="type">BIGINT</span>,</span><br><span class="line">    c_params STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    show_log.log_id <span class="keyword">as</span> s_id,</span><br><span class="line">    show_log.show_params <span class="keyword">as</span> s_params,</span><br><span class="line">    click_log.log_id <span class="keyword">as</span> c_id,</span><br><span class="line">    click_log.click_params <span class="keyword">as</span> c_params</span><br><span class="line"><span class="keyword">FROM</span> show_log <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> click_log <span class="keyword">ON</span> show_log.log_id <span class="operator">=</span> click_log.log_id</span><br><span class="line"><span class="keyword">AND</span> show_log.row_time <span class="keyword">BETWEEN</span> click_log.row_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span> <span class="keyword">AND</span> click_log.row_time <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span>;</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">+I[6, e, 6, 7]</span><br><span class="line">+I[11, d, null, null]</span><br><span class="line">+I[7, b, null, null]</span><br><span class="line">+I[8, 0, 8, 3]</span><br><span class="line">+I[13, 6, null, null]</span><br></pre></td></tr></table></figure><p>如果是 <code>Full Interval Join</code>：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> show_log (</span><br><span class="line">    log_id <span class="type">BIGINT</span>,</span><br><span class="line">    show_params STRING,</span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.show_params.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;5&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;15&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> click_log (</span><br><span class="line">    log_id <span class="type">BIGINT</span>,</span><br><span class="line">    click_params STRING,</span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time</span><br><span class="line">)</span><br><span class="line"><span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.click_params.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    s_id <span class="type">BIGINT</span>,</span><br><span class="line">    s_params STRING,</span><br><span class="line">    c_id <span class="type">BIGINT</span>,</span><br><span class="line">    c_params STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    show_log.log_id <span class="keyword">as</span> s_id,</span><br><span class="line">    show_log.show_params <span class="keyword">as</span> s_params,</span><br><span class="line">    click_log.log_id <span class="keyword">as</span> c_id,</span><br><span class="line">    click_log.click_params <span class="keyword">as</span> c_params</span><br><span class="line"><span class="keyword">FROM</span> show_log <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> click_log <span class="keyword">ON</span> show_log.log_id <span class="operator">=</span> click_log.log_id</span><br><span class="line"><span class="keyword">AND</span> show_log.row_time <span class="keyword">BETWEEN</span> click_log.row_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span> <span class="keyword">AND</span> click_log.row_time <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span>;</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">+I[6, 1, null, null]</span><br><span class="line">+I[7, 3, 7, 8]</span><br><span class="line">+I[null, null, 6, 6]</span><br><span class="line">+I[null, null, 4, d]</span><br><span class="line">+I[8, d, null, null]</span><br><span class="line">+I[null, null, 3, b]</span><br></pre></td></tr></table></figure><blockquote><p>关于 Interval Join 的注意事项：</p><p>⭐ 实时 Interval Join 可以不是 <code>等值 join</code>。<code>等值 join</code> 和 <code>非等值 join</code> 区别在于，<code>等值 join</code> 数据 shuffle 策略是 Hash，会按照 Join on 中的等值条件作为 id 发往对应的下游；<code>非等值 join</code> 数据 shuffle 策略是 Global，所有数据发往一个并发，然后将满足条件的数据进行关联输出</p></blockquote><ol start="5"><li>⭐ SQL 语义：</li></ol><p>关于详细的 SQL 语义可以参考。</p><p><a href="https://mp.weixin.qq.com/s/p9Y9qzqMgd8DTs9Fw_25kA">https://mp.weixin.qq.com/s/p9Y9qzqMgd8DTs9Fw_25kA</a></p><h3 id="3-8-3-Temporal-Join（快照-Join）"><a href="#3-8-3-Temporal-Join（快照-Join）" class="headerlink" title="3.8.3.Temporal Join（快照 Join）"></a>3.8.3.Temporal Join（快照 Join）</h3><ol><li><p>⭐ Temporal Join 定义（支持 Batch\Streaming）：Temporal Join 在离线的概念中其实是没有类似的 Join 概念的，但是离线中常常会维护一种表叫做 <code>拉链快照表</code>，使用一个明细表去 join 这个 <code>拉链快照表</code> 的 join 方式就叫做 Temporal Join。而 Flink SQL 中也有对应的概念，表叫做 <code>Versioned Table</code>，使用一个明细表去 join 这个 <code>Versioned Table</code> 的 join 操作就叫做 Temporal Join。Temporal Join 中，<code>Versioned Table</code> 其实就是对同一条 key（在 DDL 中以 primary key 标记同一个 key）的历史版本（根据时间划分版本）做一个维护，当有明细表 Join 这个表时，可以根据明细表中的时间版本选择 <code>Versioned Table</code> 对应时间区间内的快照数据进行 join。</p></li><li><p>⭐ 应用场景：比如常见的汇率数据（实时的根据汇率计算总金额），在 12:00 之前（事件时间），人民币和美元汇率是 7:1，在 12:00 之后变为 6:1，那么在 12:00 之前数据就要按照 7:1 进行计算，12:00 之后就要按照 6:1 计算。在事件时间语义的任务中，事件时间 12:00 之前的数据，要按照 7:1 进行计算，12:00 之后的数据，要按照 6:1 进行计算。这其实就是离线中快照的概念，维护具体汇率的表在 Flink SQL 体系中就叫做 <code>Versioned Table</code>。</p></li><li><p>⭐ Verisoned Table：Verisoned Table 中存储的数据通常是来源于 CDC 或者会发生更新的数据。Flink SQL 会为 Versioned Table 维护 Primary Key 下的所有历史时间版本的数据。举一个汇率的场景的案例来看一下一个 Versioned Table 的两种定义方式。</p></li></ol><ul><li>⭐ PRIMARY KEY 定义方式：</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 定义一个汇率 versioned 表，其中 versioned 表的概念下文会介绍到</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> currency_rates (</span><br><span class="line">    currency STRING,</span><br><span class="line">    conversion_rate <span class="type">DECIMAL</span>(<span class="number">32</span>, <span class="number">2</span>),</span><br><span class="line">    update_time <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> `values.source.timestamp` VIRTUAL,</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> update_time <span class="keyword">AS</span> update_time,</span><br><span class="line">    <span class="comment">-- PRIMARY KEY 定义方式</span></span><br><span class="line">    <span class="keyword">PRIMARY</span> KEY(currency) <span class="keyword">NOT</span> ENFORCED</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">   <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;value.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;debezium-json&#x27;</span>,</span><br><span class="line">   <span class="comment">/* ... */</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><ul><li>⭐ Deduplicate 定义方式：</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 定义一个 append-only 的数据源表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> currency_rates (</span><br><span class="line">    currency STRING,</span><br><span class="line">    conversion_rate <span class="type">DECIMAL</span>(<span class="number">32</span>, <span class="number">2</span>),</span><br><span class="line">    update_time <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> `values.source.timestamp` VIRTUAL,</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> update_time <span class="keyword">AS</span> update_time</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;value.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;debezium-json&#x27;</span>,</span><br><span class="line">    <span class="comment">/* ... */</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 将数据源表按照 Deduplicate 方式定义为 Versioned Table</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> versioned_rates <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span> currency, conversion_rate, update_time   <span class="comment">-- 1. 定义 `update_time` 为时间字段</span></span><br><span class="line">  <span class="keyword">FROM</span> (</span><br><span class="line">      <span class="keyword">SELECT</span> <span class="operator">*</span>,</span><br><span class="line">      <span class="built_in">ROW_NUMBER</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> currency  <span class="comment">-- 2. 定义 `currency` 为主键</span></span><br><span class="line">         <span class="keyword">ORDER</span> <span class="keyword">BY</span> update_time <span class="keyword">DESC</span>              <span class="comment">-- 3. ORDER BY 中必须是时间戳列</span></span><br><span class="line">      ) <span class="keyword">AS</span> rownum </span><br><span class="line">      <span class="keyword">FROM</span> currency_rates)</span><br><span class="line"><span class="keyword">WHERE</span> rownum <span class="operator">=</span> <span class="number">1</span>; </span><br></pre></td></tr></table></figure><ol start="4"><li><p>⭐ Temporal Join 支持的时间语义：事件时间、处理时间</p></li><li><p>⭐ 实际案例：就是上文提到的汇率计算。</p></li></ol><p>以 <code>事件时间</code> 任务举例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 1. 定义一个输入订单表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> orders (</span><br><span class="line">    order_id    STRING,</span><br><span class="line">    price       <span class="type">DECIMAL</span>(<span class="number">32</span>,<span class="number">2</span>),</span><br><span class="line">    currency    STRING,</span><br><span class="line">    order_time  <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> order_time <span class="keyword">AS</span> order_time</span><br><span class="line">) <span class="keyword">WITH</span> (<span class="comment">/* ... */</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2. 定义一个汇率 versioned 表，其中 versioned 表的概念下文会介绍到</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> currency_rates (</span><br><span class="line">    currency STRING,</span><br><span class="line">    conversion_rate <span class="type">DECIMAL</span>(<span class="number">32</span>, <span class="number">2</span>),</span><br><span class="line">    update_time <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> `values.source.timestamp` VIRTUAL,</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> update_time <span class="keyword">AS</span> update_time,</span><br><span class="line">    <span class="keyword">PRIMARY</span> KEY(currency) <span class="keyword">NOT</span> ENFORCED</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">   <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;value.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;debezium-json&#x27;</span>,</span><br><span class="line">   <span class="comment">/* ... */</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">     order_id,</span><br><span class="line">     price,</span><br><span class="line">     currency,</span><br><span class="line">     conversion_rate,</span><br><span class="line">     order_time,</span><br><span class="line"><span class="keyword">FROM</span> orders</span><br><span class="line"><span class="comment">-- 3. Temporal Join 逻辑</span></span><br><span class="line"><span class="comment">-- SQL 语法为：FOR SYSTEM_TIME AS OF</span></span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> currency_rates <span class="keyword">FOR</span> <span class="built_in">SYSTEM_TIME</span> <span class="keyword">AS</span> <span class="keyword">OF</span> orders.order_time</span><br><span class="line"><span class="keyword">ON</span> orders.currency <span class="operator">=</span> currency_rates.currency;</span><br></pre></td></tr></table></figure><p>结果如下，可以看到相同的货币汇率会根据具体数据的事件时间不同 Join 到对应时间的汇率：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">order_id  price  货币       汇率             order_time</span><br><span class="line">========  =====  ========  ===============  =========</span><br><span class="line">o_001     11.11  EUR       1.14             12:00:00</span><br><span class="line">o_002     12.51  EUR       1.10             12:06:00</span><br></pre></td></tr></table></figure><blockquote><p>注意：</p><ol><li>⭐ 事件时间的 Temporal Join 一定要给左右两张表都设置 Watermark。</li><li>⭐ 事件时间的 Temporal Join 一定要把 Versioned Table 的主键包含在 Join on 的条件中。</li></ol></blockquote><p>还是相同的案例，如果是 <code>处理时间</code> 语义：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">10</span>:<span class="number">15</span><span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> LatestRates;</span><br><span class="line"></span><br><span class="line">currency   rate</span><br><span class="line"><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span> <span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span></span><br><span class="line">US Dollar   <span class="number">102</span></span><br><span class="line">Euro        <span class="number">114</span></span><br><span class="line">Yen           <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="number">10</span>:<span class="number">30</span><span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> LatestRates;</span><br><span class="line"></span><br><span class="line">currency   rate</span><br><span class="line"><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span> <span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span></span><br><span class="line">US Dollar   <span class="number">102</span></span><br><span class="line">Euro        <span class="number">114</span></span><br><span class="line">Yen           <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 10:42 时，Euro 的汇率从 114 变为 116</span></span><br><span class="line"><span class="number">10</span>:<span class="number">52</span><span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> LatestRates;</span><br><span class="line"></span><br><span class="line">currency   rate</span><br><span class="line"><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span> <span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span></span><br><span class="line">US Dollar   <span class="number">102</span></span><br><span class="line">Euro        <span class="number">116</span>     <span class="operator">&lt;=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span> 从 <span class="number">114</span> 变为 <span class="number">116</span></span><br><span class="line">Yen           <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 从 Orders 表查询数据</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> Orders;</span><br><span class="line"></span><br><span class="line">amount currency</span><br><span class="line"><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span> <span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span></span><br><span class="line">     <span class="number">2</span> Euro             <span class="operator">&lt;=</span><span class="operator">=</span> 在处理时间 <span class="number">10</span>:<span class="number">15</span> 到达的一条数据</span><br><span class="line">     <span class="number">1</span> US Dollar        <span class="operator">&lt;=</span><span class="operator">=</span> 在处理时间 <span class="number">10</span>:<span class="number">30</span> 到达的一条数据</span><br><span class="line">     <span class="number">2</span> Euro             <span class="operator">&lt;=</span><span class="operator">=</span> 在处理时间 <span class="number">10</span>:<span class="number">52</span> 到达的一条数据</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 执行关联查询</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  o.amount, o.currency, r.rate, o.amount <span class="operator">*</span> r.rate</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  Orders <span class="keyword">AS</span> o</span><br><span class="line">  <span class="keyword">JOIN</span> LatestRates <span class="keyword">FOR</span> <span class="built_in">SYSTEM_TIME</span> <span class="keyword">AS</span> <span class="keyword">OF</span> o.proctime <span class="keyword">AS</span> r</span><br><span class="line">  <span class="keyword">ON</span> r.currency <span class="operator">=</span> o.currency</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 结果如下：</span></span><br><span class="line">amount currency     rate   amount<span class="operator">*</span>rate</span><br><span class="line"><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span> <span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span> <span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span> <span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span><span class="operator">=</span></span><br><span class="line">     <span class="number">2</span> Euro          <span class="number">114</span>          <span class="number">228</span>    <span class="operator">&lt;=</span><span class="operator">=</span> 在处理时间 <span class="number">10</span>:<span class="number">15</span> 到达的一条数据</span><br><span class="line">     <span class="number">1</span> US Dollar     <span class="number">102</span>          <span class="number">102</span>    <span class="operator">&lt;=</span><span class="operator">=</span> 在处理时间 <span class="number">10</span>:<span class="number">30</span> 到达的一条数据</span><br><span class="line">     <span class="number">2</span> Euro          <span class="number">116</span>          <span class="number">232</span>    <span class="operator">&lt;=</span><span class="operator">=</span> 在处理时间 <span class="number">10</span>:<span class="number">52</span> 到达的一条数据</span><br></pre></td></tr></table></figure><p>可以发现处理时间就比较好理解了，因为处理时间语义中是根据左流数据到达的时间决定拿到的汇率值。Flink 就只为 LatestRates 维护了最新的状态数据，不需要关心历史版本的数据。</p><h3 id="3-8-4-Lookup-Join（维表-Join）"><a href="#3-8-4-Lookup-Join（维表-Join）" class="headerlink" title="3.8.4.Lookup Join（维表 Join）"></a>3.8.4.Lookup Join（维表 Join）</h3><ol><li><p>⭐ Lookup Join 定义（支持 Batch\Streaming）：Lookup Join 其实就是维表 Join，比如拿离线数仓来说，常常会有用户画像，设备画像等数据，而对应到实时数仓场景中，这种实时获取外部缓存的 Join 就叫做维表 Join。</p></li><li><p>⭐ 应用场景：小伙伴萌会问，我们既然已经有了上面介绍的 Regular Join，Interval Join 等，为啥还需要一种 Lookup Join？因为上面说的这几种 Join 都是流与流之间的 Join，而 Lookup Join 是流与 Redis，Mysql，HBase 这种存储介质的 Join。Lookup 的意思就是实时查找，而实时的画像数据一般都是存储在 Redis，Mysql，HBase 中，这就是 Lookup Join 的由来</p></li><li><p>⭐ 实际案例：使用曝光用户日志流（show_log）关联用户画像维表（user_profile）关联到用户的维度之后，提供给下游计算分性别，年龄段的曝光用户数使用。</p></li></ol><p>来一波输入数据：</p><p>曝光用户日志流（show_log）数据（数据存储在 kafka 中）：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">log_idtimestamp        user_id</span><br><span class="line">1       2021-11-01 00:01:03a</span><br><span class="line">2       2021-11-01 00:03:00b</span><br><span class="line">3       2021-11-01 00:05:00c</span><br><span class="line">4       2021-11-01 00:06:00b</span><br><span class="line">5       2021-11-01 00:07:00c</span><br></pre></td></tr></table></figure><p>用户画像维表（user_profile）数据（数据存储在 redis 中）：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">user_id(主键)age    sex</span><br><span class="line">a               12-18   男</span><br><span class="line">b               18-24   女</span><br><span class="line">c               18-24   男</span><br></pre></td></tr></table></figure><blockquote><p>注意：</p><p>redis 中的数据结构存储是按照 key，value 去存储的。其中 key 为 user_id，value 为 age，sex 的 json。</p></blockquote><p>具体 SQL：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> show_log (</span><br><span class="line">    log_id <span class="type">BIGINT</span>,</span><br><span class="line">    `<span class="type">timestamp</span>` <span class="keyword">as</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    user_id STRING,</span><br><span class="line">    proctime <span class="keyword">AS</span> PROCTIME()</span><br><span class="line">)</span><br><span class="line"><span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_profile (</span><br><span class="line">    user_id STRING,</span><br><span class="line">    age STRING,</span><br><span class="line">    sex STRING</span><br><span class="line">    ) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;redis&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;hostname&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;127.0.0.1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;port&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;6379&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;lookup.cache.max-rows&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;500&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;lookup.cache.ttl&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;3600&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;lookup.max-retries&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    log_id <span class="type">BIGINT</span>,</span><br><span class="line">    `<span class="type">timestamp</span>` <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    user_id STRING,</span><br><span class="line">    proctime <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    age STRING,</span><br><span class="line">    sex STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- lookup join 的 query 逻辑</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    s.log_id <span class="keyword">as</span> log_id</span><br><span class="line">    , s.`<span class="type">timestamp</span>` <span class="keyword">as</span> `<span class="type">timestamp</span>`</span><br><span class="line">    , s.user_id <span class="keyword">as</span> user_id</span><br><span class="line">    , s.proctime <span class="keyword">as</span> proctime</span><br><span class="line">    , u.sex <span class="keyword">as</span> sex</span><br><span class="line">    , u.age <span class="keyword">as</span> age</span><br><span class="line"><span class="keyword">FROM</span> show_log <span class="keyword">AS</span> s</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> user_profile <span class="keyword">FOR</span> <span class="built_in">SYSTEM_TIME</span> <span class="keyword">AS</span> <span class="keyword">OF</span> s.proctime <span class="keyword">AS</span> u</span><br><span class="line"><span class="keyword">ON</span> s.user_id <span class="operator">=</span> u.user_id</span><br></pre></td></tr></table></figure><p>输出数据如下：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">log_id  timestamp           user_id age    sex</span><br><span class="line">1       2021-11-01 00:01:03 a       12-18   男</span><br><span class="line">2       2021-11-01 00:03:00 b       18-24   女</span><br><span class="line">3       2021-11-01 00:05:00 c       18-24   男</span><br><span class="line">4       2021-11-01 00:06:00 b       18-24   女</span><br><span class="line">5       2021-11-01 00:07:00 c       18-24   男</span><br></pre></td></tr></table></figure><blockquote><p>注意：</p><p>实时的 lookup 维表关联能使用 <code>处理时间</code> 去做关联。</p></blockquote><ol start="4"><li>⭐ SQL 语义：</li></ol><p>详细 SQL 语义及案例可见：<a href="https://mp.weixin.qq.com/s/ku11tCZp7CAFzpkqd4J1cQ">https://mp.weixin.qq.com/s/ku11tCZp7CAFzpkqd4J1cQ</a></p><p>其实，Flink 官方并没有提供 redis 的维表 connector 实现。</p><p>没错，博主自己实现了一套。关于 redis 维表的 connector 实现，直接参考下面的文章。都是可以从 github 上找到源码拿来用的！</p><blockquote><p>注意：</p><ol><li>⭐ 同一条数据关联到的维度数据可能不同：实时数仓中常用的实时维表都是在不断的变化中的，当前流表数据关联完维表数据后，如果同一个 key 的维表的数据发生了变化，已关联到的维表的结果数据不会再同步更新。举个例子，维表中 user_id 为 1 的数据在 08：00 时 age 由 12-18 变为了 18-24，那么当我们的任务在 08：01 failover 之后从 07：59 开始回溯数据时，原本应该关联到 12-18 的数据会关联到 18-24 的 age 数据。这是有可能会影响数据质量的。所以小伙伴萌在评估你们的实时任务时要考虑到这一点。</li><li>⭐ 会发生实时的新建及更新的维表博主建议小伙伴萌应该建立起数据延迟的监控机制，防止出现流表数据先于维表数据到达，导致关联不到维表数据</li></ol></blockquote><p>再说说维表常见的性能问题及优化思路。</p><p>所有的维表性能问题都可以总结为：<code>高 qps 下访问维表存储引擎产生的任务背压，数据产出延迟问题。</code></p><p>举个例子：</p><ol><li>⭐ 在没有使用维表的情况下：一条数据从输入 Flink 任务到输出 Flink 任务的时延假如为 <code>0.1 ms</code>，那么并行度为 1 的任务的吞吐可以达到 <code>1 query / 0.1 ms = 1w qps</code>。</li><li>⭐ 在使用维表之后：每条数据访问维表的外部存储的时长为 <code>2 ms</code>，那么一条数据从输入 Flink 任务到输出 Flink 任务的时延就会变成 <code>2.1 ms</code>，那么同样并行度为 1 的任务的吞吐只能达到<code>1 query / 2.1 ms = 476 qps</code>。两者的吞吐量相差 <code>21 倍</code>。</li></ol><p>这就是为什么维表 join 的算子会产生背压，任务产出会延迟。</p><p>那么当然，解决方案也是有很多的。抛开 Flink SQL 想一下，如果我们使用 DataStream API，甚至是在做一个后端应用，需要访问外部存储时，常用的优化方案有哪些？这里列举一下：</p><ol><li>⭐ 按照 redis 维表的 key 分桶 + local cache：通过按照 key 分桶的方式，让大多数据的维表关联的数据访问走之前访问过得 local cache 即可。这样就可以把访问外部存储 2.1 ms 处理一个 query 变为访问内存的 0.1 ms 处理一个 query 的时长。</li><li>⭐ 异步访问外存：DataStream api 有异步算子，可以利用线程池去同时多次请求维表外部存储。这样就可以把 2.1 ms 处理 1 个 query 变为 2.1 ms 处理 10 个 query。吞吐可变优化到 10 / 2.1 ms = 4761 qps。</li><li>⭐ 批量访问外存：除了异步访问之外，我们还可以批量访问外部存储。举一个例子：在访问 redis 维表的 1 query 占用 2.1 ms 时长中，其中可能有 2 ms 都是在网络请求上面的耗时 ，其中只有 0.1 ms 是 redis server 处理请求的时长。那么我们就可以使用 redis 提供的 pipeline 能力，在客户端（也就是 flink 任务 lookup join 算子中），攒一批数据，使用 pipeline 去同时访问 redis sever。这样就可以把 2.1 ms 处理 1 个 query 变为 7ms（2ms + 50 * 0.1ms） 处理 50 个 query。吞吐可变为 50 query / 7 ms = 7143 qps。博主这里测试了下使用 redis pipeline 和未使用的时长消耗对比。如下图所示。</li></ol><p>博主认为上述优化效果中，最好用的是 1 + 3，2 相比 3 还是一条一条发请求，性能会差一些。</p><p>既然 DataStream 可以这样做，Flink SQL 必须必的也可以借鉴上面的这些优化方案。具体怎么操作呢？看下文骚操作</p><ol><li>⭐ 按照 redis 维表的 key 分桶 + local cache：sql 中如果要做分桶，得先做 group by，但是如果做了 group by 的聚合，就只能在 udaf 中做访问 redis 处理，并且 udaf 产出的结果只能是一条，所以这种实现起来非常复杂。我们选择不做 keyby 分桶。但是我们可以直接使用 local cache 去做本地缓存，虽然【直接缓存】的效果比【先按照 key 分桶再做缓存】的效果差，但是也能一定程度上减少访问 redis 压力。在博主实现的 redis connector 中，内置了 local cache 的实现，小伙伴萌可以参考下面这部篇文章进行配置。</li><li>⭐ 异步访问外存：目前博主实现的 redis connector 不支持异步访问，但是官方实现的 hbase connector 支持这个功能，参考下面链接文章的，点开之后搜索 lookup.async。<a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/connectors/table/hbase/">https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/connectors/table/hbase/</a></li><li>⭐ 批量访问外存：这玩意官方必然没有实现啊，但是，但是，但是，经过博主周末两天的疯狂 debug，改了改源码，搞定了基于 redis 的批量访问外存优化的功能。具体可以参考下文。</li></ol><p>关于批量访问外存可参考：<a href="https://mp.weixin.qq.com/s/ku11tCZp7CAFzpkqd4J1cQ">https://mp.weixin.qq.com/s/ku11tCZp7CAFzpkqd4J1cQ</a></p><h3 id="3-8-5-Array-Expansion（数组列转行）"><a href="#3-8-5-Array-Expansion（数组列转行）" class="headerlink" title="3.8.5.Array Expansion（数组列转行）"></a>3.8.5.Array Expansion（数组列转行）</h3><ol><li><p>⭐ 应用场景（支持 Batch\Streaming）：将表中 ARRAY 类型字段（列）拍平，转为多行</p></li><li><p>⭐ 实际案例：比如某些场景下，日志是合并、攒批上报的，就可以使用这种方式将一个 Array 转为多行。</p></li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> show_log_table (</span><br><span class="line">    log_id <span class="type">BIGINT</span>,</span><br><span class="line">    show_params <span class="keyword">ARRAY</span><span class="operator">&lt;</span>STRING<span class="operator">&gt;</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    log_id <span class="type">BIGINT</span>,</span><br><span class="line">    show_param STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    log_id,</span><br><span class="line">    t.show_param <span class="keyword">as</span> show_param</span><br><span class="line"><span class="keyword">FROM</span> show_log_table</span><br><span class="line"><span class="comment">-- array 炸开语法</span></span><br><span class="line"><span class="keyword">CROSS</span> <span class="keyword">JOIN</span> <span class="built_in">UNNEST</span>(show_params) <span class="keyword">AS</span> t (show_param)</span><br></pre></td></tr></table></figure><p>show_log_table 原始数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">+</span>I[<span class="number">7</span>, [a, b, c]]</span><br><span class="line"><span class="operator">+</span>I[<span class="number">5</span>, [d, e, f]]</span><br></pre></td></tr></table></figure><p>输出结果如下所示：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-- +I[7, [a, b, c]] 一行转为 3 行</span><br><span class="line">+I[7, a]</span><br><span class="line">+I[7, b]</span><br><span class="line">+I[7, b]</span><br><span class="line">-- +I[5, [d, e, f]] 一行转为 3 行</span><br><span class="line">+I[5, d]</span><br><span class="line">+I[5, e]</span><br><span class="line">+I[5, f]</span><br></pre></td></tr></table></figure><h3 id="3-8-6-Table-Function（自定义列转行）"><a href="#3-8-6-Table-Function（自定义列转行）" class="headerlink" title="3.8.6.Table Function（自定义列转行）"></a>3.8.6.Table Function（自定义列转行）</h3><ol><li><p>⭐ 应用场景（支持 Batch\Streaming）：这个其实和 Array Expansion 功能类似，但是 Table Function 本质上是个 UDTF 函数，和离线 Hive SQL 一样，我们可以自定义 UDTF 去决定列转行的逻辑</p></li><li><p>⭐ Table Function 使用分类：</p></li></ol><ul><li>⭐ Inner Join Table Function：如果 UDTF 返回结果为空，则相当于 1 行转为 0 行，这行数据直接被丢弃</li><li>⭐ Left Join Table Function：如果 UDTF 返回结果为空，折行数据不会被丢弃，只会在结果中填充 null 值</li></ul><ol start="3"><li>⭐ 实际案例：直接上 SQL 。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableFunctionInnerJoin_Test</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        FlinkEnv flinkEnv = FlinkEnvUtils.getStreamTableEnv(args);</span><br><span class="line"></span><br><span class="line">        String sql = <span class="string">&quot;CREATE FUNCTION user_profile_table_func AS &#x27;flink.examples.sql._07.query._06_joins._06_table_function&quot;</span></span><br><span class="line">                + <span class="string">&quot;._01_inner_join.TableFunctionInnerJoin_Test$UserProfileTableFunction&#x27;;\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;CREATE TABLE source_table (\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    user_id BIGINT NOT NULL,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    name STRING,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    row_time AS cast(CURRENT_TIMESTAMP as timestamp(3)),\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    WATERMARK FOR row_time AS row_time - INTERVAL &#x27;5&#x27; SECOND\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;) WITH (\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;connector&#x27; = &#x27;datagen&#x27;,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;rows-per-second&#x27; = &#x27;10&#x27;,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;fields.name.length&#x27; = &#x27;1&#x27;,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;fields.user_id.min&#x27; = &#x27;1&#x27;,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;fields.user_id.max&#x27; = &#x27;10&#x27;\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;);\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;CREATE TABLE sink_table (\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    user_id BIGINT,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    name STRING,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    age INT,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    row_time TIMESTAMP(3)\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;) WITH (\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;connector&#x27; = &#x27;print&#x27;\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;);\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;INSERT INTO sink_table\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;SELECT user_id,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;       name,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;       age,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;       row_time\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;FROM source_table,\n&quot;</span></span><br><span class="line">                <span class="comment">// Table Function Join 语法对应 LATERAL TABLE</span></span><br><span class="line">                + <span class="string">&quot;LATERAL TABLE(user_profile_table_func(user_id)) t(age)&quot;</span>;</span><br><span class="line"></span><br><span class="line">        Arrays.stream(sql.split(<span class="string">&quot;;&quot;</span>))</span><br><span class="line">                .forEach(flinkEnv.streamTEnv()::executeSql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UserProfileTableFunction</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>&lt;<span class="title">Integer</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eval</span><span class="params">(<span class="keyword">long</span> userId)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 自定义输出逻辑</span></span><br><span class="line">            <span class="keyword">if</span> (userId &lt;= <span class="number">5</span>) &#123;</span><br><span class="line">                <span class="comment">// 一行转 1 行</span></span><br><span class="line">                collect(<span class="number">1</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 一行转 3 行</span></span><br><span class="line">                collect(<span class="number">1</span>);</span><br><span class="line">                collect(<span class="number">2</span>);</span><br><span class="line">                collect(<span class="number">3</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行结果如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- &lt;= 5，则只有 1 行结果</span></span><br><span class="line"><span class="operator">+</span>I[<span class="number">3</span>, <span class="number">7</span>, <span class="number">1</span>, <span class="number">2021</span><span class="number">-05</span><span class="number">-01</span>T18:<span class="number">23</span>:<span class="number">42.560</span>]</span><br><span class="line"><span class="comment">-- &gt; 5，则有行 3 结果</span></span><br><span class="line"><span class="operator">+</span>I[<span class="number">8</span>, e, <span class="number">1</span>, <span class="number">2021</span><span class="number">-05</span><span class="number">-01</span>T18:<span class="number">23</span>:<span class="number">42.560</span>]</span><br><span class="line"><span class="operator">+</span>I[<span class="number">8</span>, e, <span class="number">2</span>, <span class="number">2021</span><span class="number">-05</span><span class="number">-01</span>T18:<span class="number">23</span>:<span class="number">42.560</span>]</span><br><span class="line"><span class="operator">+</span>I[<span class="number">8</span>, e, <span class="number">3</span>, <span class="number">2021</span><span class="number">-05</span><span class="number">-01</span>T18:<span class="number">23</span>:<span class="number">42.560</span>]</span><br><span class="line"><span class="comment">-- &lt;= 5，则只有 1 行结果</span></span><br><span class="line"><span class="operator">+</span>I[<span class="number">4</span>, <span class="number">9</span>, <span class="number">1</span>, <span class="number">2021</span><span class="number">-05</span><span class="number">-01</span>T18:<span class="number">23</span>:<span class="number">42.561</span>]</span><br><span class="line"><span class="comment">-- &gt; 5，则有行 3 结果</span></span><br><span class="line"><span class="operator">+</span>I[<span class="number">8</span>, c, <span class="number">1</span>, <span class="number">2021</span><span class="number">-05</span><span class="number">-01</span>T18:<span class="number">23</span>:<span class="number">42.561</span>]</span><br><span class="line"><span class="operator">+</span>I[<span class="number">8</span>, c, <span class="number">2</span>, <span class="number">2021</span><span class="number">-05</span><span class="number">-01</span>T18:<span class="number">23</span>:<span class="number">42.561</span>]</span><br><span class="line"><span class="operator">+</span>I[<span class="number">8</span>, c, <span class="number">3</span>, <span class="number">2021</span><span class="number">-05</span><span class="number">-01</span>T18:<span class="number">23</span>:<span class="number">42.561</span>]</span><br></pre></td></tr></table></figure><h2 id="3-9-DML：集合操作"><a href="#3-9-DML：集合操作" class="headerlink" title="3.9 DML：集合操作"></a>3.9 DML：集合操作</h2><p>集合操作支持 Batch\Streaming 任务。</p><ol><li>⭐ UNION：将集合合并并且去重。</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/34.png" alt="union"></p><ol start="2"><li>⭐ UNION ALL：将集合合并，不做去重。</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">view</span> t1(s) <span class="keyword">as</span> <span class="keyword">values</span> (<span class="string">&#x27;c&#x27;</span>), (<span class="string">&#x27;a&#x27;</span>), (<span class="string">&#x27;b&#x27;</span>), (<span class="string">&#x27;b&#x27;</span>), (<span class="string">&#x27;c&#x27;</span>);</span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">view</span> t2(s) <span class="keyword">as</span> <span class="keyword">values</span> (<span class="string">&#x27;d&#x27;</span>), (<span class="string">&#x27;e&#x27;</span>), (<span class="string">&#x27;a&#x27;</span>), (<span class="string">&#x27;b&#x27;</span>), (<span class="string">&#x27;b&#x27;</span>);</span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> (<span class="keyword">SELECT</span> s <span class="keyword">FROM</span> t1) <span class="keyword">UNION</span> (<span class="keyword">SELECT</span> s <span class="keyword">FROM</span> t2);</span><br><span class="line"><span class="operator">+</span><span class="comment">---+</span></span><br><span class="line"><span class="operator">|</span>  s<span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---+</span></span><br><span class="line"><span class="operator">|</span>  c<span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>  a<span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>  b<span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>  d<span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>  e<span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---+</span></span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> (<span class="keyword">SELECT</span> s <span class="keyword">FROM</span> t1) <span class="keyword">UNION</span> <span class="keyword">ALL</span> (<span class="keyword">SELECT</span> s <span class="keyword">FROM</span> t2);</span><br><span class="line"><span class="operator">+</span><span class="comment">---+</span></span><br><span class="line"><span class="operator">|</span>  c<span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---+</span></span><br><span class="line"><span class="operator">|</span>  c<span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>  a<span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>  b<span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>  b<span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>  c<span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>  d<span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>  e<span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>  a<span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>  b<span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>  b<span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---+</span></span><br></pre></td></tr></table></figure><ol start="3"><li>⭐ Intersect：交集并且去重</li><li>⭐ Intersect ALL：交集不做去重</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">view</span> t1(s) <span class="keyword">as</span> <span class="keyword">values</span> (<span class="string">&#x27;c&#x27;</span>), (<span class="string">&#x27;a&#x27;</span>), (<span class="string">&#x27;b&#x27;</span>), (<span class="string">&#x27;b&#x27;</span>), (<span class="string">&#x27;c&#x27;</span>);</span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">view</span> t2(s) <span class="keyword">as</span> <span class="keyword">values</span> (<span class="string">&#x27;d&#x27;</span>), (<span class="string">&#x27;e&#x27;</span>), (<span class="string">&#x27;a&#x27;</span>), (<span class="string">&#x27;b&#x27;</span>), (<span class="string">&#x27;b&#x27;</span>);</span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> (<span class="keyword">SELECT</span> s <span class="keyword">FROM</span> t1) <span class="keyword">INTERSECT</span> (<span class="keyword">SELECT</span> s <span class="keyword">FROM</span> t2);</span><br><span class="line"><span class="operator">+</span><span class="comment">---+</span></span><br><span class="line"><span class="operator">|</span>  s<span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---+</span></span><br><span class="line"><span class="operator">|</span>  a<span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>  b<span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---+</span></span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> (<span class="keyword">SELECT</span> s <span class="keyword">FROM</span> t1) <span class="keyword">INTERSECT</span> <span class="keyword">ALL</span> (<span class="keyword">SELECT</span> s <span class="keyword">FROM</span> t2);</span><br><span class="line"><span class="operator">+</span><span class="comment">---+</span></span><br><span class="line"><span class="operator">|</span>  s<span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---+</span></span><br><span class="line"><span class="operator">|</span>  a<span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>  b<span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>  b<span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---+</span></span><br></pre></td></tr></table></figure><ol start="5"><li>⭐ Except：差集并且去重</li><li>⭐ Except ALL：差集不做去重</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> (<span class="keyword">SELECT</span> s <span class="keyword">FROM</span> t1) <span class="keyword">EXCEPT</span> (<span class="keyword">SELECT</span> s <span class="keyword">FROM</span> t2);</span><br><span class="line"><span class="operator">+</span><span class="comment">---+</span></span><br><span class="line"><span class="operator">|</span> s <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---+</span></span><br><span class="line"><span class="operator">|</span> c <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---+</span></span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> (<span class="keyword">SELECT</span> s <span class="keyword">FROM</span> t1) <span class="keyword">EXCEPT</span> <span class="keyword">ALL</span> (<span class="keyword">SELECT</span> s <span class="keyword">FROM</span> t2);</span><br><span class="line"><span class="operator">+</span><span class="comment">---+</span></span><br><span class="line"><span class="operator">|</span> s <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---+</span></span><br><span class="line"><span class="operator">|</span> c <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> c <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---+</span></span><br></pre></td></tr></table></figure><p>上述 SQL 在流式任务中，如果一条左流数据先来了，没有从右流集合数据中找到对应的数据时会直接输出，当右流对应数据后续来了之后，会下发回撤流将之前的数据給撤回。这也是一个回撤流。</p><ol start="7"><li>⭐ In 子查询：这个大家比较熟悉了，但是注意，In 子查询的结果集只能有一列</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">user</span>, amount</span><br><span class="line"><span class="keyword">FROM</span> Orders</span><br><span class="line"><span class="keyword">WHERE</span> product <span class="keyword">IN</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> product <span class="keyword">FROM</span> NewProducts</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>上述 SQL 的 In 子句其实就和之前介绍到的 Inner Join 类似。并且 In 子查询也会涉及到大状态问题，大家注意设置 State 的 TTL。</p><h2 id="3-10-DML：Order-By、Limit-子句"><a href="#3-10-DML：Order-By、Limit-子句" class="headerlink" title="3.10.DML：Order By、Limit 子句"></a>3.10.DML：Order By、Limit 子句</h2><h3 id="3-10-1-Order-By-子句"><a href="#3-10-1-Order-By-子句" class="headerlink" title="3.10.1.Order By 子句"></a>3.10.1.Order By 子句</h3><p>支持 Batch\Streaming，但在实时任务中一般用的非常少。</p><p>实时任务中，Order By 子句中必须要有时间属性字段，并且时间属性必须为升序时间属性，即 <code>WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL &#39;0.001&#39; SECOND</code> 或者 <code>WATERMARK FOR rowtime_column AS rowtime_column</code>。</p><p>举例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table_1 (</span><br><span class="line">    user_id <span class="type">BIGINT</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    user_id <span class="type">BIGINT</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span> user_id</span><br><span class="line"><span class="keyword">FROM</span> source_table_1</span><br><span class="line"><span class="keyword">Order</span> <span class="keyword">By</span> row_time, user_id <span class="keyword">desc</span></span><br></pre></td></tr></table></figure><h3 id="3-10-2-Limit-子句"><a href="#3-10-2-Limit-子句" class="headerlink" title="3.10.2.Limit 子句"></a>3.10.2.Limit 子句</h3><p>支持 Batch\Streaming，但实时场景一般不使用，但是此处依然举一个例子：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table_1 (</span><br><span class="line">    user_id <span class="type">BIGINT</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    user_id <span class="type">BIGINT</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span> user_id</span><br><span class="line"><span class="keyword">FROM</span> source_table_1</span><br><span class="line">Limit <span class="number">3</span></span><br></pre></td></tr></table></figure><p>结果如下，只有 3 条输出：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">+I[5]</span><br><span class="line">+I[9]</span><br><span class="line">+I[4]</span><br></pre></td></tr></table></figure><h2 id="3-11-DML：TopN-子句"><a href="#3-11-DML：TopN-子句" class="headerlink" title="3.11.DML：TopN 子句"></a>3.11.DML：TopN 子句</h2><ol><li><p>⭐ TopN 定义（支持 Batch\Streaming）：TopN 其实就是对应到离线数仓中的 row_number()，可以使用 row_number() 对某一个分组的数据进行排序</p></li><li><p>⭐ 应用场景：根据 <code>某个排序</code> 条件，计算<code>某个分组</code>下的排行榜数据</p></li><li><p>⭐ SQL 语法标准：</p></li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> [column_list]</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">   <span class="keyword">SELECT</span> [column_list],</span><br><span class="line">     <span class="built_in">ROW_NUMBER</span>() <span class="keyword">OVER</span> ([<span class="keyword">PARTITION</span> <span class="keyword">BY</span> col1[, col2...]]</span><br><span class="line">       <span class="keyword">ORDER</span> <span class="keyword">BY</span> col1 [<span class="keyword">asc</span><span class="operator">|</span><span class="keyword">desc</span>][, col2 [<span class="keyword">asc</span><span class="operator">|</span><span class="keyword">desc</span>]...]) <span class="keyword">AS</span> rownum</span><br><span class="line">   <span class="keyword">FROM</span> table_name)</span><br><span class="line"><span class="keyword">WHERE</span> rownum <span class="operator">&lt;=</span> N [<span class="keyword">AND</span> conditions]</span><br></pre></td></tr></table></figure><ul><li>⭐ <code>ROW_NUMBER()</code>：标识 TopN 排序子句</li><li>⭐ <code>PARTITION BY col1[, col2...]</code>：标识分区字段，代表按照这个 col 字段作为分区粒度对数据进行排序取 topN，比如下述案例中的 <code>partition by key</code>，就是根据需求中的搜索关键词（key）做为分区</li><li>⭐ <code>ORDER BY col1 [asc|desc][, col2 [asc|desc]...]</code>：标识 TopN 的排序规则，是按照哪些字段、顺序或逆序进行排序</li><li>⭐ <code>WHERE rownum &lt;= N</code>：这个子句是一定需要的，只有加上了这个子句，Flink 才能将其识别为一个 TopN 的查询，其中 N 代表 TopN 的条目数</li><li>⭐ <code>[AND conditions]</code>：其他的限制条件也可以加上</li></ul><ol start="4"><li>⭐ 实际案例：取某个搜索关键词下的搜索热度前 10 名的词条数据。</li></ol><p>输入数据为搜索词条数据的搜索热度数据，当搜索热度发生变化时，会将变化后的数据写入到数据源的 Kafka 中：</p><p>数据源 schema：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 字段名        备注</span></span><br><span class="line"><span class="comment">-- key         搜索关键词</span></span><br><span class="line"><span class="comment">-- name        搜索热度名称</span></span><br><span class="line"><span class="comment">-- search_cnt   热搜消费热度（比如 3000）</span></span><br><span class="line"><span class="comment">-- timestamp       消费词条时间戳</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    name <span class="type">BIGINT</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">    search_cnt <span class="type">BIGINT</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">    key <span class="type">BIGINT</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 数据汇 schema：</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- key         搜索关键词</span></span><br><span class="line"><span class="comment">-- name        搜索热度名称</span></span><br><span class="line"><span class="comment">-- search_cnt   热搜消费热度（比如 3000）</span></span><br><span class="line"><span class="comment">-- timestamp       消费词条时间戳</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    key <span class="type">BIGINT</span>,</span><br><span class="line">    name <span class="type">BIGINT</span>,</span><br><span class="line">    search_cnt <span class="type">BIGINT</span>,</span><br><span class="line">    `<span class="type">timestamp</span>` <span class="type">TIMESTAMP</span>(<span class="number">3</span>)</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- DML 逻辑</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span> key, name, search_cnt, row_time <span class="keyword">as</span> `<span class="type">timestamp</span>`</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">   <span class="keyword">SELECT</span> key, name, search_cnt, row_time, </span><br><span class="line">     <span class="comment">-- 根据热搜关键词 key 作为 partition key，然后按照 search_cnt 倒排取前 100 名</span></span><br><span class="line">     <span class="built_in">ROW_NUMBER</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> key</span><br><span class="line">       <span class="keyword">ORDER</span> <span class="keyword">BY</span> search_cnt <span class="keyword">desc</span>) <span class="keyword">AS</span> rownum</span><br><span class="line">   <span class="keyword">FROM</span> source_table)</span><br><span class="line"><span class="keyword">WHERE</span> rownum <span class="operator">&lt;=</span> <span class="number">100</span></span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-D[关键词1, 词条1, 4944]</span><br><span class="line">+I[关键词1, 词条1, 8670]</span><br><span class="line">+I[关键词1, 词条2, 1735]</span><br><span class="line">-D[关键词1, 词条3, 6641]</span><br><span class="line">+I[关键词1, 词条3, 6928]</span><br><span class="line">-D[关键词1, 词条4, 6312]</span><br><span class="line">+I[关键词1, 词条4, 7287]</span><br></pre></td></tr></table></figure><p>可以看到输出数据是有回撤数据的，为什么会出现回撤，我们来看看 SQL 语义。</p><ol start="5"><li>⭐ SQL 语义</li></ol><p>上面的 SQL 会翻译成以下三个算子：</p><ul><li>⭐ <code>数据源</code>：数据源即最新的词条下面的搜索词的搜索热度数据，消费到 Kafka 中数据后，按照 partition key 将数据进行 hash 分发到下游排序算子，相同的 key 数据将会发送到一个并发中</li><li>⭐ <code>排序算子</code>：为每个 Key 维护了一个 TopN 的榜单数据，接受到上游的一条数据后，如果 TopN 榜单还没有到达 N 条，则将这条数据加入 TopN 榜单后，直接下发数据，如果到达 N 条之后，经过 TopN 计算，发现这条数据比原有的数据排序靠前，那么新的 TopN 排名就会有变化，就变化了的这部分数据之前下发的排名数据撤回（即回撤数据），然后下发新的排名数据</li><li>⭐ <code>数据汇</code>：接收到上游的数据之后，然后输出到外部存储引擎中</li></ul><p>上面三个算子也是会 24 小时一直运行的。</p><h2 id="3-12-DML：Window-TopN"><a href="#3-12-DML：Window-TopN" class="headerlink" title="3.12.DML：Window TopN"></a>3.12.DML：Window TopN</h2><ol><li><p>⭐ Window TopN 定义（支持 Streaming）：Window TopN 是一种特殊的 TopN，它的返回结果是每一个窗口内的 N 个最小值或者最大值。</p></li><li><p>⭐ 应用场景：小伙伴萌会问了，我有了 TopN 为啥还需要 Window TopN 呢？还记得上文介绍 TopN 说道的 TopN 时会出现中间结果，从而出现回撤数据的嘛？Window TopN 不会出现回撤数据，因为 Window TopN 实现是在窗口结束时输出最终结果，不会产生中间结果。而且注意，因为是窗口上面的操作，Window TopN 在窗口结束时，会自动把 State 给清除。</p></li><li><p>⭐ SQL 语法标准：</p></li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> [column_list]</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">   <span class="keyword">SELECT</span> [column_list],</span><br><span class="line">     <span class="built_in">ROW_NUMBER</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> window_start, window_end [, col_key1...]</span><br><span class="line">       <span class="keyword">ORDER</span> <span class="keyword">BY</span> col1 [<span class="keyword">asc</span><span class="operator">|</span><span class="keyword">desc</span>][, col2 [<span class="keyword">asc</span><span class="operator">|</span><span class="keyword">desc</span>]...]) <span class="keyword">AS</span> rownum</span><br><span class="line">   <span class="keyword">FROM</span> table_name) <span class="comment">-- windowing TVF</span></span><br><span class="line"><span class="keyword">WHERE</span> rownum <span class="operator">&lt;=</span> N [<span class="keyword">AND</span> conditions]</span><br></pre></td></tr></table></figure><ol start="4"><li>⭐ 实际案例：取当前这一分钟的搜索关键词下的搜索热度前 10 名的词条数据</li></ol><p>输入表字段：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 字段名        备注</span></span><br><span class="line"><span class="comment">-- key             搜索关键词</span></span><br><span class="line"><span class="comment">-- name             搜索热度名称</span></span><br><span class="line"><span class="comment">-- search_cnt       热搜消费热度（比如 3000）</span></span><br><span class="line"><span class="comment">-- timestamp        消费词条时间戳</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    name <span class="type">BIGINT</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">    search_cnt <span class="type">BIGINT</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">    key <span class="type">BIGINT</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 输出表字段：</span></span><br><span class="line"><span class="comment">-- 字段名        备注</span></span><br><span class="line"><span class="comment">-- key              搜索关键词</span></span><br><span class="line"><span class="comment">-- name            搜索热度名称</span></span><br><span class="line"><span class="comment">-- search_cnt       热搜消费热度（比如 3000）</span></span><br><span class="line"><span class="comment">-- window_start     窗口开始时间戳</span></span><br><span class="line"><span class="comment">-- window_end       窗口结束时间戳</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    key <span class="type">BIGINT</span>,</span><br><span class="line">    name <span class="type">BIGINT</span>,</span><br><span class="line">    search_cnt <span class="type">BIGINT</span>,</span><br><span class="line">    window_start <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    window_end <span class="type">TIMESTAMP</span>(<span class="number">3</span>)</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  ...</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 处理 sql：</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span> key, name, search_cnt, window_start, window_end</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">   <span class="keyword">SELECT</span> key, name, search_cnt, window_start, window_end, </span><br><span class="line">     <span class="built_in">ROW_NUMBER</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> window_start, window_end, key</span><br><span class="line">       <span class="keyword">ORDER</span> <span class="keyword">BY</span> search_cnt <span class="keyword">desc</span>) <span class="keyword">AS</span> rownum</span><br><span class="line">   <span class="keyword">FROM</span> (</span><br><span class="line">      <span class="keyword">SELECT</span> window_start, window_end, key, name, <span class="built_in">max</span>(search_cnt) <span class="keyword">as</span> search_cnt</span><br><span class="line">      <span class="comment">-- window tvf 写法</span></span><br><span class="line">      <span class="keyword">FROM</span> <span class="keyword">TABLE</span>(TUMBLE(<span class="keyword">TABLE</span> source_table, DESCRIPTOR(row_time), <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> MINUTES))</span><br><span class="line">      <span class="keyword">GROUP</span> <span class="keyword">BY</span> window_start, window_end, key, name</span><br><span class="line">   )</span><br><span class="line">)</span><br><span class="line"><span class="keyword">WHERE</span> rownum <span class="operator">&lt;=</span> <span class="number">100</span></span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">+I[关键词1, 词条1, 8670, 2021-1-28T22:34, 2021-1-28T22:35]</span><br><span class="line">+I[关键词1, 词条2, 6928, 2021-1-28T22:34, 2021-1-28T22:35]</span><br><span class="line">+I[关键词1, 词条3, 1735, 2021-1-28T22:34, 2021-1-28T22:35]</span><br><span class="line">+I[关键词1, 词条4, 7287, 2021-1-28T22:34, 2021-1-28T22:35]</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>可以看到结果是符合预期的，其中没有回撤数据。</p><ol start="5"><li>⭐ SQL 语义</li></ol><ul><li>⭐ <code>数据源</code>：数据源即最新的词条下面的搜索词的搜索热度数据，消费到 Kafka 中数据后，将数据按照窗口聚合的 key 通过 hash 分发策略发送到下游窗口聚合算子</li><li>⭐ <code>窗口聚合算子</code>：进行窗口聚合计算，随着时间的推进，将窗口聚合结果计算完成发往下游窗口排序算子</li><li>⭐ <code>窗口排序算子</code>：这个算子其实也是一个窗口算子，只不过这个窗口算子为每个 Key 维护了一个 TopN 的榜单数据，接受到上游发送的窗口结果数据进行排序，随着时间的推进，窗口的结束，将排序的结果输出到下游数据汇算子。</li><li>⭐ <code>数据汇</code>：接收到上游的数据之后，然后输出到外部存储引擎中</li></ul><h2 id="3-13-DML：Deduplication"><a href="#3-13-DML：Deduplication" class="headerlink" title="3.13.DML：Deduplication"></a>3.13.DML：Deduplication</h2><ol><li><p>⭐ Deduplication 定义（支持 Batch\Streaming）：Deduplication 其实就是去重，也即上文介绍到的 TopN 中 row_number = 1 的场景，但是这里有一点不一样在于其排序字段一定是时间属性列，不能是其他非时间属性的普通列。在 row_number = 1 时，如果排序字段是普通列 planner 会翻译成 TopN 算子，如果是时间属性列 planner 会翻译成 Deduplication，这两者最终的执行算子是不一样的，Deduplication 相比 TopN 算子专门做了对应的优化，性能会有很大提升。</p></li><li><p>⭐ 应用场景：比如上游数据发重了，或者计算 DAU 明细数据等场景，都可以使用 Deduplication 语法去做去重。</p></li><li><p>⭐ SQL 语法标准：</p></li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> [column_list]</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">   <span class="keyword">SELECT</span> [column_list],</span><br><span class="line">     <span class="built_in">ROW_NUMBER</span>() <span class="keyword">OVER</span> ([<span class="keyword">PARTITION</span> <span class="keyword">BY</span> col1[, col2...]]</span><br><span class="line">       <span class="keyword">ORDER</span> <span class="keyword">BY</span> time_attr [<span class="keyword">asc</span><span class="operator">|</span><span class="keyword">desc</span>]) <span class="keyword">AS</span> rownum</span><br><span class="line">   <span class="keyword">FROM</span> table_name)</span><br><span class="line"><span class="keyword">WHERE</span> rownum <span class="operator">=</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p>其中：</p><ul><li>⭐ <code>ROW_NUMBER()</code>：标识当前数据的排序值</li><li>⭐ <code>PARTITION BY col1[, col2...]</code>：标识分区字段，代表按照这个 col 字段作为分区粒度对数据进行排序</li><li>⭐ <code>ORDER BY time_attr [asc|desc]</code>：标识排序规则，必须为时间戳列，当前 Flink SQL 支持处理时间、事件时间，ASC 代表保留第一行，DESC 代表保留最后一行</li><li>⭐ <code>WHERE rownum = 1</code>：这个子句是一定需要的，而且必须为 rownum = 1</li></ul><ol start="4"><li>⭐ 实际案例：</li></ol><p>博主这里举两个案例：</p><ul><li>⭐ 案例 1（事件时间）：是腾讯 QQ 用户等级的场景，每一个 QQ 用户都有一个 QQ 用户等级，需要求出当前用户等级在 <code>星星</code>，<code>月亮</code>，<code>太阳</code> 的用户数分别有多少。</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 数据源：当每一个用户的等级初始化及后续变化的时候的数据，即用户等级变化明细数据。</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    user_id <span class="type">BIGINT</span> COMMENT <span class="string">&#x27;用户 id&#x27;</span>,</span><br><span class="line">    level STRING COMMENT <span class="string">&#x27;用户等级&#x27;</span>,</span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)) COMMENT <span class="string">&#x27;事件时间戳&#x27;</span>,</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.level.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1000000&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 数据汇：输出即每一个等级的用户数</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    level STRING COMMENT <span class="string">&#x27;等级&#x27;</span>,</span><br><span class="line">    uv <span class="type">BIGINT</span> COMMENT <span class="string">&#x27;当前等级用户数&#x27;</span>,</span><br><span class="line">    row_time <span class="type">timestamp</span>(<span class="number">3</span>) COMMENT <span class="string">&#x27;时间戳&#x27;</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 处理逻辑：</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">    level</span><br><span class="line">    , <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">as</span> uv</span><br><span class="line">    , <span class="built_in">max</span>(row_time) <span class="keyword">as</span> row_time</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">      <span class="keyword">SELECT</span></span><br><span class="line">          user_id,</span><br><span class="line">          level,</span><br><span class="line">          row_time,</span><br><span class="line">          <span class="built_in">row_number</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> row_time) <span class="keyword">as</span> rn</span><br><span class="line">      <span class="keyword">FROM</span> source_table</span><br><span class="line">)</span><br><span class="line"><span class="keyword">where</span> rn <span class="operator">=</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> </span><br><span class="line">    level</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">+I[等级 1, 6928, 2021-1-28T22:34]</span><br><span class="line">-I[等级 1, 6928, 2021-1-28T22:34]</span><br><span class="line">+I[等级 1, 8670, 2021-1-28T22:34]</span><br><span class="line">-I[等级 1, 8670, 2021-1-28T22:34]</span><br><span class="line">+I[等级 1, 77287, 2021-1-28T22:34]</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>可以看到其有回撤数据。</p><p>其对应的 SQL 语义如下：</p><ul><li><p>⭐ <code>数据源</code>：消费到 Kafka 中数据后，将数据按照 partition by 的 key 通过 hash 分发策略发送到下游去重算子</p></li><li><p>⭐ <code>Deduplication 去重算子</code>：接受到上游数据之后，根据 order by 中的条件判断当前的这条数据和之前数据时间戳大小，以上面案例来说，如果当前数据时间戳大于之前数据时间戳，则撤回之前向下游发的中间结果，然后将最新的结果发向下游（发送策略也为 hash，具体的 hash 策略为按照 group by 中 key 进行发送），如果当前数据时间戳小于之前数据时间戳，则不做操作。次算子产出的结果就是每一个用户的对应的最新等级信息。</p></li><li><p>⭐ <code>Group by 聚合算子</code>：接受到上游数据之后，根据 Group by 聚合粒度对数据进行聚合计算结果（每一个等级的用户数），发往下游数据汇算子</p></li><li><p>⭐ <code>数据汇</code>：接收到上游的数据之后，然后输出到外部存储引擎中</p></li><li><p>⭐ 案例 2（处理时间）：最原始的日志是明细数据，需要我们根据用户 id 筛选出这个用户当天的第一条数据，发往下游，下游可以据此计算分各种维度的 DAU</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 数据源：原始日志明细数据</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    user_id <span class="type">BIGINT</span> COMMENT <span class="string">&#x27;用户 id&#x27;</span>,</span><br><span class="line">    name STRING COMMENT <span class="string">&#x27;用户姓名&#x27;</span>,</span><br><span class="line">    server_timestamp <span class="type">BIGINT</span> COMMENT <span class="string">&#x27;用户访问时间戳&#x27;</span>,</span><br><span class="line">    proctime <span class="keyword">AS</span> PROCTIME()</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.name.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.server_timestamp.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.server_timestamp.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 数据汇：根据 user_id 去重的第一条数据</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    user_id <span class="type">BIGINT</span>,</span><br><span class="line">    name STRING,</span><br><span class="line">    server_timestamp <span class="type">BIGINT</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 处理逻辑：</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">select</span> user_id,</span><br><span class="line">       name,</span><br><span class="line">       server_timestamp</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">      <span class="keyword">SELECT</span></span><br><span class="line">          user_id,</span><br><span class="line">          name,</span><br><span class="line">          server_timestamp,</span><br><span class="line">          <span class="built_in">row_number</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> proctime) <span class="keyword">as</span> rn</span><br><span class="line">      <span class="keyword">FROM</span> source_table</span><br><span class="line">)</span><br><span class="line"><span class="keyword">where</span> rn <span class="operator">=</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">+I[1, 用户 1, 2021-1-28T22:34]</span><br><span class="line">+I[2, 用户 2, 2021-1-28T22:34]</span><br><span class="line">+I[3, 用户 3, 2021-1-28T22:34]</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>可以看到这个处理逻辑是没有回撤数据的。其对应的 SQL 语义如下：</p><ul><li>⭐ <code>数据源</code>：消费到 Kafka 中数据后，将数据按照 partition by 的 key 通过 hash 分发策略发送到下游去重算子</li><li>⭐ <code>Deduplication 去重算子</code>：处理时间语义下，如果是当前 key 的第一条数据，则直接发往下游，如果判断（根据 state 中是否存储过改 key）不是第一条，则直接丢弃</li><li>⭐ <code>数据汇</code>：接收到上游的数据之后，然后输出到外部存储引擎中</li></ul><blockquote><p>注意：</p><p>在 Deduplication 关于是否会出现回撤流，博主总结如下：</p><ol><li>⭐ Order by 事件时间 DESC：会出现回撤流，因为当前 key 下 <code>可能会有</code> 比当前事件时间还大的数据</li><li>⭐ Order by 事件时间 ASC：会出现回撤流，因为当前 key 下 <code>可能会有</code> 比当前事件时间还小的数据</li><li>⭐ Order by 处理时间 DESC：会出现回撤流，因为当前 key 下 <code>可能会有</code> 比当前处理时间还大的数据</li><li>⭐ Order by 处理时间 ASC：不会出现回撤流，因为当前 key 下 <code>不可能会有</code> 比当前处理时间还小的数据</li></ol></blockquote><h2 id="3-14-EXPLAIN-子句"><a href="#3-14-EXPLAIN-子句" class="headerlink" title="3.14.EXPLAIN 子句"></a>3.14.EXPLAIN 子句</h2><ol><li><p>⭐ 应用场景：EXPLAIN 子句其实就是用于查看当前这个 sql 查询的逻辑计划以及优化的执行计划。</p></li><li><p>⭐ SQL 语法标准：</p></li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN PLAN <span class="keyword">FOR</span> <span class="operator">&lt;</span>query_statement_or_insert_statement<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure><ol start="3"><li>⭐ 实际案例：</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Explain_Test</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        FlinkEnv flinkEnv = FlinkEnvUtils.getStreamTableEnv(args);</span><br><span class="line"></span><br><span class="line">        flinkEnv.env().setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        String sql = <span class="string">&quot;CREATE TABLE source_table (\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    user_id BIGINT COMMENT &#x27;用户 id&#x27;,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    name STRING COMMENT &#x27;用户姓名&#x27;,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    server_timestamp BIGINT COMMENT &#x27;用户访问时间戳&#x27;,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    proctime AS PROCTIME()\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;) WITH (\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;connector&#x27; = &#x27;datagen&#x27;,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;rows-per-second&#x27; = &#x27;1&#x27;,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;fields.name.length&#x27; = &#x27;1&#x27;,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;fields.user_id.min&#x27; = &#x27;1&#x27;,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;fields.user_id.max&#x27; = &#x27;10&#x27;,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;fields.server_timestamp.min&#x27; = &#x27;1&#x27;,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;fields.server_timestamp.max&#x27; = &#x27;100000&#x27;\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;);\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;CREATE TABLE sink_table (\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    user_id BIGINT,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    name STRING,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    server_timestamp BIGINT\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;) WITH (\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  &#x27;connector&#x27; = &#x27;print&#x27;\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;);\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;EXPLAIN PLAN FOR\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;INSERT INTO sink_table\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;select user_id,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;       name,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;       server_timestamp\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;from (\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;      SELECT\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;          user_id,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;          name,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;          server_timestamp,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;          row_number() over(partition by user_id order by proctime) as rn\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;      FROM source_table\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;)\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;where rn = 1&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 算子 &#123;<span class="doctag">@link</span> org.apache.flink.streaming.api.operators.KeyedProcessOperator&#125;</span></span><br><span class="line"><span class="comment">         *      -- &#123;<span class="doctag">@link</span> org.apache.flink.table.runtime.operators.deduplicate.ProcTimeDeduplicateKeepFirstRowFunction&#125;</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String innerSql : sql.split(<span class="string">&quot;;&quot;</span>)) &#123;</span><br><span class="line">            TableResult tableResult = flinkEnv.streamTEnv().executeSql(innerSql);</span><br><span class="line"></span><br><span class="line">            tableResult.print();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述代码执行结果如下：</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">1. 抽象语法树</span><br><span class="line">== Abstract Syntax Tree ==</span><br><span class="line">LogicalSink(table=[default_catalog.default_database.sink_table], fields=[user_id, name, server_timestamp])</span><br><span class="line">+- LogicalProject(user_id=[$0], name=[$1], server_timestamp=[$2])</span><br><span class="line">   +- LogicalFilter(condition=[=($3, 1)])</span><br><span class="line">      +- LogicalProject(user_id=[$0], name=[$1], server_timestamp=[$2], rn=[ROW_NUMBER() OVER (PARTITION BY $0 ORDER BY PROCTIME() NULLS FIRST)])</span><br><span class="line">         +- LogicalTableScan(table=[[default_catalog, default_database, source_table]])</span><br><span class="line"></span><br><span class="line">2. 优化后的物理计划</span><br><span class="line">== Optimized Physical Plan ==</span><br><span class="line">Sink(table=[default_catalog.default_database.sink_table], fields=[user_id, name, server_timestamp])</span><br><span class="line">+- Calc(select=[user_id, name, server_timestamp])</span><br><span class="line">   +- Deduplicate(keep=[FirstRow], key=[user_id], order=[PROCTIME])</span><br><span class="line">      +- Exchange(distribution=[hash[user_id]])</span><br><span class="line">         +- Calc(select=[user_id, name, server_timestamp, PROCTIME() AS $3])</span><br><span class="line">            +- TableSourceScan(table=[[default_catalog, default_database, source_table]], fields=[user_id, name, server_timestamp])</span><br><span class="line"></span><br><span class="line">3. 优化后的执行计划</span><br><span class="line">== Optimized Execution Plan ==</span><br><span class="line">Sink(table=[default_catalog.default_database.sink_table], fields=[user_id, name, server_timestamp])</span><br><span class="line">+- Calc(select=[user_id, name, server_timestamp])</span><br><span class="line">   +- Deduplicate(keep=[FirstRow], key=[user_id], order=[PROCTIME])</span><br><span class="line">      +- Exchange(distribution=[hash[user_id]])</span><br><span class="line">         +- Calc(select=[user_id, name, server_timestamp, PROCTIME() AS $3])</span><br><span class="line">            +- TableSourceScan(table=[[default_catalog, default_database, source_table]], fields=[user_id, name, server_timestamp])</span><br></pre></td></tr></table></figure><h2 id="3-15-USE-子句"><a href="#3-15-USE-子句" class="headerlink" title="3.15.USE 子句"></a>3.15.USE 子句</h2><ol><li><p>⭐ 应用场景：如果熟悉 MySQL 的同学会非常熟悉这个子句，在 MySQL 中，USE 子句通常被用于切换库，那么在 Flink SQL 体系中，它的作用也是和 MySQL 中 USE 子句的功能基本一致，用于切换 Catalog，DataBase，使用 Module</p></li><li><p>⭐ SQL 语法标准：</p></li></ol><ul><li>⭐ 切换 Catalog</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">USE CATALOG catalog_name</span><br></pre></td></tr></table></figure><ul><li>⭐ 使用 Module</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">USE MODULES module_name1[, module_name2, ...]</span><br></pre></td></tr></table></figure><ul><li>⭐ 切换 Database</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">USE db名称</span><br></pre></td></tr></table></figure><ol start="3"><li>⭐ 实际案例：</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line"><span class="comment">// create a catalog</span></span><br><span class="line">tEnv.executeSql(<span class="string">&quot;CREATE CATALOG cat1 WITH (...)&quot;</span>);</span><br><span class="line">tEnv.executeSql(<span class="string">&quot;SHOW CATALOGS&quot;</span>).print();</span><br><span class="line"><span class="comment">// +-----------------+</span></span><br><span class="line"><span class="comment">// |    catalog name |</span></span><br><span class="line"><span class="comment">// +-----------------+</span></span><br><span class="line"><span class="comment">// | default_catalog |</span></span><br><span class="line"><span class="comment">// | cat1            |</span></span><br><span class="line"><span class="comment">// +-----------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// change default catalog</span></span><br><span class="line">tEnv.executeSql(<span class="string">&quot;USE CATALOG cat1&quot;</span>);</span><br><span class="line"></span><br><span class="line">tEnv.executeSql(<span class="string">&quot;SHOW DATABASES&quot;</span>).print();</span><br><span class="line"><span class="comment">// databases are empty</span></span><br><span class="line"><span class="comment">// +---------------+</span></span><br><span class="line"><span class="comment">// | database name |</span></span><br><span class="line"><span class="comment">// +---------------+</span></span><br><span class="line"><span class="comment">// +---------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// create a database</span></span><br><span class="line">tEnv.executeSql(<span class="string">&quot;CREATE DATABASE db1 WITH (...)&quot;</span>);</span><br><span class="line">tEnv.executeSql(<span class="string">&quot;SHOW DATABASES&quot;</span>).print();</span><br><span class="line"><span class="comment">// +---------------+</span></span><br><span class="line"><span class="comment">// | database name |</span></span><br><span class="line"><span class="comment">// +---------------+</span></span><br><span class="line"><span class="comment">// |        db1    |</span></span><br><span class="line"><span class="comment">// +---------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// change default database</span></span><br><span class="line">tEnv.executeSql(<span class="string">&quot;USE db1&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// change module resolution order and enabled status</span></span><br><span class="line">tEnv.executeSql(<span class="string">&quot;USE MODULES hive&quot;</span>);</span><br><span class="line">tEnv.executeSql(<span class="string">&quot;SHOW FULL MODULES&quot;</span>).print();</span><br><span class="line"><span class="comment">// +-------------+-------+</span></span><br><span class="line"><span class="comment">// | module name |  used |</span></span><br><span class="line"><span class="comment">// +-------------+-------+</span></span><br><span class="line"><span class="comment">// |        hive |  true |</span></span><br><span class="line"><span class="comment">// |        core | false |</span></span><br><span class="line"><span class="comment">// +-------------+-------+</span></span><br></pre></td></tr></table></figure><h2 id="3-16-SHOW-子句"><a href="#3-16-SHOW-子句" class="headerlink" title="3.16.SHOW 子句"></a>3.16.SHOW 子句</h2><ol><li><p>⭐ 应用场景：如果熟悉 MySQL 的同学会非常熟悉这个子句，在 MySQL 中，SHOW 子句常常用于查询库、表、函数等，在 Flink SQL 体系中也类似。Flink SQL 支持 SHOW 以下内容。</p></li><li><p>⭐ SQL 语法标准：</p></li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> CATALOGS：展示所有 Catalog</span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">CURRENT</span> CATALOG：展示当前的 Catalog</span><br><span class="line"><span class="keyword">SHOW</span> DATABASES：展示当前 Catalog 下所有 Database</span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">CURRENT</span> DATABASE：展示当前的 Database</span><br><span class="line"><span class="keyword">SHOW</span> TABLES：展示当前 Database 下所有表</span><br><span class="line"><span class="keyword">SHOW</span> VIEWS：展示所有视图</span><br><span class="line"><span class="keyword">SHOW</span> FUNCTIONS：展示所有的函数</span><br><span class="line"><span class="keyword">SHOW</span> MODULES：展示所有的 <span class="keyword">Module</span>（<span class="keyword">Module</span> 是用于 UDF 扩展）</span><br></pre></td></tr></table></figure><ol start="3"><li>⭐ 实际案例：</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line"><span class="comment">// show catalogs</span></span><br><span class="line">tEnv.executeSql(<span class="string">&quot;SHOW CATALOGS&quot;</span>).print();</span><br><span class="line"><span class="comment">// +-----------------+</span></span><br><span class="line"><span class="comment">// |    catalog name |</span></span><br><span class="line"><span class="comment">// +-----------------+</span></span><br><span class="line"><span class="comment">// | default_catalog |</span></span><br><span class="line"><span class="comment">// +-----------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// show current catalog</span></span><br><span class="line">tEnv.executeSql(<span class="string">&quot;SHOW CURRENT CATALOG&quot;</span>).print();</span><br><span class="line"><span class="comment">// +----------------------+</span></span><br><span class="line"><span class="comment">// | current catalog name |</span></span><br><span class="line"><span class="comment">// +----------------------+</span></span><br><span class="line"><span class="comment">// |      default_catalog |</span></span><br><span class="line"><span class="comment">// +----------------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// show databases</span></span><br><span class="line">tEnv.executeSql(<span class="string">&quot;SHOW DATABASES&quot;</span>).print();</span><br><span class="line"><span class="comment">// +------------------+</span></span><br><span class="line"><span class="comment">// |    database name |</span></span><br><span class="line"><span class="comment">// +------------------+</span></span><br><span class="line"><span class="comment">// | default_database |</span></span><br><span class="line"><span class="comment">// +------------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// show current database</span></span><br><span class="line">tEnv.executeSql(<span class="string">&quot;SHOW CURRENT DATABASE&quot;</span>).print();</span><br><span class="line"><span class="comment">// +-----------------------+</span></span><br><span class="line"><span class="comment">// | current database name |</span></span><br><span class="line"><span class="comment">// +-----------------------+</span></span><br><span class="line"><span class="comment">// |      default_database |</span></span><br><span class="line"><span class="comment">// +-----------------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// create a table</span></span><br><span class="line">tEnv.executeSql(<span class="string">&quot;CREATE TABLE my_table (...) WITH (...)&quot;</span>);</span><br><span class="line"><span class="comment">// show tables</span></span><br><span class="line">tEnv.executeSql(<span class="string">&quot;SHOW TABLES&quot;</span>).print();</span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// | table name |</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |   my_table |</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// create a view</span></span><br><span class="line">tEnv.executeSql(<span class="string">&quot;CREATE VIEW my_view AS ...&quot;</span>);</span><br><span class="line"><span class="comment">// show views</span></span><br><span class="line">tEnv.executeSql(<span class="string">&quot;SHOW VIEWS&quot;</span>).print();</span><br><span class="line"><span class="comment">// +-----------+</span></span><br><span class="line"><span class="comment">// | view name |</span></span><br><span class="line"><span class="comment">// +-----------+</span></span><br><span class="line"><span class="comment">// |   my_view |</span></span><br><span class="line"><span class="comment">// +-----------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// show functions</span></span><br><span class="line">tEnv.executeSql(<span class="string">&quot;SHOW FUNCTIONS&quot;</span>).print();</span><br><span class="line"><span class="comment">// +---------------+</span></span><br><span class="line"><span class="comment">// | function name |</span></span><br><span class="line"><span class="comment">// +---------------+</span></span><br><span class="line"><span class="comment">// |           mod |</span></span><br><span class="line"><span class="comment">// |        sha256 |</span></span><br><span class="line"><span class="comment">// |           ... |</span></span><br><span class="line"><span class="comment">// +---------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// create a user defined function</span></span><br><span class="line">tEnv.executeSql(<span class="string">&quot;CREATE FUNCTION f1 AS ...&quot;</span>);</span><br><span class="line"><span class="comment">// show user defined functions</span></span><br><span class="line">tEnv.executeSql(<span class="string">&quot;SHOW USER FUNCTIONS&quot;</span>).print();</span><br><span class="line"><span class="comment">// +---------------+</span></span><br><span class="line"><span class="comment">// | function name |</span></span><br><span class="line"><span class="comment">// +---------------+</span></span><br><span class="line"><span class="comment">// |            f1 |</span></span><br><span class="line"><span class="comment">// |           ... |</span></span><br><span class="line"><span class="comment">// +---------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// show modules</span></span><br><span class="line">tEnv.executeSql(<span class="string">&quot;SHOW MODULES&quot;</span>).print();</span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// | module name |</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |        core |</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// show full modules</span></span><br><span class="line">tEnv.executeSql(<span class="string">&quot;SHOW FULL MODULES&quot;</span>).print();</span><br><span class="line"><span class="comment">// +-------------+-------+</span></span><br><span class="line"><span class="comment">// | module name |  used |</span></span><br><span class="line"><span class="comment">// +-------------+-------+</span></span><br><span class="line"><span class="comment">// |        core |  true |</span></span><br><span class="line"><span class="comment">// |        hive | false |</span></span><br><span class="line"><span class="comment">// +-------------+-------+</span></span><br></pre></td></tr></table></figure><h2 id="3-17-LOAD、UNLOAD-子句"><a href="#3-17-LOAD、UNLOAD-子句" class="headerlink" title="3.17.LOAD、UNLOAD 子句"></a>3.17.LOAD、UNLOAD 子句</h2><ol><li><p>⭐ 应用场景：我们可以使用 LOAD 子句去加载 Flink SQL 体系内置的或者用户自定义的 Module，UNLOAD 子句去卸载 Flink SQL 体系内置的或者用户自定义的 Module</p></li><li><p>⭐ SQL 语法标准：</p></li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 加载</span></span><br><span class="line">LOAD <span class="keyword">MODULE</span> module_name [<span class="keyword">WITH</span> (<span class="string">&#x27;key1&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;val1&#x27;</span>, <span class="string">&#x27;key2&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;val2&#x27;</span>, ...)]</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 卸载</span></span><br><span class="line">UNLOAD <span class="keyword">MODULE</span> module_name</span><br></pre></td></tr></table></figure><ol start="3"><li>⭐ 实际案例：</li></ol><ul><li>⭐ LOAD 案例：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 加载 Flink SQL 体系内置的 Hive module</span></span><br><span class="line">tEnv.executeSql(<span class="string">&quot;LOAD MODULE hive WITH (&#x27;hive-version&#x27; = &#x27;3.1.2&#x27;)&quot;</span>);</span><br><span class="line">tEnv.executeSql(<span class="string">&quot;SHOW MODULES&quot;</span>).print();</span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// | module name |</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |        core |</span></span><br><span class="line"><span class="comment">// |        hive |</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br></pre></td></tr></table></figure><ul><li>⭐ UNLOAD 案例：</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 卸载唯一的一个 CoreModule</span></span><br><span class="line">tEnv.executeSql(<span class="string">&quot;UNLOAD MODULE core&quot;</span>);</span><br><span class="line">tEnv.executeSql(<span class="string">&quot;SHOW MODULES&quot;</span>).print();</span><br><span class="line"><span class="comment">// 结果啥 Moudle 都没有了</span></span><br></pre></td></tr></table></figure><h2 id="3-18-SET、RESET-子句"><a href="#3-18-SET、RESET-子句" class="headerlink" title="3.18.SET、RESET 子句"></a>3.18.SET、RESET 子句</h2><ol><li><p>⭐ 应用场景：SET 子句可以用于修改一些 Flink SQL 的环境配置，RESET 子句是可以将所有的环境配置恢复成默认配置，但只能在 SQL CLI 中进行使用，主要是为了让用户更纯粹的使用 SQL 而不必使用其他方式或者切换系统环境。</p></li><li><p>⭐ SQL 语法标准：</p></li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SET</span> (key <span class="operator">=</span> <span class="keyword">value</span>)?</span><br><span class="line"></span><br><span class="line">RESET (key)?</span><br></pre></td></tr></table></figure><ol start="3"><li>⭐ 实际案例：</li></ol><p>启动一个 SQL CLI 之后，在 SQL CLI 中可以进行以下 SET 设置：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SET</span> table.planner <span class="operator">=</span> blink;</span><br><span class="line">[INFO] Session property has been set.</span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">SET</span>;</span><br><span class="line">table.planner<span class="operator">=</span>blink;</span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> RESET table.planner;</span><br><span class="line">[INFO] Session property has been reset.</span><br><span class="line"></span><br><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> RESET;</span><br><span class="line">[INFO] <span class="keyword">All</span> session properties have been <span class="keyword">set</span> <span class="keyword">to</span> their <span class="keyword">default</span> values.</span><br></pre></td></tr></table></figure><h2 id="3-19-SQL-Hints"><a href="#3-19-SQL-Hints" class="headerlink" title="3.19.SQL Hints"></a>3.19.SQL Hints</h2><ol><li><p>⭐ 应用场景：比如有一个 kafka 数据源表 kafka_table1，用户想直接从 <code>latest-offset</code> select 一些数据出来预览，其元数据已经存储在 Hive MetaStore 中，但是 Hive MetaStore 中存储的配置中的 <code>scan.startup.mode</code> 是 <code>earliest-offset</code>，通过 SQL Hints，用户可以在 DML 语句中将 <code>scan.startup.mode</code> 改为 <code>latest-offset</code> 查询，因此可以看出 SQL Hints 常用语这种比较临时的参数修改，比如 Ad-hoc 这种临时查询中，方便用户使用自定义的新的表参数而不是 Catalog 中已有的表参数。</p></li><li><p>⭐ SQL 语法标准：</p></li></ol><p>以下 DML SQL 中的 <code>/*+ OPTIONS(key=val [, key=val]*) */</code> 就是 SQL Hints。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> table_path <span class="comment">/*+ OPTIONS(key=val [, key=val]*) */</span></span><br></pre></td></tr></table></figure><ol start="3"><li>⭐ 实际案例：</li></ol><p>启动一个 SQL CLI 之后，在 SQL CLI 中可以进行以下 SET 设置：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> kafka_table1 (id <span class="type">BIGINT</span>, name STRING, age <span class="type">INT</span>) <span class="keyword">WITH</span> (...);</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> kafka_table2 (id <span class="type">BIGINT</span>, name STRING, age <span class="type">INT</span>) <span class="keyword">WITH</span> (...);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 1. 使用 &#x27;scan.startup.mode&#x27;=&#x27;earliest-offset&#x27; 覆盖原来的 scan.startup.mode</span></span><br><span class="line"><span class="keyword">select</span> id, name <span class="keyword">from</span> kafka_table1 <span class="comment">/*+ OPTIONS(&#x27;scan.startup.mode&#x27;=&#x27;earliest-offset&#x27;) */</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2. 使用 &#x27;scan.startup.mode&#x27;=&#x27;earliest-offset&#x27; 覆盖原来的 scan.startup.mode</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span></span><br><span class="line">    kafka_table1 <span class="comment">/*+ OPTIONS(&#x27;scan.startup.mode&#x27;=&#x27;earliest-offset&#x27;) */</span> t1</span><br><span class="line">    <span class="keyword">join</span></span><br><span class="line">    kafka_table2 <span class="comment">/*+ OPTIONS(&#x27;scan.startup.mode&#x27;=&#x27;earliest-offset&#x27;) */</span> t2</span><br><span class="line">    <span class="keyword">on</span> t1.id <span class="operator">=</span> t2.id;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 3. 使用 &#x27;sink.partitioner&#x27;=&#x27;round-robin&#x27; 覆盖原来的 Sink 表的 sink.partitioner</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> kafka_table1 <span class="comment">/*+ OPTIONS(&#x27;sink.partitioner&#x27;=&#x27;round-robin&#x27;) */</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> kafka_table2;</span><br></pre></td></tr></table></figure><h1 id="4-SQL-UDF-篇"><a href="#4-SQL-UDF-篇" class="headerlink" title="4.SQL UDF 篇"></a>4.SQL UDF 篇</h1><p>Flink Table\SQL API 允许用户使用函数进行数据处理、字段标准化等处理。</p><h2 id="4-1-SQL-函数的归类"><a href="#4-1-SQL-函数的归类" class="headerlink" title="4.1.SQL 函数的归类"></a>4.1.SQL 函数的归类</h2><p>Flink 中的函数有两个维度的归类标准。</p><ol><li><p>⭐ 一个归类标准是：系统（内置）函数和 Catalog 函数。系统函数没有命名空间，只能通过其名称来进行引用。Catalog 函数属于 Catalog 和数据库，因此它们拥有 Catalog 和数据库的命名空间。用户可以通过全/部分限定名（catalog.db.func 或 db.func）或者函数来对 Catalog 函数进行引用。</p></li><li><p>⭐ 另一个归类标准是：临时函数和持久化函数。临时函数由用户创建，它仅在会话的生命周期（也就是一个 Flink 任务的一次运行生命周期内）内有效。持久化函数不是由系统提供的，是存储在 Catalog 中，它在不同会话的生命周期内都有效。</p></li></ol><p>这两个维度归类标准组合下，Flink SQL 总共提供了 4 种函数：</p><ol><li>⭐ 临时性系统内置函数</li><li>⭐ 系统内置函数</li><li>⭐ 临时性 Catalog 函数（例如：Create Temporary Function）</li><li>⭐ Catalog 函数（例如：Create Function）</li></ol><p>请注意，在用户使用函数时，系统函数始终优先于 Catalog 函数解析，临时函数始终优先于持久化函数解析。</p><h2 id="4-2-SQL-函数的引用方式"><a href="#4-2-SQL-函数的引用方式" class="headerlink" title="4.2.SQL 函数的引用方式"></a>4.2.SQL 函数的引用方式</h2><p>用户在 Flink 中可以通过精确、模糊两种引用方式引用函数。</p><h3 id="4-2-1-精确函数"><a href="#4-2-1-精确函数" class="headerlink" title="4.2.1.精确函数"></a>4.2.1.精确函数</h3><p>精确函数引用是让用户限定 Catalog，数据库名称进行精准定位一个 UDF 然后调用。</p><p>例如：select mycatalog.mydb.myfunc(x) from mytable 或者 select mydb.myfunc(x) from mytable。</p><h3 id="4-2-2-模糊函数"><a href="#4-2-2-模糊函数" class="headerlink" title="4.2.2.模糊函数"></a>4.2.2.模糊函数</h3><p>在模糊函数引用中，用户只需在 SQL 查询中指定函数名就可以引用 UDF，例如： select myfunc(x) from mytable。</p><p>当然小伙伴萌问到，如果系统函数和 Catalog 函数的名称是重复的，Flink 体系是会使用哪一个函数呢？这就是下文要介绍的 UDF 解析顺序</p><h2 id="4-3-SQL-函数的解析顺序"><a href="#4-3-SQL-函数的解析顺序" class="headerlink" title="4.3.SQL 函数的解析顺序"></a>4.3.SQL 函数的解析顺序</h2><h3 id="4-3-1-精确函数"><a href="#4-3-1-精确函数" class="headerlink" title="4.3.1.精确函数"></a>4.3.1.精确函数</h3><p>由于精确函数应用一定会带上 Catalog 或者数据库名称，所以 Flink 中的精确函数引用一定是指向临时性 Catalog 函数或 Catalog 函数的。</p><p>比如：<code>select mycatalog.mydb.myfunc(x) from mytable</code>。</p><p>那么 Flink 对其解析顺序以及使用顺序如下：</p><ol><li>⭐ 临时性 catalog 函数</li><li>⭐ Catalog 函数</li></ol><h3 id="4-3-2-模糊函数"><a href="#4-3-2-模糊函数" class="headerlink" title="4.3.2.模糊函数"></a>4.3.2.模糊函数</h3><p>比如 <code>select myfunc(x) from mytable</code>。</p><p>解析顺序以及使用顺序如下：</p><ol><li>⭐ 临时性系统内置函数</li><li>⭐ 系统内置函数</li><li>⭐ 临时性 Catalog 函数, 只会在当前会话的当前 Catalog 和当前数据库中查找函数及解析函数</li><li>⭐ Catalog 函数, 在当前 Catalog 和当前数据库中查找函数及解析函数</li></ol><h2 id="4-4-系统内置函数"><a href="#4-4-系统内置函数" class="headerlink" title="4.4.系统内置函数"></a>4.4.系统内置函数</h2><p>系统内置函数小伙伴萌可以直接在 Flink 官网进行查询，博主这里就不多进行介绍。</p><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/dev/table/functions/systemfunctions/#hash-functions">https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/dev/table/functions/systemfunctions/#hash-functions</a></p><blockquote><p>注意：</p><p>在目前 1.13 版本的 Flink 体系中，内置的系统函数没有像 Hive 内置的函数那么丰富，比如 Hive 中常见的 get_json_object 之类的，Flink 都是没有的，但是 Flink 提供了插件化 Module 的能力，能扩充一些 UDF，下文会进行介绍。</p></blockquote><h2 id="4-5-SQL-自定义函数（UDF）"><a href="#4-5-SQL-自定义函数（UDF）" class="headerlink" title="4.5.SQL 自定义函数（UDF）"></a>4.5.SQL 自定义函数（UDF）</h2><p>！！！Flink 体系也提供了类似于其他大数据引擎的 UDF 体系。</p><p>自定义函数（UDF）是一种扩展开发机制，可以用来在查询语句里调用难以用 SQL 进行 <code>直接</code> 表达的频繁使用或自定义的逻辑。</p><p>目前 Flink 自定义函数可以基于 JVM 语言（例如 Java 或 Scala）或 Python 实现，实现者可以在 UDF 中使用任意第三方库，本章聚焦于使用 Java 语言开发自定义函数。</p><p>当前 Flink 提供了一下几种 UDF 能力：</p><ol><li>标量函数（Scalar functions 或 <code>UDAF</code>）：输入一条输出一条，将标量值转换成一个新标量值，对标 Hive 中的 UDF；</li><li>表值函数（Table functions 或 <code>UDTF</code>）：输入一条条输出多条，对标 Hive 中的 UDTF；</li><li>聚合函数（Aggregate functions 或 <code>UDAF</code>）：输入多条输出一条，对标 Hive 中的 UDAF；</li><li>表值聚合函数（Table aggregate functions 或 <code>UDTAF</code>）：仅仅支持 Table API，不支持 SQL API，其可以将多行转为多行；</li><li>异步表值函数（Async table functions）：这是一种特殊的 UDF，支持异步查询外部数据系统，用在前文介绍到的 lookup join 中作为查询外部系统的函数。</li></ol><p>先直接给一个案例看看，怎么创建并在 Flink SQL 中使用一个 UDF：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.ScalarFunction;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.flink.table.api.Expressions.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义一个标量函数</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SubstringFunction</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">eval</span><span class="params">(String s, Integer begin, Integer end)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> s.substring(begin, end);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">TableEnvironment env = TableEnvironment.create(...);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 Table API 可以直接以引用 class 方式使用 UDF</span></span><br><span class="line">env.from(<span class="string">&quot;MyTable&quot;</span>).select(call(SubstringFunction.class, $(<span class="string">&quot;myField&quot;</span>), <span class="number">5</span>, <span class="number">12</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册 UDF</span></span><br><span class="line">env.createTemporarySystemFunction(<span class="string">&quot;SubstringFunction&quot;</span>, SubstringFunction.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Table API 调用 UDF</span></span><br><span class="line">env.from(<span class="string">&quot;MyTable&quot;</span>).select(call(<span class="string">&quot;SubstringFunction&quot;</span>, $(<span class="string">&quot;myField&quot;</span>), <span class="number">5</span>, <span class="number">12</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL API 调用 UDF</span></span><br><span class="line">env.sqlQuery(<span class="string">&quot;SELECT SubstringFunction(myField, 5, 12) FROM MyTable&quot;</span>);</span><br></pre></td></tr></table></figure><p>注意：如果你的函数在初始化时，是有入参的，那么需要你的入参是 <code>Serializable</code> 的。即 Java 中需要继承 <code>Serializable</code> 接口。</p><p>案例如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.ScalarFunction;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.flink.table.api.Expressions.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义一个带有输入参数的标量函数</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SubstringFunction</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  -- <span class="keyword">boolean</span> 默认就是 Serializable 的</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">boolean</span> endInclusive;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">SubstringFunction</span><span class="params">(<span class="keyword">boolean</span> endInclusive)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.endInclusive = endInclusive;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">eval</span><span class="params">(String s, Integer begin, Integer end)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> s.substring(begin, endInclusive ? end + <span class="number">1</span> : end);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">TableEnvironment env = TableEnvironment.create(...);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Table API 调用 UDF</span></span><br><span class="line">env.from(<span class="string">&quot;MyTable&quot;</span>).select(call(<span class="keyword">new</span> SubstringFunction(<span class="keyword">true</span>), $(<span class="string">&quot;myField&quot;</span>), <span class="number">5</span>, <span class="number">12</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册 UDF</span></span><br><span class="line">env.createTemporarySystemFunction(<span class="string">&quot;SubstringFunction&quot;</span>, <span class="keyword">new</span> SubstringFunction(<span class="keyword">true</span>));</span><br></pre></td></tr></table></figure><h2 id="4-6-开发-UDF-之前的需知事项"><a href="#4-6-开发-UDF-之前的需知事项" class="headerlink" title="4.6.开发 UDF 之前的需知事项"></a>4.6.开发 UDF 之前的需知事项</h2><p>总结这几个事项主要包含以下步骤：</p><ol><li>首先需要继承 Flink SQL UDF 体系提供的基类，每种 UDF 实现都有不同的基类</li><li>实现 UDF 执行逻辑函数，不同类型的 UDF 需要实现不同的执行逻辑函数</li><li>注意 UDF 入参、出参类型推导，Flink 在一些基础类型上的是可以直接推导出类型信息的，但是一些复杂类型就无能为力了，这里需要用户主动介入</li><li>明确 UDF 输出结果是否是定值，如果是定值则 Flink 会在生成计划时就执行一遍，得出结果，然后使用这个定值的结果作为后续的执行逻辑的参数，这样可以做到不用在 Flink SQL 任务运行时每次都执行一次，会有性能优化</li><li>巧妙运用运行时上下文，可以在任务运行前加载到一些外部资源、上下文配置信息，扩展 UDF 能力</li></ol><h3 id="4-6-1-继承-UDF-基类"><a href="#4-6-1-继承-UDF-基类" class="headerlink" title="4.6.1.继承 UDF 基类"></a>4.6.1.继承 UDF 基类</h3><p>和 Hive UDF 实现思路类似，在 Flink UDF 体系中，需要注意一下事项：</p><ol><li>⭐ Flink UDF 要继承一个基类（比如标量 UDF 要继承 <code>org.apache.flink.table.functions.ScalarFunction</code>）。</li><li>⭐ 类必须声明为 <code>public</code>，不能是 <code>abstract</code> 类，不能使用非静态内部类或匿名类。</li><li>⭐ 为了在 Catalog 中存储此类，该类必须要有默认构造函数并且在运行时可以进行实例化。</li></ol><h3 id="4-6-2-实现-UDF-执行逻辑函数"><a href="#4-6-2-实现-UDF-执行逻辑函数" class="headerlink" title="4.6.2.实现 UDF 执行逻辑函数"></a>4.6.2.实现 UDF 执行逻辑函数</h3><p>基类提供了一组可以被重写的方法，来给用户进行使用，这些可被重写的方法就是主要承担 UDF 自定义执行逻辑的地方。</p><p>举例在 <code>ScalarFunction</code> 中：</p><ol><li>⭐ <code>open()</code>：用于初始化资源（比如连接外部资源），程序初始化时进行调用</li><li>⭐ <code>close()</code>：用于关闭资源，程序结束时进行调用</li><li>⭐ <code>isDeterministic()</code>：用于判断返回结果是否是确定的，如果是确定的，结果会被直接执行</li><li>⭐ <code>eval(xxx)</code>：Flink 用于处理每一条数据的主要处理逻辑函数</li></ol><p>你可以自定义 eval 的入参，比如：</p><ul><li>eval(Integer) 和 eval(LocalDateTime)；</li><li>使用变长参数，例如 eval(Integer…);</li><li>使用对象，例如 eval(Object) 可接受 LocalDateTime、Integer 作为参数，只要是 Object 都可以；</li><li>也可组合使用，例如 eval(Object…) 可接受所有类型的参数。</li></ul><p>并且你可以在一个 UDF 中重载 eval 函数来实现不同的逻辑，比如：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.ScalarFunction;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 有多个重载求和方法的函数</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SumFunction</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 入参为 Integer</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Integer <span class="title">eval</span><span class="params">(Integer a, Integer b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a + b;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 入参为 String</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Integer <span class="title">eval</span><span class="params">(String a, String b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Integer.valueOf(a) + Integer.valueOf(b);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 入参为多个 Double</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Integer <span class="title">eval</span><span class="params">(Double... d)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">double</span> result = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">double</span> value : d)</span><br><span class="line">      result += value;</span><br><span class="line">    <span class="keyword">return</span> (<span class="keyword">int</span>) result;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>注意：<br>由于 Flink 在运行时会调用这些方法，所以这些方法必须声明为 public，并且包含明确的输入和输出参数。</p></blockquote><h3 id="4-6-3-注意-UDF-入参、出参类型推导"><a href="#4-6-3-注意-UDF-入参、出参类型推导" class="headerlink" title="4.6.3.注意 UDF 入参、出参类型推导"></a>4.6.3.注意 UDF 入参、出参类型推导</h3><p>从两个角度来说，为什么函数的入参、出参类型会对 UDF 这么重要。</p><ol><li>⭐ 从开发人员角度讲，在设计 UDF 的时候，肯定会涉及到 UDF 预期的入参、出参类型信息、也包括一些数据的精度、小数位数等信息</li><li>⭐ 从程序运行角度讲，Flink SQL 程序运行时，肯定也需要知道怎么将 SQL 中的类型数据与 UDF 的入参、出参类型，这样才能做数据序列化等操作</li></ol><p>而 Flink 也提供了三种方式帮助 Flink 程序获取参数类型信息。</p><ol><li><p>⭐ 自动类型推导功能：Flink 具备 UDF 自动类型推导功能，该功能可以通过反射从函数的类及其求值方法派生数据类型。比如如果你的 UDF 的方法或者类的签名中已经有了对应的入参、出参的类型，Flink 一般都可以推导并获取到这些类型信息。</p></li><li><p>⭐ 添加类型注解：当 1 中的隐式反射提取方法不成功，则可以通过使用 Flink 提供的 <code>@DataTypeHint</code> 和 <code>@FunctionHint</code> 注解对应的参数、类或方法来显示的支持 Flink 参数类型提取。</p></li><li><p>⭐ 重写 <code>getTypeInference()</code>：你可以使用 Flink 提供的更高级的类型推导方法，你可以在 UDF 实现类中重写 <code>getTypeInference()</code> 方法去显示声明函数的参数类型信息</p></li></ol><p>接下来介绍几个例子。</p><ol><li>⭐ 自动类型推导案例：</li></ol><p>自动类型推导会检查函数的 <code>类</code> 签名和 <code>eval</code> 方法签名，从而推导出函数入参和出参的数据类型，<code>@DataTypeHint</code> 和 <code>@FunctionHint</code> 注解也可以辅助支持自动类型推导。</p><p>关于自动类型推导具体将 Java 的对象会映射成 SQL 的具体哪个数据类型，可以参考 <a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/types/#data-type-extraction">https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/types/#data-type-extraction</a></p><p>案例如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.table.annotation.DataTypeHint;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.annotation.InputGroup;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.ScalarFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.Row;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 有多个重载求值方法的函数</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">OverloadedFunction</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 不需要任何声明，可以直接推导出类型信息，即入参和出参对应到 SQL 中的 bigint 类型</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Long <span class="title">eval</span><span class="params">(<span class="keyword">long</span> a, <span class="keyword">long</span> b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a + b;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 使用 @DataTypeHint(&quot;DECIMAL(12, 3)&quot;) 定义 decimal 的精度和小数位</span></span><br><span class="line">  <span class="keyword">public</span> <span class="meta">@DataTypeHint(&quot;DECIMAL(12, 3)&quot;)</span> <span class="function">BigDecimal <span class="title">eval</span><span class="params">(<span class="keyword">double</span> a, <span class="keyword">double</span> b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> BigDecimal.valueOf(a + b);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 使用注解定义嵌套数据类型</span></span><br><span class="line">  <span class="meta">@DataTypeHint(&quot;ROW&lt;s STRING, t TIMESTAMP_LTZ(3)&gt;&quot;)</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Row <span class="title">eval</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Row.of(String.valueOf(i), Instant.ofEpochSecond(i));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 允许任意类型的输入，并输出序列化定制后的值</span></span><br><span class="line">  <span class="meta">@DataTypeHint(value = &quot;RAW&quot;, bridgedTo = ByteBuffer.class)</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> ByteBuffer <span class="title">eval</span><span class="params">(<span class="meta">@DataTypeHint(inputGroup = InputGroup.ANY)</span> Object o)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> MyUtils.serializeToByteBuffer(o);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="2"><li>⭐ 根据 @FunctionHint 注解自动推导类型案例：</li></ol><p>使用 @DataTypeHint 注解虽好，但是有些场景下，使用起来比较复杂，比如：</p><ul><li>⭐ 我们不希望 eval 函数的入参和出参都是一个非常具体的类型，比如 long，int，double 等。我们希望它是一个通用的类型，比如 Object。这样的话就不用重载那么多的函数，可以直接使用一个 eval 函数实现不同的处理逻辑，返回不同类型的结果</li><li>⭐ 多个 eval 方法的返回结果类型都是相同的，我们懒得写多次 @DataTypeHint</li></ul><p>那么就可以使用 @FunctionHint 实现，@FunctionHint 是声明在类上面的，举例如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.table.annotation.DataTypeHint;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.annotation.FunctionHint;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.TableFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.Row;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. 解耦类型推导与 eval 方法，类型推导根据 FunctionHint 注解中的信息来，下面的案例说明当前这个 UDF 有三种输入输出类型信息组合</span></span><br><span class="line"><span class="meta">@FunctionHint(</span></span><br><span class="line"><span class="meta">  input = &#123;@DataTypeHint(&quot;INT&quot;), @DataTypeHint(&quot;INT&quot;)&#125;,</span></span><br><span class="line"><span class="meta">  output = @DataTypeHint(&quot;INT&quot;)</span></span><br><span class="line"><span class="meta">)</span></span><br><span class="line"><span class="meta">@FunctionHint(</span></span><br><span class="line"><span class="meta">  input = &#123;@DataTypeHint(&quot;BIGINT&quot;), @DataTypeHint(&quot;BIGINT&quot;)&#125;,</span></span><br><span class="line"><span class="meta">  output = @DataTypeHint(&quot;BIGINT&quot;)</span></span><br><span class="line"><span class="meta">)</span></span><br><span class="line"><span class="meta">@FunctionHint(</span></span><br><span class="line"><span class="meta">  input = &#123;&#125;,</span></span><br><span class="line"><span class="meta">  output = @DataTypeHint(&quot;BOOLEAN&quot;)</span></span><br><span class="line"><span class="meta">)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">OverloadedFunction</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>&lt;<span class="title">Object</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eval</span><span class="params">(Object... o)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (o.length == <span class="number">0</span>) &#123;</span><br><span class="line">      collect(<span class="keyword">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    collect(o[<span class="number">0</span>]);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 为函数类的所有 eval 方法指定同一个输出类型</span></span><br><span class="line"><span class="meta">@FunctionHint(output = @DataTypeHint(&quot;ROW&lt;s STRING, i INT&gt;&quot;))</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">OverloadedFunction</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>&lt;<span class="title">Row</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eval</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span> </span>&#123;</span><br><span class="line">    collect(Row.of(<span class="string">&quot;Sum&quot;</span>, a + b));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eval</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    collect(Row.of(<span class="string">&quot;Empty args&quot;</span>, -<span class="number">1</span>));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="3"><li>⭐ getTypeInference()</li></ol><p>getTypeInference() 可以做到根据小伙伴萌自定义的方式去定义类型推导过程及结果，具有高度自定义的能力。举例如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.DataTypes;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.catalog.DataTypeFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.ScalarFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.types.inference.TypeInference;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.Row;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">LiteralFunction</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> Object <span class="title">eval</span><span class="params">(String s, String type)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">switch</span> (type) &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">&quot;INT&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> Integer.valueOf(s);</span><br><span class="line">      <span class="keyword">case</span> <span class="string">&quot;DOUBLE&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> Double.valueOf(s);</span><br><span class="line">      <span class="keyword">case</span> <span class="string">&quot;STRING&quot;</span>:</span><br><span class="line">      <span class="keyword">default</span>:</span><br><span class="line">        <span class="keyword">return</span> s;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 如果实现了 getTypeInference，则会禁用自动的反射式类型推导，使用如下逻辑进行类型推导</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> TypeInference <span class="title">getTypeInference</span><span class="params">(DataTypeFactory typeFactory)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> TypeInference.newBuilder()</span><br><span class="line">      <span class="comment">// 指定输入参数的类型，必要时参数会被隐式转换</span></span><br><span class="line">      .typedArguments(DataTypes.STRING(), DataTypes.STRING())</span><br><span class="line">      <span class="comment">// 用户高度自定义的类型推导逻辑</span></span><br><span class="line">      .outputTypeStrategy(callContext -&gt; &#123;</span><br><span class="line">        <span class="keyword">if</span> (!callContext.isArgumentLiteral(<span class="number">1</span>) || callContext.isArgumentNull(<span class="number">1</span>)) &#123;</span><br><span class="line">          <span class="keyword">throw</span> callContext.newValidationError(<span class="string">&quot;Literal expected for second argument.&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 基于第一个入参决定具体的返回数据类型</span></span><br><span class="line">        <span class="keyword">final</span> String literal = callContext.getArgumentValue(<span class="number">1</span>, String.class).orElse(<span class="string">&quot;STRING&quot;</span>);</span><br><span class="line">        <span class="keyword">switch</span> (literal) &#123;</span><br><span class="line">          <span class="keyword">case</span> <span class="string">&quot;INT&quot;</span>:</span><br><span class="line">            <span class="keyword">return</span> Optional.of(DataTypes.INT().notNull());</span><br><span class="line">          <span class="keyword">case</span> <span class="string">&quot;DOUBLE&quot;</span>:</span><br><span class="line">            <span class="keyword">return</span> Optional.of(DataTypes.DOUBLE().notNull());</span><br><span class="line">          <span class="keyword">case</span> <span class="string">&quot;STRING&quot;</span>:</span><br><span class="line">          <span class="keyword">default</span>:</span><br><span class="line">            <span class="keyword">return</span> Optional.of(DataTypes.STRING());</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">      .build();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-6-4-明确-UDF-输出结果是否是定值"><a href="#4-6-4-明确-UDF-输出结果是否是定值" class="headerlink" title="4.6.4.明确 UDF 输出结果是否是定值"></a>4.6.4.明确 UDF 输出结果是否是定值</h3><p>用户可以通过重写 <code>isDeterministic()</code> 函数来声明这个 UDF 产出的结果是否是一个定值。</p><p>对于纯函数（即没有入参的函数，比如 random(), date(), or now() 等）来说，默认情况下 <code>isDeterministic()</code> 返回 true，小伙伴萌可以自定义返回 false。</p><p>如果函数不是一个纯函数（即没有入参的函数，比如 random(), date(), or now() 等），这个方法必须返回 <code>false</code>。</p><p>那么 <code>isDeterministic()</code> 方法的返回值到底影响什么呢？</p><p>答案：影响 Flink 任务在什么时候就直接执行这个 UDF。主要在以下两个方面体现：</p><ol><li><p>⭐ Flink 在生成计划期间直接执行 UDF 获得结果：如果使用常量表达式调用函数，或者使用常量作为函数的入参，则 Flink 任务可能不会在任务正式运行时执行该函数。举个例子，<code>SELECT ABS(-1) FROM t</code>，<code>SELECT ABS(field) FROM t WHERE field = -1</code>，这两种都会被 Flink 进行优化，直接把 ABS(-1) 的结果在客户端生成执行计划时就将结果运行出来。如果不想在生成执行计划阶段直接将结果运行出来，可以实现 <code>isDeterministic()</code> 返回 false。</p></li><li><p>⭐ Flink 在程序运行期间执行 UDF 获得结果：如果 UDF 的入参不是常量表达式，或者 <code>isDeterministic()</code> 返回 false，则 Flink 会在程序运行期间执行 UDF。</p></li></ol><p>那么小伙伴会问到，有些场景下 Flink SQL 是做了各种优化之后然后推断出表达式是否是常量，我怎么判断能够更加方便的判断出这个 Flink 是否将这个 UDF 的优化为固定结果了呢？</p><p>结论：这些都是可以在 Flink SQL 生成的算子图中看到，在 Flink web ui 中，每一个算子上面都可以详细看到 Flink 最终生成的算子执行逻辑。</p><h3 id="4-6-5-巧妙运用运行时上下文"><a href="#4-6-5-巧妙运用运行时上下文" class="headerlink" title="4.6.5.巧妙运用运行时上下文"></a>4.6.5.巧妙运用运行时上下文</h3><p>有时候我们想在 UDF 需要获取一些 Flink 任务运行的全局信息，或者在 UDF 真正处理数据之前做一些配置（setup）/清理（clean-up）的工作。UDF 为我们提供了 open() 和 close() 方法，你可以重写这两个方法做到类似于 DataStream API 中 RichFunction 的功能。</p><ol><li>⭐ <code>open()</code> 方法：在任务初始化时被调用，常常用于加载一些外部资源；</li><li>⭐ <code>close()</code> 方法：在任务结束时被调用，常常用于关闭一些外部资源；</li></ol><p>其中 open() 方法提供了一个 FunctionContext，它包含了一些 UDF 被执行时的上下文信息，比如 metric group、分布式文件缓存，或者是全局的作业参数等。</p><p>比如可以获取到下面的信息：</p><ol><li>⭐ getMetricGroup()：执行该函数的 subtask 的 Metric Group</li><li>⭐ getCachedFile(name)：分布式文件缓存的本地临时文件副本</li><li>⭐ getJobParameter(name, defaultValue)：获取 Flink 任务的全局作业参数</li><li>⭐ getExternalResourceInfos(resourceName)：获取一些外部资源</li></ol><p>下面的例子展示了如何在一个标量函数中通过 FunctionContext 来获取一个全局的任务参数：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.FunctionContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.ScalarFunction;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">HashCodeFunction</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> factor = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(FunctionContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 4. 在 UDF 中获取全局参数 hashcode_factor</span></span><br><span class="line">        <span class="comment">// 用户可以配置全局作业参数 &quot;hashcode_factor&quot;</span></span><br><span class="line">        <span class="comment">// 获取参数 &quot;hashcode_factor&quot;</span></span><br><span class="line">        <span class="comment">// 如果不存在，则使用默认值 &quot;12&quot;</span></span><br><span class="line">        factor = Integer.parseInt(context.getJobParameter(<span class="string">&quot;hashcode_factor&quot;</span>, <span class="string">&quot;12&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">eval</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> s.hashCode() * factor;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">TableEnvironment env = TableEnvironment.create(...);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. 设置任务参数</span></span><br><span class="line">env.getConfig().addJobParameter(<span class="string">&quot;hashcode_factor&quot;</span>, <span class="string">&quot;31&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 注册函数</span></span><br><span class="line">env.createTemporarySystemFunction(<span class="string">&quot;hashCode&quot;</span>, HashCodeFunction.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 调用函数</span></span><br><span class="line">env.sqlQuery(<span class="string">&quot;SELECT myField, hashCode(myField) FROM MyTable&quot;</span>);</span><br></pre></td></tr></table></figure><p>以上就是关于开发一个 UDF 之前，你需要注意的一些事项，这些内容不但包含了一些基础必备知识，也包含了一些扩展知识，帮助我们开发更强大的 UDF。</p><h2 id="4-7-SQL-标量函数（Scalar-Function）"><a href="#4-7-SQL-标量函数（Scalar-Function）" class="headerlink" title="4.7.SQL 标量函数（Scalar Function）"></a>4.7.SQL 标量函数（Scalar Function）</h2><p>标量函数即 UDF，常常用于进一条数据出一条数据的场景。</p><p>使用 Java\Scala 开发一个 Scalar Function 必须包含以下几点：</p><ol><li>⭐ 实现 <code>org.apache.flink.table.functions.ScalarFunction</code> 接口</li><li>⭐ 实现一个或者多个自定义的 eval 函数，名称必须叫做 eval，eval 方法签名必须是 public 的</li><li>⭐ eval 方法的入参、出参都是直接体现在 eval 函数的签名中</li></ol><p>举例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.table.annotation.InputGroup;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.ScalarFunction;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.flink.table.api.Expressions.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">HashFunction</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 接受任意类型输入，返回 INT 型输出</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">eval</span><span class="params">(<span class="meta">@DataTypeHint(inputGroup = InputGroup.ANY)</span> Object o)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> o.hashCode();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">TableEnvironment env = TableEnvironment.create(...);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 Table API 里不经注册直接调用函数</span></span><br><span class="line">env.from(<span class="string">&quot;MyTable&quot;</span>).select(call(HashFunction.class, $(<span class="string">&quot;myField&quot;</span>)));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册函数</span></span><br><span class="line">env.createTemporarySystemFunction(<span class="string">&quot;HashFunction&quot;</span>, HashFunction.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 Table API 里调用注册好的函数</span></span><br><span class="line">env.from(<span class="string">&quot;MyTable&quot;</span>).select(call(<span class="string">&quot;HashFunction&quot;</span>, $(<span class="string">&quot;myField&quot;</span>)));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 SQL 里调用注册好的函数</span></span><br><span class="line">env.sqlQuery(<span class="string">&quot;SELECT HashFunction(myField) FROM MyTable&quot;</span>);</span><br></pre></td></tr></table></figure><h2 id="4-8-SQL-表值函数（Table-Function）"><a href="#4-8-SQL-表值函数（Table-Function）" class="headerlink" title="4.8.SQL 表值函数（Table Function）"></a>4.8.SQL 表值函数（Table Function）</h2><p>表值函数即 UDTF，常用于进一条数据，出多条数据的场景。</p><p>使用 Java\Scala 开发一个 Table Function 必须包含以下几点：</p><ol><li>⭐ 实现 <code>org.apache.flink.table.functions.TableFunction</code> 接口</li><li>⭐ 实现一个或者多个自定义的 eval 函数，名称必须叫做 eval，eval 方法签名必须是 public 的</li><li>⭐ eval 方法的入参是直接体现在 eval 函数签名中，出参是体现在 TableFunction 类的泛型参数 T 中，eval 是没有返回值的，这一点是和标量函数不同的，Flink TableFunction 接口提供了 <code>collect(T)</code> 来发送输出的数据。这一点也比较好理解，如果都体现在函数签名上，那就成了标量函数了，而使用 <code>collect(T)</code> 才能体现出 <code>进一条数据</code> <code>出多条数据</code></li></ol><p>在 SQL 中是用 SQL 中的 <code>LATERAL TABLE(&lt;TableFunction&gt;)</code> 配合 <code>JOIN</code>、<code>LEFT JOIN</code> xxx <code>ON TRUE</code> 使用。</p><p>举例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.table.annotation.DataTypeHint;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.annotation.FunctionHint;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.TableFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.Row;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.flink.table.api.Expressions.*;</span><br><span class="line"></span><br><span class="line"><span class="meta">@FunctionHint(output = @DataTypeHint(&quot;ROW&lt;word STRING, length INT&gt;&quot;))</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SplitFunction</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>&lt;<span class="title">Row</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eval</span><span class="params">(String str)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (String s : str.split(<span class="string">&quot; &quot;</span>)) &#123;</span><br><span class="line">      <span class="comment">// 输出结果</span></span><br><span class="line">      collect(Row.of(s, s.length()));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">TableEnvironment env = TableEnvironment.create(...);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 Table API 里可以直接调用 UDF</span></span><br><span class="line">env</span><br><span class="line">  .from(<span class="string">&quot;MyTable&quot;</span>)</span><br><span class="line">  .joinLateral(call(SplitFunction.class, $(<span class="string">&quot;myField&quot;</span>)))</span><br><span class="line">  .select($(<span class="string">&quot;myField&quot;</span>), $(<span class="string">&quot;word&quot;</span>), $(<span class="string">&quot;length&quot;</span>));</span><br><span class="line"></span><br><span class="line">env</span><br><span class="line">  .from(<span class="string">&quot;MyTable&quot;</span>)</span><br><span class="line">  .leftOuterJoinLateral(call(SplitFunction.class, $(<span class="string">&quot;myField&quot;</span>)))</span><br><span class="line">  .select($(<span class="string">&quot;myField&quot;</span>), $(<span class="string">&quot;word&quot;</span>), $(<span class="string">&quot;length&quot;</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 Table API 里重命名 UDF 的结果字段</span></span><br><span class="line">env</span><br><span class="line">  .from(<span class="string">&quot;MyTable&quot;</span>)</span><br><span class="line">  .leftOuterJoinLateral(call(SplitFunction.class, $(<span class="string">&quot;myField&quot;</span>)).as(<span class="string">&quot;newWord&quot;</span>, <span class="string">&quot;newLength&quot;</span>))</span><br><span class="line">  .select($(<span class="string">&quot;myField&quot;</span>), $(<span class="string">&quot;newWord&quot;</span>), $(<span class="string">&quot;newLength&quot;</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册函数</span></span><br><span class="line">env.createTemporarySystemFunction(<span class="string">&quot;SplitFunction&quot;</span>, SplitFunction.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 Table API 里调用注册好的 UDF</span></span><br><span class="line">env</span><br><span class="line">  .from(<span class="string">&quot;MyTable&quot;</span>)</span><br><span class="line">  .joinLateral(call(<span class="string">&quot;SplitFunction&quot;</span>, $(<span class="string">&quot;myField&quot;</span>)))</span><br><span class="line">  .select($(<span class="string">&quot;myField&quot;</span>), $(<span class="string">&quot;word&quot;</span>), $(<span class="string">&quot;length&quot;</span>));</span><br><span class="line"></span><br><span class="line">env</span><br><span class="line">  .from(<span class="string">&quot;MyTable&quot;</span>)</span><br><span class="line">  .leftOuterJoinLateral(call(<span class="string">&quot;SplitFunction&quot;</span>, $(<span class="string">&quot;myField&quot;</span>)))</span><br><span class="line">  .select($(<span class="string">&quot;myField&quot;</span>), $(<span class="string">&quot;word&quot;</span>), $(<span class="string">&quot;length&quot;</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 SQL 里调用注册好的 UDF</span></span><br><span class="line">env.sqlQuery(</span><br><span class="line">  <span class="string">&quot;SELECT myField, word, length &quot;</span> +</span><br><span class="line">  <span class="string">&quot;FROM MyTable, LATERAL TABLE(SplitFunction(myField))&quot;</span>);</span><br><span class="line"></span><br><span class="line">env.sqlQuery(</span><br><span class="line">  <span class="string">&quot;SELECT myField, word, length &quot;</span> +</span><br><span class="line">  <span class="string">&quot;FROM MyTable &quot;</span> +</span><br><span class="line">  <span class="string">&quot;LEFT JOIN LATERAL TABLE(SplitFunction(myField)) ON TRUE&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 SQL 里重命名 UDF 字段</span></span><br><span class="line">env.sqlQuery(</span><br><span class="line">  <span class="string">&quot;SELECT myField, newWord, newLength &quot;</span> +</span><br><span class="line">  <span class="string">&quot;FROM MyTable &quot;</span> +</span><br><span class="line">  <span class="string">&quot;LEFT JOIN LATERAL TABLE(SplitFunction(myField)) AS T(newWord, newLength) ON TRUE&quot;</span>);</span><br></pre></td></tr></table></figure><blockquote><p>注意：</p><p>如果你是使用 Scala 实现函数，不要使用 Scala 中 object 实现 UDF，Scala object 是单例的，有可能会导致并发问题。</p></blockquote><h2 id="4-9-SQL-聚合函数（Aggregate-Function）"><a href="#4-9-SQL-聚合函数（Aggregate-Function）" class="headerlink" title="4.9.SQL 聚合函数（Aggregate Function）"></a>4.9.SQL 聚合函数（Aggregate Function）</h2><p>聚合函数即 UDAF，常用于进多条数据，出一条数据的场景。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/19.png" alt="UDAF"></p><p>上面的图片展示了一个聚合函数的例子以及聚合函数包含的几个重要方法。</p><p>假设你有一个关于饮料的表。表里面有三个字段，分别是 id、name、price，表里有 5 行数据。</p><p>假设你需要找到所有饮料里最贵的饮料的价格，即执行一个 max() 聚合就能拿到结果。那么 max() 聚合的执行旧需要遍历所有 5 行数据，最终结果就只有一个数值。</p><p>使用 Java\Scala 开发一个 Aggregate Function 必须包含以下几点：</p><ol><li>⭐ 实现 <code>AggregateFunction</code> 接口，其中所有的方法必须是 public 的、非 static 的</li><li>⭐ 必须实现以下几个方法：</li></ol><ul><li>⭐ <code>Acc聚合中间结果 createAccumulator()</code>：为当前 Key 初始化一个空的 accumulator，其存储了聚合的中间结果，比如在执行 max() 时会存储当前的 max 值</li><li>⭐ <code>accumulate(Acc accumulator, Input输入参数)</code>：对于每一行数据，都会调用 accumulate() 方法来更新 accumulator，这个方法就是用于处理每一条输入数据；这个方法必须声明为 public 和非 static 的。accumulate 方法可以重载，每个方法的参数类型可以不同，并且支持变长参数。</li><li>⭐ <code>Output输出参数 getValue(Acc accumulator)</code>：通过调用 getValue 方法来计算和返回最终的结果</li></ul><ol start="3"><li>⭐ 还有几个方法是在某些场景下才必须实现的：</li></ol><ul><li>⭐ <code>retract(Acc accumulator, Input输入参数)</code>：在回撤流的场景下必须要实现，Flink 在计算回撤数据时需要进行调用，如果没有实现则会直接报错</li><li>⭐ <code>merge(Acc accumulator, Iterable&lt;Acc&gt; it)</code>：在许多批式聚合以及流式聚合中的 Session、Hop 窗口聚合场景下都是必须要实现的。除此之外，这个方法对于优化也很多帮助。例如，如果你打开了两阶段聚合优化，就需要 AggregateFunction 实现 merge 方法，从而可以做到在数据进行 shuffle 前先进行一次聚合计算。</li><li>⭐ <code>resetAccumulator()</code>：在批式聚合中是必须实现的。</li></ul><ol start="4"><li>⭐ 还有几个关于入参、出参数据类型信息的方法，默认情况下，用户的 <code>Input输入参数</code>（<code>accumulate(Acc accumulator, Input输入参数)</code> 的入参 <code>Input输入参数</code>）、accumulator（<code>Acc聚合中间结果 createAccumulator()</code> 的返回结果）、<code>Output输出参数</code> 数据类型（<code>Output输出参数 getValue(Acc accumulator)</code> 的 <code>Output输出参数</code>）都会被 Flink 使用反射获取到。但是对于 <code>accumulator</code> 和 <code>Output输出参数</code> 类型来说，Flink SQL 的类型推导在遇到复杂类型的时候可能会推导出错误的结果（注意：<code>Input输入参数</code> 因为是上游算子传入的，所以类型信息是确认的，不会出现推导错误的情况），比如那些非基本类型 POJO 的复杂类型。所以跟 ScalarFunction 和 TableFunction 一样，AggregateFunction 提供了 <code>AggregateFunction#getResultType()</code> 和 <code>AggregateFunction#getAccumulatorType()</code> 来分别指定最终返回值类型和 accumulator 的类型，两个函数的返回值类型都是 TypeInformation，所以熟悉 DataStream 的小伙伴很容易上手。</li></ol><ul><li>⭐ <code>getResultType()</code>：即 <code>Output输出参数 getValue(Acc accumulator)</code> 的输出结果数据类型</li><li>⭐ <code>getAccumulatorType()</code>：即 <code>Acc聚合中间结果 createAccumulator()</code> 的返回结果数据类型</li></ul><p>这个时候，我们直接来举一个加权平均值的例子看下，总共 3 个步骤：</p><ul><li>⭐ 定义一个聚合函数来计算某一列的加权平均</li><li>⭐ 在 TableEnvironment 中注册函数</li><li>⭐ 在查询中使用函数</li></ul><p>为了计算加权平均值，accumulator 需要存储加权总和以及数据的条数。在我们的例子里，我们定义了一个类 WeightedAvgAccumulator 来作为 accumulator。</p><p>Flink 的 checkpoint 机制会自动保存 accumulator，在失败时进行恢复，以此来保证精确一次的语义。</p><p>我们的 WeightedAvg（聚合函数）的 accumulate 方法有三个输入参数。第一个是 WeightedAvgAccum accumulator，另外两个是用户自定义的输入：输入的值 ivalue 和 输入的权重 iweight。</p><p>尽管 retract()、merge()、resetAccumulator() 这几个方法在大多数聚合类型中都不是必须实现的，博主也在样例中提供了他们的实现。并且定义了 getResultType() 和 getAccumulatorType()。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.AggregateFunction;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.flink.table.api.Expressions.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义一个计算权重 avg 的 accmulator</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WeightedAvgAccumulator</span> </span>&#123;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">long</span> sum = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输入：Long iValue, Integer iWeight</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WeightedAvg</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Long</span>, <span class="title">WeightedAvgAccumulator</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span> </span><br><span class="line">  <span class="comment">// 创建一个 accumulator</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> WeightedAvgAccumulator <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> WeightedAvgAccumulator();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accumulate</span><span class="params">(WeightedAvgAccumulator acc, Long iValue, Integer iWeight)</span> </span>&#123;</span><br><span class="line">    acc.sum += iValue * iWeight;</span><br><span class="line">    acc.count += iWeight;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">retract</span><span class="params">(WeightedAvgAccumulator acc, Long iValue, Integer iWeight)</span> </span>&#123;</span><br><span class="line">    acc.sum -= iValue * iWeight;</span><br><span class="line">    acc.count -= iWeight;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="comment">// 获取返回结果</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Long <span class="title">getValue</span><span class="params">(WeightedAvgAccumulator acc)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (acc.count == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> acc.sum / acc.count;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Session window 可以使用这个方法将几个单独窗口的结果合并</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">merge</span><span class="params">(WeightedAvgAccumulator acc, Iterable&lt;WeightedAvgAccumulator&gt; it)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (WeightedAvgAccumulator a : it) &#123;</span><br><span class="line">      acc.count += a.count;</span><br><span class="line">      acc.sum += a.sum;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">resetAccumulator</span><span class="params">(WeightedAvgAccumulator acc)</span> </span>&#123;</span><br><span class="line">    acc.count = <span class="number">0</span>;</span><br><span class="line">    acc.sum = <span class="number">0L</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">TableEnvironment env = TableEnvironment.create(...);</span><br><span class="line"></span><br><span class="line">env</span><br><span class="line">  .from(<span class="string">&quot;MyTable&quot;</span>)</span><br><span class="line">  .groupBy($(<span class="string">&quot;myField&quot;</span>))</span><br><span class="line">  .select($(<span class="string">&quot;myField&quot;</span>), call(WeightedAvg.class, $(<span class="string">&quot;value&quot;</span>), $(<span class="string">&quot;weight&quot;</span>)));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册函数</span></span><br><span class="line">env.createTemporarySystemFunction(<span class="string">&quot;WeightedAvg&quot;</span>, WeightedAvg.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Table API 调用函数</span></span><br><span class="line">env</span><br><span class="line">  .from(<span class="string">&quot;MyTable&quot;</span>)</span><br><span class="line">  .groupBy($(<span class="string">&quot;myField&quot;</span>))</span><br><span class="line">  .select($(<span class="string">&quot;myField&quot;</span>), call(<span class="string">&quot;WeightedAvg&quot;</span>, $(<span class="string">&quot;value&quot;</span>), $(<span class="string">&quot;weight&quot;</span>)));</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL API 调用函数</span></span><br><span class="line">env.sqlQuery(</span><br><span class="line">  <span class="string">&quot;SELECT myField, WeightedAvg(`value`, weight) FROM MyTable GROUP BY myField&quot;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><h2 id="4-10-SQL-表值聚合函数（Table-Aggregate-Function）"><a href="#4-10-SQL-表值聚合函数（Table-Aggregate-Function）" class="headerlink" title="4.10.SQL 表值聚合函数（Table Aggregate Function）"></a>4.10.SQL 表值聚合函数（Table Aggregate Function）</h2><p>表值聚合函数即 UDTAF。首先说明这个函数目前只能在 Table API 中进行使用，不能在 SQL API 中使用。那么这个函数有什么作用呢，为什么被创建出来？</p><p>因为在 SQL 表达式中，如果我们想对数据先分组再进行聚合取值，能选择的就是 <code>select max(xxx) from source_table group by key1, key2</code>。但是上面这个 SQL 的 max 语义最后产出的结果只有一条最终结果，如果我想取聚合结果最大的 n 条数据，并且 n 条数据，每一条都要输出一次结果数据，上面的 SQL 就没有办法实现了（因为在聚合的情况下还输出多条，从上述 SQL 语义上来说就是不正确的）。</p><p>所以 UDTAF 就是为了处理这种场景，他可以让我们自定义 <code>怎么去</code>，<code>取多少条</code> 最终的聚合结果。所以可以看到 UDTAF 和 UDAF 是类似的。如下图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/20.png" alt="UDTAF"></p><p>上图展示了一个表值聚合函数的例子。</p><p>假设你有一个饮料的表，这个表有 3 列，分别是 id、name 和 price，一共有 5 行。</p><p>假设你需要找到价格最高的两个饮料，类似于 top2() 表值聚合函数。你需要遍历所有 5 行数据，输出结果为 2 行数据的一个表。</p><p>使用 Java\Scala 开发一个 Table Aggregate Function 必须包含以下几点：</p><ol><li>⭐ 实现 <code>TableAggregateFunction</code> 接口，其中所有的方法必须是 public 的、非 static 的</li><li>⭐ 必须实现以下几个方法：</li></ol><ul><li>⭐ <code>Acc聚合中间结果 createAccumulator()</code>：为当前 Key 初始化一个空的 accumulator，其存储了聚合的中间结果，比如在执行 max() 时会存储每一条中间结果的 max 值</li><li>⭐ <code>accumulate(Acc accumulator, Input输入参数)</code>：对于每一行数据，都会调用 accumulate() 方法来更新 accumulator，这个方法就是对每一条输入数据进行执行，比如执行 max() 时，遍历每一条数据执行；在实现这个方法是必须声明为 public 和非 static 的。accumulate 方法可以重载，每个方法的参数类型不同，并且支持变长参数。</li><li>⭐ <code>emitValue(Acc accumulator, Collector&lt;OutPut&gt; collector)</code> 或者 <code>emitUpdateWithRetract(Acc accumulator, RetractableCollector&lt;OutPut&gt; collector)</code>：当遍历所有的数据，当所有的数据都处理完了之后，通过调用 emit 方法来计算和输出最终的结果，在这里你就可以自定义到底输出多条少以及怎么样去输出结果。那么对于 emitValue 以及 emitUpdateWithRetract 的区别来说，拿 TopN 实现来说，emitValue 每次都会发送所有的最大的 n 个值，而这在流式任务中可能会有一些性能问题。为了提升性能，用户可以实现 emitUpdateWithRetract 方法。这个方法在 retract 模式下会增量的输出结果，比如只在有数据更新时，可以做到撤回老的数据，然后再发送新的数据，而不需要每次都发出全量的最新数据。如果我们同时定义了 emitUpdateWithRetract、emitValue 方法，那 emitUpdateWithRetract 会优先于 emitValue 方法被使用，因为引擎会认为 emitUpdateWithRetract 会更加高效，因为它的输出是增量的。 </li></ul><ol start="3"><li>⭐ 还有几个方法是在某些场景下才必须实现的：</li></ol><ul><li>⭐ <code>retract(Acc accumulator, Input输入参数)</code>：在回撤流的场景下必须要实现，Flink 在计算回撤数据时需要进行调用，如果没有实现则会直接报错</li><li>⭐ <code>merge(Acc accumulator, Iterable&lt;Acc&gt; it)</code>：在许多批式聚合以及流式聚合中的 Session、Hop 窗口聚合场景下都是必须要实现的。除此之外，这个方法对于优化也很多帮助。例如，如果你打开了两阶段聚合优化，就需要 AggregateFunction 实现 merge 方法，从而在第一阶段先进行数据聚合。</li><li>⭐ <code>resetAccumulator()</code>：在批式聚合中是必须实现的。</li></ul><ol start="4"><li>⭐ 还有几个关于入参、出参数据类型信息的方法，默认情况下，用户的 <code>Input输入参数</code>（<code>accumulate(Acc accumulator, Input输入参数)</code> 的入参 <code>Input输入参数</code>）、accumulator（<code>Acc聚合中间结果 createAccumulator()</code> 的返回结果）、<code>Output输出参数</code> 数据类型（<code>emitValue(Acc acc, Collector&lt;Output输出参数&gt; out)</code> 的 <code>Output输出参数</code>）都会被 Flink 使用反射获取到。但是对于 <code>accumulator</code> 和 <code>Output输出参数</code> 类型来说，Flink SQL 的类型推导在遇到复杂类型的时候可能会推导出错误的结果（注意：<code>Input输入参数</code> 因为是上游算子传入的，所以类型信息是确认的，不会出现推导错误的情况），比如那些非基本类型 POJO 的复杂类型。所以跟 ScalarFunction 和 TableFunction 一样，AggregateFunction 提供了 <code>TableAggregateFunction#getResultType()</code> 和 <code>TableAggregateFunction#getAccumulatorType()</code> 来分别指定最终返回值类型和 accumulator 的类型，两个函数的返回值类型都是 TypeInformation，所以熟悉 DataStream 的小伙伴很容易上手。</li></ol><ul><li>⭐ <code>getResultType()</code>：即 <code>emitValue(Acc acc, Collector&lt;Output输出参数&gt; out)</code> 的输出结果数据类型</li><li>⭐ <code>getAccumulatorType()</code>：即 <code>Acc聚合中间结果 createAccumulator()</code> 的返回结果数据类型</li></ul><p>这个时候，我们直接来举一个 Top2 的例子看下吧：</p><ul><li>⭐ 定义一个 TableAggregateFunction 来计算给定列的最大的 2 个值</li><li>⭐ 在 TableEnvironment 中注册函数</li><li>⭐ 在 Table API 查询中使用函数（当前只在 Table API 中支持 TableAggregateFunction）</li></ul><p>为了计算最大的 2 个值，accumulator 需要保存当前看到的最大的 2 个值。</p><p>在我们的例子中，我们定义了类 Top2Accum 来作为 accumulator。</p><p>Flink 的 checkpoint 机制会自动保存 accumulator，并且在失败时进行恢复，来保证精确一次的语义。</p><p>我们的 Top2 表值聚合函数（TableAggregateFunction）的 accumulate() 方法有两个输入，第一个是 Top2Accum accumulator，另一个是用户定义的输入：输入的值 v。尽管 merge() 方法在大多数聚合类型中不是必须的，我们也在样例中提供了它的实现。并且定义了 getResultType() 和 getAccumulatorType() 方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Accumulator for Top2.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Top2Accum</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> Integer first;</span><br><span class="line">    <span class="keyword">public</span> Integer second;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Top2</span> <span class="keyword">extends</span> <span class="title">TableAggregateFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">Integer</span>, <span class="title">Integer</span>&gt;, <span class="title">Top2Accum</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Top2Accum <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Top2Accum acc = <span class="keyword">new</span> Top2Accum();</span><br><span class="line">        acc.first = Integer.MIN_VALUE;</span><br><span class="line">        acc.second = Integer.MIN_VALUE;</span><br><span class="line">        <span class="keyword">return</span> acc;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accumulate</span><span class="params">(Top2Accum acc, Integer v)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (v &gt; acc.first) &#123;</span><br><span class="line">            acc.second = acc.first;</span><br><span class="line">            acc.first = v;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (v &gt; acc.second) &#123;</span><br><span class="line">            acc.second = v;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">merge</span><span class="params">(Top2Accum acc, java.lang.Iterable&lt;Top2Accum&gt; iterable)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (Top2Accum otherAcc : iterable) &#123;</span><br><span class="line">            accumulate(acc, otherAcc.first);</span><br><span class="line">            accumulate(acc, otherAcc.second);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">emitValue</span><span class="params">(Top2Accum acc, Collector&lt;Tuple2&lt;Integer, Integer&gt;&gt; out)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// emit the value and rank</span></span><br><span class="line">        <span class="keyword">if</span> (acc.first != Integer.MIN_VALUE) &#123;</span><br><span class="line">            out.collect(Tuple2.of(acc.first, <span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (acc.second != Integer.MIN_VALUE) &#123;</span><br><span class="line">            out.collect(Tuple2.of(acc.second, <span class="number">2</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册函数</span></span><br><span class="line">StreamTableEnvironment tEnv = ...</span><br><span class="line">tEnv.registerFunction(<span class="string">&quot;top2&quot;</span>, <span class="keyword">new</span> Top2());</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始化表</span></span><br><span class="line">Table tab = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用函数</span></span><br><span class="line">tab.groupBy(<span class="string">&quot;key&quot;</span>)</span><br><span class="line">    .flatAggregate(<span class="string">&quot;top2(a) as (v, rank)&quot;</span>)</span><br><span class="line">    .select(<span class="string">&quot;key, v, rank&quot;</span>);</span><br></pre></td></tr></table></figure><p>下面的例子展示了如何使用 emitUpdateWithRetract 方法来只发送更新的数据。</p><p>为了只发送更新的结果，accumulator 保存了上一次的最大的 2 个值，也保存了当前最大的 2 个值。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Accumulator for Top2.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Top2Accum</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> Integer first;</span><br><span class="line">    <span class="keyword">public</span> Integer second;</span><br><span class="line">    <span class="keyword">public</span> Integer oldFirst;</span><br><span class="line">    <span class="keyword">public</span> Integer oldSecond;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Top2</span> <span class="keyword">extends</span> <span class="title">TableAggregateFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">Integer</span>, <span class="title">Integer</span>&gt;, <span class="title">Top2Accum</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Top2Accum <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Top2Accum acc = <span class="keyword">new</span> Top2Accum();</span><br><span class="line">        acc.first = Integer.MIN_VALUE;</span><br><span class="line">        acc.second = Integer.MIN_VALUE;</span><br><span class="line">        acc.oldFirst = Integer.MIN_VALUE;</span><br><span class="line">        acc.oldSecond = Integer.MIN_VALUE;</span><br><span class="line">        <span class="keyword">return</span> acc;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accumulate</span><span class="params">(Top2Accum acc, Integer v)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (v &gt; acc.first) &#123;</span><br><span class="line">            acc.second = acc.first;</span><br><span class="line">            acc.first = v;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (v &gt; acc.second) &#123;</span><br><span class="line">            acc.second = v;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">emitUpdateWithRetract</span><span class="params">(Top2Accum acc, RetractableCollector&lt;Tuple2&lt;Integer, Integer&gt;&gt; out)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!acc.first.equals(acc.oldFirst)) &#123;</span><br><span class="line">            <span class="comment">// if there is an update, retract old value then emit new value.</span></span><br><span class="line">            <span class="keyword">if</span> (acc.oldFirst != Integer.MIN_VALUE) &#123;</span><br><span class="line">                out.retract(Tuple2.of(acc.oldFirst, <span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">            out.collect(Tuple2.of(acc.first, <span class="number">1</span>));</span><br><span class="line">            acc.oldFirst = acc.first;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!acc.second.equals(acc.oldSecond)) &#123;</span><br><span class="line">            <span class="comment">// if there is an update, retract old value then emit new value.</span></span><br><span class="line">            <span class="keyword">if</span> (acc.oldSecond != Integer.MIN_VALUE) &#123;</span><br><span class="line">                out.retract(Tuple2.of(acc.oldSecond, <span class="number">2</span>));</span><br><span class="line">            &#125;</span><br><span class="line">            out.collect(Tuple2.of(acc.second, <span class="number">2</span>));</span><br><span class="line">            acc.oldSecond = acc.second;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册函数</span></span><br><span class="line">StreamTableEnvironment tEnv = ...</span><br><span class="line">tEnv.registerFunction(<span class="string">&quot;top2&quot;</span>, <span class="keyword">new</span> Top2());</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始化表</span></span><br><span class="line">Table tab = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用函数</span></span><br><span class="line">tab.groupBy(<span class="string">&quot;key&quot;</span>)</span><br><span class="line">    .flatAggregate(<span class="string">&quot;top2(a) as (v, rank)&quot;</span>)</span><br><span class="line">    .select(<span class="string">&quot;key, v, rank&quot;</span>);</span><br></pre></td></tr></table></figure><h1 id="5-SQL-能力扩展篇"><a href="#5-SQL-能力扩展篇" class="headerlink" title="5.SQL 能力扩展篇"></a>5.SQL 能力扩展篇</h1><h2 id="5-1-SQL-UDF-扩展-Module"><a href="#5-1-SQL-UDF-扩展-Module" class="headerlink" title="5.1.SQL UDF 扩展 - Module"></a>5.1.SQL UDF 扩展 - Module</h2><p>在介绍 Flink Module 具体能力之前，我们先来聊聊博主讲述的思路：</p><ol><li>⭐ 背景及应用场景介绍</li><li>⭐ Flink Module 功能介绍</li><li>⭐ 应用案例：Flink SQL 支持 Hive UDF</li></ol><h3 id="5-1-1-Flink-SQL-Module-应用场景"><a href="#5-1-1-Flink-SQL-Module-应用场景" class="headerlink" title="5.1.1.Flink SQL Module 应用场景"></a>5.1.1.Flink SQL Module 应用场景</h3><p>兄弟们，想想其实大多数公司都是从离线数仓开始建设的。相信大家必然在自己的生产环境中开发了非常多的 Hive UDF。随着需求对于时效性要求的增高，越来越多的公司也开始建设起实时数仓。很多场景下实时数仓的建设都是随着离线数仓而建设的。实时数据使用 Flink 产出，离线数据使用 Hive/Spark 产出。</p><p>那么回到我们的问题：为什么需要给 Flink UDF 做扩展呢？可能这个问题比较大，那么博主分析的具体一些，如果 Flink 扩展支持 Hive UDF 对我们有哪些好处呢？</p><p>博主分析了下，结论如下：</p><p>站在数据需求的角度来说，一般会有以下两种情况：</p><ol><li>⭐ 以前已经有了离线数据链路，需求方也想要实时数据。如果直接能用已经开发好的 hive udf，则不用将相同的逻辑迁移到 flink udf 中，并且后续无需费时费力维护两个 udf 的逻辑一致性。</li><li>⭐ 实时和离线的需求都是新的，需要新开发。如果只开发一套 UDF，则事半功倍。</li></ol><p>因此在 Flink 中支持 Hive UDF（也即扩展 Flink 的 UDF 能力）这件事对开发人员提效来说是非常有好处的。</p><h3 id="5-1-2-Flink-SQL-Module-功能介绍"><a href="#5-1-2-Flink-SQL-Module-功能介绍" class="headerlink" title="5.1.2.Flink SQL Module 功能介绍"></a>5.1.2.Flink SQL Module 功能介绍</h3><p>Module 允许 Flink 扩展函数能力。它是可插拔的，Flink 官方本身已经提供了一些 Module，用户也可以编写自己的 Module。</p><p>例如，用户可以定义自己的函数，并将其作为加载进入 Flink，以在 Flink SQL 和 Table API 中使用。</p><p>再举一个例子，用户可以加载官方已经提供的的 Hive Module，将 Hive 已有的内置函数作为 Flink 的内置函数。</p><p>目前 Flink 包含了以下三种 Module：</p><ol><li>⭐ CoreModule：CoreModule 是 Flink 内置的 Module，其包含了目前 Flink 内置的所有 UDF，Flink 默认开启的 Module 就是 CoreModule，我们可以直接使用其中的 UDF</li><li>⭐ HiveModule：HiveModule 可以将 Hive 内置函数作为 Flink 的系统函数提供给 SQL\Table API 用户进行使用，比如 get_json_object 这类 Hive 内置函数（Flink 默认的 CoreModule 是没有的）</li><li>⭐ 用户自定义 Module：用户可以实现 Module 接口实现自己的 UDF 扩展 Module</li></ol><p>在 Flink 中，Module 可以被 <code>加载</code>、<code>启用</code>、<code>禁用</code>、<code>卸载</code> Module，当 TableEnvironment 加载（见 SQL 语法篇的 Load Module） Module 之后，默认就是开启的。</p><p>Flink 是同时支持多个 Module 的，并且根据加载 Module 的顺序去按顺序查找和解析 UDF，先查到的先解析使用。</p><p>此外，Flink 只会解析已经启用了的 Module。那么当两个 Module 中出现两个同名的函数时，会有以下三种情况：</p><ol><li>⭐ 如果两个 Module 都启用的话，Flink 会根据加载 Module 的顺序进行解析，结果就是会使用顺序为第一个的 Module 的 UDF</li><li>⭐ 如果只有一个 Module 启用的话，Flink 就只会从启用的 Module 解析 UDF</li><li>⭐ 如果两个 Module 都没有启用，Flink 就无法解析这个 UDF</li></ol><p>当然如果出现第一种情况时，用户也可以改变使用 Module 的顺序。比如用户可以使用 <code>USE MODULE hive, core</code> 语句去将 Hive Module 设为第一个使用及解析的 Module。</p><p>另外，用户可以使用 <code>USE MODULES hive</code> 去禁用默认的 core Module，注意，禁用不是卸载 Module，用户之后还可以再次启用 Module，并且使用 <code>USE MODULES core</code> 去将 core Module 设置为启用的。如果使用未加载的 Module，则会直接抛出异常。</p><p>禁用和卸载 Module 的区别在于禁用依然会在 TableEnvironment 保留 Module，用户依然可以使用使用 list 命令看到禁用的 Module。</p><blockquote><p>注意：</p><p>由于 Module 的 UDF 是被 Flink 认为是 Flink 系统内置的，它不和任何 Catalog，数据库绑定，所以这部分 UDF 没有对应的命名空间，即没有 Catalog，数据库命名空间。</p></blockquote><ol><li>⭐ 使用 SQL API 加载、卸载、使用、列出 Module</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner().build();</span><br><span class="line">TableEnvironment tableEnv = TableEnvironment.create(settings);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 展示加载和启用的 Module</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;SHOW MODULES&quot;</span>).print();</span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// | module name |</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |        core |</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;SHOW FULL MODULES&quot;</span>).print();</span><br><span class="line"><span class="comment">// +-------------+------+</span></span><br><span class="line"><span class="comment">// | module name | used |</span></span><br><span class="line"><span class="comment">// +-------------+------+</span></span><br><span class="line"><span class="comment">// |        core | true |</span></span><br><span class="line"><span class="comment">// +-------------+------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 加载 hive module</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;LOAD MODULE hive WITH (&#x27;hive-version&#x27; = &#x27;...&#x27;)&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 展示所有启用的 module</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;SHOW MODULES&quot;</span>).print();</span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// | module name |</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |        core |</span></span><br><span class="line"><span class="comment">// |        hive |</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 展示所有加载的 module 以及它们的启用状态</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;SHOW FULL MODULES&quot;</span>).print();</span><br><span class="line"><span class="comment">// +-------------+------+</span></span><br><span class="line"><span class="comment">// | module name | used |</span></span><br><span class="line"><span class="comment">// +-------------+------+</span></span><br><span class="line"><span class="comment">// |        core | true |</span></span><br><span class="line"><span class="comment">// |        hive | true |</span></span><br><span class="line"><span class="comment">// +-------------+------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 改变 module 解析顺序</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;USE MODULES hive, core&quot;</span>);</span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;SHOW MODULES&quot;</span>).print();</span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// | module name |</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |        hive |</span></span><br><span class="line"><span class="comment">// |        core |</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;SHOW FULL MODULES&quot;</span>).print();</span><br><span class="line"><span class="comment">// +-------------+------+</span></span><br><span class="line"><span class="comment">// | module name | used |</span></span><br><span class="line"><span class="comment">// +-------------+------+</span></span><br><span class="line"><span class="comment">// |        hive | true |</span></span><br><span class="line"><span class="comment">// |        core | true |</span></span><br><span class="line"><span class="comment">// +-------------+------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 禁用 core module</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;USE MODULES hive&quot;</span>);</span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;SHOW MODULES&quot;</span>).print();</span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// | module name |</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |        hive |</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;SHOW FULL MODULES&quot;</span>).print();</span><br><span class="line"><span class="comment">// +-------------+-------+</span></span><br><span class="line"><span class="comment">// | module name |  used |</span></span><br><span class="line"><span class="comment">// +-------------+-------+</span></span><br><span class="line"><span class="comment">// |        hive |  true |</span></span><br><span class="line"><span class="comment">// |        core | false |</span></span><br><span class="line"><span class="comment">// +-------------+-------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 卸载 hive module</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;UNLOAD MODULE hive&quot;</span>);</span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;SHOW MODULES&quot;</span>).print();</span><br><span class="line"><span class="comment">// Empty set</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;SHOW FULL MODULES&quot;</span>).print();</span><br><span class="line"><span class="comment">// +-------------+-------+</span></span><br><span class="line"><span class="comment">// | module name |  used |</span></span><br><span class="line"><span class="comment">// +-------------+-------+</span></span><br><span class="line"><span class="comment">// |        hive | false |</span></span><br><span class="line"><span class="comment">// +-------------+-------+</span></span><br></pre></td></tr></table></figure><ol start="2"><li>⭐ 使用 Java API 加载、卸载、使用、列出 Module</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner().build();</span><br><span class="line">TableEnvironment tableEnv = TableEnvironment.create(settings);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Show initially loaded and enabled modules</span></span><br><span class="line">tableEnv.listModules();</span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// | module name |</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |        core |</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line">tableEnv.listFullModules();</span><br><span class="line"><span class="comment">// +-------------+------+</span></span><br><span class="line"><span class="comment">// | module name | used |</span></span><br><span class="line"><span class="comment">// +-------------+------+</span></span><br><span class="line"><span class="comment">// |        core | true |</span></span><br><span class="line"><span class="comment">// +-------------+------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Load a hive module</span></span><br><span class="line">tableEnv.loadModule(<span class="string">&quot;hive&quot;</span>, <span class="keyword">new</span> HiveModule());</span><br><span class="line"></span><br><span class="line"><span class="comment">// Show all enabled modules</span></span><br><span class="line">tableEnv.listModules();</span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// | module name |</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |        core |</span></span><br><span class="line"><span class="comment">// |        hive |</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Show all loaded modules with both name and use status</span></span><br><span class="line">tableEnv.listFullModules();</span><br><span class="line"><span class="comment">// +-------------+------+</span></span><br><span class="line"><span class="comment">// | module name | used |</span></span><br><span class="line"><span class="comment">// +-------------+------+</span></span><br><span class="line"><span class="comment">// |        core | true |</span></span><br><span class="line"><span class="comment">// |        hive | true |</span></span><br><span class="line"><span class="comment">// +-------------+------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Change resolution order</span></span><br><span class="line">tableEnv.useModules(<span class="string">&quot;hive&quot;</span>, <span class="string">&quot;core&quot;</span>);</span><br><span class="line">tableEnv.listModules();</span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// | module name |</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |        hive |</span></span><br><span class="line"><span class="comment">// |        core |</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line">tableEnv.listFullModules();</span><br><span class="line"><span class="comment">// +-------------+------+</span></span><br><span class="line"><span class="comment">// | module name | used |</span></span><br><span class="line"><span class="comment">// +-------------+------+</span></span><br><span class="line"><span class="comment">// |        hive | true |</span></span><br><span class="line"><span class="comment">// |        core | true |</span></span><br><span class="line"><span class="comment">// +-------------+------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Disable core module</span></span><br><span class="line">tableEnv.useModules(<span class="string">&quot;hive&quot;</span>);</span><br><span class="line">tableEnv.listModules();</span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// | module name |</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |        hive |</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line">tableEnv.listFullModules();</span><br><span class="line"><span class="comment">// +-------------+-------+</span></span><br><span class="line"><span class="comment">// | module name |  used |</span></span><br><span class="line"><span class="comment">// +-------------+-------+</span></span><br><span class="line"><span class="comment">// |        hive |  true |</span></span><br><span class="line"><span class="comment">// |        core | false |</span></span><br><span class="line"><span class="comment">// +-------------+-------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Unload hive module</span></span><br><span class="line">tableEnv.unloadModule(<span class="string">&quot;hive&quot;</span>);</span><br><span class="line">tableEnv.listModules();</span><br><span class="line"><span class="comment">// Empty set</span></span><br><span class="line">tableEnv.listFullModules();</span><br><span class="line"><span class="comment">// +-------------+-------+</span></span><br><span class="line"><span class="comment">// | module name |  used |</span></span><br><span class="line"><span class="comment">// +-------------+-------+</span></span><br><span class="line"><span class="comment">// |        hive | false |</span></span><br><span class="line"><span class="comment">// +-------------+-------+</span></span><br></pre></td></tr></table></figure><h3 id="5-1-3-应用案例：Flink-SQL-支持-Hive-UDF"><a href="#5-1-3-应用案例：Flink-SQL-支持-Hive-UDF" class="headerlink" title="5.1.3.应用案例：Flink SQL 支持 Hive UDF"></a>5.1.3.应用案例：Flink SQL 支持 Hive UDF</h3><p>Flink 支持 hive UDF 这件事分为两个部分。</p><ol><li>⭐ Flink 扩展支持 hive 内置 UDF</li><li>⭐ Flink 扩展支持用户自定义 hive UDF</li></ol><p>第一部分：Flink 扩展支持 Hive 内置 UDF，比如 <code>get_json_object</code>，<code>rlike</code> 等等。</p><p>有同学问了，这么基本的 UDF，Flink 都没有吗？</p><p>确实没有。关于 Flink SQL 内置的 UDF 见如下链接，大家可以看看 Flink 支持了哪些 UDF：<br><a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/functions/systemfunctions/">https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/functions/systemfunctions/</a></p><p>那么如果我如果强行使用 get_json_object 这个 UDF，会发生啥呢？结果如下图。</p><p>直接报错找不到 UDF。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/20_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89%EF%BC%9Aflinksqludf/2.png" alt="error"></p><p>第二部分：Flink 扩展支持用户自定义 Hive UDF。</p><p>内置函数解决不了用户的复杂需求，用户就需要自己写 Hive UDF，并且这部分自定义 UDF 也想在 flink sql 中使用。</p><p>下面看看怎么在 Flink SQL 中进行这两种扩展。</p><ol><li>⭐ flink 扩展支持 hive 内置 udf</li></ol><p>步骤如下：</p><ul><li>⭐ 引入 hive 的 connector。其中包含了 flink 官方提供的一个 <code>HiveModule</code>。在 <code>HiveModule</code> 中包含了 hive 内置的 udf。</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-hive_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>⭐ 在 <code>StreamTableEnvironment</code> 中加载 <code>HiveModule</code>。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">String name = <span class="string">&quot;default&quot;</span>;</span><br><span class="line">String version = <span class="string">&quot;3.1.2&quot;</span>;</span><br><span class="line">tEnv.loadModule(name, <span class="keyword">new</span> HiveModule(version));</span><br></pre></td></tr></table></figure><p>然后在控制台打印一下目前有的 module。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">String[] modules = tEnv.listModules();</span><br><span class="line">Arrays.stream(modules).forEach(System.out::println);</span><br></pre></td></tr></table></figure><p>然后可以看到除了 <code>core</code> module，还有我们刚刚加载进去的 <code>default</code> module。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">default</span><br><span class="line">core</span><br></pre></td></tr></table></figure><ul><li>⭐ 查看所有 module 的所有 udf。在控制台打印一下。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">String[] functions = tEnv.listFunctions();</span><br><span class="line">Arrays.stream(functions).forEach(System.out::println);</span><br></pre></td></tr></table></figure><p>就会将 default 和 core module 中的所有包含的 udf 给列举出来，当然也就包含了 hive module 中的 get_json_object。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/20_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89%EF%BC%9Aflinksqludf/3.png" alt="get_json_object"></p><p>然后我们再去在 Flink SQL 中使用 get_json_object 这个 UDF，就没有报错，能正常输出结果了。</p><p>使用 Flink Hive connector 自带的 <code>HiveModule</code>，已经能够解决很大一部分常见 UDF 使用的问题了。</p><ol start="2"><li>⭐ Flink 扩展支持用户自定义 Hive UDF</li></ol><p>原本博主是直接想要使用 Flink SQL 中的 <code>create temporary function</code> 去执行引入自定义 Hive UDF 的。</p><p>举例如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> TEMPORARY <span class="keyword">FUNCTION</span> test_hive_udf <span class="keyword">as</span> <span class="string">&#x27;flink.examples.sql._09.udf._02_stream_hive_udf.TestGenericUDF&#x27;</span>;</span><br></pre></td></tr></table></figure><p>发现在执行这句 SQL 时，是可以执行成功，将 UDF 注册进去的。</p><p>但是在后续 UDF 初始化时就报错了。具体错误如下图。直接报错 ClassCastException。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/20_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89%EF%BC%9Aflinksqludf/4.png" alt="ddl hive udf error"></p><p>看了下源码，Flink 流任务模式下（未连接 Hive MetaStore 时）在创建 UDF 时会认为这个 UDF 是 Flink 生态体系中的 UDF。</p><p>所以在初始化我们引入的 <code>TestGenericUDF</code> 时，默认会按照 Flink 的 <code>UserDefinedFunction</code> 强转，因此才会报强转错误。</p><p>那么我们就不能使用 Hive UDF 了吗？</p><p>错误，小伙伴萌岂敢有这种想法。博主都把这个标题列出来了（牛逼都吹出去了），还能给不出解决方案嘛。</p><p>思路见下一节。</p><ol start="3"><li>⭐ Flink 扩展支持用户自定义 Hive UDF 的增强 module</li></ol><p>其实思路很简单。</p><p>使用 Flink SQL 中的 <code>create temporary function</code> 虽然不能执行，但是 Flink 提供了插件化的自定义 module。</p><p>我们可以扩展一个支持用户自定义 Hive UDF 的 module，使用这个 module 来支持自定义的 Hive UDF。</p><p>实现的代码也非常简单。简单的把 Flink Hive connector 提供的 <code>HiveModule</code> 做一个增强即可，即下图中的 <code>HiveModuleV2</code>。使用方式如下图所示：</p><p>源码公众号后台回复<strong>1.13.2 sql hive udf</strong>获取。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/20_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89%EF%BC%9Aflinksqludf/5.png" alt="hive module enhance"></p><p>然后程序就正常跑起来了。</p><p>肥肠滴好用！</p><h2 id="5-2-SQL-元数据扩展-Catalog"><a href="#5-2-SQL-元数据扩展-Catalog" class="headerlink" title="5.2.SQL 元数据扩展 - Catalog"></a>5.2.SQL 元数据扩展 - Catalog</h2><h3 id="5-2-1-Flink-Catalog-功能介绍"><a href="#5-2-1-Flink-Catalog-功能介绍" class="headerlink" title="5.2.1.Flink Catalog 功能介绍"></a>5.2.1.Flink Catalog 功能介绍</h3><p>数据处理最关键的方面之一是管理元数据。元数据可以是临时的，例如临时表、UDF。 元数据也可以是持久化的，例如 Hive MetaStore 中的元数据。</p><p>Flink SQL 中是由 Catalog 提供了元数据信息，例如数据库、表、分区、视图以及数据库或其他外部系统中存储的函数和信息。对标 Hive 去理解就是 Hive 的 MetaStore，都是用于存储计算引擎涉及到的元数据信息。</p><p>Catalog 允许用户引用其数据存储系统中现有的元数据，并自动将其映射到 Flink 的相应元数据。例如，Flink 可以直接使用 Hive MetaStore 中的表的元数据，也可以将 Flink SQL 中的元数据存储到 Hive MetaStore 中。Catalog 极大地简化了用户开始使用 Flink 的步骤，提升了用户体验。</p><p>目前 Flink 包含了以下四种 Catalog：</p><ol><li>⭐ GenericInMemoryCatalog：GenericInMemoryCatalog 是基于内存实现的 Catalog，所有元数据只在 session 的生命周期（即一个 Flink 任务一次运行生命周期内）内可用。</li><li>⭐ JdbcCatalog：JdbcCatalog 使得用户可以将 Flink 通过 JDBC 协议连接到关系数据库。PostgresCatalog 是当前实现的唯一一种 JDBC Catalog，即可以将 Flink SQL 的预案数据存储在 Postgres 中。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// PostgresCatalog 方法支持的方法</span></span><br><span class="line">PostgresCatalog.databaseExists(String databaseName)</span><br><span class="line">PostgresCatalog.listDatabases()</span><br><span class="line">PostgresCatalog.getDatabase(String databaseName)</span><br><span class="line">PostgresCatalog.listTables(String databaseName)</span><br><span class="line">PostgresCatalog.getTable(ObjectPath tablePath)</span><br><span class="line">PostgresCatalog.tableExists(ObjectPath tablePath)</span><br></pre></td></tr></table></figure><ol start="3"><li>⭐ HiveCatalog：HiveCatalog 有两个用途，作为 Flink 元数据的持久化存储，以及作为读写现有 Hive 元数据的接口。注意：Hive MetaStore 以小写形式存储所有元数据对象名称。而 GenericInMemoryCatalog 会区分大小写。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">TableEnvironment tableEnv = TableEnvironment.create(settings);</span><br><span class="line"></span><br><span class="line">String name            = <span class="string">&quot;myhive&quot;</span>;</span><br><span class="line">String defaultDatabase = <span class="string">&quot;mydatabase&quot;</span>;</span><br><span class="line">String hiveConfDir     = <span class="string">&quot;/opt/hive-conf&quot;</span>;</span><br><span class="line"></span><br><span class="line">HiveCatalog hive = <span class="keyword">new</span> HiveCatalog(name, defaultDatabase, hiveConfDir);</span><br><span class="line">tableEnv.registerCatalog(<span class="string">&quot;myhive&quot;</span>, hive);</span><br><span class="line"></span><br><span class="line"><span class="comment">// set the HiveCatalog as the current catalog of the session</span></span><br><span class="line">tableEnv.useCatalog(<span class="string">&quot;myhive&quot;</span>);</span><br></pre></td></tr></table></figure><ol start="4"><li>⭐ 用户自定义 Catalog：用户可以实现 Catalog 接口实现自定义 Catalog</li></ol><p>下面看看 Flink Catalog 提供了什么 API，以及对应 API 的使用案例：</p><ol><li>⭐ 使用 SQL API 将表创建注册进 Catalog</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">TableEnvironment tableEnv = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建 HiveCatalog </span></span><br><span class="line">Catalog catalog = <span class="keyword">new</span> HiveCatalog(<span class="string">&quot;myhive&quot;</span>, <span class="keyword">null</span>, <span class="string">&quot;&lt;path_of_hive_conf&gt;&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册 catalog</span></span><br><span class="line">tableEnv.registerCatalog(<span class="string">&quot;myhive&quot;</span>, catalog);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 catalog 中创建 database</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;CREATE DATABASE mydb WITH (...)&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 catalog 中创建表</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;CREATE TABLE mytable (name STRING, age INT) WITH (...)&quot;</span>);</span><br><span class="line"></span><br><span class="line">tableEnv.listTables(); <span class="comment">// 列出当前 myhive.mydb 中的所有表</span></span><br></pre></td></tr></table></figure><ol start="2"><li>⭐ 使用 Java API 将表创建注册进 Catalog</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.catalog.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.catalog.hive.HiveCatalog;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.descriptors.Kafka;</span><br><span class="line"></span><br><span class="line">TableEnvironment tableEnv = TableEnvironment.create(EnvironmentSettings.newInstance().build());</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建 HiveCatalog </span></span><br><span class="line">Catalog catalog = <span class="keyword">new</span> HiveCatalog(<span class="string">&quot;myhive&quot;</span>, <span class="keyword">null</span>, <span class="string">&quot;&lt;path_of_hive_conf&gt;&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册 catalog</span></span><br><span class="line">tableEnv.registerCatalog(<span class="string">&quot;myhive&quot;</span>, catalog);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 catalog 中创建 database</span></span><br><span class="line">catalog.createDatabase(<span class="string">&quot;mydb&quot;</span>, <span class="keyword">new</span> CatalogDatabaseImpl(...));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 catalog 中创建表</span></span><br><span class="line">TableSchema schema = TableSchema.builder()</span><br><span class="line">    .field(<span class="string">&quot;name&quot;</span>, DataTypes.STRING())</span><br><span class="line">    .field(<span class="string">&quot;age&quot;</span>, DataTypes.INT())</span><br><span class="line">    .build();</span><br><span class="line"></span><br><span class="line">catalog.createTable(</span><br><span class="line">        <span class="keyword">new</span> ObjectPath(<span class="string">&quot;mydb&quot;</span>, <span class="string">&quot;mytable&quot;</span>), </span><br><span class="line">        <span class="keyword">new</span> CatalogTableImpl(</span><br><span class="line">            schema,</span><br><span class="line">            <span class="keyword">new</span> Kafka()</span><br><span class="line">                .version(<span class="string">&quot;0.11&quot;</span>)</span><br><span class="line">                ....</span><br><span class="line">                .startFromEarlist()</span><br><span class="line">                .toProperties(),</span><br><span class="line">            <span class="string">&quot;my comment&quot;</span></span><br><span class="line">        ),</span><br><span class="line">        <span class="keyword">false</span></span><br><span class="line">    );</span><br><span class="line">    </span><br><span class="line">List&lt;String&gt; tables = catalog.listTables(<span class="string">&quot;mydb&quot;</span>); <span class="comment">// 列出当前 myhive.mydb 中的所有表</span></span><br></pre></td></tr></table></figure><h3 id="5-2-2-操作-Catalog-的-API"><a href="#5-2-2-操作-Catalog-的-API" class="headerlink" title="5.2.2.操作 Catalog 的 API"></a>5.2.2.操作 Catalog 的 API</h3><p>这里只列出了 Java 的 Catalog API，用户也可以使用 SQL DDL API 实现相同的功能。关于 DDL 的详细信息请参考之前介绍到的 SQL CREATE DDL 章节。</p><ol><li>⭐ Catalog 操作</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 注册 Catalog</span></span><br><span class="line">tableEnv.registerCatalog(<span class="keyword">new</span> CustomCatalog(<span class="string">&quot;myCatalog&quot;</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 切换 Catalog 和 Database</span></span><br><span class="line">tableEnv.useCatalog(<span class="string">&quot;myCatalog&quot;</span>);</span><br><span class="line">tableEnv.useDatabase(<span class="string">&quot;myDb&quot;</span>);</span><br><span class="line"><span class="comment">// 也可以通过以下方式访问对应的表</span></span><br><span class="line">tableEnv.from(<span class="string">&quot;not_the_current_catalog.not_the_current_db.my_table&quot;</span>);</span><br></pre></td></tr></table></figure><ol start="2"><li>⭐ 数据库操作</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create database</span></span><br><span class="line">catalog.createDatabase(<span class="string">&quot;mydb&quot;</span>, <span class="keyword">new</span> CatalogDatabaseImpl(...), <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// drop database</span></span><br><span class="line">catalog.dropDatabase(<span class="string">&quot;mydb&quot;</span>, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// alter database</span></span><br><span class="line">catalog.alterDatabase(<span class="string">&quot;mydb&quot;</span>, <span class="keyword">new</span> CatalogDatabaseImpl(...), <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// get databse</span></span><br><span class="line">catalog.getDatabase(<span class="string">&quot;mydb&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// check if a database exist</span></span><br><span class="line">catalog.databaseExists(<span class="string">&quot;mydb&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// list databases in a catalog</span></span><br><span class="line">catalog.listDatabases(<span class="string">&quot;mycatalog&quot;</span>);</span><br></pre></td></tr></table></figure><ol start="3"><li>⭐ 表操作</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create table</span></span><br><span class="line">catalog.createTable(<span class="keyword">new</span> ObjectPath(<span class="string">&quot;mydb&quot;</span>, <span class="string">&quot;mytable&quot;</span>), <span class="keyword">new</span> CatalogTableImpl(...), <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// drop table</span></span><br><span class="line">catalog.dropTable(<span class="keyword">new</span> ObjectPath(<span class="string">&quot;mydb&quot;</span>, <span class="string">&quot;mytable&quot;</span>), <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// alter table</span></span><br><span class="line">catalog.alterTable(<span class="keyword">new</span> ObjectPath(<span class="string">&quot;mydb&quot;</span>, <span class="string">&quot;mytable&quot;</span>), <span class="keyword">new</span> CatalogTableImpl(...), <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// rename table</span></span><br><span class="line">catalog.renameTable(<span class="keyword">new</span> ObjectPath(<span class="string">&quot;mydb&quot;</span>, <span class="string">&quot;mytable&quot;</span>), <span class="string">&quot;my_new_table&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// get table</span></span><br><span class="line">catalog.getTable(<span class="string">&quot;mytable&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// check if a table exist or not</span></span><br><span class="line">catalog.tableExists(<span class="string">&quot;mytable&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// list tables in a database</span></span><br><span class="line">catalog.listTables(<span class="string">&quot;mydb&quot;</span>);</span><br></pre></td></tr></table></figure><ol start="4"><li>⭐ 视图操作</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create view</span></span><br><span class="line">catalog.createTable(<span class="keyword">new</span> ObjectPath(<span class="string">&quot;mydb&quot;</span>, <span class="string">&quot;myview&quot;</span>), <span class="keyword">new</span> CatalogViewImpl(...), <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// drop view</span></span><br><span class="line">catalog.dropTable(<span class="keyword">new</span> ObjectPath(<span class="string">&quot;mydb&quot;</span>, <span class="string">&quot;myview&quot;</span>), <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// alter view</span></span><br><span class="line">catalog.alterTable(<span class="keyword">new</span> ObjectPath(<span class="string">&quot;mydb&quot;</span>, <span class="string">&quot;mytable&quot;</span>), <span class="keyword">new</span> CatalogViewImpl(...), <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// rename view</span></span><br><span class="line">catalog.renameTable(<span class="keyword">new</span> ObjectPath(<span class="string">&quot;mydb&quot;</span>, <span class="string">&quot;myview&quot;</span>), <span class="string">&quot;my_new_view&quot;</span>, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// get view</span></span><br><span class="line">catalog.getTable(<span class="string">&quot;myview&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// check if a view exist or not</span></span><br><span class="line">catalog.tableExists(<span class="string">&quot;mytable&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// list views in a database</span></span><br><span class="line">catalog.listViews(<span class="string">&quot;mydb&quot;</span>);</span><br></pre></td></tr></table></figure><ol start="5"><li>⭐ 分区操作</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create view</span></span><br><span class="line">catalog.createPartition(</span><br><span class="line">    <span class="keyword">new</span> ObjectPath(<span class="string">&quot;mydb&quot;</span>, <span class="string">&quot;mytable&quot;</span>),</span><br><span class="line">    <span class="keyword">new</span> CatalogPartitionSpec(...),</span><br><span class="line">    <span class="keyword">new</span> CatalogPartitionImpl(...),</span><br><span class="line">    <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// drop partition</span></span><br><span class="line">catalog.dropPartition(<span class="keyword">new</span> ObjectPath(<span class="string">&quot;mydb&quot;</span>, <span class="string">&quot;mytable&quot;</span>), <span class="keyword">new</span> CatalogPartitionSpec(...), <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// alter partition</span></span><br><span class="line">catalog.alterPartition(</span><br><span class="line">    <span class="keyword">new</span> ObjectPath(<span class="string">&quot;mydb&quot;</span>, <span class="string">&quot;mytable&quot;</span>),</span><br><span class="line">    <span class="keyword">new</span> CatalogPartitionSpec(...),</span><br><span class="line">    <span class="keyword">new</span> CatalogPartitionImpl(...),</span><br><span class="line">    <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// get partition</span></span><br><span class="line">catalog.getPartition(<span class="keyword">new</span> ObjectPath(<span class="string">&quot;mydb&quot;</span>, <span class="string">&quot;mytable&quot;</span>), <span class="keyword">new</span> CatalogPartitionSpec(...));</span><br><span class="line"></span><br><span class="line"><span class="comment">// check if a partition exist or not</span></span><br><span class="line">catalog.partitionExists(<span class="keyword">new</span> ObjectPath(<span class="string">&quot;mydb&quot;</span>, <span class="string">&quot;mytable&quot;</span>), <span class="keyword">new</span> CatalogPartitionSpec(...));</span><br><span class="line"></span><br><span class="line"><span class="comment">// list partitions of a table</span></span><br><span class="line">catalog.listPartitions(<span class="keyword">new</span> ObjectPath(<span class="string">&quot;mydb&quot;</span>, <span class="string">&quot;mytable&quot;</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// list partitions of a table under a give partition spec</span></span><br><span class="line">catalog.listPartitions(<span class="keyword">new</span> ObjectPath(<span class="string">&quot;mydb&quot;</span>, <span class="string">&quot;mytable&quot;</span>), <span class="keyword">new</span> CatalogPartitionSpec(...));</span><br><span class="line"></span><br><span class="line"><span class="comment">// list partitions of a table by expression filter</span></span><br><span class="line">catalog.listPartitionsByFilter(<span class="keyword">new</span> ObjectPath(<span class="string">&quot;mydb&quot;</span>, <span class="string">&quot;mytable&quot;</span>), Arrays.asList(epr1, ...));</span><br></pre></td></tr></table></figure><ol start="6"><li>⭐ 函数操作</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create function</span></span><br><span class="line">catalog.createFunction(<span class="keyword">new</span> ObjectPath(<span class="string">&quot;mydb&quot;</span>, <span class="string">&quot;myfunc&quot;</span>), <span class="keyword">new</span> CatalogFunctionImpl(...), <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// drop function</span></span><br><span class="line">catalog.dropFunction(<span class="keyword">new</span> ObjectPath(<span class="string">&quot;mydb&quot;</span>, <span class="string">&quot;myfunc&quot;</span>), <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// alter function</span></span><br><span class="line">catalog.alterFunction(<span class="keyword">new</span> ObjectPath(<span class="string">&quot;mydb&quot;</span>, <span class="string">&quot;myfunc&quot;</span>), <span class="keyword">new</span> CatalogFunctionImpl(...), <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// get function</span></span><br><span class="line">catalog.getFunction(<span class="string">&quot;myfunc&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// check if a function exist or not</span></span><br><span class="line">catalog.functionExists(<span class="string">&quot;myfunc&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// list functions in a database</span></span><br><span class="line">catalog.listFunctions(<span class="string">&quot;mydb&quot;</span>);</span><br></pre></td></tr></table></figure><h2 id="5-3-SQL-任务参数配置"><a href="#5-3-SQL-任务参数配置" class="headerlink" title="5.3.SQL 任务参数配置"></a>5.3.SQL 任务参数配置</h2><p>关于 Flink SQL 详细的配置项及功能如下链接所示，详细内容大家可以点击链接去看，博主下面只介绍常用的性能优化参数及其功能：</p><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/config/">https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/config/</a></p><h3 id="5-3-1-参数设置方式"><a href="#5-3-1-参数设置方式" class="headerlink" title="5.3.1.参数设置方式"></a>5.3.1.参数设置方式</h3><p>Flink SQL 相关参数需要在 TableEnvironment 中设置。如下案例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// instantiate table environment</span></span><br><span class="line">TableEnvironment tEnv = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// access flink configuration</span></span><br><span class="line">Configuration configuration = tEnv.getConfig().getConfiguration();</span><br><span class="line"><span class="comment">// set low-level key-value options</span></span><br><span class="line">configuration.setString(<span class="string">&quot;table.exec.mini-batch.enabled&quot;</span>, <span class="string">&quot;true&quot;</span>);</span><br><span class="line">configuration.setString(<span class="string">&quot;table.exec.mini-batch.allow-latency&quot;</span>, <span class="string">&quot;5 s&quot;</span>);</span><br><span class="line">configuration.setString(<span class="string">&quot;table.exec.mini-batch.size&quot;</span>, <span class="string">&quot;5000&quot;</span>);</span><br></pre></td></tr></table></figure><p>具体参数分为以下 3 类：</p><ol><li>⭐ 运行时参数：优化 Flink SQL 任务在执行时的任务性能</li><li>⭐ 优化器参数：Flink SQL 任务在生成执行计划时，经过优化器优化生成更优的执行计划</li><li>⭐ 表参数：用于调整 Flink SQL table 的执行行为</li></ol><h3 id="5-3-2-运行时参数"><a href="#5-3-2-运行时参数" class="headerlink" title="5.3.2.运行时参数"></a>5.3.2.运行时参数</h3><p>用于优化 Flink SQL 任务在执行时的任务性能。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">//</span> <span class="string">默认值：100</span></span><br><span class="line"><span class="string">//</span> <span class="string">值类型：Integer</span></span><br><span class="line"><span class="string">//</span> <span class="string">流批任务：流、批任务都支持</span></span><br><span class="line"><span class="string">//</span> <span class="string">用处：异步</span> <span class="string">lookup</span> <span class="string">join</span> <span class="string">中最大的异步</span> <span class="string">IO</span> <span class="string">执行数目</span></span><br><span class="line"><span class="attr">table.exec.async-lookup.buffer-capacity:</span> <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="string">//</span> <span class="string">默认值：false</span></span><br><span class="line"><span class="string">//</span> <span class="string">值类型：Boolean</span></span><br><span class="line"><span class="string">//</span> <span class="string">流批任务：流任务支持</span></span><br><span class="line"><span class="string">//</span> <span class="string">用处：MiniBatch</span> <span class="string">优化是一种专门针对</span> <span class="string">unbounded</span> <span class="string">流任务的优化（即非窗口类应用），其机制是在</span> <span class="string">`允许的延迟时间间隔内`</span> <span class="string">以及</span> <span class="string">`达到最大缓冲记录数`</span> <span class="string">时触发以减少</span> <span class="string">`状态访问`</span> <span class="string">的优化，从而节约处理时间。下面两个参数一个代表</span> <span class="string">`允许的延迟时间间隔`，另一个代表</span> <span class="string">`达到最大缓冲记录数`。</span></span><br><span class="line"><span class="attr">table.exec.mini-batch.enabled:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="string">//</span> <span class="string">默认值：0</span> <span class="string">ms</span></span><br><span class="line"><span class="string">//</span> <span class="string">值类型：Duration</span></span><br><span class="line"><span class="string">//</span> <span class="string">流批任务：流任务支持</span></span><br><span class="line"><span class="string">//</span> <span class="string">用处：此参数设置为多少就代表</span> <span class="string">MiniBatch</span> <span class="string">机制最大允许的延迟时间。注意这个参数要配合</span> <span class="string">`table.exec.mini-batch.enabled`</span> <span class="string">为</span> <span class="literal">true</span> <span class="string">时使用，而且必须大于</span> <span class="number">0</span> <span class="string">ms</span></span><br><span class="line"><span class="attr">table.exec.mini-batch.allow-latency:</span> <span class="number">0</span> <span class="string">ms</span></span><br><span class="line"></span><br><span class="line"><span class="string">//</span> <span class="string">默认值：-1</span></span><br><span class="line"><span class="string">//</span> <span class="string">值类型：Long</span></span><br><span class="line"><span class="string">//</span> <span class="string">流批任务：流任务支持</span></span><br><span class="line"><span class="string">//</span> <span class="string">用处：此参数设置为多少就代表</span> <span class="string">MiniBatch</span> <span class="string">机制最大缓冲记录数。注意这个参数要配合</span> <span class="string">`table.exec.mini-batch.enabled`</span> <span class="string">为</span> <span class="literal">true</span> <span class="string">时使用，而且必须大于</span> <span class="number">0</span></span><br><span class="line"><span class="attr">table.exec.mini-batch.size:</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"><span class="string">//</span> <span class="string">默认值：-1</span></span><br><span class="line"><span class="string">//</span> <span class="string">值类型：Integer</span></span><br><span class="line"><span class="string">//</span> <span class="string">流批任务：流、批任务都支持</span></span><br><span class="line"><span class="string">//</span> <span class="string">用处：可以用此参数设置</span> <span class="string">Flink</span> <span class="string">SQL</span> <span class="string">中算子的并行度，这个参数的优先级</span> <span class="string">`高于`</span> <span class="string">StreamExecutionEnvironment</span> <span class="string">中设置的并行度优先级，如果这个值设置为</span> <span class="number">-1</span><span class="string">，则代表没有设置，会默认使用</span> <span class="string">StreamExecutionEnvironment</span> <span class="string">设置的并行度</span></span><br><span class="line"><span class="attr">table.exec.resource.default-parallelism:</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"><span class="string">//</span> <span class="string">默认值：ERROR</span></span><br><span class="line"><span class="string">//</span> <span class="string">值类型：Enum【ERROR,</span> <span class="string">DROP】</span></span><br><span class="line"><span class="string">//</span> <span class="string">流批任务：流、批任务都支持</span></span><br><span class="line"><span class="string">//</span> <span class="string">用处：表上的</span> <span class="string">NOT</span> <span class="literal">NULL</span> <span class="string">列约束强制不能将</span> <span class="literal">NULL</span> <span class="string">值插入表中。Flink</span> <span class="string">支持</span> <span class="string">`ERROR`（默认）和</span> <span class="string">`DROP`</span> <span class="string">配置。默认情况下，当</span> <span class="literal">NULL</span> <span class="string">值写入</span> <span class="string">NOT</span> <span class="literal">NULL</span> <span class="string">列时，Flink</span> <span class="string">会产生运行时异常。用户可以将行为更改为</span> <span class="string">`DROP`，直接删除此类记录，而不会引发异常。</span></span><br><span class="line"><span class="attr">table.exec.sink.not-null-enforcer:</span> <span class="string">ERROR</span></span><br><span class="line"></span><br><span class="line"><span class="string">//</span> <span class="string">默认值：false</span></span><br><span class="line"><span class="string">//</span> <span class="string">值类型：Boolean</span></span><br><span class="line"><span class="string">//</span> <span class="string">流批任务：流任务</span></span><br><span class="line"><span class="string">//</span> <span class="string">用处：接入了</span> <span class="string">CDC</span> <span class="string">的数据源，上游</span> <span class="string">CDC</span> <span class="string">如果产生重复的数据，可以使用此参数在</span> <span class="string">Flink</span> <span class="string">数据源算子进行去重操作，去重会引入状态开销</span></span><br><span class="line"><span class="attr">table.exec.source.cdc-events-duplicate:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="string">//</span> <span class="string">默认值：0</span> <span class="string">ms</span></span><br><span class="line"><span class="string">//</span> <span class="string">值类型：Duration</span></span><br><span class="line"><span class="string">//</span> <span class="string">流批任务：流任务</span></span><br><span class="line"><span class="string">//</span> <span class="string">用处：如果此参数设置为</span> <span class="number">60</span> <span class="string">s，当</span> <span class="string">Source</span> <span class="string">算子在</span> <span class="number">60</span> <span class="string">s</span> <span class="string">内未收到任何元素时，这个</span> <span class="string">Source</span> <span class="string">将被标记为临时空闲，此时下游任务就不依赖此</span> <span class="string">Source</span> <span class="string">的</span> <span class="string">Watermark</span> <span class="string">来推进整体的</span> <span class="string">Watermark</span> <span class="string">了。</span></span><br><span class="line"><span class="string">//</span> <span class="string">默认值为</span> <span class="number">0</span> <span class="string">时，代表未启用检测源空闲。</span></span><br><span class="line"><span class="attr">table.exec.source.idle-timeout:</span> <span class="number">0</span> <span class="string">ms</span></span><br><span class="line"></span><br><span class="line"><span class="string">//</span> <span class="string">默认值：0</span> <span class="string">ms</span></span><br><span class="line"><span class="string">//</span> <span class="string">值类型：Duration</span></span><br><span class="line"><span class="string">//</span> <span class="string">流批任务：流任务</span></span><br><span class="line"><span class="string">//</span> <span class="string">用处：指定空闲状态（即未更新的状态）将保留多长时间。尤其是在</span> <span class="string">unbounded</span> <span class="string">场景中很有用。默认</span> <span class="number">0</span> <span class="string">ms</span> <span class="string">为不清除空闲状态</span></span><br><span class="line"><span class="attr">table.exec.state.ttl:</span> <span class="number">0</span> <span class="string">ms</span></span><br></pre></td></tr></table></figure><p>其中上述参数中最常被用到为一下两种：</p><ol><li>⭐ MiniBatch 聚合</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">table.exec.mini-batch.enabled:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">table.exec.mini-batch.allow-latency:</span> <span class="number">60</span> <span class="string">s</span></span><br><span class="line"><span class="attr">table.exec.mini-batch.size:</span> <span class="number">1000000000</span></span><br></pre></td></tr></table></figure><p>具体使用场景如下链接：</p><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/tuning/#minibatch-aggregation">https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/tuning/#minibatch-aggregation</a></p><ol start="2"><li>⭐ state ttl 状态过期</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 状态清除如下流 SQL 案例场景很有用，随着实时任务的运行，前几天（即前几天的 p_date）的 state 不会被更新的情况下，就可以使用空闲状态删除机制把 state 给删除</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">  p_date</span><br><span class="line">  , <span class="built_in">count</span>(<span class="keyword">distinct</span> user_id) <span class="keyword">as</span> uv</span><br><span class="line"><span class="keyword">from</span> source_table</span><br><span class="line"><span class="keyword">group</span> </span><br><span class="line">  p_date</span><br></pre></td></tr></table></figure><h3 id="5-3-3-优化器参数"><a href="#5-3-3-优化器参数" class="headerlink" title="5.3.3.优化器参数"></a>5.3.3.优化器参数</h3><p>Flink SQL 任务在生成执行计划时，优化生成更优的执行计划</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">//</span> <span class="string">默认值：AUTO</span></span><br><span class="line"><span class="string">//</span> <span class="string">值类型：String</span></span><br><span class="line"><span class="string">//</span> <span class="string">流批任务：流、批任务都支持</span></span><br><span class="line"><span class="string">//</span> <span class="string">用处：聚合阶段的策略。和</span> <span class="string">MapReduce</span> <span class="string">的</span> <span class="string">Combiner</span> <span class="string">功能类似，可以在数据</span> <span class="string">shuffle</span> <span class="string">前做一些提前的聚合，可以选择以下三种方式</span></span><br><span class="line"><span class="string">//</span> <span class="string">TWO_PHASE：强制使用具有</span> <span class="string">localAggregate</span> <span class="string">和</span> <span class="string">globalAggregate</span> <span class="string">的两阶段聚合。请注意，如果聚合函数不支持优化为两个阶段，Flink</span> <span class="string">仍将使用单阶段聚合。</span></span><br><span class="line"><span class="string">//</span> <span class="string">两阶段优化在计算</span> <span class="string">count，sum</span> <span class="string">时很有用，但是在计算</span> <span class="string">count</span> <span class="string">distinct</span> <span class="string">时需要注意，key</span> <span class="string">的稀疏程度，如果</span> <span class="string">key</span> <span class="string">不稀疏，那么很可能两阶段优化的效果会适得其反</span></span><br><span class="line"><span class="string">//</span> <span class="string">ONE_PHASE：强制使用只有</span> <span class="string">CompleteGlobalAggregate</span> <span class="string">的一个阶段聚合。</span></span><br><span class="line"><span class="string">//</span> <span class="string">AUTO：聚合阶段没有特殊的执行器。选择</span> <span class="string">TWO_PHASE</span> <span class="string">或者</span> <span class="string">ONE_PHASE</span> <span class="string">取决于优化器的成本。</span></span><br><span class="line"><span class="string">//</span> </span><br><span class="line"><span class="string">//</span> <span class="string">注意！！！：此优化在窗口聚合中会自动生效，但是在</span> <span class="string">unbounded</span> <span class="string">agg</span> <span class="string">中需要与</span> <span class="string">minibatch</span> <span class="string">参数相结合使用才会生效</span></span><br><span class="line"><span class="attr">table.optimizer.agg-phase-strategy:</span> <span class="string">AUTO</span></span><br><span class="line"></span><br><span class="line"><span class="string">//</span> <span class="string">默认值：false</span></span><br><span class="line"><span class="string">//</span> <span class="string">值类型：Boolean</span></span><br><span class="line"><span class="string">//</span> <span class="string">流批任务：流任务</span></span><br><span class="line"><span class="string">//</span> <span class="string">用处：避免</span> <span class="string">group</span> <span class="string">by</span> <span class="string">计算</span> <span class="string">count</span> <span class="string">distinct\sum</span> <span class="string">distinct</span> <span class="string">数据时的</span> <span class="string">group</span> <span class="string">by</span> <span class="string">的</span> <span class="string">key</span> <span class="string">较少导致的数据倾斜，比如</span> <span class="string">group</span> <span class="string">by</span> <span class="string">中一个</span> <span class="string">key</span> <span class="string">的</span> <span class="string">distinct</span> <span class="string">要去重</span> <span class="string">500w</span> <span class="string">数据，而另一个</span> <span class="string">key</span> <span class="string">只需要去重</span> <span class="number">3</span> <span class="string">个</span> <span class="string">key，那么就需要先需要按照</span> <span class="string">distinct</span> <span class="string">的</span> <span class="string">key</span> <span class="string">进行分桶。将此参数设置为</span> <span class="literal">true</span> <span class="string">之后，下面的</span> <span class="string">table.optimizer.distinct-agg.split.bucket-num</span> <span class="string">可以用于决定分桶数是多少</span></span><br><span class="line"><span class="string">//</span> <span class="string">后文会介绍具体的案例</span></span><br><span class="line"><span class="attr">table.optimizer.distinct-agg.split.enabled:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="string">//</span> <span class="string">默认值：1024</span></span><br><span class="line"><span class="string">//</span> <span class="string">值类型：Integer</span></span><br><span class="line"><span class="string">//</span> <span class="string">流批任务：流任务</span></span><br><span class="line"><span class="string">//</span> <span class="string">用处：避免</span> <span class="string">group</span> <span class="string">by</span> <span class="string">计算</span> <span class="string">count</span> <span class="string">distinct</span> <span class="string">数据时的</span> <span class="string">group</span> <span class="string">by</span> <span class="string">较少导致的数据倾斜。加了此参数之后，会先根据</span> <span class="string">group</span> <span class="string">by</span> <span class="string">key</span> <span class="string">结合</span> <span class="string">hash_code（distinct_key）进行分桶，然后再自动进行合桶。</span></span><br><span class="line"><span class="string">//</span> <span class="string">后文会介绍具体的案例</span></span><br><span class="line"><span class="attr">table.optimizer.distinct-agg.split.bucket-num:</span> <span class="number">1024</span></span><br><span class="line"></span><br><span class="line"><span class="string">//</span> <span class="string">默认值：true</span></span><br><span class="line"><span class="string">//</span> <span class="string">值类型：Boolean</span></span><br><span class="line"><span class="string">//</span> <span class="string">流批任务：流任务</span></span><br><span class="line"><span class="string">//</span> <span class="string">用处：如果设置为</span> <span class="literal">true</span><span class="string">，Flink</span> <span class="string">优化器将会尝试找出重复的自计划并重用。默认为</span> <span class="literal">true</span> <span class="string">不需要改动</span></span><br><span class="line"><span class="attr">table.optimizer.reuse-sub-plan-enabled:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="string">//</span> <span class="string">默认值：true</span></span><br><span class="line"><span class="string">//</span> <span class="string">值类型：Boolean</span></span><br><span class="line"><span class="string">//</span> <span class="string">流批任务：流任务</span></span><br><span class="line"><span class="string">//</span> <span class="string">用处：如果设置为</span> <span class="literal">true</span><span class="string">，Flink</span> <span class="string">优化器会找出重复使用的</span> <span class="string">table</span> <span class="string">source</span> <span class="string">并且重用。默认为</span> <span class="literal">true</span> <span class="string">不需要改动</span></span><br><span class="line"><span class="attr">table.optimizer.reuse-source-enabled:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="string">//</span> <span class="string">默认值：true</span></span><br><span class="line"><span class="string">//</span> <span class="string">值类型：Boolean</span></span><br><span class="line"><span class="string">//</span> <span class="string">流批任务：流任务</span></span><br><span class="line"><span class="string">//</span> <span class="string">用处：如果设置为</span> <span class="literal">true</span><span class="string">，Flink</span> <span class="string">优化器将会做谓词下推到</span> <span class="string">FilterableTableSource</span> <span class="string">中，将一些过滤条件前置，提升性能。默认为</span> <span class="literal">true</span> <span class="string">不需要改动</span></span><br><span class="line"><span class="attr">table.optimizer.source.predicate-pushdown-enabled:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>其中上述参数中最常被用到为以下两种：</p><ol><li>⭐ 两阶段优化：</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">table.optimizer.agg-phase-strategy:</span> <span class="string">AUTO</span></span><br></pre></td></tr></table></figure><p>在计算 count(1)，sum(col) 场景汇总提效很高，因为 count(1)，sum(col) 在经过本地 localAggregate 之后，每个 group by 的 key 就一个结果值。</p><p>注意！！！：此优化在窗口聚合中会自动生效，但是在 unbounded agg 中需要与 minibatch 参数相结合使用才会生效。</p><ol start="2"><li>⭐ split 分桶：</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">table.optimizer.distinct-agg.split.enabled:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">table.optimizer.distinct-agg.split.bucket-num:</span> <span class="number">1024</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    <span class="built_in">count</span>(<span class="keyword">distinct</span> user_id) <span class="keyword">as</span> uv,</span><br><span class="line">    <span class="built_in">max</span>(<span class="built_in">cast</span>(server_timestamp <span class="keyword">as</span> <span class="type">bigint</span>)) <span class="keyword">as</span> server_timestamp</span><br><span class="line"><span class="keyword">FROM</span> source_table</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 上述 SQL 打开了 split 分桶之后的效果等同于以下 SQL</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    <span class="built_in">sum</span>(bucket_uv) <span class="keyword">as</span> uv</span><br><span class="line">    , <span class="built_in">max</span>(server_timestamp) <span class="keyword">as</span> server_timestamp</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">    <span class="keyword">SELECT</span></span><br><span class="line">        <span class="built_in">count</span>(<span class="keyword">distinct</span> user_id) <span class="keyword">as</span> bucket_uv,</span><br><span class="line">        <span class="built_in">max</span>(<span class="built_in">cast</span>(server_timestamp <span class="keyword">as</span> <span class="type">bigint</span>)) <span class="keyword">as</span> server_timestamp</span><br><span class="line">    <span class="keyword">FROM</span> source_table</span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">        <span class="built_in">mod</span>(hash_code(user_id), <span class="number">1024</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>注意！！！：如果有多个 distinct key，则多个 distinct key 都会被作为分桶 key。</p><h3 id="5-3-4-表参数"><a href="#5-3-4-表参数" class="headerlink" title="5.3.4.表参数"></a>5.3.4.表参数</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">//</span> <span class="string">默认值：false</span></span><br><span class="line"><span class="string">//</span> <span class="string">值类型：Boolean</span></span><br><span class="line"><span class="string">//</span> <span class="string">流批任务：流、批任务都支持</span></span><br><span class="line"><span class="string">//</span> <span class="string">用处：DML</span> <span class="string">SQL（即执行</span> <span class="string">insert</span> <span class="string">into</span> <span class="string">操作）是异步执行还是同步执行。默认为异步（false），即可以同时提交多个</span> <span class="string">DML</span> <span class="string">SQL</span> <span class="string">作业，如果设置为</span> <span class="literal">true</span><span class="string">，则为同步，第二个</span> <span class="string">DML</span> <span class="string">将会等待第一个</span> <span class="string">DML</span> <span class="string">操作执行结束之后再执行</span></span><br><span class="line"><span class="attr">table.dml-sync:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="string">//</span> <span class="string">默认值：64000</span></span><br><span class="line"><span class="string">//</span> <span class="string">值类型：Integer</span></span><br><span class="line"><span class="string">//</span> <span class="string">流批任务：流、批任务都支持</span></span><br><span class="line"><span class="string">//</span> <span class="string">用处：Flink</span> <span class="string">SQL</span> <span class="string">会通过生产</span> <span class="string">java</span> <span class="string">代码来执行具体的</span> <span class="string">SQL</span> <span class="string">逻辑，但是</span> <span class="string">jvm</span> <span class="string">限制了一个</span> <span class="string">java</span> <span class="string">方法的最大长度不能超过</span> <span class="string">64KB，但是某些场景下</span> <span class="string">Flink</span> <span class="string">SQL</span> <span class="string">生产的</span> <span class="string">java</span> <span class="string">代码会超过</span> <span class="string">64KB，这时</span> <span class="string">jvm</span> <span class="string">就会直接报错。因此此参数可以用于限制生产的</span> <span class="string">java</span> <span class="string">代码的长度来避免超过</span> <span class="string">64KB，从而避免</span> <span class="string">jvm</span> <span class="string">报错。</span></span><br><span class="line"><span class="attr">table.generated-code.max-length:</span> <span class="number">64000</span></span><br><span class="line"></span><br><span class="line"><span class="string">//</span> <span class="string">默认值：default</span></span><br><span class="line"><span class="string">//</span> <span class="string">值类型：String</span></span><br><span class="line"><span class="string">//</span> <span class="string">流批任务：流、批任务都支持</span></span><br><span class="line"><span class="string">//</span> <span class="string">用处：在使用天级别的窗口时，通常会遇到时区问题。举个例子，Flink</span> <span class="string">开一天的窗口，默认是按照</span> <span class="string">UTC</span> <span class="string">零时区进行划分，那么在北京时区划分出来的一天的窗口是第一天的早上</span> <span class="number">8</span><span class="string">:00</span> <span class="string">到第二天的早上</span> <span class="number">8</span><span class="string">:00，但是实际场景中想要的效果是第一天的早上</span> <span class="number">0</span><span class="string">:00</span> <span class="string">到第二天的早上</span> <span class="number">0</span><span class="string">:00</span> <span class="string">点。因此可以将此参数设置为</span> <span class="string">GMT+08:00</span> <span class="string">来解决这个问题。</span></span><br><span class="line"><span class="attr">table.local-time-zone:</span> <span class="string">default</span></span><br><span class="line"></span><br><span class="line"><span class="string">//</span> <span class="string">默认值：default</span></span><br><span class="line"><span class="string">//</span> <span class="string">值类型：Enum【BLINK、OLD】</span></span><br><span class="line"><span class="string">//</span> <span class="string">流批任务：流、批任务都支持</span></span><br><span class="line"><span class="string">//</span> <span class="string">用处：Flink</span> <span class="string">SQL</span> <span class="string">planner，默认为</span> <span class="string">BLINK</span> <span class="string">planner，也可以选择</span> <span class="string">old</span> <span class="string">planner，但是推荐使用</span> <span class="string">BLINK</span> <span class="string">planner</span></span><br><span class="line"><span class="attr">table.planner:</span> <span class="string">BLINK</span></span><br><span class="line"></span><br><span class="line"><span class="string">//</span> <span class="string">默认值：default</span></span><br><span class="line"><span class="string">//</span> <span class="string">值类型：String</span></span><br><span class="line"><span class="string">//</span> <span class="string">流批任务：流、批任务都支持</span></span><br><span class="line"><span class="string">//</span> <span class="string">用处：Flink</span> <span class="string">解析一个</span> <span class="string">SQL</span> <span class="string">的解析器，目前有</span> <span class="string">Flink</span> <span class="string">SQL</span> <span class="string">默认的解析器和</span> <span class="string">Hive</span> <span class="string">SQL</span> <span class="string">解析器，其区别在于两种解析器支持的语法会有不同，比如</span> <span class="string">Hive</span> <span class="string">SQL</span> <span class="string">解析器支持</span> <span class="string">between</span> <span class="string">and、rlike</span> <span class="string">语法，Flink</span> <span class="string">SQL</span> <span class="string">不支持</span></span><br><span class="line"><span class="attr">table.sql-dialect:</span> <span class="string">default</span></span><br></pre></td></tr></table></figure><h2 id="5-4-SQL-性能调优"><a href="#5-4-SQL-性能调优" class="headerlink" title="5.4.SQL 性能调优"></a>5.4.SQL 性能调优</h2><p>本小节主要介绍 Flink SQL 中的聚合算子的优化，在某些场景下应用这些优化后，性能提升会非常大。本小节主要包含以下四种优化：</p><ol><li>⭐ <code>（常用）</code>MiniBatch 聚合：unbounded group agg 中，可以使用 minibatch 聚合来做到微批计算、访问状态、输出结果，避免每来一条数据就计算、访问状态、输出一次结果，从而减少访问 state 的时长（尤其是 Rocksdb）提升性能。</li><li>⭐ <code>（常用）</code>两阶段聚合：类似 MapReduce 中的 Combiner 的效果，可以先在 shuffle 数据之前先进行一次聚合，减少 shuffle 数据量</li><li>⭐ <code>（不常用）</code>split 分桶：在 count distinct、sum distinct 的去重的场景中，如果出现数据倾斜，任务性能会非常差，所以如果先按照 distinct key 进行分桶，将数据打散到各个 TM 进行计算，然后将分桶的结果再进行聚合，性能就会提升很大</li><li>⭐ <code>（常用）</code>去重 filter 子句：在 count distinct 中使用 filter 子句于 Hive SQL 中的 count(distinct if(xxx, user_id, null)) 子句，但是 state 中同一个 key 会按照 bit 位会进行复用，这对状态大小优化非常有用</li></ol><p>上面简单介绍了聚合场景的四种优化，下面详细介绍一下其最终效果以及实现原理。</p><h3 id="5-4-1-MiniBatch-聚合"><a href="#5-4-1-MiniBatch-聚合" class="headerlink" title="5.4.1.MiniBatch 聚合"></a>5.4.1.MiniBatch 聚合</h3><ol><li>⭐ 问题场景：默认情况下，unbounded agg 算子是逐条处理输入的记录，其处理流程如下：</li></ol><ul><li>⭐ 从状态中读取 accumulator；</li><li>⭐ 累加/撤回的数据记录至 accumulator；</li><li>⭐ 将 accumulator 写回状态；</li><li>⭐ 下一条记录将再次从流程 1 开始处理。</li></ul><p>但是上述处理流程的问题在于会增加 StateBackend 的访问性能开销（尤其是对于 RocksDB StateBackend）。</p><ol start="2"><li>⭐ MiniBatch 聚合如何解决上述问题：其核心思想是将一组输入的数据缓存在聚合算子内部的缓冲区中。当输入的数据被触发处理时，每个 key 只需要访问一次状态后端，这样可以大大减少访问状态的时间开销从而获得更好的吞吐量。但是，其会增加一些数据产出的延迟，因为它会缓冲一些数据再去处理。因此如果你要做这个优化，需要提前做一下吞吐量和延迟之间的权衡，但是大多数情况下，buffer 数据的延迟都是可以被接受的。所以非常建议在 unbounded agg 场景下使用这项优化。</li></ol><p>下图说明了 MiniBatch 聚合如何减少状态访问的。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/21.png" alt="MiniBatch"></p><p>上图展示了加 MiniBatch 和没加 MiniBatch 之前的执行区别。</p><ol start="3"><li>⭐ 启用 MiniBatch 聚合的参数：</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">TableEnvironment tEnv = ...</span><br><span class="line"></span><br><span class="line">Configuration configuration = tEnv.getConfig().getConfiguration();</span><br><span class="line">configuration.setString(<span class="string">&quot;table.exec.mini-batch.enabled&quot;</span>, <span class="string">&quot;true&quot;</span>); <span class="comment">// 启用 MiniBatch 聚合</span></span><br><span class="line">configuration.setString(<span class="string">&quot;table.exec.mini-batch.allow-latency&quot;</span>, <span class="string">&quot;5 s&quot;</span>); <span class="comment">// buffer 最多 5s 的输入数据记录</span></span><br><span class="line">configuration.setString(<span class="string">&quot;table.exec.mini-batch.size&quot;</span>, <span class="string">&quot;5000&quot;</span>); <span class="comment">// buffer 最多的输入数据记录数目</span></span><br></pre></td></tr></table></figure><blockquote><p>注意！！！</p><ol><li>⭐ <code>table.exec.mini-batch.allow-latency</code> 和 <code>table.exec.mini-batch.size</code> 两者只要其中一项满足条件就会执行 batch 访问状态操作。</li><li>⭐ 上述 MiniBatch 配置不会对 Window TVF 生效，因为！！！Window TVF 默认就会启用小批量优化，Window TVF 会将 buffer 的输入记录记录在托管内存中，而不是 JVM 堆中，因此 Window TVF 不会有 GC 过高或者 OOM 的问题。</li></ol></blockquote><h3 id="5-4-2-两阶段聚合"><a href="#5-4-2-两阶段聚合" class="headerlink" title="5.4.2.两阶段聚合"></a>5.4.2.两阶段聚合</h3><ol><li>⭐ 问题场景：在聚合数据处理场景中，很可能会由于热点数据导致数据倾斜，如下 SQL 所示，当 color = RED 为 50000w 条，而 color = BLUE 为 5 条，就产生了数据倾斜，而器数据处理的算子产生性能瓶颈。</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> color, <span class="built_in">sum</span>(id)</span><br><span class="line"><span class="keyword">FROM</span> T</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> color</span><br></pre></td></tr></table></figure><ol start="2"><li>⭐ 两阶段聚合如何解决上述问题：其核心思想类似于 MapReduce 中的 Combiner + Reduce，先将聚合操作在本地做一次 local 聚合，这样 shuffle 到下游的数据就会变少。</li></ol><p>还是上面的 SQL 案例，如果在 50000w 条的 color = RED 的数据 shuffle 之前，在本地将 color = RED 的数据聚合成为 1 条结果，那么 shuffle 给下游的数据量就被极大地减少了。</p><p>下图说明了两阶段聚合是如何处理热点数据的：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/22.png" alt="两阶段聚合"></p><ol start="3"><li>⭐ 启用两阶段聚合的参数：</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">TableEnvironment tEnv = ...</span><br><span class="line"></span><br><span class="line">Configuration configuration = tEnv.getConfig().getConfiguration();</span><br><span class="line">configuration.setString(<span class="string">&quot;table.exec.mini-batch.enabled&quot;</span>, <span class="string">&quot;true&quot;</span>); <span class="comment">// 打开 minibatch</span></span><br><span class="line">configuration.setString(<span class="string">&quot;table.exec.mini-batch.allow-latency&quot;</span>, <span class="string">&quot;5 s&quot;</span>);</span><br><span class="line">configuration.setString(<span class="string">&quot;table.exec.mini-batch.size&quot;</span>, <span class="string">&quot;5000&quot;</span>);</span><br><span class="line">configuration.setString(<span class="string">&quot;table.optimizer.agg-phase-strategy&quot;</span>, <span class="string">&quot;TWO_PHASE&quot;</span>); <span class="comment">// 打开两阶段聚合</span></span><br></pre></td></tr></table></figure><blockquote><p>注意！！！</p><ol><li>⭐ 此优化在窗口聚合中会自动生效，大家在使用 Window TVF 时可以看到 localagg + globalagg 两部分</li><li>⭐ 但是在 unbounded agg 中需要与 MiniBatch 参数相结合使用才会生效。</li></ol></blockquote><h3 id="5-4-3-split-分桶"><a href="#5-4-3-split-分桶" class="headerlink" title="5.4.3.split 分桶"></a>5.4.3.split 分桶</h3><ol><li>⭐ 问题场景：使用两阶段聚合虽然能够很好的处理 count，sum 等常规聚合算子，但是在 count distinct，sum distinct 等算子的两阶段聚合效果在大多数场景下都不太满足预期。</li></ol><p>因为 100w 条数据的 count 聚合能够在 local 算子聚合为 1 条数据，但是 count distinct 聚合 100w 条在 local 聚合之后的结果和可能是 90w 条，那么依然会有数据倾斜，如下 SQL 案例所示：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> color, <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> user_id)</span><br><span class="line"><span class="keyword">FROM</span> T</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> color</span><br></pre></td></tr></table></figure><ol start="2"><li>⭐ split 分桶如何解决上述问题：其核心思想在于按照 distinct 的 key，即 user_id，先做数据的分桶，将数据打散，分散到 Flink 的多个 TM 上进行计算，然后再将数据合桶计算。打开 split 分桶之后的效果就等同于以下 SQL：</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> color, <span class="built_in">SUM</span>(cnt)</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> color, <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> user_id) <span class="keyword">as</span> cnt</span><br><span class="line">    <span class="keyword">FROM</span> T</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> color, <span class="built_in">MOD</span>(HASH_CODE(user_id), <span class="number">1024</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> color</span><br></pre></td></tr></table></figure><p>下图说明了 split 分桶的处理流程：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/23.png" alt="split 聚合"></p><ol start="3"><li>⭐ 启用 split 分桶的参数：</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">TableEnvironment tEnv = ...</span><br><span class="line"></span><br><span class="line">tEnv.getConfig()</span><br><span class="line">  .getConfiguration()</span><br><span class="line">  .setString(<span class="string">&quot;table.optimizer.distinct-agg.split.enabled&quot;</span>, <span class="string">&quot;true&quot;</span>);  <span class="comment">// 打开 split 分桶</span></span><br></pre></td></tr></table></figure><blockquote><p>注意！！！</p><ol><li>⭐ 如果有多个 distinct key，则多个 distinct key 都会被作为分桶 key。比如 count(distinct a)，sum(distinct b) 这种多个 distinct key 也支持。</li><li>⭐ 小伙伴萌自己写的 UDAF 不支持！</li><li>⭐ 其实此种优化很少使用，因为大家直接自己按照分桶的写法自己就可以写了，而且最后生成的算子图和自己写的 SQL 的语法也能对应的上</li></ol></blockquote><h3 id="5-4-4-去重-filter-子句"><a href="#5-4-4-去重-filter-子句" class="headerlink" title="5.4.4.去重 filter 子句"></a>5.4.4.去重 filter 子句</h3><ol><li>⭐ 问题场景：在一些场景下，用户可能需要从不同维度计算 UV，例如 Android 的 UV、iPhone 的 UV、Web 的 UV 和总 UV。许多用户会选择 CASE WHEN 支持此功能，如下 SQL 所示：</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line"> <span class="keyword">day</span>,</span><br><span class="line"> <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> user_id) <span class="keyword">AS</span> total_uv,</span><br><span class="line"> <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> <span class="keyword">CASE</span> <span class="keyword">WHEN</span> flag <span class="keyword">IN</span> (<span class="string">&#x27;android&#x27;</span>, <span class="string">&#x27;iphone&#x27;</span>) <span class="keyword">THEN</span> user_id <span class="keyword">ELSE</span> <span class="keyword">NULL</span> <span class="keyword">END</span>) <span class="keyword">AS</span> app_uv,</span><br><span class="line"> <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> <span class="keyword">CASE</span> <span class="keyword">WHEN</span> flag <span class="keyword">IN</span> (<span class="string">&#x27;wap&#x27;</span>, <span class="string">&#x27;other&#x27;</span>) <span class="keyword">THEN</span> user_id <span class="keyword">ELSE</span> <span class="keyword">NULL</span> <span class="keyword">END</span>) <span class="keyword">AS</span> web_uv</span><br><span class="line"><span class="keyword">FROM</span> T</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">day</span></span><br></pre></td></tr></table></figure><p>但是如果你想实现类似的效果，Flink SQL 提供了更好性能的写法，就是本小节的 filter 子句。</p><ol start="2"><li>⭐ Filter 子句重写上述场景：</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line"> <span class="keyword">day</span>,</span><br><span class="line"> <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> user_id) <span class="keyword">AS</span> total_uv,</span><br><span class="line"> <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> user_id) <span class="keyword">FILTER</span> (<span class="keyword">WHERE</span> flag <span class="keyword">IN</span> (<span class="string">&#x27;android&#x27;</span>, <span class="string">&#x27;iphone&#x27;</span>)) <span class="keyword">AS</span> app_uv,</span><br><span class="line"> <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> user_id) <span class="keyword">FILTER</span> (<span class="keyword">WHERE</span> flag <span class="keyword">IN</span> (<span class="string">&#x27;web&#x27;</span>, <span class="string">&#x27;other&#x27;</span>)) <span class="keyword">AS</span> web_uv</span><br><span class="line"><span class="keyword">FROM</span> T</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">day</span></span><br></pre></td></tr></table></figure><p>Filter 子句的优化点在于，Flink 会识别出三个去重的 key 都是 user_id，因此会把三个去重的 key 存在一个共享的状态中。而不是上文 case when 中的三个状态中。其具体实现区别在于：</p><ul><li>⭐ case when：total_uv、app_uv、web_uv 在去重时，state 是存在三个 MapState 中的，MapState key 为 user_id，value 为默认值，判断是否重复直接按照 key 是在 MapState 中的出现过进行判断。如果总 uv 为 1 亿，’android’, ‘iphone’ uv 为 5kw，’wap’, ‘other’ uv 为 5kw，则 3 个 state 要存储总共 2 亿条数据</li><li>⭐ filter：total_uv、app_uv、web_uv 在去重时，state 是存在一个 MapState 中的，MapState key 为 user_id，value 为 long，其中 long 的第一个 bit 位标识在计算总 uv 时此 user_id 是否来光顾哦，第二个标识 ‘android’, ‘iphone’，第三个标识 ‘wap’, ‘other’，因此在上述 case when 相同的数据量的情况下，总共只需要存储 1 亿条数据，state 容量减小了几乎 50%</li></ul><p>或者下面的场景也可以使用 filter 子句进行替换。</p><ol><li>⭐ 优化前：</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">day</span></span><br><span class="line">    , app_typp</span><br><span class="line">    , <span class="built_in">count</span>(<span class="keyword">distinct</span> user_id) <span class="keyword">as</span> uv</span><br><span class="line"><span class="keyword">from</span> source_table</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    <span class="keyword">day</span></span><br><span class="line">    , app_type</span><br></pre></td></tr></table></figure><p>如果能够确定 app_type 是可以枚举的，比如为 android、iphone、web 三种，则可以使用 filter 子句做性能优化：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">day</span></span><br><span class="line">    , <span class="built_in">count</span>(<span class="keyword">distinct</span> user_id) <span class="keyword">filter</span> (<span class="keyword">where</span> app_type <span class="operator">=</span> <span class="string">&#x27;android&#x27;</span>) <span class="keyword">as</span> android_uv</span><br><span class="line">    , <span class="built_in">count</span>(<span class="keyword">distinct</span> user_id) <span class="keyword">filter</span> (<span class="keyword">where</span> app_type <span class="operator">=</span> <span class="string">&#x27;iphone&#x27;</span>) <span class="keyword">as</span> iphone_uv</span><br><span class="line">    , <span class="built_in">count</span>(<span class="keyword">distinct</span> user_id) <span class="keyword">filter</span> (<span class="keyword">where</span> app_type <span class="operator">=</span> <span class="string">&#x27;web&#x27;</span>) <span class="keyword">as</span> web_uv</span><br><span class="line"><span class="keyword">from</span> source_table</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    <span class="keyword">day</span></span><br></pre></td></tr></table></figure><p>经过上述优化之后，state 大小的优化效果也会是成倍提升的。</p><h2 id="5-5-SQL-Connector-扩展-自定义-Source-Sink"><a href="#5-5-SQL-Connector-扩展-自定义-Source-Sink" class="headerlink" title="5.5.SQL Connector 扩展 - 自定义 Source\Sink"></a>5.5.SQL Connector 扩展 - 自定义 Source\Sink</h2><h3 id="5-5-1-自定义-Source-Sink"><a href="#5-5-1-自定义-Source-Sink" class="headerlink" title="5.5.1.自定义 Source\Sink"></a>5.5.1.自定义 Source\Sink</h3><p>详细内容可见：<a href="https://mp.weixin.qq.com/s/xIXh8B_suAlKSp56aO5aEg">https://mp.weixin.qq.com/s/xIXh8B_suAlKSp56aO5aEg</a></p><h3 id="5-5-2-自定义-Source-Sink-的扩展接口"><a href="#5-5-2-自定义-Source-Sink-的扩展接口" class="headerlink" title="5.5.2.自定义 Source\Sink 的扩展接口"></a>5.5.2.自定义 Source\Sink 的扩展接口</h3><p>Flink SQL 中除了自定义的 Source 的基础接口之外，还提供了一部分扩展接口用于性能的优化、能力扩展，接下来详细进行介绍。在 Source\Sink 中主要包含了以下接口：</p><ol><li>⭐ Source 算子的接口：</li></ol><ul><li>⭐ <code>SupportsFilterPushDown</code>：将过滤条件下推到 Source 中提前过滤，减少下游处理的数据量。案例可见 <code>org.apache.flink.table.filesystem.FileSystemTableSource</code></li><li>⭐ <code>SupportsLimitPushDown</code>：将 limit 条目数下推到 Source 中提前限制处理的条目数。案例可见 <code>org.apache.flink.table.filesystem.FileSystemTableSource</code></li><li>⭐ <code>SupportsPartitionPushDown</code>：（常用于批处理场景）将带有 Partition 属性的 Source，将所有的 Partition 数据获取到之后，然后在 Source 决定哪个 Source 读取哪些 Partition 的数据，而不必在 Source 后过滤。比如 Hive 表的 Partition，将所有 Partition 获取到之后，然后决定某个 Source 应该读取哪些 Partition，详细可见 <code>org.apache.flink.table.filesystem.FileSystemTableSource</code>。</li><li>⭐ <code>SupportsProjectionPushDown</code>：将下游用到的字段下推到 Source 中，然后 Source 中只取这些字段，不使用的字段就不往下游发。案例可见 <code>org.apache.flink.connector.jdbc.table.JdbcDynamicTableSource</code></li><li>⭐ <code>SupportsReadingMetadata</code>：支持读取 Source 的 metadata，比如在 Kafka Source 中读取 Kafka 的 offset，写入时间戳等数据。案例可见 <code>org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSource</code></li><li>⭐ <code>SupportsWatermarkPushDown</code>：支持将 Watermark 的分配方式下推到 Source 中，比如 Kafka Source 中一个 Source Task 可以读取多个 Partition，然后为每个 Partition 单独分配 Watermark Generator，这样 Watermark 的生成粒度就是单 Partition，在事件时间下数据计算会更加准确。案例可见 <code>org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSource</code></li><li>⭐ <code>SupportsSourceWatermark</code>：支持自定义的 Source Watermark 分配方式，比如目前已有的 Watermark 分配方式不满足需求，需要自定义 Source 的 Watermark 生成方式，则可以实现此接口 + <code>在 DDL 中声明 SOURCE_WATERMARK()</code> 来声明使用自定义 Source 的 Watermark 生成方式。案例可见 <code>org.apache.flink.table.planner.connectors.ExternalDynamicSource</code></li></ul><ol start="2"><li>⭐ Sink 算子的接口：</li></ol><ul><li>⭐ <code>SupportsOverwrite</code>：（常用于批处理场景）支持类似于 Hive SQL 的 insert overwrite table xxx 的能力，将已有分区内的数据进行覆盖。案例可见 <code>org.apache.flink.connectors.hive.HiveTableSink</code></li><li>⭐ <code>SupportsPartitioning</code>：（常用于批处理场景）支持类似于 Hive SQL 的 insert INTO xxx partition(key = ‘A’) xxx 的能力，支持将结果数据写入某个静态分区。案例可见 <code>org.apache.flink.connectors.hive.HiveTableSink</code></li><li>⭐ <code>SupportsWritingMetadata</code>：支持将 metadata 写入到 Sink 中，比如可以往 Kafka Sink 中写入 Kafka 的 timestamp、header 等。案例可见 <code>org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSink</code></li></ul><h3 id="5-5-3-Source：SupportsFilterPushDown"><a href="#5-5-3-Source：SupportsFilterPushDown" class="headerlink" title="5.5.3.Source：SupportsFilterPushDown"></a>5.5.3.Source：SupportsFilterPushDown</h3><ol><li><p>⭐ 应用场景：将 where 中的一些过滤条件下推到 Source 中进行处理，这样不需要的数据就可以不往下游发送了，性能会有提升。</p></li><li><p>⭐ 优化前：如下图 web ui 算子图，过滤条件都在 Source 节点之后有单独的 filter 算子进行承接</p></li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/24.png" alt="filter 前"></p><ol start="3"><li>⭐ 优化方案及实现：在 DynamicTableSource 中实现 SupportsFilterPushDown 接口的方法，具体实现方案如下：</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Abilities_TableSource</span> <span class="keyword">implements</span> <span class="title">ScanTableSource</span></span></span><br><span class="line"><span class="class">        , <span class="title">SupportsFilterPushDown</span> // 过滤条件下推 </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> List&lt;ResolvedExpression&gt; filters;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 方法输入参数：List&lt;ResolvedExpression&gt; filters：引擎下推过来的过滤条件，然后在此方法中来决定哪些条件需要被下推</span></span><br><span class="line">    <span class="comment">// 方法输出参数：Result：Result 记录哪些过滤条件在 Source 中应用，哪些条件不能在 Source 中应用</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Result <span class="title">applyFilters</span><span class="params">(List&lt;ResolvedExpression&gt; filters)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.filters = <span class="keyword">new</span> LinkedList&lt;&gt;(filters);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.不上推任何过滤条件</span></span><br><span class="line">        <span class="comment">// Result.of(上推的 filter, 没有做上推的 filter)</span></span><br><span class="line"><span class="comment">//        return Result.of(Lists.newLinkedList(), filters);</span></span><br><span class="line">        <span class="comment">// 2.将所有的过滤条件都上推到 source</span></span><br><span class="line">        <span class="keyword">return</span> Result.of(filters, Lists.newLinkedList());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="4"><li>⭐ 优化效果：如下图 web ui 算子图，过滤条件在 Source 节点执行</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/25.png" alt="filter 后"></p><h3 id="5-5-4-Source：SupportsLimitPushDown"><a href="#5-5-4-Source：SupportsLimitPushDown" class="headerlink" title="5.5.4.Source：SupportsLimitPushDown"></a>5.5.4.Source：SupportsLimitPushDown</h3><ol><li><p>⭐ 应用场景：将 limit 子句下推到 Source 中，在批场景中可以过滤大部分不需要的数据</p></li><li><p>⭐ 优化前：如下图 web ui 算子图，limit 条件都在 Source 节点之后有单独的 Limit 算子进行承接</p></li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/27.png" alt="limit 前"></p><ol start="3"><li>⭐ 优化方案及实现：在 DynamicTableSource 中实现 SupportsLimitPushDown 接口的方法，具体实现方案如下：</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Abilities_TableSource</span> <span class="keyword">implements</span> <span class="title">ScanTableSource</span></span></span><br><span class="line"><span class="class">        , <span class="title">SupportsLimitPushDown</span> // <span class="title">limit</span> 条件下推 </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> limit = -<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">// 方法输入参数：long limit：引擎下推过来的 limit 条目数</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">applyLimit</span><span class="params">(<span class="keyword">long</span> limit)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 将 limit 数接收到之后，然后在 SourceFunction 中可以进行过滤</span></span><br><span class="line">        <span class="keyword">this</span>.limit = limit;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="4"><li>⭐ 优化效果：如下图 web ui 算子图，limit 条件在 Source 节点执行</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/26.png" alt="limit 后"></p><h3 id="5-5-5-Source：SupportsProjectionPushDown"><a href="#5-5-5-Source：SupportsProjectionPushDown" class="headerlink" title="5.5.5.Source：SupportsProjectionPushDown"></a>5.5.5.Source：SupportsProjectionPushDown</h3><ol><li><p>⭐ 应用场景：将下游用到的字段下推到 Source 中，然后 Source 中可以做到只取这些字段，不使用的字段就不往下游发</p></li><li><p>⭐ 优化前：如下图 web ui 算子图，limit 条件都在 Source 节点之后有单独的 Limit 算子进行承接</p></li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/28.png" alt="project 前"></p><ol start="3"><li>⭐ 优化方案及实现：在 DynamicTableSource 中实现 SupportsProjectionPushDown 接口的方法，具体实现方案如下：</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Abilities_TableSource</span> <span class="keyword">implements</span> <span class="title">ScanTableSource</span></span></span><br><span class="line"><span class="class">        , <span class="title">SupportsProjectionPushDown</span> // <span class="title">select</span> 字段下推 </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> TableSchema tableSchema;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@SneakyThrows</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ScanRuntimeProvider <span class="title">getScanRuntimeProvider</span><span class="params">(ScanContext runtimeProviderContext)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// create runtime classes that are shipped to the cluster</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> DeserializationSchema&lt;RowData&gt; deserializer = decodingFormat.createRuntimeDecoder(</span><br><span class="line">                runtimeProviderContext,</span><br><span class="line">                getSchemaWithMetadata(<span class="keyword">this</span>.tableSchema).toRowDataType());</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">// 方法输入参数：</span></span><br><span class="line">    <span class="comment">// int[][] projectedFields：下游算子 `使用到的那些字段` 的下标，可以通过 projectSchemaWithMetadata 方法结合 table schema 信息生成 Source 新的需要写出 schema 信息</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">applyProjection</span><span class="params">(<span class="keyword">int</span>[][] projectedFields)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.tableSchema = projectSchemaWithMetadata(<span class="keyword">this</span>.tableSchema, projectedFields);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="4"><li>⭐ 优化效果：如下图 web ui 算子图，下游没有用到的字段直接在 Source 节点过滤掉，不输出</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/29.png" alt="project 后"></p><h3 id="5-5-6-Source：SupportsReadingMetadata"><a href="#5-5-6-Source：SupportsReadingMetadata" class="headerlink" title="5.5.6.Source：SupportsReadingMetadata"></a>5.5.6.Source：SupportsReadingMetadata</h3><ol><li><p>⭐ 应用场景：支持读取 Source 的 metadata，比如在 Kafka Source 中读取 Kafka 的 offset，写入时间戳等数据</p></li><li><p>⭐ 支持之前：比如想获取 Kafka 中的 offset 字段，在之前是不支持的</p></li><li><p>⭐ 支持方案及实现：在 DynamicTableSource 中实现 SupportsReadingMetadata 接口的方法，我们来看看 Flink Kafka Consumer 的具体实现方案：</p></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 注意！！！先执行 listReadableMetadata()，然后执行 applyReadableMetadata(xxx, xxx) 方法</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 方法输出参数：列出 Kafka Source 可以从 Kafka 中读取的 metadata 数据</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Map&lt;String, DataType&gt; <span class="title">listReadableMetadata</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> Map&lt;String, DataType&gt; metadataMap = <span class="keyword">new</span> LinkedHashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// add value format metadata with prefix</span></span><br><span class="line">    valueDecodingFormat</span><br><span class="line">            .listReadableMetadata()</span><br><span class="line">            .forEach((key, value) -&gt; metadataMap.put(VALUE_METADATA_PREFIX + key, value));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// add connector metadata</span></span><br><span class="line">    Stream.of(ReadableMetadata.values())</span><br><span class="line">            .forEachOrdered(m -&gt; metadataMap.putIfAbsent(m.key, m.dataType));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> metadataMap;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方法输入参数：</span></span><br><span class="line"><span class="comment">// List&lt;String&gt; metadataKeys：用户 SQL 中写入到 Sink 表的的 metadata 字段名称（metadataKeys）</span></span><br><span class="line"><span class="comment">// DataType producedDataType：将用户 SQL 写入到 Sink 表的所有字段的类型信息传进来，包括了 metadata 字段的类型信息</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">applyReadableMetadata</span><span class="params">(List&lt;String&gt; metadataKeys, DataType producedDataType)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> List&lt;String&gt; formatMetadataKeys =</span><br><span class="line">            metadataKeys.stream()</span><br><span class="line">                    .filter(k -&gt; k.startsWith(VALUE_METADATA_PREFIX))</span><br><span class="line">                    .collect(Collectors.toList());</span><br><span class="line">    <span class="keyword">final</span> List&lt;String&gt; connectorMetadataKeys = <span class="keyword">new</span> ArrayList&lt;&gt;(metadataKeys);</span><br><span class="line">    connectorMetadataKeys.removeAll(formatMetadataKeys);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> Map&lt;String, DataType&gt; formatMetadata = valueDecodingFormat.listReadableMetadata();</span><br><span class="line">    <span class="keyword">if</span> (formatMetadata.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">final</span> List&lt;String&gt; requestedFormatMetadataKeys =</span><br><span class="line">                formatMetadataKeys.stream()</span><br><span class="line">                        .map(k -&gt; k.substring(VALUE_METADATA_PREFIX.length()))</span><br><span class="line">                        .collect(Collectors.toList());</span><br><span class="line">        valueDecodingFormat.applyReadableMetadata(requestedFormatMetadataKeys);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.metadataKeys = connectorMetadataKeys;</span><br><span class="line">    <span class="keyword">this</span>.producedDataType = producedDataType;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="4"><li>⭐ 支持之后的效果：</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> KafkaTable (</span><br><span class="line">   <span class="operator">/</span><span class="operator">/</span> METADATA 字段用于声明可以从 Source 读取的 metadata</span><br><span class="line">   <span class="operator">/</span><span class="operator">/</span> 关于 Flink Kafka Source 可以读取的 metadata 见以下链接</span><br><span class="line">   <span class="operator">/</span><span class="operator">/</span> https:<span class="operator">/</span><span class="operator">/</span>nightlies.apache.org<span class="operator">/</span>flink<span class="operator">/</span>flink<span class="operator">-</span>docs<span class="operator">-</span><span class="keyword">release</span><span class="number">-1.13</span><span class="operator">/</span>docs<span class="operator">/</span>connectors<span class="operator">/</span><span class="keyword">table</span><span class="operator">/</span>kafka<span class="operator">/</span>#available<span class="operator">-</span>metadata</span><br><span class="line">  `event_time` <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">&#x27;timestamp&#x27;</span>,</span><br><span class="line">  `<span class="keyword">partition</span>` <span class="type">BIGINT</span> METADATA VIRTUAL,</span><br><span class="line">  `<span class="keyword">offset</span>` <span class="type">BIGINT</span> METADATA VIRTUAL,</span><br><span class="line">  `user_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `item_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `behavior` STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;user_behavior&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;scan.startup.mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;earliest-offset&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;csv&#x27;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>在后续的 DML SQL 语句中就可以正常使用这些 metadata 字段的数据了。</p><h3 id="5-5-7-Source：SupportsWatermarkPushDown"><a href="#5-5-7-Source：SupportsWatermarkPushDown" class="headerlink" title="5.5.7.Source：SupportsWatermarkPushDown"></a>5.5.7.Source：SupportsWatermarkPushDown</h3><ol><li><p>⭐ 应用场景：支持将 Watermark 的分配方式下推到 Source 中，比如 Kafka Source 中一个 Source Task 可以读取多个 Partition，Watermark 分配器下推到 Source 算子中后，就可以为每个 Partition 单独分配 Watermark Generator，这样 Watermark 的生成粒度就是 Kafka 的单 Partition，在事件时间下数据乱序会更小。</p></li><li><p>⭐ 支持之前：可以看到下图，Watermark 的分配是在 Source 节点之后的。</p></li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/32.png" alt="watermark 前"></p><ol start="3"><li>⭐ 支持方案及实现：在 DynamicTableSource 中实现 SupportsWatermarkPushDown 接口的方法，我们来看看 Flink Kafka Consumer 的具体实现方案：</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 方法输入参数：</span></span><br><span class="line"><span class="comment">// WatermarkStrategy&lt;RowData&gt; watermarkStrategy：将用户 DDL 中的 watermark 生成方式传入</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">applyWatermark</span><span class="params">(WatermarkStrategy&lt;RowData&gt; watermarkStrategy)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.watermarkStrategy = watermarkStrategy;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="4"><li>⭐ 支持之后的效果：</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/21_flinksql%E7%89%9B%E9%80%BC%E8%BD%B0%E8%BD%B0/31.png" alt="watermark 前"></p><h3 id="5-5-8-Sink：SupportsOverwrite"><a href="#5-5-8-Sink：SupportsOverwrite" class="headerlink" title="5.5.8.Sink：SupportsOverwrite"></a>5.5.8.Sink：SupportsOverwrite</h3><ol><li><p>⭐ 应用场景：（常用于批处理场景）支持类似于 Hive SQL 的 insert overwrite table xxx 的能力，将已有分区内的数据进行覆盖。</p></li><li><p>⭐ 支持方案及实现：在 DynamicTableSink 中实现 SupportsOverwrite 接口的方法，我们来看看 <code>HiveTableSink</code> 的具体实现方案：</p></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> DataStreamSink&lt;Row&gt; <span class="title">createBatchSink</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    DataStream&lt;RowData&gt; dataStream,</span></span></span><br><span class="line"><span class="function"><span class="params">    DataStructureConverter converter,</span></span></span><br><span class="line"><span class="function"><span class="params">    StorageDescriptor sd,</span></span></span><br><span class="line"><span class="function"><span class="params">    HiveWriterFactory recordWriterFactory,</span></span></span><br><span class="line"><span class="function"><span class="params">    OutputFileConfig fileNaming,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> <span class="keyword">int</span> parallelism)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">FileSystemOutputFormat.Builder&lt;Row&gt; builder = <span class="keyword">new</span> FileSystemOutputFormat.Builder&lt;&gt;();</span><br><span class="line">...</span><br><span class="line">--- <span class="number">2.</span> 将 overwrite 字段设置到 FileSystemOutputFormat 中，在后续写入数据到 Hive 表时，如果 overwrite = <span class="keyword">true</span>，则会覆盖直接覆盖已有数据</span><br><span class="line">builder.setOverwrite(overwrite);</span><br><span class="line">builder.setStaticPartitions(staticPartitionSpec);</span><br><span class="line">...</span><br><span class="line"><span class="keyword">return</span> dataStream</span><br><span class="line">        .map((MapFunction&lt;RowData, Row&gt;) value -&gt; (Row) converter.toExternal(value))</span><br><span class="line">        .writeUsingOutputFormat(builder.build())</span><br><span class="line">        .setParallelism(parallelism);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. 方法输入参数：</span></span><br><span class="line"><span class="comment">// boolean overwrite：用户写的 SQL 中如果包含了 overwrite 关键字，则方法入参 overwrite = true</span></span><br><span class="line"><span class="comment">// 如果不包含 overwrite 关键字，则方法入参 overwrite = false</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">applyOverwrite</span><span class="params">(<span class="keyword">boolean</span> overwrite)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.overwrite = overwrite;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="4"><li>⭐ 支持之后的效果：</li></ol><p>支持在批任务中 insert overwrite xxx。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite hive_sink_table</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    user_id</span><br><span class="line">    , order_amount</span><br><span class="line">    , server_timestamp_bigint</span><br><span class="line">    , server_timestamp </span><br><span class="line"><span class="keyword">from</span> hive_source_table</span><br></pre></td></tr></table></figure><h3 id="5-5-9-Sink：SupportsPartitioning"><a href="#5-5-9-Sink：SupportsPartitioning" class="headerlink" title="5.5.9.Sink：SupportsPartitioning"></a>5.5.9.Sink：SupportsPartitioning</h3><ol><li><p>⭐ 应用场景：（常用于批处理场景）支持类似于 Hive SQL 的 insert INTO xxx partition(key = ‘A’) 的能力，支持将结果数据写入某个静态分区。</p></li><li><p>⭐ 支持方案及实现：在 DynamicTableSink 中实现 SupportsPartitioning 接口的方法，我们来看看 <code>HiveTableSink</code> 的具体实现方案：</p></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> DataStreamSink&lt;Row&gt; <span class="title">createBatchSink</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    DataStream&lt;RowData&gt; dataStream,</span></span></span><br><span class="line"><span class="function"><span class="params">    DataStructureConverter converter,</span></span></span><br><span class="line"><span class="function"><span class="params">    StorageDescriptor sd,</span></span></span><br><span class="line"><span class="function"><span class="params">    HiveWriterFactory recordWriterFactory,</span></span></span><br><span class="line"><span class="function"><span class="params">    OutputFileConfig fileNaming,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> <span class="keyword">int</span> parallelism)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">FileSystemOutputFormat.Builder&lt;Row&gt; builder = <span class="keyword">new</span> FileSystemOutputFormat.Builder&lt;&gt;();</span><br><span class="line">...</span><br><span class="line">builder.setMetaStoreFactory(msFactory());</span><br><span class="line">builder.setOverwrite(overwrite);</span><br><span class="line">--- <span class="number">2.</span> 将 staticPartitionSpec 字段设置到 FileSystemOutputFormat 中，在后续写入数据到 Hive 表时，如果有静态分区，则会将数据写入到对应的静态分区中</span><br><span class="line">builder.setStaticPartitions(staticPartitionSpec);</span><br><span class="line">...</span><br><span class="line"><span class="keyword">return</span> dataStream</span><br><span class="line">        .map((MapFunction&lt;RowData, Row&gt;) value -&gt; (Row) converter.toExternal(value))</span><br><span class="line">        .writeUsingOutputFormat(builder.build())</span><br><span class="line">        .setParallelism(parallelism);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. 方法输入参数：</span></span><br><span class="line"><span class="comment">// Map&lt;String, String&gt; partitionMap：用户写的 SQL 中如果包含了 partition(partition_key = &#x27;A&#x27;) 关键字</span></span><br><span class="line"><span class="comment">// 则方法入参 Map&lt;String, String&gt; partitionMap 的输入值转为 JSON 后为：&#123;&quot;partition_key&quot;: &quot;A&quot;&#125;</span></span><br><span class="line"><span class="comment">// 用户可以自己将方法入参的 partitionMap 保存到自定义变量中，后续写出到 Hive 表时进行使用</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">applyStaticPartition</span><span class="params">(Map&lt;String, String&gt; partitionMap)</span> </span>&#123;</span><br><span class="line">    staticPartitionSpec = <span class="keyword">new</span> LinkedHashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (String partitionCol : getPartitionKeys()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (partitionMap.containsKey(partitionCol)) &#123;</span><br><span class="line">            staticPartitionSpec.put(partitionCol, partitionMap.get(partitionCol));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="4"><li>⭐ 支持之后的效果：</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite hive_sink_table <span class="keyword">partition</span>(<span class="type">date</span> <span class="operator">=</span> <span class="string">&#x27;2022-01-01&#x27;</span>)</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    user_id</span><br><span class="line">    , order_amount</span><br><span class="line">    , server_timestamp_bigint</span><br><span class="line">    , server_timestamp </span><br><span class="line"><span class="keyword">from</span> hive_source_table</span><br></pre></td></tr></table></figure><h3 id="5-5-9-Sink：SupportsWritingMetadata"><a href="#5-5-9-Sink：SupportsWritingMetadata" class="headerlink" title="5.5.9.Sink：SupportsWritingMetadata"></a>5.5.9.Sink：SupportsWritingMetadata</h3><ol><li><p>⭐ 应用场景：支持将 metadata 写入到 Sink 中。举例：可以往 Kafka Sink 中写入 Kafka 的 timestamp、header 等。案例可见 <code>org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicSink</code></p></li><li><p>⭐ 支持方案及实现：在 DynamicTableSink 中实现 SupportsWritingMetadata 接口的方法，我们来看看 <code>KafkaDynamicSink</code> 的具体实现方案：</p></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 注意！！！先执行 listWritableMetadata()，然后执行 applyWritableMetadata(xxx, xxx) 方法</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. 方法返回参数 Map&lt;String, DataType&gt;：Flink 会获取到可以写入到 Kafka Sink 中的 metadata 都有哪些</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Map&lt;String, DataType&gt; <span class="title">listWritableMetadata</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> Map&lt;String, DataType&gt; metadataMap = <span class="keyword">new</span> LinkedHashMap&lt;&gt;();</span><br><span class="line">    Stream.of(WritableMetadata.values())</span><br><span class="line">            .forEachOrdered(m -&gt; metadataMap.put(m.key, m.dataType));</span><br><span class="line">    <span class="keyword">return</span> metadataMap;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 方法输入参数：</span></span><br><span class="line"><span class="comment">// List&lt;String&gt; metadataKeys：通过解析用户的 SQL 语句，得出用户写出到 Sink 的 metadata 列信息，是 listWritableMetadata() 返回结果的子集</span></span><br><span class="line"><span class="comment">// DataType consumedDataType：写出到 Sink 字段的 DataType 类型信息，包括了写出的 metadata 列的类型信息（注意！！！metadata 列会被添加到最后一列）。</span></span><br><span class="line"><span class="comment">// 用户可以将这两个信息获取到，然后传入构造的 SinkFunction 中实现将对应字段写入 metadata 流程。</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">applyWritableMetadata</span><span class="params">(List&lt;String&gt; metadataKeys, DataType consumedDataType)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.metadataKeys = metadataKeys;</span><br><span class="line">    <span class="keyword">this</span>.consumedDataType = consumedDataType;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="4"><li>⭐ 支持之后的效果：</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> KafkaSourceTable (</span><br><span class="line">  `user_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `item_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `behavior` STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;source_topic&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;scan.startup.mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;earliest-offset&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> KafkaSinkTable (</span><br><span class="line">  <span class="comment">-- 1. 定义 kafka 中 metadata 的 timestamp 列</span></span><br><span class="line">  `<span class="type">timestamp</span>` TIMESTAMP_LTZ(<span class="number">3</span>) METADATA,</span><br><span class="line">  `user_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `item_id` <span class="type">BIGINT</span>,</span><br><span class="line">  `behavior` STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;sink_topic&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;scan.startup.mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;earliest-offset&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> KafkaSinkTable</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="comment">-- 2. 写入到 kafka 的 metadata 中的 timestamp</span></span><br><span class="line">    <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> TIMESTAMP_LTZ(<span class="number">3</span>)) <span class="keyword">as</span> `<span class="type">timestamp</span>`</span><br><span class="line">    , user_id</span><br><span class="line">    , item_id</span><br><span class="line">    , behavior</span><br><span class="line"><span class="keyword">from</span> KafkaSourceTable</span><br></pre></td></tr></table></figure><h2 id="5-6-SQL-Format-扩展"><a href="#5-6-SQL-Format-扩展" class="headerlink" title="5.6.SQL Format 扩展"></a>5.6.SQL Format 扩展</h2><p>关于怎么实现一个自定义的 Format 可以参考文章：<a href="https://mp.weixin.qq.com/s/STUC4trW-HA3cnrsqT-N6g">https://mp.weixin.qq.com/s/STUC4trW-HA3cnrsqT-N6g</a></p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink sql 知其所以然（十五）：改了改源码，实现了个 batch lookup join</title>
    <link href="https://yangyichao-mango.github.io/2021/11/15/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/16_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9Aflink-sql-batch-lookup-join/"/>
    <id>https://yangyichao-mango.github.io/2021/11/15/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/16_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9Aflink-sql-batch-lookup-join/</id>
    <published>2021-11-15T07:26:59.000Z</published>
    <updated>2021-12-04T16:43:52.244Z</updated>
    
    <content type="html"><![CDATA[<p>看了那么多的技术文，你能明白作者想让你在读完文章后学到什么吗？</p><p>大数据羊说的文章会让你明白</p><ol><li><p>博主会阐明博主期望本文能给小伙伴们带来什么帮助，让小伙伴萌能直观明白博主的心思</p></li><li><p>博主会以实际的应用场景和案例入手，不只是知识点的简单堆砌</p></li><li><p>博主会把重要的知识点的原理进行剖析，让小伙伴萌做到深入浅出</p></li></ol><h1 id="1-序篇"><a href="#1-序篇" class="headerlink" title="1.序篇"></a>1.序篇</h1><p>源码公众号后台回复<strong>1.13.2 sql batch lookup join</strong>获取。</p><p>TODO 上节</p><p>书接上回，上节说到了博主发现由于在 flink sql 中 lookup join 访问外部维表存在的性能问题。</p><p>由此诞生了一个想法，以 Redis 维表为例，Redis 支持 pipeline 批量访问模式，因此 flink sql lookup join 能不能按照 DataStream 方式一样，先攒一批数据<br>，然后使用 Redis pipeline 批量访问外部存储。博主亲切的将这个功能称为 flink sql batch lookup join，本节就是讲述博主基于 flink 源码对此功能的实现。</p><p>废话不多说，咱们先直接上本文的目录和结论，小伙伴可以先看结论快速了解博主期望本文能给小伙伴们带来什么帮助：</p><ol><li><strong>直接来一个实战案例</strong>：博主以曝光用户日志流关联用户画像（年龄、性别）维表为例介绍 batch lookup join 具有的基本能力（怎么配置参数，怎么写 sql，最终效果咋样）。</li><li><strong>batch lookup join</strong>：主要介绍 batch lookup join 的功能是从 flink transformation 出发，确定要 batch lookup join 涉及改动的地方以及其实现思路、原理。也会教给大家一些改动源码来实现自己想要的一些功能的思路。</li><li><strong>总结及展望</strong>：目前的 batch lookup join 实现其实不符合 sql 的原始语义，后续大家可以按照 sql 标准自己做一些实现</li></ol><h1 id="2-来一个实战案例"><a href="#2-来一个实战案例" class="headerlink" title="2.来一个实战案例"></a>2.来一个实战案例</h1><h2 id="2-1-预期的输入、输出数据"><a href="#2-1-预期的输入、输出数据" class="headerlink" title="2.1.预期的输入、输出数据"></a>2.1.预期的输入、输出数据</h2><p>来看看在具体场景下，对应输入值的输出值应该长啥样。</p><p>需求指标：使用曝光用户日志流（show_log）关联用户画像维表（user_profile）关联到用户的画像（性别，年龄段）数据。</p><p>来一波输入数据：</p><p><strong>曝光用户日志流（show_log）数据（数据存储在 kafka 中）：</strong></p><table><thead><tr><th>log_id</th><th>timestamp</th><th>user_id</th></tr></thead><tbody><tr><td>1</td><td>2021-11-01 00:01:03</td><td>a</td></tr><tr><td>2</td><td>2021-11-01 00:03:00</td><td>b</td></tr><tr><td>3</td><td>2021-11-01 00:05:00</td><td>c</td></tr><tr><td>4</td><td>2021-11-01 00:06:00</td><td>b</td></tr><tr><td>5</td><td>2021-11-01 00:07:00</td><td>c</td></tr></tbody></table><p><strong>用户画像维表（user_profile）数据（数据存储在 redis 中）：</strong></p><table><thead><tr><th>user_id(主键)</th><th>age</th><th>sex</th></tr></thead><tbody><tr><td>a</td><td>12-18</td><td>男</td></tr><tr><td>b</td><td>18-24</td><td>女</td></tr><tr><td>c</td><td>18-24</td><td>男</td></tr></tbody></table><p>注意：redis 中的数据结构存储是按照 key，value 去存储的。其中 key 为 user_id，value 为 age，sex 的 json。如下图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/16_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%EF%BC%9Aflink_sql_lookup_join/2.png" alt="user_profile redis"></p><p>预期输出数据如下：</p><table><thead><tr><th>log_id</th><th>timestamp</th><th>user_id</th><th>age</th><th>sex</th></tr></thead><tbody><tr><td>1</td><td>2021-11-01 00:01:03</td><td>a</td><td>12-18</td><td>男</td></tr><tr><td>2</td><td>2021-11-01 00:03:00</td><td>b</td><td>18-24</td><td>女</td></tr><tr><td>3</td><td>2021-11-01 00:05:00</td><td>c</td><td>18-24</td><td>男</td></tr><tr><td>4</td><td>2021-11-01 00:06:00</td><td>b</td><td>18-24</td><td>女</td></tr><tr><td>5</td><td>2021-11-01 00:07:00</td><td>c</td><td>18-24</td><td>男</td></tr></tbody></table><h2 id="2-2-batch-lookup-join-sql-代码"><a href="#2-2-batch-lookup-join-sql-代码" class="headerlink" title="2.2.batch lookup join sql 代码"></a>2.2.batch lookup join sql 代码</h2><p>batch lookup join sql 代码和原来的 lookup join sql 代码一模一样。如下 sql。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> show_log (</span><br><span class="line">    log_id <span class="type">BIGINT</span>,</span><br><span class="line">    `<span class="type">timestamp</span>` <span class="keyword">as</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    user_id STRING,</span><br><span class="line">    proctime <span class="keyword">AS</span> PROCTIME()</span><br><span class="line">)</span><br><span class="line"><span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_profile (</span><br><span class="line">    user_id STRING,</span><br><span class="line">    age STRING,</span><br><span class="line">    sex STRING</span><br><span class="line">    ) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;redis&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;hostname&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;127.0.0.1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;port&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;6379&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;lookup.cache.max-rows&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;500&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;lookup.cache.ttl&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;3600&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;lookup.max-retries&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    log_id <span class="type">BIGINT</span>,</span><br><span class="line">    `<span class="type">timestamp</span>` <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    user_id STRING,</span><br><span class="line">    proctime <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    age STRING,</span><br><span class="line">    sex STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- lookup join 的 query 逻辑</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    s.log_id <span class="keyword">as</span> log_id</span><br><span class="line">    , s.`<span class="type">timestamp</span>` <span class="keyword">as</span> `<span class="type">timestamp</span>`</span><br><span class="line">    , s.user_id <span class="keyword">as</span> user_id</span><br><span class="line">    , s.proctime <span class="keyword">as</span> proctime</span><br><span class="line">    , u.sex <span class="keyword">as</span> sex</span><br><span class="line">    , u.age <span class="keyword">as</span> age</span><br><span class="line"><span class="keyword">FROM</span> show_log <span class="keyword">AS</span> s</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> user_profile <span class="keyword">FOR</span> <span class="built_in">SYSTEM_TIME</span> <span class="keyword">AS</span> <span class="keyword">OF</span> s.proctime <span class="keyword">AS</span> u</span><br><span class="line"><span class="keyword">ON</span> s.user_id <span class="operator">=</span> u.user_id</span><br></pre></td></tr></table></figure><p><code>可以看到 lookup join 和 batch lookup join 的代码是完全相同的，唯一的不同之处在于，batch lookup join 需要设置 table config 参数，如下图所示：</code></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/17_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9Aflinksqlbatchjoin/3.png" alt="table config"></p><h2 id="2-2-batch-lookup-join-效果"><a href="#2-2-batch-lookup-join-效果" class="headerlink" title="2.2.batch lookup join 效果"></a>2.2.batch lookup join 效果</h2><p>将原生 lookup join 和 batch lookup join 的效果做个对比：</p><p>原生的 lookup join：每输入一条数据，访问外部维表获取到结果输出一条数据，如下图所示。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/17_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9Aflinksqlbatchjoin/1.gif" alt="lookup join"></p><p>博主实现的 batch lookup join：是<code>每攒够 30 条数据</code>或者<code>每 5s（防止数据量少的情况下，长时间不输出数据）</code> 就利用 redis pipeline 能力访问外部存储一次。然后批量输出结果，如下图所示。大大提高了吞吐。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/17_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9Aflinksqlbatchjoin/2.gif" alt="batch lookup join"></p><h1 id="3-batch-lookup-join-实现"><a href="#3-batch-lookup-join-实现" class="headerlink" title="3.batch lookup join 实现"></a>3.batch lookup join 实现</h1><h2 id="3-1-怎么知道应该改哪部分源码？"><a href="#3-1-怎么知道应该改哪部分源码？" class="headerlink" title="3.1.怎么知道应该改哪部分源码？"></a>3.1.怎么知道应该改哪部分源码？</h2><p>博主将通过下面几个问题去交给大家怎么改源码去实现自己的功能。</p><ol><li>改源码的有哪些比较好的思路？</li></ol><ul><li>结论：首先就是参考类似模块的实现（不会写，但是我会抄啊！），比如本文要实现 batch lookup join，必然要参考原生的 lookup join 去实现。</li></ul><ol start="2"><li>大家在改 flink 源码时，因为 flink 源码的模块太多了，项目非常庞大，往往第一步碰到的问题不是怎么去实现这个功能，而是应该在什么地方去改才能实现！</li></ol><ul><li>结论：一个 flink 的任务（DataStream\Table\SQL）所有的精华精华精华都集中在 transformation 中！！！只要是涉及到算子实现的东西，小伙伴萌就可以到 transformation 中去寻找。<br>可以将断点打在每一个 operator 的构造器或者 open 方法中就可以看到其实在哪一步构造和初始化的。这样就能顺着调用栈往前回溯而确定要改哪部分代码了。</li></ul><h2 id="3-2-lookup-join-原理"><a href="#3-2-lookup-join-原理" class="headerlink" title="3.2.lookup join 原理"></a>3.2.lookup join 原理</h2><h3 id="3-2-1-transformation"><a href="#3-2-1-transformation" class="headerlink" title="3.2.1.transformation"></a>3.2.1.transformation</h3><p>在实现 batch lookup join 之前，当然要从原生的 lookup join 的实现开始入手，看看 flink 官方大大是怎么实现的，具体 transformation 如下图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/17_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9Aflinksqlbatchjoin/4.png" alt="transformation"></p><p>具体的实现逻辑承载在 <code>org.apache.flink.streaming.api.operators.ProcessOperator</code>，<code>org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner</code> 中。</p><h3 id="3-2-2-LookupJoinRunner"><a href="#3-2-2-LookupJoinRunner" class="headerlink" title="3.2.2.LookupJoinRunner"></a>3.2.2.LookupJoinRunner</h3><p>LookupJoinRunner 中的数据处理逻辑集中在 <code>processElement</code> 中。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/17_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9Aflinksqlbatchjoin/5.png" alt="LookupJoinRunner"></p><p>可以看到上图，LookupJoinRunner 又内嵌了一层 fetcher 来实现具体的 lookup 逻辑。</p><ol><li>其中 fetcher：就是根据 flink sql lookup join 逻辑生成的 lookup join 的代码实例；</li><li>其中 collector：collector 的主要功能就是将<code>原始数据 RowData</code> 和 <code>lookup 到的 RowData</code> 的数据合并为 <code>JoinedRowData</code> 结果，然后输出。</li></ol><p>接下来详细看看 fetcher 和 collector。</p><h3 id="3-2-3-fetcher"><a href="#3-2-3-fetcher" class="headerlink" title="3.2.3.fetcher"></a>3.2.3.fetcher</h3><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/17_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9Aflinksqlbatchjoin/6.png" alt="transformation fetcher"></p><p>把这个 fetcher 的代码 copy 出来瞅瞅。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/17_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9Aflinksqlbatchjoin/8.png" alt="fetcher"></p><p>fetcher 内嵌了 <code>RedisRowDataLookupFunction</code> 来作为最终访问外部维表的函数。</p><h3 id="3-2-4-RedisRowDataLookupFunction"><a href="#3-2-4-RedisRowDataLookupFunction" class="headerlink" title="3.2.4.RedisRowDataLookupFunction"></a>3.2.4.RedisRowDataLookupFunction</h3><p>访问 redis 获取到数据。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/17_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9Aflinksqlbatchjoin/17.png" alt="RedisRowDataLookupFunction"></p><h3 id="3-2-5-collector"><a href="#3-2-5-collector" class="headerlink" title="3.2.5.collector"></a>3.2.5.collector</h3><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/17_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9Aflinksqlbatchjoin/7.png" alt="transformation collector"></p><p>把这个 collector 的代码 copy 出来瞅瞅。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/17_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9Aflinksqlbatchjoin/9.png" alt="collector"></p><h2 id="3-3-lookup-join-算子实现调用链"><a href="#3-3-lookup-join-算子实现调用链" class="headerlink" title="3.3.lookup join 算子实现调用链"></a>3.3.lookup join 算子实现调用链</h2><p>是不是感觉一个 lookup join 的调用链贼复杂。</p><p>因为 batch lookup join 是完全参考 lookup join 去实现的，所以接下来博主介绍一下整体的调用链关系，这就会方便后续设计 batch lookup join 实现方案的时候去确定具体修改哪一部分代码。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/17_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9Aflinksqlbatchjoin/10.png" alt="collector"></p><p>整体的调用逻辑如下：</p><ol><li><code>ProcessOpeartor</code> 把 <code>原始 RowData</code> 传给 <code>LookupJoinRunner</code></li><li><code>LookupJoinRunner</code> 把 <code>原始 RowData</code> 传给根据 sql 代码生成的 <code>fetcher</code></li><li><code>fetcher</code> 中把 <code>原始 RowData</code> 传给 <code>RedisRowDataLookupFunction</code> 然后去 lookup 维表，lookup 到的结果数据为 <code>lookup RowData</code></li><li><code>collector</code> 把 <code>原始 RowData</code> 和 <code>lookup RowData</code> 数据合并为 <code>JoinedRowData</code> 然后输出。</li></ol><h2 id="3-4-batch-lookup-join-设计思路"><a href="#3-4-batch-lookup-join-设计思路" class="headerlink" title="3.4.batch lookup join 设计思路"></a>3.4.batch lookup join 设计思路</h2><p>还是一样，先看看设计思路最终的结论，batch lookup join 算子调用链设计如下：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/17_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9Aflinksqlbatchjoin/11.png" alt="batch lookup"></p><p>详细说明一下设计思路：</p><ol><li>如果想做到批量访问外部存储（Redis）的数据。可以推断出 <code>RedisRowDataLookupFunction</code> 的输入需要是 <code>List&lt;原始 RowData&gt;</code> ，输出需要是 <code>List&lt;lookup RowData&gt;</code>。<br>其中输入数据输入到 <code>RedisRowDataLookupFunction</code> 中后，使用 Redis pipeline 去批量访问外部存储，然后把结果 <code>List&lt;lookup RowData&gt;</code> 输出。</li><li>由 <code>RedisRowDataLookupFunction</code> 的输出数据为 <code>List&lt;lookup RowData&gt;</code> 推断出 <code>collector</code> 输入数据格式必然是 <code>List&lt;原始 RowData&gt;</code>。由于在 lookup join 中 <code>collector</code> 的逻辑就是将 <code>原始 RowData</code> 和 <code>lookup RowData</code> 合并为 <code>JoinedRowData</code>，将结果输出。<br>因此 <code>collector</code> 这里就是将 <code>List&lt;原始 RowData&gt;</code> 和 <code>List&lt;lookup RowData&gt;</code> 进行遍历合并，一条一条的输出 <code>JoinedRowData</code>。</li><li>同样 <code>RedisRowDataLookupFunction</code> 的输入数据是 <code>fetcher</code> 传入的，则推断出 <code>fetcher</code> 输入数据格式必然是 <code>List&lt;原始 RowData&gt;</code>。</li><li>由于 <code>fetcher</code> 输入是 <code>List&lt;原始 RowData&gt;</code>，则 <code>LookupJoinRunner</code> 输出到 <code>fetcher</code> 的数据也需要是 <code>List&lt;原始 RowData&gt;</code>。<br>但是 <code>ProcessOpeartor</code> 只能传给 <code>LookupJoinRunner</code> <code>原始 RowData</code>，因此可以得出我们的<code>每攒 30 条数据</code>或者<code>每隔 5s</code> 的逻辑就能确定需要在 <code>LookupJoinRunner</code> 中做了。</li></ol><p>思路有了，那么 batch lookup join 涉及到的改动项也就能确认了。</p><ol><li>新建一个 <code>BatchLookupJoinRunner</code>：实现攒批逻辑（<code>每攒 30 条数据</code>或者<code>每隔 5s</code>），其中攒批的数据放在 ListState 中，以防止丢失，在 table config 中的 <code>is.dim.batch.mode</code> 设置为 true 时使用此 <code>BatchLookupJoinRunner</code>。</li><li>代码生成的 <code>fetcher</code>：将原来输入的 <code>原始 RowData</code> 改为 <code>List&lt;原始 RowData&gt;</code>。</li><li>新建一个 <code>RedisRowDataBatchLookupFunction</code>：实现将输入的批量数据 <code>List&lt;原始 RowData&gt;</code> 拿到之后使用 redis pipeline 批量访问外部存储，获取到 <code>List&lt;lookup RowData&gt;</code> 结果数据给 <code>collector</code>。</li><li>代码生成的 <code>collector</code>：将原来 lookup join 中的输入 <code>原始 RowData</code>，<code>lookup RowData</code> 改为 <code>List&lt;原始 RowData&gt;</code>，<code>List&lt;lookup RowData&gt;</code>，添加遍历循环 <code>List&lt;原始 RowData&gt;</code>，<code>List&lt;lookup RowData&gt;</code>，按顺序合并 List 中的每一项 <code>原始 RowData</code>，<code>lookup RowData</code> 输出 <code>JoinedRowData</code> 的逻辑。</li></ol><h2 id="3-5-batch-lookup-join-的最终效果"><a href="#3-5-batch-lookup-join-的最终效果" class="headerlink" title="3.5.batch lookup join 的最终效果"></a>3.5.batch lookup join 的最终效果</h2><h3 id="3-5-1-transformation"><a href="#3-5-1-transformation" class="headerlink" title="3.5.1.transformation"></a>3.5.1.transformation</h3><p>可以看到 <code>is.dim.batch.mode</code> 设置为 true 时，transformation 如下。transformation 中的重点处理逻辑就是 <code>BatchLookupJoinRunner</code></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/17_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9Aflinksqlbatchjoin/12.png" alt="batch transformation"></p><h3 id="3-5-2-BatchLookupJoinRunner"><a href="#3-5-2-BatchLookupJoinRunner" class="headerlink" title="3.5.2.BatchLookupJoinRunner"></a>3.5.2.BatchLookupJoinRunner</h3><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/17_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9Aflinksqlbatchjoin/13.png" alt="BatchLookupJoinRunner"></p><h3 id="3-5-3-fetcher"><a href="#3-5-3-fetcher" class="headerlink" title="3.5.3.fetcher"></a>3.5.3.fetcher</h3><p>sql 生成的 fetcher 代码如下：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/17_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9Aflinksqlbatchjoin/14.png" alt="fetcher"></p><h3 id="3-5-4-RedisRowDataBatchLookupFunction"><a href="#3-5-4-RedisRowDataBatchLookupFunction" class="headerlink" title="3.5.4.RedisRowDataBatchLookupFunction"></a>3.5.4.RedisRowDataBatchLookupFunction</h3><p>RedisRowDataBatchLookupFunction 拿到输入的 List 数据，调用 Redis pipeline 批量访问外部存储。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/17_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9Aflinksqlbatchjoin/18.png" alt="RedisRowDataBatchLookupFunction"></p><h3 id="3-5-5-collector"><a href="#3-5-5-collector" class="headerlink" title="3.5.5.collector"></a>3.5.5.collector</h3><p>sql 生成的 collector 代码如下：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/17_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9Aflinksqlbatchjoin/16.png" alt="collector"></p><h2 id="3-6-待改进项"><a href="#3-6-待改进项" class="headerlink" title="3.6.待改进项"></a>3.6.待改进项</h2><p>目前上述方案实现的不足之处如下：</p><ol><li>batch 的执行逻辑与 sql 原始的语义不一致。因为从 sql 上看是完全没有这种 batch lookup join 的语义的。</li><li>其中<code>每 5s</code>博主简单实现了下，完全基于数据驱动的每 5s 攒一批，不是基于 onTimer 驱动的。可能会出现来了一条数据之后，5 min 内都没有来数据，则数据就不输出了。</li><li>没有考虑实现代码的抽象，以实现功能为主，所以很多基于源码的改动都是直接 copy 出来了另一个方法实现。</li></ol><h1 id="4-xdm-怎么使用这个功能？"><a href="#4-xdm-怎么使用这个功能？" class="headerlink" title="4.xdm 怎么使用这个功能？"></a>4.xdm 怎么使用这个功能？</h1><ol><li>git clone <a href="https://github.com/yangyichao-mango/flink/tree/release-1.13.2">https://github.com/yangyichao-mango/flink/tree/release-1.13.2</a></li><li>在 clone 下来的项目的中，重新把下面两个模块 install (mvn clean install) 到本地仓库中。<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/17_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%EF%BC%9Aflinksqlbatchjoin/19.png" alt="blink"></li><li>然后在你的项目中引用两个 blink 包即可使用。使用方法就是只需要把 table config 的 <code>is.dim.batch.mode</code> 设置为 true，代码还按照 lookup join 的方式写即可。</li></ol><h1 id="4-总结与展望"><a href="#4-总结与展望" class="headerlink" title="4.总结与展望"></a>4.总结与展望</h1><p>源码公众号后台回复<strong>1.13.2 sql batch lookup join</strong>获取。</p><p>本文主要介绍了 flink sql batch lookup join 的使用方式，并介绍了其实现思路以及效果，主要内容如下：</p><ol><li><strong>直接来一个实战案例</strong>：博主以曝光用户日志流关联用户画像（年龄、性别）维表为例介绍 batch lookup join 具有的基本能力（怎么配置参数，怎么写 sql，最终效果咋样）。</li><li><strong>batch lookup join</strong>：主要介绍 batch lookup join 的功能是从 flink transformation 出发，确定要 batch lookup join 涉及改动的地方以及其实现思路、原理。也会教给大家一些改动源码来实现自己想要的一些功能的思路。</li><li><strong>总结及展望</strong>：目前的 batch lookup join 实现其实不符合 sql 的原始语义，后续大家可以按照 sql 标准自己做一些实现</li></ol>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink sql 知其所以然（十六）：flink sql 开发企业级利器之 Dlink</title>
    <link href="https://yangyichao-mango.github.io/2021/11/15/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/17_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89%EF%BC%9Aflink-sql-%E5%BC%80%E5%8F%91%E5%88%A9%E5%99%A8%E4%B9%8Bdlink/"/>
    <id>https://yangyichao-mango.github.io/2021/11/15/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/17_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89%EF%BC%9Aflink-sql-%E5%BC%80%E5%8F%91%E5%88%A9%E5%99%A8%E4%B9%8Bdlink/</id>
    <published>2021-11-15T07:26:59.000Z</published>
    <updated>2021-12-05T13:35:32.857Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-序篇"><a href="#1-序篇" class="headerlink" title="1.序篇"></a>1.序篇</h1><p>博主这个系列都是讲 flink sql 相关的实践的。</p><p>讲到这个章节，其实挺多常用的 flink sql 语法及实战案例都已经讲了。</p><p>那么原理讲了，得在自己家公司把 flink sql 这等好东西用起来啊。</p><p>搞大数据开发的同学基本都知道在 HUE 上面写 hive sql 贼爽。那么有没有写 flink sql 的企业级的 web IDE 推荐的呢？</p><p>经过博主调研之后，发现有两款非常优秀的利器：</p><ul><li>Apache Zeppelin</li><li>Dlink</li></ul><p>为啥先介绍 Dlink 呢？</p><p>因为博主和其开发人员混的很熟了，所以就先拿 Dlink 来尝试尝试。</p><p>废话不多说，大家都想先看效果再看怎么部署。先看看最终效果。</p><h1 id="2-Dlink-平台效果"><a href="#2-Dlink-平台效果" class="headerlink" title="2.Dlink 平台效果"></a>2.Dlink 平台效果</h1><h2 id="2-1-登录"><a href="#2-1-登录" class="headerlink" title="2.1.登录"></a>2.1.登录</h2><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/18_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89%EF%BC%9Aflinksql-dlink/9.png" alt="登录"></p><p>登录账号和密码默认为 admin/admin。</p><h2 id="2-2-flink-sql-开发界面"><a href="#2-2-flink-sql-开发界面" class="headerlink" title="2.2.flink sql 开发界面"></a>2.2.flink sql 开发界面</h2><p>具体功能如下图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/18_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89%EF%BC%9Aflinksql-dlink/10.png" alt="sql 开发"></p><blockquote><p>注意：</p><p>预跑就是用于快速验证 sql 是否是正确的，可以快速以 standalone 模式跑出来一个结果。肥肠地好用。<br>想看到预跑的结果，在 sql 中不能写 insert into xxx 这段。</p></blockquote><p>可以看到是一个功能很齐全的 web IDE。</p><p>接下来我们看看怎么安装部署 Dlink 0.4.0。</p><h1 id="3-安装部署篇"><a href="#3-安装部署篇" class="headerlink" title="3.安装部署篇"></a>3.安装部署篇</h1><h2 id="3-1-Dlink-的-github"><a href="#3-1-Dlink-的-github" class="headerlink" title="3.1.Dlink 的 github"></a>3.1.Dlink 的 github</h2><p>源码直接去 github 上看，已经开源了，链接如下。</p><p><a href="https://github.com/DataLinkDC/dlink">https://github.com/DataLinkDC/dlink</a></p><h2 id="3-2-部署环境准备"><a href="#3-2-部署环境准备" class="headerlink" title="3.2.部署环境准备"></a>3.2.部署环境准备</h2><p>此部署示例是在 Mac OS 上进行，其他环境未测试。</p><table><thead><tr><th>环境</th><th>版本</th><th>备注</th></tr></thead><tbody><tr><td>jdk</td><td>1.8.0_201</td><td>web 基础环境</td></tr><tr><td>mysql</td><td>8.0+</td><td>存储 web IDE 作业，集群等信息</td></tr><tr><td>nginx</td><td>博主使用的是 1.21.1</td><td>web 前端访问</td></tr></tbody></table><h2 id="3-3-下载解压-Dlink-安装包"><a href="#3-3-下载解压-Dlink-安装包" class="headerlink" title="3.3.下载解压 Dlink 安装包"></a>3.3.下载解压 Dlink 安装包</h2><p>博主是基于 Dlink 0.4.0 版本部署安装的。</p><p>第一步：下载 Dlink 0.4.0 安装包。</p><p>我们打开 Dlink 0.4.0 release Notes 看看，链接如下：</p><p><a href="https://github.com/DataLinkDC/dlink/releases/tag/0.4.0">https://github.com/DataLinkDC/dlink/releases/tag/0.4.0</a></p><p>打开上述 0.4.0 release Notes 链接后，点击下图中的 <code>dlink-release-0.4.0.tar.gz</code> 下载 Dlink 0.4.0 的安装包。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/18_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89%EF%BC%9Aflinksql-dlink/8.png" alt="Dlink 下载"></p><p>第二步：解压 Dlink 0.4.0 安装包看看。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/18_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89%EF%BC%9Aflinksql-dlink/1.png" alt="Dlink"></p><p>解压后得到的 <code>dlink-release-0.4.0</code> 目录结构如下：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">config/ -- 配置文件</span><br><span class="line">|- application.yml</span><br><span class="line">lib/ -- 外部依赖及Connector</span><br><span class="line">|- dlink-client-1.13-0.4.0.jar</span><br><span class="line">|- dlink-connector-jdbc-1.13-0.4.0.jar</span><br><span class="line">|- dlink-function-0.4.0.jar</span><br><span class="line">|- dlink-metadata-clickhouse-0.4.0.jar</span><br><span class="line">|- dlink-metadata-mysql-0.4.0.jar</span><br><span class="line">|- dlink-metadata-oracle-0.4.0.jar</span><br><span class="line">|- dlink-metadata-postgresql-0.4.0.jar</span><br><span class="line">sql/</span><br><span class="line">|- dlink.sql -- Mysql初始化脚本</span><br><span class="line">auto.sh -- 启动停止脚本</span><br><span class="line">dlink-admin-0.4.0.jar -- 程序包</span><br></pre></td></tr></table></figure><h2 id="3-4-配置-Dlink-MySQL"><a href="#3-4-配置-Dlink-MySQL" class="headerlink" title="3.4.配置 Dlink MySQL"></a>3.4.配置 Dlink MySQL</h2><p>既然是一个 web IDE，必然会存储一些 web 应用相关的信息。这些信息就是存储在 MySQL 中的。</p><p>第一步：创建 MySQL <code>dlink</code> 库。</p><p>使用 MySQL-cli 连接 MySQL 创建库。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; mysql -u用户名 -p密码</span><br><span class="line"></span><br><span class="line">mysql&gt; create database dlink;</span><br><span class="line">mysql&gt; show databases;</span><br></pre></td></tr></table></figure><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/18_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89%EF%BC%9Aflinksql-dlink/4.png" alt="Dlink MySQL"></p><p>第二步：使用 Dlink 的 sql 脚本（<code>dlink 目录\sql 目录\dlink.sql</code>）初始化 Dlink 数据库表信息，具体初始化命令行如下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; mysql -h localhost -u用户名 -p密码 -Ddlink &lt; dlink.sql</span><br><span class="line">&gt; mysql -u用户名 -p密码</span><br><span class="line">mysql&gt; use dlink;</span><br><span class="line">mysql&gt; show tables;</span><br></pre></td></tr></table></figure><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/18_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89%EF%BC%9Aflinksql-dlink/5.png" alt="Dlink MySQL Table"></p><p>第三步：Dlink 也是个 web 项目，用的是 SpringBoot 那一套东西，所以连接 MySQL 得需要进行 Spring 相关的配置。</p><p>所以需要去 <code>dlink 目录\config 目录\application.yml</code> 中修改 MySQL 相关的配置。由于博主是在本地部署。所以涉及到改动的只有用户名和密码，改完之后保存。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/18_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89%EF%BC%9Aflinksql-dlink/6.png" alt="application.yml"></p><h2 id="3-5-启动-web-后端"><a href="#3-5-启动-web-后端" class="headerlink" title="3.5.启动 web 后端"></a>3.5.启动 web 后端</h2><p>Dlink 是前后端分离的，刚刚我们配置了 MySQL 相关的环境，则可以直接启动后端了，后端占用的端口是 8888，启动命令如下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh auto.sh start</span><br></pre></td></tr></table></figure><p>在 Mac OS 下启动可能会遇到下面的问题：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/18_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89%EF%BC%9Aflinksql-dlink/12.png" alt="bash error"></p><p>这个错误的原因如下链接：</p><p><a href="https://jingyan.baidu.com/article/9f63fb91d014b8c8410f0e7a.html">https://jingyan.baidu.com/article/9f63fb91d014b8c8410f0e7a.html</a></p><p>解决方案如下：</p><p>直接把 <code>auto.sh</code> 在其他编辑器中重新复制出来一个 <code>auto1.sh</code> 启动就可以。</p><p>其他命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 停止</span><br><span class="line">sh auto.sh stop</span><br><span class="line"># 重启</span><br><span class="line">sh auto.sh restart</span><br><span class="line"># 状态</span><br><span class="line">sh auto.sh status</span><br></pre></td></tr></table></figure><p>运行日志：</p><p>控制台输出：项目根目录下的 dlink.log 文件。</p><p>日志归档输出：项目根目录下的 logs 目录下。</p><h2 id="3-6-配置-web-前端"><a href="#3-6-配置-web-前端" class="headerlink" title="3.6.配置 web 前端"></a>3.6.配置 web 前端</h2><p>前端都是一些静态文件，Dlink 使用 Nginx 作为访问前端静态文件的服务器。</p><p>第一步：Nginx 在 Mac OS 的安装，如下链接：</p><p><a href="https://www.jianshu.com/p/4f433d219ab7">https://www.jianshu.com/p/4f433d219ab7</a></p><p>第二步：在 nginx.conf 文件中配置 Dlink 的 server 信息。</p><p>本地安装的话，直接把下面这段 copy 到 nginx.conf 中对应的 server 配置下就行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen       9999;</span><br><span class="line">    server_name localhost;</span><br><span class="line">    </span><br><span class="line">    # gzip config</span><br><span class="line">    gzip on;</span><br><span class="line">    gzip_min_length 1k;</span><br><span class="line">    gzip_comp_level 9;</span><br><span class="line">    gzip_types text&#x2F;plain application&#x2F;javascript application&#x2F;x-javascript text&#x2F;css application&#x2F;xml text&#x2F;javascript application&#x2F;x-httpd-php image&#x2F;jpeg image&#x2F;gif image&#x2F;png;</span><br><span class="line">    gzip_vary on;</span><br><span class="line">    gzip_disable &quot;MSIE [1-6]\.&quot;;</span><br><span class="line">    </span><br><span class="line">    #charset koi8-r;</span><br><span class="line">    </span><br><span class="line">    #access_log logs&#x2F;host.access.log main;</span><br><span class="line">    </span><br><span class="line">    location &#x2F; &#123;</span><br><span class="line">        root   html;</span><br><span class="line">        index index.html index.htm;</span><br><span class="line">        try_files $uri $uri&#x2F; &#x2F;index.html;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    #error_page 404             &#x2F;404.html;</span><br><span class="line">    </span><br><span class="line">    # redirect server error pages to the static page &#x2F;50x.html</span><br><span class="line">    #</span><br><span class="line">    error_page   500 502 503 504 &#x2F;50x.html;</span><br><span class="line">    location &#x3D; &#x2F;50x.html &#123;</span><br><span class="line">        root   html;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    location ^~ &#x2F;api &#123;</span><br><span class="line">        proxy_pass http:&#x2F;&#x2F;127.0.0.1:8888;</span><br><span class="line">        proxy_set_header   X-Forwarded-Proto $scheme;</span><br><span class="line">        proxy_set_header   X-Real-IP         $remote_addr;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第三步：注意，重点。<code>一定要将 【dlink 目录\html 目录\所有文件】上传至 Nginx 的 [html 目录]下。</code>不然访问 <a href="http://127.0.0.1:9999">http://127.0.0.1:9999</a> 啥都看不到。</p><p>第四步：reload Nginx 配置信息。</p><p>第五步：然后我们就可以 happy 的使用 Dlink 了。</p><h1 id="4-总结及展望"><a href="#4-总结及展望" class="headerlink" title="4.总结及展望"></a>4.总结及展望</h1><p>其他功能博主还在测试中。</p><p>引用 Dlink 官网的介绍，Dlink 将紧跟 Flink 官方社区发展，为推广及发展 Flink 的应用而奋斗，打造 FlinkSQL 的最佳搭档的形象。</p><h1 id="5-Dlink-社区交流"><a href="#5-Dlink-社区交流" class="headerlink" title="5.Dlink 社区交流"></a>5.Dlink 社区交流</h1><p>欢迎您加入社区交流分享与批评，也欢迎您为社区贡献自己的力量。</p><p>QQ社区群：543709668，申请备注 <code>Dlink</code>，不写不批。</p><p>微信社区群（推荐）：添加 <code>wenmo_ai</code> ，申请备注 <code>Dlink</code>，邀请进群。</p><p>公众号：<code>DataLink数据中台</code></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/18_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89%EF%BC%9Aflinksql-dlink/11.png" alt="DataLink数据中台"></p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink sql 知其所以然（十四）：维表 join 的性能优化之路（上）</title>
    <link href="https://yangyichao-mango.github.io/2021/11/15/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/15_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%EF%BC%9Aflink-sql-lookup-join/"/>
    <id>https://yangyichao-mango.github.io/2021/11/15/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/15_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%EF%BC%9Aflink-sql-lookup-join/</id>
    <published>2021-11-15T06:26:59.000Z</published>
    <updated>2021-11-30T15:05:31.957Z</updated>
    
    <content type="html"><![CDATA[<p>看了那么多的技术文，你能明白作者想让你在读完文章后学到什么吗？</p><p>大数据羊说的文章会让你明白</p><ol><li><p>博主会阐明博主期望本文能给小伙伴们带来什么帮助，让小伙伴萌能直观明白博主的心思</p></li><li><p>博主会以实际的应用场景和案例入手，不只是知识点的简单堆砌</p></li><li><p>博主会把重要的知识点的原理进行剖析，让小伙伴萌做到深入浅出</p></li></ol><h1 id="1-序篇"><a href="#1-序篇" class="headerlink" title="1.序篇"></a>1.序篇</h1><p>源码公众号后台回复<strong>1.13.2 sql lookup join</strong>获取。</p><p>废话不多说，咱们先直接上本文的目录和结论，小伙伴可以先看结论快速了解博主期望本文能给小伙伴们带来什么帮助：</p><ol><li><strong>背景及应用场景介绍</strong>：博主期望你能了解到，flink sql 提供了轻松访问<strong>外部存储</strong>的 lookup join（与上节不同，上节说的是流与流的 join）。lookup join 可以简单理解为使用 flatmap 访问外部存储数据然后将维度字段拼接到当前这条数据上面</li><li><strong>来一个实战案例</strong>：博主以曝光用户日志流关联用户画像（年龄、性别）维表为例介绍 lookup join 应该达到的关联的预期效果。</li><li><strong>flink sql lookup join 的解决方案以及原理的介绍</strong>：主要介绍 lookup join 的在上述实战案例的 sql 写法，博主期望你能了解到，lookup join 是基于处理时间的，并且 lookup join 经常会由于访问外部存储的 qps 过高而导致背压，产出延迟等性能问题。<br>我们可以借鉴在 DataStream api 中的维表 join 优化思路在 flink sql 使用 <code>local cache</code>，<code>异步访问维表</code>，<code>批量访问维表</code>三种方式去解决性能问题。</li><li><strong>总结及展望</strong>：官方并没有提供 <code>批量访问维表</code> 的能力，因此博主自己实现了一套，具体使用方式和原理实现敬请期待下篇文章。</li></ol><h1 id="2-背景及应用场景介绍"><a href="#2-背景及应用场景介绍" class="headerlink" title="2.背景及应用场景介绍"></a>2.背景及应用场景介绍</h1><p>维表作为 sql 任务中一种常见表的类型，其本质就是关联表数据的额外数据属性，通常在 join 语句中进行使用。比如源数据有人的 id，你现在想要得到人的性别、年龄，那么可以通过用户 id 去关联人的性别、年龄，就可以得到更全的数据。</p><p>维表 join 在离线数仓中是最常见的一种数据处理方式了，在实时数仓的场景中，flink sql 目前也支持了维表的 join，即 lookup join，生产环境可以用 mysql，redis，hbase 来作为高速维表存储引擎。</p><blockquote><p>Notes：</p><p>在实时数仓中，常用实时维表有两种更新频率</p><ol><li>实时的更新：维度信息是实时新建的，实时写入到高速存储引擎中。然后其他实时任务在做处理时实时的关联这些维度信息。</li><li>周期性的更新：对于一些缓慢变化维度，比如年龄、性别的用户画像等，几万年都不变化一次的东西😂，实时维表的更新可以是小时级别，天级别的。</li></ol></blockquote><h1 id="3-来一个实战案例"><a href="#3-来一个实战案例" class="headerlink" title="3.来一个实战案例"></a>3.来一个实战案例</h1><p>来看看在具体场景下，对应输入值的输出值应该长啥样。</p><p>需求指标：使用曝光用户日志流（show_log）关联用户画像维表（user_profile）关联到用户的维度之后，提供给下游计算分性别，年龄段的曝光用户数使用。<br>此处我们只关心关联维表这一部分的输入输出数据。</p><p>来一波输入数据：</p><p><strong>曝光用户日志流（show_log）数据（数据存储在 kafka 中）：</strong></p><table><thead><tr><th>log_id</th><th>timestamp</th><th>user_id</th></tr></thead><tbody><tr><td>1</td><td>2021-11-01 00:01:03</td><td>a</td></tr><tr><td>2</td><td>2021-11-01 00:03:00</td><td>b</td></tr><tr><td>3</td><td>2021-11-01 00:05:00</td><td>c</td></tr><tr><td>4</td><td>2021-11-01 00:06:00</td><td>b</td></tr><tr><td>5</td><td>2021-11-01 00:07:00</td><td>c</td></tr></tbody></table><p><strong>用户画像维表（user_profile）数据（数据存储在 redis 中）：</strong></p><table><thead><tr><th>user_id(主键)</th><th>age</th><th>sex</th></tr></thead><tbody><tr><td>a</td><td>12-18</td><td>男</td></tr><tr><td>b</td><td>18-24</td><td>女</td></tr><tr><td>c</td><td>18-24</td><td>男</td></tr></tbody></table><p>注意：redis 中的数据结构存储是按照 key，value 去存储的。其中 key 为 user_id，value 为 age，sex 的 json。如下图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/16_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%EF%BC%9Aflink_sql_lookup_join/2.png" alt="user_profile redis"></p><p>预期输出数据如下：</p><table><thead><tr><th>log_id</th><th>timestamp</th><th>user_id</th><th>age</th><th>sex</th></tr></thead><tbody><tr><td>1</td><td>2021-11-01 00:01:03</td><td>a</td><td>12-18</td><td>男</td></tr><tr><td>2</td><td>2021-11-01 00:03:00</td><td>b</td><td>18-24</td><td>女</td></tr><tr><td>3</td><td>2021-11-01 00:05:00</td><td>c</td><td>18-24</td><td>男</td></tr><tr><td>4</td><td>2021-11-01 00:06:00</td><td>b</td><td>18-24</td><td>女</td></tr><tr><td>5</td><td>2021-11-01 00:07:00</td><td>c</td><td>18-24</td><td>男</td></tr></tbody></table><p>flink sql lookup join 登场。下面是官网的链接。</p><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/sql/queries/joins/#lookup-join">https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/sql/queries/joins/#lookup-join</a></p><h1 id="4-flink-sql-lookup-join"><a href="#4-flink-sql-lookup-join" class="headerlink" title="4.flink sql lookup join"></a>4.flink sql lookup join</h1><h2 id="4-1-lookup-join-定义"><a href="#4-1-lookup-join-定义" class="headerlink" title="4.1.lookup join 定义"></a>4.1.lookup join 定义</h2><p>以上述案例来说，lookup join 其实简单理解来，就是每来一条数据去 redis 里面搂一次数据。然后把关联到的维度数据给拼接到当前数据中。</p><p>熟悉 DataStream api 的小伙伴萌，简单来理解，就是 lookup join 的算子就是 DataStream api 中的 flatmap 算子中处理每一条来的数据，针对每一条数据去访问用户画像的 redis。（实际上，flink sql api 中也确实是这样实现的！sql 生成的 lookup join 代码就是继承了 flatmap）</p><h2 id="4-2-上述案例解决方案"><a href="#4-2-上述案例解决方案" class="headerlink" title="4.2.上述案例解决方案"></a>4.2.上述案例解决方案</h2><p>来看看上述案例的 flink sql lookup join sql 怎么写：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> show_log (</span><br><span class="line">    log_id <span class="type">BIGINT</span>,</span><br><span class="line">    `<span class="type">timestamp</span>` <span class="keyword">as</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    user_id STRING,</span><br><span class="line">    proctime <span class="keyword">AS</span> PROCTIME()</span><br><span class="line">)</span><br><span class="line"><span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.log_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_profile (</span><br><span class="line">    user_id STRING,</span><br><span class="line">    age STRING,</span><br><span class="line">    sex STRING</span><br><span class="line">    ) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;redis&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;hostname&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;127.0.0.1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;port&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;6379&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;lookup.cache.max-rows&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;500&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;lookup.cache.ttl&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;3600&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;lookup.max-retries&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    log_id <span class="type">BIGINT</span>,</span><br><span class="line">    `<span class="type">timestamp</span>` <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    user_id STRING,</span><br><span class="line">    proctime <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    age STRING,</span><br><span class="line">    sex STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- lookup join 的 query 逻辑</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    s.log_id <span class="keyword">as</span> log_id</span><br><span class="line">    , s.`<span class="type">timestamp</span>` <span class="keyword">as</span> `<span class="type">timestamp</span>`</span><br><span class="line">    , s.user_id <span class="keyword">as</span> user_id</span><br><span class="line">    , s.proctime <span class="keyword">as</span> proctime</span><br><span class="line">    , u.sex <span class="keyword">as</span> sex</span><br><span class="line">    , u.age <span class="keyword">as</span> age</span><br><span class="line"><span class="keyword">FROM</span> show_log <span class="keyword">AS</span> s</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> user_profile <span class="keyword">FOR</span> <span class="built_in">SYSTEM_TIME</span> <span class="keyword">AS</span> <span class="keyword">OF</span> s.proctime <span class="keyword">AS</span> u</span><br><span class="line"><span class="keyword">ON</span> s.user_id <span class="operator">=</span> u.user_id</span><br></pre></td></tr></table></figure><p>这里使用了 <code>for SYSTEM_TIME as of</code> 时态表的语法来作为维表关联的标识语法。</p><blockquote><p>Notes：</p><p>实时的 lookup 维表关联能使用处理时间去做关联。</p></blockquote><p>运行结果如下：</p><table><thead><tr><th>log_id</th><th>timestamp</th><th>user_id</th><th>age</th><th>sex</th></tr></thead><tbody><tr><td>1</td><td>2021-11-01 00:01:03</td><td>a</td><td>12-18</td><td>男</td></tr><tr><td>2</td><td>2021-11-01 00:03:00</td><td>b</td><td>18-24</td><td>女</td></tr><tr><td>3</td><td>2021-11-01 00:05:00</td><td>c</td><td>18-24</td><td>男</td></tr><tr><td>4</td><td>2021-11-01 00:06:00</td><td>b</td><td>18-24</td><td>女</td></tr><tr><td>5</td><td>2021-11-01 00:07:00</td><td>c</td><td>18-24</td><td>男</td></tr></tbody></table><p>flink web ui 算子图如下：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/16_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%EF%BC%9Aflink_sql_lookup_join/3.png" alt="user_profile redis"></p><p>但是！！！<br>但是！！！<br>但是！！！</p><p>flink 官方并没有提供 redis 的维表 connector 实现。</p><p>没错，博主自己实现了一套。关于 redis 维表的 connector 实现，直接参考下面的文章。都是可以从 github 上找到源码拿来用的！</p><p>TODO</p><h2 id="4-3-关于维表使用的一些注意事项"><a href="#4-3-关于维表使用的一些注意事项" class="headerlink" title="4.3.关于维表使用的一些注意事项"></a>4.3.关于维表使用的一些注意事项</h2><ol><li><strong>同一条数据关联到的维度数据可能不同</strong>：实时数仓中常用的<strong>实时维表</strong>都是在不断的变化中的，当前流表数据关联完维表数据后，如果同一个 key 的维表的数据发生了变化，已关联到的维表的结果数据<strong>不会</strong>再同步更新。<br>举个例子，维表中 user_id 为 1 的数据在 08：00 时 age 由 12-18 变为了 18-24，那么当我们的任务在 08：01 failover 之后从 07：59 开始回溯数据时，原本应该关联到 12-18 的数据会关联到 18-24 的 age 数据。这是有可能会影响数据质量的。<br>所以小伙伴萌在评估你们的实时任务时要考虑到这一点。</li><li><strong>会发生实时的新建及更新的维表博主建议小伙伴萌应该建立起数据延迟的监控机制，防止出现流表数据先于维表数据到达，导致关联不到维表数据</strong></li></ol><h2 id="4-4-再说说维表常见的性能问题及优化思路"><a href="#4-4-再说说维表常见的性能问题及优化思路" class="headerlink" title="4.4.再说说维表常见的性能问题及优化思路"></a>4.4.再说说维表常见的性能问题及优化思路</h2><p>所有的维表性能问题都可以总结为：高 qps 下访问维表存储引擎产生的任务背压，数据产出延迟问题。</p><p>举个例子：</p><ul><li><strong>在没有使用维表的情况下</strong>：一条数据从输入 flink 任务到输出 flink 任务的时延假如为 <code>0.1 ms</code>，那么并行度为 <code>1</code> 的任务的吞吐可以达到 <code>1 query / 0.1 ms = 1w qps</code>。</li><li><strong>在使用维表之后</strong>：每条数据访问维表的外部存储的时长为 <code>2 ms</code>，那么一条数据从输入 flink 任务到输出 flink 任务的时延就会变成 <code>2.1 ms</code>，那么同样并行度为 <code>1</code> 的任务的吞吐只能达到 <code>1 query / 2.1 ms = 476 qps</code>。两者的吞吐量相差 <code>21 倍</code>。</li></ul><p>这就是为什么维表 join 的算子会产生背压，任务产出会延迟。</p><p>那么当然，解决方案也是有很多的。抛开 flink sql 想一下，如果我们使用 DataStream api，甚至是在做一个后端应用，需要访问外部存储时，常用的优化方案有哪些？这里列举一下：</p><ol><li><strong>按照 redis 维表的 key 分桶 + local cache</strong>：通过按照 key 分桶的方式，让大多数据的维表关联的数据访问走之前访问过得 local cache 即可。这样就可以把访问外部存储 2.1 ms 处理一个 query 变为访问内存的 0.1 ms 处理一个 query 的时长。</li><li><strong>异步访问外存</strong>：DataStream api 有异步算子，可以利用线程池去同时多次请求维表外部存储。这样就可以把 2.1 ms 处理 1 个 query 变为 2.1 ms 处理 10 个 query。吞吐可变优化到 <code>10 / 2.1 ms = 4761 qps</code>。</li><li><strong>批量访问外存</strong>：除了异步访问之外，我们还可以批量访问外部存储。举一个例子：在访问 redis 维表的 1 query 占用 2.1 ms 时长中，其中可能有 2 ms 都是在网络请求上面的耗时<br>，其中只有 0.1 ms 是 redis server 处理请求的时长。那么我们就可以使用 redis 提供的 pipeline 能力，在客户端（也就是 flink 任务 lookup join 算子中），攒一批数据，使用 pipeline 去同时访问 redis sever。<br>这样就可以把 2.1 ms 处理 1 个 query 变为 7ms（2ms + 50 * 0.1ms） 处理 50 个 query。吞吐可变为 <code>50 query / 7 ms = 7143 qps</code>。博主这里测试了下使用 redis pipeline 和未使用的时长消耗对比。如下图所示。</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/16_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%EF%BC%9Aflink_sql_lookup_join/1.png" alt="redis pipeline"></p><p>博主认为上述优化效果中，最好用的是 1 + 3，2 相比 3 还是一条一条发请求，性能会差一些。</p><p>既然 DataStream 可以这样做，flink sql 必须必的也可以借鉴上面的这些优化方案。具体怎么操作呢？看下文骚操作</p><h2 id="4-5-lookup-join-的具体性能优化方案"><a href="#4-5-lookup-join-的具体性能优化方案" class="headerlink" title="4.5.lookup join 的具体性能优化方案"></a>4.5.lookup join 的具体性能优化方案</h2><ol><li><strong>按照 redis 维表的 key 分桶 + local cache</strong>：sql 中如果要做分桶，得先做 group by，但是如果做了 group by 的聚合，就只能在 udaf 中做访问 redis 处理，并且 udaf 产出的结果只能是一条，所以这种实现起来非常复杂。我们选择不做 keyby 分桶。<br>但是我们可以直接使用 local cache 去做本地缓存，虽然【直接缓存】的效果比【先按照 key 分桶再做缓存】的效果差，但是也能一定程度上减少访问 redis 压力。在博主实现的 redis connector 中，内置了 local cache 的实现，小伙伴萌可以参考下面这部篇文章进行配置。<br>TODO</li><li><strong>异步访问外存</strong>：目前博主实现的 redis connector 不支持异步访问，但是官方实现的 hbase connector 支持这个功能，参考下面链接文章的，点开之后搜索 <code>lookup.async</code>。<br><a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/connectors/table/hbase/">https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/connectors/table/hbase/</a></li><li><strong>批量访问外存</strong>：这玩意官方必然没有实现啊，但是，但是，但是，经过博主周末两天的疯狂 debug，改了改源码，搞定了基于 redis 的批量访问外存优化的功能。</li></ol><h2 id="4-6-基于-redis-connector-的批量访问机制优化"><a href="#4-6-基于-redis-connector-的批量访问机制优化" class="headerlink" title="4.6.基于 redis connector 的批量访问机制优化"></a>4.6.基于 redis connector 的批量访问机制优化</h2><p>先描述一下大概是个什么东西，具体怎么用。</p><p>你只需要在 StreamTableEnvironment 中的 table config 配置上 <code>is.dim.batch.mode</code> 为 <code>true</code>，sql 不用做任何改动的情况下，flink lookup join 算子会自动优化，优化效果如下：</p><p>lookup join 算子的每个 task 上，<code>每攒够 30 条数据</code> or <code>每隔五秒（处理时间）</code> 去触发一次批量访问 redis 的请求，使用的是 jedis client 的 pipeline 功能访问 redis server。实测性能有很大提升。</p><p>关于这个批量访问机制的优化介绍和使用方式介绍，小伙伴们先别急，下篇文章会详细介绍到。</p><h1 id="5-总结与展望"><a href="#5-总结与展望" class="headerlink" title="5.总结与展望"></a>5.总结与展望</h1><p>源码公众号后台回复<strong>1.13.2 sql lookup join</strong>获取。</p><p>本文主要介绍了 flink sql lookup join 的使用方式，并介绍了一些经常出现的性能问题以及优化思路，总结如下：</p><ol><li><strong>背景及应用场景介绍</strong>：博主期望你能了解到，flink sql 提供了轻松访问<strong>外部存储</strong>的 lookup join（与上节不同，上节说的是流与流的 join）。lookup join 可以简单理解为使用 flatmap 访问外部存储数据然后将维度字段拼接到当前这条数据上面</li><li><strong>来一个实战案例</strong>：博主以曝光用户日志流关联用户画像（年龄、性别）维表为例介绍 lookup join 应该达到的关联的预期效果。</li><li><strong>flink sql lookup join 的解决方案以及原理的介绍</strong>：主要介绍 lookup join 的在上述实战案例的 sql 写法，博主期望你能了解到，lookup join 是基于处理时间的，并且 lookup join 经常会由于访问外部存储的 qps 过高而导致背压，产出延迟等性能问题。<br>我们可以借鉴在 DataStream api 中的维表 join 优化思路在 flink sql 使用 <code>local cache</code>，<code>异步访问维表</code>，<code>批量访问维表</code>三种方式去解决性能问题。</li><li><strong>总结及展望</strong>：官方并没有提供 <code>批量访问维表</code> 的能力，因此博主自己实现了一套，具体使用方式和原理实现敬请期待下篇文章。</li></ol>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink sql 知其所以然（十二）：流 join 很难嘛？？？（上）</title>
    <link href="https://yangyichao-mango.github.io/2021/11/15/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/13_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%EF%BC%9Aflink-sql-join/"/>
    <id>https://yangyichao-mango.github.io/2021/11/15/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/13_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%EF%BC%9Aflink-sql-join/</id>
    <published>2021-11-15T06:25:59.000Z</published>
    <updated>2021-11-26T01:48:06.149Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>想啥呢，小宝贝，还不三连？？？（关注  +  点赞 + 再看）</strong>，对博主的肯定，会督促博主持续的输出更多的优质实战内容！！！</p></blockquote><h1 id="1-序篇"><a href="#1-序篇" class="headerlink" title="1.序篇"></a>1.序篇</h1><p>源码公众号后台回复<strong>1.13.2 sql join 的奇妙解析之路</strong>获取。</p><p>废话不多说，咱们先直接上本文的目录和结论，小伙伴可以先看结论快速了解博主期望本文能给小伙伴们带来什么帮助：</p><ol><li>背景及应用场景介绍：博主期望你能了解到，flink sql 提供的丰富的 join 方式（总结 6 种：regular join，维表 join，快照 join，interval join，array 拍平，table function）对我们满足需求提供了强大的后盾，<br>这 6 种 join 中涉及到流与流的 join 最常用的是 regular join 以及 interval join</li><li>来一个实战案例：博主以一个曝光日志流 left join 点击日志流为案例展开，介绍离线 hive sql left join 的解决方案及 flink sql left join 的解决方案，主要是想告诉小伙伴 flink sql 也有类似于 hive sql 的 regular join</li><li>flink sql regular join 的解决方案以及存在问题的介绍：主要介绍 regular join 的在上述实战案例的运行结果及分析源码机制，博主期望你能了解到，left join，right join，full join 虽然写起来简单，但是会存在着 retract 的问题，所以在使用前，你应该充分了解其运行机制，避免出现写入 kafka 的数据发重，发多的问题</li><li>本文主要介绍 flink sql regular join retract 的问题，下节介绍怎么使用 interval join 来避免这种 retract 问题，并满足第 2 点的实战案例需求</li></ol><h1 id="2-背景及应用场景介绍"><a href="#2-背景及应用场景介绍" class="headerlink" title="2.背景及应用场景介绍"></a>2.背景及应用场景介绍</h1><p>在我们的日常场景中，应用最广的一种操作必然有 join 的一席之地，例如</p><ol><li>计算曝光数据和点击数据的 CTR，需要通过唯一 id 进行 join 关联</li><li>事实数据关联维度数据获取维度，进而计算维度指标</li></ol><p>上述场景，在离线数仓应用之广就不多说了。</p><p>那么，实时流之间的关联要怎么操作呢？</p><p>flink sql 为我们提供了六种强大的关联方式，帮助我们在流式场景中达到流关联的目的。如下图官网截图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/14_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%EF%BC%9Aflinksqljoin/4.png" alt="join"></p><ol><li>（流 join）regular join：即 left join，right join，full join，inner join</li><li>维表 lookup join：维表关联</li><li>temporal join：快照表 join</li><li>（流 join）interval join：两条流在一段时间区间之内的 join</li><li>array 炸开：列转行，类似 hive lateral view explode</li><li>table function join：通过 table function 自定义函数实现 join（类似于 hive lateral view explode，用户可以自定义这个 explode 函数）</li></ol><p>在实时数仓中，设计到流与流 join 就是 regular join 以及 interval join。所以本文主要介绍这两种（太长的篇幅大家可能也不想看，所以之后的文章就以简洁，短为目标）。</p><h1 id="3-来一个实战案例"><a href="#3-来一个实战案例" class="headerlink" title="3.来一个实战案例"></a>3.来一个实战案例</h1><p>先来一个实际案例来看看在具体输入值的场景下，输出值应该长啥样。</p><p>场景：即常见的曝光日志流（show_log）通过 log_id 关联点击日志流（click_log），将数据的关联结果进行下发。</p><p>来一波输入数据：</p><p>曝光数据：</p><table><thead><tr><th>log_id</th><th>timestamp</th><th>show_params</th></tr></thead><tbody><tr><td>1</td><td>2021-11-01 00:01:03</td><td>show_params</td></tr><tr><td>2</td><td>2021-11-01 00:03:00</td><td>show_params2</td></tr><tr><td>3</td><td>2021-11-01 00:05:00</td><td>show_params3</td></tr></tbody></table><p>点击数据：</p><table><thead><tr><th>log_id</th><th>timestamp</th><th>click_params</th></tr></thead><tbody><tr><td>1</td><td>2021-11-01 00:01:53</td><td>click_params</td></tr><tr><td>2</td><td>2021-11-01 00:02:01</td><td>click_params2</td></tr></tbody></table><p>预期输出数据如下：</p><table><thead><tr><th>log_id</th><th>timestamp</th><th>show_params</th><th>click_params</th></tr></thead><tbody><tr><td>1</td><td>2021-11-01 00:01:00</td><td>show_params</td><td>click_params</td></tr><tr><td>2</td><td>2021-11-01 00:01:00</td><td>show_params2</td><td>click_params2</td></tr><tr><td>3</td><td>2021-11-01 00:02:00</td><td>show_params3</td><td>null</td></tr></tbody></table><p>熟悉离线 hive sql 的同学可能 10s 就写完上面这个 sql 了，如下</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    show_log.log_id <span class="keyword">as</span> log_id,</span><br><span class="line">    show_log.timestamp <span class="keyword">as</span> <span class="type">timestamp</span>,</span><br><span class="line">    show_log.show_params <span class="keyword">as</span> show_params,</span><br><span class="line">    click_log.click_params <span class="keyword">as</span> click_params</span><br><span class="line"><span class="keyword">FROM</span> show_log</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> click_log <span class="keyword">ON</span> show_log.log_id <span class="operator">=</span> click_log.log_id;</span><br></pre></td></tr></table></figure><p>那么我们看看上述需求如果要以 flink sql 实现需要怎么做呢？</p><p>虽然 flink sql 提供了 left join 的能力，但是在实际使用时，可能会出现预期之外的问题。下节详述。</p><h1 id="4-flink-sql-regular-join"><a href="#4-flink-sql-regular-join" class="headerlink" title="4.flink sql regular join"></a>4.flink sql regular join</h1><h2 id="4-1-flink-sql"><a href="#4-1-flink-sql" class="headerlink" title="4.1.flink sql"></a>4.1.flink sql</h2><p>还是上面的案例，我们先用 flink sql 实际跑一遍看看结果：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    show_log.log_id <span class="keyword">as</span> log_id,</span><br><span class="line">    show_log.timestamp <span class="keyword">as</span> <span class="type">timestamp</span>,</span><br><span class="line">    show_log.show_params <span class="keyword">as</span> show_params,</span><br><span class="line">    click_log.click_params <span class="keyword">as</span> click_params</span><br><span class="line"><span class="keyword">FROM</span> show_log</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> click_log <span class="keyword">ON</span> show_log.log_id <span class="operator">=</span> click_log.log_id;</span><br></pre></td></tr></table></figure><p>flink web ui 算子图如下：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/14_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%EF%BC%9Aflinksqljoin/5.png" alt="flink web ui"></p><p>结果如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">+</span>[<span class="number">1</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-11</span><span class="number">-01</span> <span class="number">00</span>:<span class="number">01</span>:<span class="number">03</span> <span class="operator">|</span> show_params <span class="operator">|</span> <span class="keyword">null</span>]</span><br><span class="line"><span class="operator">-</span>[<span class="number">1</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-11</span><span class="number">-01</span> <span class="number">00</span>:<span class="number">01</span>:<span class="number">03</span> <span class="operator">|</span> show_params <span class="operator">|</span> <span class="keyword">null</span>]</span><br><span class="line"><span class="operator">+</span>[<span class="number">1</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-11</span><span class="number">-01</span> <span class="number">00</span>:<span class="number">01</span>:<span class="number">03</span> <span class="operator">|</span> show_params <span class="operator">|</span> click_params]</span><br><span class="line"><span class="operator">+</span>[<span class="number">2</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-11</span><span class="number">-01</span> <span class="number">00</span>:<span class="number">03</span>:<span class="number">00</span> <span class="operator">|</span> show_params <span class="operator">|</span> click_params]</span><br><span class="line"><span class="operator">+</span>[<span class="number">3</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-11</span><span class="number">-01</span> <span class="number">00</span>:<span class="number">05</span>:<span class="number">00</span> <span class="operator">|</span> show_params <span class="operator">|</span> <span class="keyword">null</span>]</span><br></pre></td></tr></table></figure><p>从结果上看，其输出数据有 <code>+</code>，<code>-</code>，代表其输出的数据是一个 retract 流的数据。分析原因发现是，由于第一条 show_log 先于 click_log 到达，<br>所以就先直接发出 <code>+[1 | 2021-11-01 00:01:03 | show_params | null]</code>，后面 click_log 到达之后，将上一次未关联到 click log 的 show_log 消息撤回，<br>然后将关联到的 <code>+[1 | 2021-11-01 00:01:03 | show_params | click_params]</code> 下发。</p><p>但是 retract 流会导致写入到 kafka 的数据变多，这是我们不希望碰到的。我们期望的结果应该是一个 append 数据流。</p><p>那为什么 left join 会出现这种问题呢？就要从 left join 的原理说起了。</p><p>来定位到具体的实现源码。先看一下 transformations。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/14_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%EF%BC%9Aflinksqljoin/1.png" alt="transformations"></p><p>可以看到 left join 的具体 operator 是 <code>org.apache.flink.table.runtime.operators.join.stream.StreamingJoinOperator</code>。</p><p>其核心逻辑就集中在 <code>processElement</code> 方法上面。并且源码对于  <code>processElement</code> 的处理逻辑有详细的注释说明，如下图所示。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/14_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%EF%BC%9Aflinksqljoin/2.png" alt="StreamingJoinOperator#processElement"></p><p>注释看起来逻辑比较难理解，是经过抽象的。我们这里按照 left join，inner join，right join，full join 分类给大家解释一下。</p><h2 id="4-2-left-join"><a href="#4-2-left-join" class="headerlink" title="4.2.left join"></a>4.2.left join</h2><p>首先是 left join，以上面的 show_log（左表） left join click_log（右表） 为例：</p><ol><li>首先如果 join xxx on 中的条件是等式则代表 join 是在相同 key 下进行的，join 的 key 即 show_log.log_id，click_log.log_id，相同 key 的数据会被发送到一个并发中进行处理。<br>如果 join xxx on 中的条件是不等式，则两个流的 source 算子向 join 算子下发数据是按照 global 的 partition 策略进行下发的，并且 join 算子并发会被设置为 1，所有的数据会被发送到这一个并发中处理。</li><li>相同 key 下，当 show_log 来一条数据，如果 click_log 有数据：则 show_log 与 click_log 中的所有数据进行遍历关联一遍输出[+（show_log，click_log）]数据，并且把 show_log 保存到左表的状态中（以供后续 join 使用）。</li><li>相同 key 下，当 show_log 来一条数据，如果 click_log 中没有数据：则 show_log 不会等待，直接输出[+（show_log，null）]数据，并且把 show_log 保存到左表的状态中（以供后续 join 使用）。</li><li>相同 key 下，当 click_log 来一条数据，如果 show_log 有数据：则 click_log 对 show_log 中所有的数据进行遍历关联一遍。<br>在输出数据前，会判断，如果被关联的这条 show_log 之前没有关联到过 click_log（即往下发过[+（show_log，null）]），则先发一条[-（show_log，null）]，后发一条[+（show_log，click_log）]<br>，代表把之前的那条没有关联到 click_log 数据的 show_log 中间结果给撤回，把当前关联到的最新结果进行下发，并把 click_log 保存到右表的状态中（以供后续左表进行关联）。这也就解释了为什么输出流是一个 retract 流。</li><li>相同 key 下，当 click_log 来一条数据，如果 show_log 没有数据：把 click_log 保存到右表的状态中（以供后续左表进行关联）。</li></ol><h2 id="4-3-inner-join"><a href="#4-3-inner-join" class="headerlink" title="4.3.inner join"></a>4.3.inner join</h2><p>以上面的 show_log（左表） left join click_log（右表） 为例：</p><ol><li>首先如果 join xxx on 中的条件是等式则代表 join 是在相同 key 下进行的，join 的 key 即 show_log.log_id，click_log.log_id，相同 key 的数据会被发送到一个并发中进行处理。<br>如果 join xxx on 中的条件是不等式，则两个流的 source 算子向 join 算子下发数据是按照 global 的 partition 策略进行下发的，并且 join 算子并发会被设置为 1，所有的数据会被发送到这一个并发中处理。</li><li>相同 key 下，当 show_log 来一条数据，如果 click_log 有数据：则 show_log 与 click_log 中的所有数据进行遍历关联一遍输出[+（show_log，click_log）]数据，并且把 show_log 保存到左表的状态中（以供后续 join 使用）。</li><li>相同 key 下，当 show_log 来一条数据，如果 click_log 中没有数据：则 show_log 不会输出数据，会把 show_log 保存到左表的状态中（以供后续 join 使用）。</li><li>相同 key 下，当 click_log 来一条数据，如果 show_log 有数据：则 click_log 与 show_log 中的所有数据进行遍历关联一遍输出[+（show_log，click_log）]数据，并且把 click_log 保存到右表的状态中（以供后续 join 使用）。</li><li>相同 key 下，当 click_log 来一条数据，如果 show_log 没有数据：则 click_log 不会输出数据，会把 click_log 保存到右表的状态中（以供后续 join 使用）。</li></ol><h2 id="4-4-right-join"><a href="#4-4-right-join" class="headerlink" title="4.4.right join"></a>4.4.right join</h2><p>right join 和 left join 一样，只不过顺序反了，这里不再赘述。</p><h2 id="4-5-full-join"><a href="#4-5-full-join" class="headerlink" title="4.5.full join"></a>4.5.full join</h2><p>以上面的 show_log（左表） left join click_log（右表） 为例：</p><ol><li>首先如果 join xxx on 中的条件是等式则代表 join 是在相同 key 下进行的，join 的 key 即 show_log.log_id，click_log.log_id，相同 key 的数据会被发送到一个并发中进行处理。如果 join xxx on 中的条件是不等式，则两个流的 source 算子向 join 算子下发数据是按照 global 的 partition 策略进行下发的，并且 join 算子并发会被设置为 1，所有的数据会被发送到这一个并发中处理。</li><li>相同 key 下，当 show_log 来一条数据，如果 click_log 有数据：则 show_log 对 click_log 中所有的数据进行遍历关联一遍。<br>在输出数据前，会判断，如果被关联的这条 click_log 之前没有关联到过 show_log（即往下发过[+（null，click_log）]），则先发一条[-（null，click_log）]，后发一条[+（show_log，click_log）]<br>，代表把之前的那条没有关联到 show_log 数据的 click_log 中间结果给撤回，把当前关联到的最新结果进行下发，并把 show_log 保存到左表的状态中（以供后续 join 使用）</li><li>相同 key 下，当 show_log 来一条数据，如果 click_log 中没有数据：则 show_log 不会等待，直接输出[+（show_log，null）]数据，并且把 show_log 保存到左表的状态中（以供后续 join 使用）。</li><li>相同 key 下，当 click_log 来一条数据，如果 show_log 有数据：则 click_log 对 show_log 中所有的数据进行遍历关联一遍。<br>在输出数据前，会判断，如果被关联的这条 show_log 之前没有关联到过 click_log（即往下发过[+（show_log，null）]），则先发一条[-（show_log，null）]，后发一条[+（show_log，click_log）]<br>，代表把之前的那条没有关联到 click_log 数据的 show_log 中间结果给撤回，把当前关联到的最新结果进行下发，并把 click_log 保存到右表的状态中（以供后续 join 使用）</li><li>相同 key 下，当 click_log 来一条数据，如果 show_log 中没有数据：则 click_log 不会等待，直接输出[+（null，click_log）]数据，并且把 click_log 保存到右表的状态中（以供后续 join 使用）。</li></ol><h2 id="4-6-regular-join-的总结"><a href="#4-6-regular-join-的总结" class="headerlink" title="4.6.regular join 的总结"></a>4.6.regular join 的总结</h2><p>总的来说上述四种 join 可以按照以下这么划分。</p><ol><li>inner join 会互相等，直到有数据才下发。</li><li>left join，right join，full join 不会互相等，只要来了数据，会尝试关联，能关联到则发出的消息字段是全的，关联不到则另一边的字段为 null。后续数据来了之后，发现之前下发过的数据为没有关联到的数据时，就会做回撤，把关联到的结果进行下发</li></ol><h2 id="4-7-怎样才能解决-retract-导致数据重复下发到-kafka-这个问题呢？"><a href="#4-7-怎样才能解决-retract-导致数据重复下发到-kafka-这个问题呢？" class="headerlink" title="4.7.怎样才能解决 retract 导致数据重复下发到 kafka 这个问题呢？"></a>4.7.怎样才能解决 retract 导致数据重复下发到 kafka 这个问题呢？</h2><p>既然 flink sql 在 left join、right join、full join 实现上的原理就是以这种 retract 的方式去实现的，就不能通过这种方式来满足业务了。</p><p>我们来转变一下思路，上述 join 的特点就是不会相互等，那有没有一种 join 是可以相互等待的呢。<br>以 left join 的思路为例，左表在关联不到右表的时候，可以选择等待一段时间，如果超过这段时间还等不到再下发 (show_log，null)，如果等到了就下发（show_log，click_log）。</p><p>interval join 闪亮登场。关于 interval join 是如何实现上述场景，及其原理实现，本篇的（下）会详细介绍，敬请期待。</p><h1 id="5-总结与展望"><a href="#5-总结与展望" class="headerlink" title="5.总结与展望"></a>5.总结与展望</h1><p>源码公众号后台回复<strong>1.13.2 sql join 的奇妙解析之路</strong>获取。</p><p>本文主要介绍了 flink sql regular 的在满足 join 场景时存在的问题，并通过解析其实现说明了运行原理，博主期望你读完本文之后能了解到：</p><ol><li>背景及应用场景介绍：博主期望你能了解到，flink sql 提供的丰富的 join 方式（总结 6 种：regular join，维表 join，快照 join，interval join，array 拍平，table function）对我们满足需求提供了强大的后盾，<br>这 6 种 join 中涉及到流与流的 join 最常用的是 regular join 以及 interval join</li><li>来一个实战案例：博主以一个曝光日志流 left join 点击日志流为案例展开，介绍离线 hive sql left join 的解决方案及 flink sql left join 的解决方案，主要是想告诉小伙伴 flink sql 也有类似于 hive sql 的 regular join</li><li>flink sql regular join 的解决方案以及存在问题的介绍：主要介绍 regular join 的在上述实战案例的运行结果及分析源码机制，博主期望你能了解到，left join，right join，full join 虽然写起来简单，但是会存在着 retract 的问题，所以在使用前，你应该充分了解其运行机制，避免出现写入 kafka 的数据发重，发多的问题</li><li>本文主要介绍 flink sql regular join retract 的问题，下节介绍怎么使用 interval join 来避免这种 retract 问题，并满足第 2 点的实战案例需求</li></ol>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink sql 知其所以然（十三）：流 join 很难嘛？？？（下）</title>
    <link href="https://yangyichao-mango.github.io/2021/11/15/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/14_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89%EF%BC%9Aflink-sql-interval-join/"/>
    <id>https://yangyichao-mango.github.io/2021/11/15/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/14_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89%EF%BC%9Aflink-sql-interval-join/</id>
    <published>2021-11-15T06:25:59.000Z</published>
    <updated>2021-11-27T04:03:39.780Z</updated>
    
    <content type="html"><![CDATA[<p>看了那么多的技术文，你能明白作者想让你在读完文章后学到什么吗？</p><p>大数据羊说的文章会让你明白</p><ol><li><p>博主会阐明博主期望本文能给小伙伴们带来什么帮助，让小伙伴萌能直观明白博主的心思</p></li><li><p>博主会以实际的应用场景和案例入手，不只是知识点的简单堆砌</p></li><li><p>博主会把重要的知识点的原理进行剖析，让小伙伴萌做到深入浅出</p></li></ol><h1 id="1-序篇"><a href="#1-序篇" class="headerlink" title="1.序篇"></a>1.序篇</h1><p>源码公众号后台回复<strong>1.13.2 sql interval join 的奇妙解析之路</strong>获取。</p><p>本节是 flink sql 流 join 系列的下篇，上篇的链接如下：</p><p>废话不多说，咱们先直接上本文的目录和结论，小伙伴可以先看结论快速了解博主期望本文能给小伙伴们带来什么帮助：</p><ol><li><strong>背景及应用场景介绍</strong>：博主期望你能了解到，flink sql 提供的丰富的 join 方式（总结 6 种：regular join，维表 join，快照 join，interval join，array 拍平，table function）对我们满足需求提供了强大的后盾，<br>这 6 种 join 中涉及到流与流的 join 最常用的是 regular join 以及 interval join，本节主要介绍 interval join</li><li><strong>来一个实战案例</strong>：博主以上节说到的曝光日志流点击日志流为案例展开，主要是想告诉小伙伴 flink sql left join 数据不会互相等待，存在 retract 问题，会导致写入 kafka 的数据量变大，<br>然后转变思路为使用 flink sql interval join 的方式可以使得数据互相等待一段时间进行 join，这种方式不会存在 retract 问题</li><li><strong>flink sql interval join 的解决方案以及原理的介绍</strong>：主要介绍 interval join 的在上述实战案例的运行结果及分析源码机制，博主期望你能了解到，interval join 的执行机制是会在你设置的 interval 区间之内互相等待一段时间，一旦时间推进（事件时间由 watermark 推进）到区间之外（即当前这条数据再也不可能被另一条流的数据 join 到时），outer join 会输出没有 join 到的数据，inner join 会从 state 中删除这条数据</li><li><strong>总结及展望</strong></li></ol><h1 id="2-背景及应用场景介绍"><a href="#2-背景及应用场景介绍" class="headerlink" title="2.背景及应用场景介绍"></a>2.背景及应用场景介绍</h1><p>书接上文，上文介绍了曝光流在关联点击流时，使用 flink sql regular join 存在的 retract 问题。</p><p>本文介绍怎么使用 flink sql interval join 解决这些问题。</p><h1 id="3-来一个实战案例"><a href="#3-来一个实战案例" class="headerlink" title="3.来一个实战案例"></a>3.来一个实战案例</h1><p>看看上节的实际案例，来看看在具体输入值的场景下，输出值应该长啥样。</p><p>场景：即常见的曝光日志流（show_log）通过 log_id 关联点击日志流（click_log），将数据的关联结果进行下发。</p><p>来一波输入数据：</p><p>曝光数据：</p><table><thead><tr><th>log_id</th><th>timestamp</th><th>show_params</th></tr></thead><tbody><tr><td>1</td><td>2021-11-01 00:01:03</td><td>show_params</td></tr><tr><td>2</td><td>2021-11-01 00:03:00</td><td>show_params2</td></tr><tr><td>3</td><td>2021-11-01 00:05:00</td><td>show_params3</td></tr></tbody></table><p>点击数据：</p><table><thead><tr><th>log_id</th><th>timestamp</th><th>click_params</th></tr></thead><tbody><tr><td>1</td><td>2021-11-01 00:01:53</td><td>click_params</td></tr><tr><td>2</td><td>2021-11-01 00:02:01</td><td>click_params2</td></tr></tbody></table><p>预期输出数据如下：</p><table><thead><tr><th>log_id</th><th>timestamp</th><th>show_params</th><th>click_params</th></tr></thead><tbody><tr><td>1</td><td>2021-11-01 00:01:00</td><td>show_params</td><td>click_params</td></tr><tr><td>2</td><td>2021-11-01 00:01:00</td><td>show_params2</td><td>click_params2</td></tr><tr><td>3</td><td>2021-11-01 00:02:00</td><td>show_params3</td><td>null</td></tr></tbody></table><p>上节的 flink sql regular join 解决方案如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    show_log.log_id <span class="keyword">as</span> log_id,</span><br><span class="line">    show_log.timestamp <span class="keyword">as</span> <span class="type">timestamp</span>,</span><br><span class="line">    show_log.show_params <span class="keyword">as</span> show_params,</span><br><span class="line">    click_log.click_params <span class="keyword">as</span> click_params</span><br><span class="line"><span class="keyword">FROM</span> show_log</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> click_log <span class="keyword">ON</span> show_log.log_id <span class="operator">=</span> click_log.log_id;</span><br></pre></td></tr></table></figure><p>上节说道，flink sql left join 在流数据到达时，如果左表流（show_log）join 不到右表流（click_log）<br>，则不会等待右流直接输出（show_log，null），在后续右表流数据代打时，会将（show_log，null）撤回，发送（show_log，click_log）。<br>这就是为什么产生了 retract 流，从而导致重复写入 kafka。</p><p>对此，我们也是提出了对应的解决思路，既然 left join 中左流不会等待右流，那么能不能让左流强行等待右流一段时间，实在等不到在数据关联不到的数据即可。</p><p>当当当！！！</p><p>本文的 flink sql interval join 登场，它就能等。</p><h1 id="4-flink-sql-interval-join"><a href="#4-flink-sql-interval-join" class="headerlink" title="4.flink sql interval join"></a>4.flink sql interval join</h1><h2 id="4-1-interval-join-定义"><a href="#4-1-interval-join-定义" class="headerlink" title="4.1.interval join 定义"></a>4.1.interval join 定义</h2><p>大家先通过下面这句话和图简单了解一下 interval join 的作用（熟悉 DataStream 的小伙伴萌可能已经使用过了），后续会详细介绍原理。</p><p>interval join 就是用一个流的数据去关联另一个流的一段时间区间内的数据。关联到就下发关联到的数据，关联不到且在超时后就根据是否是 outer join（left join，right join，full join）下发没关联到的数据。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/15_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89%EF%BC%9Aflinksqlintervaljoin/1.png" alt="interval join"></p><h2 id="4-2-案例解决方案"><a href="#4-2-案例解决方案" class="headerlink" title="4.2.案例解决方案"></a>4.2.案例解决方案</h2><p>来看看上述案例的 flink sql interval join sql 怎么写：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    show_log.log_id <span class="keyword">as</span> log_id,</span><br><span class="line">    show_log.timestamp <span class="keyword">as</span> <span class="type">timestamp</span>,</span><br><span class="line">    show_log.show_params <span class="keyword">as</span> show_params,</span><br><span class="line">    click_log.click_params <span class="keyword">as</span> click_params</span><br><span class="line"><span class="keyword">FROM</span> show_log <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> click_log <span class="keyword">ON</span> show_log.log_id <span class="operator">=</span> click_log.log_id</span><br><span class="line"><span class="keyword">AND</span> show_log.row_time </span><br><span class="line">    <span class="keyword">BETWEEN</span> click_log.row_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">MINUTE</span> </span><br><span class="line">    <span class="keyword">AND</span> click_log.row_time <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="string">&#x27;10&#x27;</span> <span class="keyword">MINUTE</span>;</span><br></pre></td></tr></table></figure><p>这里设置了 <code>show_log.row_time BETWEEN click_log.row_time - INTERVAL &#39;10&#39; MINUTE AND click_log.row_time + INTERVAL &#39;10&#39; MINUTE</code><br>代表 show_log 表中的数据会和 click_log 表中的 row_time 在前后 10 分钟之内的数据进行关联。</p><p>运行结果如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">+</span>[<span class="number">1</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-11</span><span class="number">-01</span> <span class="number">00</span>:<span class="number">01</span>:<span class="number">03</span> <span class="operator">|</span> show_params <span class="operator">|</span> click_params]</span><br><span class="line"><span class="operator">+</span>[<span class="number">2</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-11</span><span class="number">-01</span> <span class="number">00</span>:<span class="number">03</span>:<span class="number">00</span> <span class="operator">|</span> show_params <span class="operator">|</span> click_params]</span><br><span class="line"><span class="operator">+</span>[<span class="number">3</span> <span class="operator">|</span> <span class="number">2021</span><span class="number">-11</span><span class="number">-01</span> <span class="number">00</span>:<span class="number">05</span>:<span class="number">00</span> <span class="operator">|</span> show_params <span class="operator">|</span> <span class="keyword">null</span>]</span><br></pre></td></tr></table></figure><p>如上就是我们期望的正确结果了。</p><p>flink web ui 算子图如下：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/15_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89%EF%BC%9Aflinksqlintervaljoin/2.png" alt="flink web ui"></p><p>那么此时你可能有一个问题，结果中的前两条数据 join 到了输出我是理解的，那当 show_log join 不到 click_log 时为啥也输出了？原理是啥？</p><p>博主带你们来定位到具体的实现源码。先看一下 transformations。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/15_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89%EF%BC%9Aflinksqlintervaljoin/3.png" alt="transformations"></p><p>可以看到事件时间下 interval join 的具体 operator 是 <code>org.apache.flink.table.runtime.operators.join.KeyedCoProcessOperatorWithWatermarkDelay</code>。</p><p>其核心逻辑就集中在 <code>processElement1</code> 和 <code>processElement2</code> 中，在 <code>processElement1</code> 和 <code>processElement2</code> 中使用 <code>org.apache.flink.table.runtime.operators.join.interval.RowTimeIntervalJoin</code> 来处理具体 join 逻辑。<code>RowTimeIntervalJoin</code> 重要方法如下图所示。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/15_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89%EF%BC%9Aflinksqlintervaljoin/4.png" alt="TimeIntervalJoin"></p><p>下面详细给大家解释一下。</p><h2 id="4-3-TimeIntervalJoin-简版说明"><a href="#4-3-TimeIntervalJoin-简版说明" class="headerlink" title="4.3.TimeIntervalJoin 简版说明"></a>4.3.TimeIntervalJoin 简版说明</h2><p>join 时，左流和右流会在 interval 时间之内相互等待，如果等到了则输出数据[+（show_log，click_log）]，如果等不到，并且另一条流的时间已经推进到当前这条数据在也不可能 join 到另一条流的数据时，则直接输出[+（show_log，null）]，[+（null，click_log）]。</p><p>举个例子，<code>show_log.row_time BETWEEN click_log.row_time - INTERVAL &#39;10&#39; MINUTE AND click_log.row_time + INTERVAL &#39;10&#39; MINUTE</code>，<br>当 click_log 的时间推进到 <code>2021-11-01 11:00:00</code> 时，这时 show_log 来一条 <code>2021-11-01 02:00:00</code> 的数据，<br>那这条 show_log 必然不可能和 click_log 中的数据 join 到了，因为 click_log 中 <code>2021-11-01 01:50:00</code> 到 <code>2021-11-01 02:10:00</code> 之间的数据以及过期删除了。则 show_log 直接输出 [+（show_log，null）]</p><blockquote><p>Notes：<br>如果你设置了 allowLateness，join 不到的数据的输出和 state 的清理会多保留 allowLateness 时间</p></blockquote><h2 id="4-4-TimeIntervalJoin-详细实现说明"><a href="#4-4-TimeIntervalJoin-详细实现说明" class="headerlink" title="4.4.TimeIntervalJoin 详细实现说明"></a>4.4.TimeIntervalJoin 详细实现说明</h2><p>以上面案例的 show_log（左表） interval join click_log（右表） 为例（不管是 inner interval join，left interval join，right interval join 还是 full interval join，都会按照下面的流程执行）：</p><ol><li><strong>第一步</strong>，首先如果 join xxx on 中的条件是等式则代表 join 是在相同 key 下进行的（上述案例中 join 的 key 即 show_log.log_id，click_log.log_id），相同 key 的数据会被发送到一个并发中进行处理。如果 join xxx on 中的条件是不等式，则两个流的 source 算子向 join 算子下发数据是按照 global 的 partition 策略进行下发的，并且 join 算子并发会被设置为 1，所有的数据会被发送到这一个并发中处理。</li><li><strong>第二步</strong>，相同 key 下，一条 show_log 的数据先到达，首先会计算出下面要使用的最重要的三类时间戳：</li></ol><ul><li>根据 show_log 的时间戳（l_time）计算出能关联到的右流的时间区间下限（r_lower）、上限（r_upper）</li><li>根据 show_log 目前的 watermark 计算出目前右流的数据能够过期做过期处理的时间的最小值（r_expire）</li><li>获取左流的 l_watermark，右流的 r_watermark，这两个时间戳在事件语义的任务中都是 watermark</li></ul><ol start="3"><li><strong>第三步</strong>，遍历所有同 key 下的 click_log 来做 join</li></ol><ul><li>对于遍历的每一条 click_log，走如下步骤</li><li>经过判断，如果 on 中的条件为 true，则和 click_log 关联，输出[+（show_log，click_log）]数据；如果 on 中的条件为 false，则啥也不干</li><li>接着判断当前这条 click_log 的数据时间（r_time）是否小于右流的数据过期时间的最小值（r_expire）（即判断这条 click_log 是否永远不会再被 show_log join 到了）。<br>如果小于，并且当前 click_log 这一侧是 outer join，则不用等直接输出[+（null，click_log）]），从状态删除这条 click_log；如果 click_log 这一侧不是 outer join，则直接从状态里删除这条 click_log。</li></ul><ol start="4"><li><strong>第四步</strong>，判断右流的时间戳（r_watermark）是否小于能关联到的右流的时间区间上限（r_upper）：</li></ol><ul><li>如果是，则说明这条 show_log 还有可能被 click_log join 到，则 show_log 放到 state 中，并注册后面用于状态清除的 timer。</li><li>如果否，则说明关联不到了，则输出[+（show_log，null）]</li></ul><ol start="5"><li><strong>第五步</strong>，timer 触发时：</li></ol><ul><li>timer 触发时，根据当前 l_watermark，r_watermark 以及 state 中存储的 show_log，click_log 的 l_time，r_time 判断是否再也不会被对方 join 到，如果是，则根据是否为 outer join 对应输出[+（show_log，null）]，[+（null，click_log）]，并从状态中删除对应的 show_log，click_log。</li></ul><p>上面只是左流 show_log 数据到达时的执行流程（即 <code>ProcessElement1</code>），当右流 click_log 到达时也是完全类似的执行流程（即 <code>ProcessElement2</code>）。</p><h2 id="4-5-使用注意事项"><a href="#4-5-使用注意事项" class="headerlink" title="4.5.使用注意事项"></a>4.5.使用注意事项</h2><p>小伙伴萌在使用 interval join 需要注意的两点事项：</p><ol><li><strong>interval join 的时间区间取决于日志的真实情况</strong>：设置大了容易造成任务的 state 太大，并且时效性也会变差。设置小了，join 不到，下发的数据在后续使用时，数据质量会存在问题。所以小伙伴萌在使用时建议先使用离线数据做一遍两条流的时间戳 diff 比较，来确定真实情况下的时间戳 diff 的分布是怎样的。<br>举例：你通过离线数据 join 并做时间戳 diff 后发现 99% 的数据都能在时间戳相差 5min 以内 join 到，那么你就有依据去设置 interval 时间差为 5min。</li><li><strong>interval join 中的时间区间条件即支持事件时间，也支持处理时间</strong>。事件时间由 watermark 推进。</li></ol><h1 id="5-总结与展望"><a href="#5-总结与展望" class="headerlink" title="5.总结与展望"></a>5.总结与展望</h1><p>源码公众号后台回复<strong>1.13.2 sql interval join 的奇妙解析之路</strong>获取。</p><p>本文主要介绍了 flink sql interval 是怎么避免出现 flink regular join 存在的 retract 问题的，并通过解析其实现说明了运行原理，博主期望你读完本文之后能了解到：</p><ol><li><strong>背景及应用场景介绍</strong>：博主期望你能了解到，flink sql 提供的丰富的 join 方式（总结 6 种：regular join，维表 join，快照 join，interval join，array 拍平，table function）对我们满足需求提供了强大的后盾，<br>这 6 种 join 中涉及到流与流的 join 最常用的是 regular join 以及 interval join，本节主要介绍 interval join</li><li><strong>来一个实战案例</strong>：博主以上节说到的曝光日志流点击日志流为案例展开，主要是想告诉小伙伴 flink sql left join 数据不会互相等待，存在 retract 问题，会导致写入 kafka 的数据量变大，<br>然后转变思路为使用 flink sql interval join 的方式可以使得数据互相等待一段时间进行 join，这种方式不会存在 retract 问题</li><li><strong>flink sql interval join 的解决方案以及原理的介绍</strong>：主要介绍 interval join 的在上述实战案例的运行结果及分析源码机制，博主期望你能了解到，interval join 的执行机制是会在你设置的 interval 区间之内互相等待一段时间，一旦时间推进（事件时间由 watermark 推进）到区间之外（即当前这条数据再也不可能被另一条流的数据 join 到时），outer join 会输出没有 join 到的数据，inner join 会从 state 中删除这条数据</li><li><strong>总结及展望</strong></li></ol>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>当我们在做流批一体时，我们在做什么？</title>
    <link href="https://yangyichao-mango.github.io/2021/11/15/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/11_%E6%B5%81%E6%89%B9%E4%B8%80%E4%BD%93%E7%9A%84%E7%AE%80%E5%8D%95%E6%80%9D%E8%80%83/"/>
    <id>https://yangyichao-mango.github.io/2021/11/15/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/11_%E6%B5%81%E6%89%B9%E4%B8%80%E4%BD%93%E7%9A%84%E7%AE%80%E5%8D%95%E6%80%9D%E8%80%83/</id>
    <published>2021-11-15T06:25:58.000Z</published>
    <updated>2021-11-18T07:50:03.165Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1.前言"></a>1.前言</h1><p>本文主要是分享目前博主理解的流批一体产生的背景，想解决的问题，以及后续可能实现的思路，并以几个案例进行介绍。抛砖引玉，让大家不止停留在做流批一体这件事，而是能更深入思考背后的原因。</p><h1 id="2-背景"><a href="#2-背景" class="headerlink" title="2.背景"></a>2.背景</h1><p>在介绍流批一体之前，首先看看目前流和批领域常用的引擎：</p><ol><li>批任务：常用 Hive、Spark </li><li>流任务：常用 Flink</li></ol><h1 id="3-什么问题导致产生了流批一体的概念呢？"><a href="#3-什么问题导致产生了流批一体的概念呢？" class="headerlink" title="3.什么问题导致产生了流批一体的概念呢？"></a>3.什么问题导致产生了流批一体的概念呢？</h1><ol><li><p>一个前提：在生产场景中：即同一个口径的指标分别用流任务产出了实时数据，用批任务产出了离线数据，才会去考虑是否需要做流批一体。如果一个指标只需要产出离线，何谈流批一体呢？</p></li><li><p>一个角度：流批一体应该站在流的角度思考，去将流任务产出的结果在批领域（或者以批数据的形式）进行复用，而不仅仅是在引擎侧面，API 接口层面的统一</p></li><li><p>解决的问题：在上述前提和思考角度的基础上，博主认为，流批一体目前需要解决的最重要的就是数据质量问题（与阿里在 FFA 2020 上分享的流批一体要解决的数据质量问题一样），用过 Flink 做实时数据开发的同学应该都碰到过 Flink 产出数据的时候，总会由于一些异常（比如使用了窗口可能会导致丢数）导致和离线 Hive、Spark 产出的数据有一些微小的差别，这样就没法做到实时数据在离线领域的复用。博主理解，流批一体的重点就是要解决这个问题，其他的在资源节约、人效提高方面的优势都是基于此的附加价值。</p></li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/11_%E6%B5%81%E6%89%B9%E4%B8%80%E4%BD%93%E7%9A%84%E7%AE%80%E5%8D%95%E6%80%9D%E8%80%83/1.png" alt="1"></p><blockquote><p>Notes:<br>有同学可能就要问了，为什么说资源节约、人效提高只是附加价值呢？<br>举一个例子，如果我们能保证产出的实时数据和离线数据是完全一致的，将实时数据进行复用，那我们那我们就不需要离线数据了！这样离线的资源就被节约了</p></blockquote><h1 id="4-那么导致流任务产生数据质量问题的原因是什么，有哪些常见场景？"><a href="#4-那么导致流任务产生数据质量问题的原因是什么，有哪些常见场景？" class="headerlink" title="4.那么导致流任务产生数据质量问题的原因是什么，有哪些常见场景？"></a>4.那么导致流任务产生数据质量问题的原因是什么，有哪些常见场景？</h1><p>博主认为，目前最重要的一个原因就是数据乱序导致的数据质量问题。</p><p>在实时领域最常用和常见的场景有以下两种：</p><p>第一种是 Flink 任务开窗口的场景。举例，一个开了 TUMBLE WINDOW 的 Flink 任务，遇到严重的数据乱序的情况（用户配置的最大乱序、允许延迟等参数都解决不了），那么任务就会把数据给丢掉，这种场景下就会导致实时数据与离线数据产生差异。</p><p>第二种是实时维表关联的场景。如果事实表的数据先到，就有关联不到维表中的数据。从而产生与离线的差异。</p><h1 id="5-想要解决上述数据质量问题，可行的思路有哪些？"><a href="#5-想要解决上述数据质量问题，可行的思路有哪些？" class="headerlink" title="5.想要解决上述数据质量问题，可行的思路有哪些？"></a>5.想要解决上述数据质量问题，可行的思路有哪些？</h1><ol><li>理想化的思路：以 TUMBLE WINDOW 为例， TUMBLE WINDOW 的初衷就是为了产出不变的结果（即 append 流），因此遇到延迟很大的数据才无法处理，那么我们可以将 TUMBLE WINDOW 使用 GROUP AGG（retract 流、或者叫做 CDC 模式）替换去计算。当有迟到的数据时，GROUP AGG 会正常的处理及将上次的结果给撤回，将重新计算的新结果下发下去。但是这有问题在于如果我们想用 CDC 的模式去运行任务，我们需要全链路都是以 CDC 的模式去运行，包括计算引擎、消息队列、OLAP 引擎等。（CDC 是不是想到了数据湖？）。<br>再以一个分钟\小时累计指标举例，我们看看阿里在 FFA 2020 上的分享是怎么做的。可以看到实际阿里就是使用 GROUP AGG 做的计算。但是对于后续的链路不知道是否是使用 CDC 的方式运行的。</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/11_%E6%B5%81%E6%89%B9%E4%B8%80%E4%BD%93%E7%9A%84%E7%AE%80%E5%8D%95%E6%80%9D%E8%80%83/3.png" alt="1"></p><ol start="2"><li>阿里 FFA 2020：如下图阿里做的所示，第一种情况是如果流批一体输入源不同，需要批任务调度订正结果，第二种情况是如果流批结果相同，就不跑批任务了。第一种情况没有啥可说的；但是如果是第二种情况，验证流批结果相同的前提是，跑了批任务产出了结果主动去和流任务的结果去做对比，但是实际是批任务并没有调度！！！所以这就需要做很多的监控来保障流任务产出的整体流程没有问题，保障能达到和<strong>预期批任务</strong>产出的结果相同。</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/11_%E6%B5%81%E6%89%B9%E4%B8%80%E4%BD%93%E7%9A%84%E7%AE%80%E5%8D%95%E6%80%9D%E8%80%83/2.png" alt="1"></p><p>上述的第一种思路相对比较理想化，基本是站在流任务产出的数据可以以批的模式进行复用角度去思考的。基本撇开了批任务执行这一个过程。<br>第二种阿里 FFA 2020 的思路相比来说对于链路软硬件条件没那么高，博主认为是更具可行性的。</p><h1 id="6-总结"><a href="#6-总结" class="headerlink" title="6.总结"></a>6.总结</h1><p>本文主要介绍了以下三部分内容：</p><ol><li>流批一体的诞生是为了解决同一个指标在离线、实时任务产出数据差异问题（数据质量）</li><li>导致数据差异的根本原因就是数据乱序</li><li>如果想解决这个问题，理想化就是全链路 CDC，更具操作行可以参考阿里 FFA 2020</li></ol>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>晋升答辩</title>
    <link href="https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/05_%E6%95%B0%E6%8D%AE%E5%9B%A2%E9%98%9F%E5%BB%BA%E8%AE%BE/03_%E4%B8%AA%E4%BA%BA/gogogo/"/>
    <id>https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/05_%E6%95%B0%E6%8D%AE%E5%9B%A2%E9%98%9F%E5%BB%BA%E8%AE%BE/03_%E4%B8%AA%E4%BA%BA/gogogo/</id>
    <published>2021-11-13T06:25:58.000Z</published>
    <updated>2021-11-14T10:25:48.623Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1.前言"></a>1.前言</h1><p>准备答辩内容及材料是一个非常耗时耗力的过程，因此有一些方法的沉淀的话，后续对自己以及帮你 review 材料的人都会更轻松一些。</p><h1 id="2-本质"><a href="#2-本质" class="headerlink" title="2.本质"></a>2.本质</h1><p>晋升永远是有了下一个职级的能力才会有晋升机会。如果你只是做好了你这个职级的事情，没有跨越职级的能力。同样是没有晋升机会的。</p><h1 id="3-平常怎么要求自己"><a href="#3-平常怎么要求自己" class="headerlink" title="3.平常怎么要求自己"></a>3.平常怎么要求自己</h1><p>在平常工作生活中，按照下一个职级（一般公司内部都会有透明的职级能力模型）的要求做事情。明确业务需要达到什么地步？技术需要达到什么地步？</p><h1 id="4-怎么准备答辩材料"><a href="#4-怎么准备答辩材料" class="headerlink" title="4.怎么准备答辩材料"></a>4.怎么准备答辩材料</h1><h2 id="4-1-准备材料前你需要知道的事情"><a href="#4-1-准备材料前你需要知道的事情" class="headerlink" title="4.1.准备材料前你需要知道的事情"></a>4.1.准备材料前你需要知道的事情</h2><ol><li><p><strong>确认下一个职级的要求</strong>：审视一下自己在这些要求上面是否已经达到，或者在什么项目上体现出了这些能力，这些项目将成为之后做 PPT 的素材。</p></li><li><p><strong>站在评委的角度上去思考答辩 PPT 的内容</strong>：所以需要明确评委都是谁，是否和你做的是一个领域的东西，涉及到后续做 PPT 或者准备答辩内容时，其中涉及到的概念是否需要稍微展开举例。举一个例子，比如说想体现出实时计算中的状态，管理是非常复杂的，但是如果评委是做离线数据的话，他们其实是对状态处理没有什么概念的，所以你这里就得考虑多解释一下状态管理为什么复杂，状态管理为什么难。如果你的评委本身就是做实时计算的话，那么这里你就可以简单的介绍一下即可，不用解释的那么复杂。</p></li><li><p><strong>晋升答辩不是技术分享</strong>：准备的内容要能给评委讲明白讲懂，明确这是一场晋升答辩，不是硬核的技术分享（硬核技术分享一般都是针对于某一个点体现深度），也不是给别人上课，因此晋升答辩内容既不能太难，也不能太简单。</p></li><li><p><strong>评委关心的是你解决问题的能力</strong>：评委关心的是解决问题的能力有没有达到下一个职级，因此答辩的内容是要重点突出做项目的过程，包括你的分析过程，剖析难点问题的过程，解决难点问题的过程，而不是简单的介绍一下这个项目最终的结果，结果只是一个点缀，不是重点。</p></li><li><p><strong>所有的工作都需要有价值，有目标</strong>：所有的在答辩过程中描述的工作都需要有价值，有目标。以一种碰到问题 → 解决问题的思路去说，而不是凭空就出现了问题的解决方法。</p></li><li><p><strong>挑重点说</strong>：时间很宝贵，不可能将所有的问题及解决方案都说完，要调能突出能力的重点说。信息量不能爆炸，本身评委可能就不太了解你这部分内容，如果你还说了非常多的东西，评委的接受度会急剧降低。</p></li><li><p><strong>成果量化</strong>：体现出你做的好的一个最简单的方式就是将成果进行量化，比如对什么东西进行了优化，那么优化的效果就要体现出来，比如有了一倍的性能提升。</p></li><li><p><strong>注意体现工作大小的方式</strong>：负责的东西尽量以模块的方式体现出来，不然如果说的太小的话，评委会认为做的没有什么技术含量，或者没有什么价值。</p></li><li><p><strong>不要超越当前职级太多去说事情</strong>：需要站在当前的职级以及下一个职级的角度上去说事情，不建议说超出自己职责能力范围太多的事情，比如说，如果是应届生晋升，如果在整个过程中去说指导组内其他老人做了什么事情，评委就会觉得你可能是不是把自己拔的太高了，也会认为你们的团队管理是不是有问题，怎么能让一个新手去教一个老手。</p></li><li><p><strong>PPT 慎用标红加粗</strong>：做 PPT 时，有时会回去把一些自己认为重要的字体进行家族标红，但是一定要想清楚标红的逻辑，你要说出来为什么标红加粗，不然评委会懵逼。</p></li><li><p><strong>PPT 上的内容要简单易理解</strong>：不要写一句你自己都看不懂的话在上面</p></li><li><p><strong>PPT 尽量写短语</strong>：用短语引申</p></li><li><p><strong>划清和他人工作的界限</strong>：如果涉及到和他人一起合作，需要说明、划分清楚界限，让评委知道你在项目中干了什么。不可能所有的工作都是一个人完成的。</p></li><li><p><strong>PPT 尽量在同一页中体现问题以及对应的解法</strong>：如果在两页会有割裂感。如果一页写不下，那么在下一页也要将问题体现出来。</p></li></ol><h2 id="4-2-答辩材料应该包含的内容"><a href="#4-2-答辩材料应该包含的内容" class="headerlink" title="4.2.答辩材料应该包含的内容"></a>4.2.答辩材料应该包含的内容</h2><p>以 PPT 为例，一般会包含以下几个部分的内容。</p><ol><li>个人简介 &amp; 工作内容概述</li><li>工作成果及心得体会</li><li>专业影响力及专业贡献</li><li>未来思考及规划</li></ol><h3 id="4-2-1-个人简介-amp-工作内容概述"><a href="#4-2-1-个人简介-amp-工作内容概述" class="headerlink" title="4.2.1.个人简介 &amp; 工作内容概述"></a>4.2.1.个人简介 &amp; 工作内容概述</h3><p>个人简介 &amp; 工作内容概述主要是让评委知道你是做什么方向的，参与过什么项目，目前负责哪一块的。</p><ol><li><p><strong>个人简介</strong>：主要介绍在什么时间在什么公司主要干了什么项目？比如说在 2015年5月 - 2016年5月在阿里巴巴负责什么数据项目的建设。</p></li><li><p><strong>工作内容概述</strong>：主要是介绍目前的工作内容及负责的工作。可以简单挑几个重点项目中你负责的工作模块，做一个对应的介绍。</p></li></ol><h3 id="4-2-2-工作成果及心得体会"><a href="#4-2-2-工作成果及心得体会" class="headerlink" title="4.2.2.工作成果及心得体会"></a>4.2.2.工作成果及心得体会</h3><p>这部分是主要介绍以及突出自己解决问题能力的部分，主要去介绍你工作重点突出的项目以及解决的难点问题。让评委知道你是怎么分析问题、解决问题、以及在项目中干了什么牛逼的事情。可以介绍一到两个项目（两个项目需要各有侧重，比如第一个项目体现技术方案牛逼，第二个可以体现保障方案牛逼）。</p><p>可以通过以下几点进行介绍。</p><ol><li><p><strong>项目背景</strong>：主要是让评委知道这个项目是做什么的，包括了<strong>业务本身的背景以及目标，你负责的模块的定位以及目标</strong>（讲清楚为什么需要你负责的模块，你负责的模块在这个项目中是干啥的，其重要性）。</p></li><li><p><strong>需求分析以及难点问题分析</strong>：从<strong>需求以及业务的保障要求</strong>出发，<strong>分析业务特征以及目前组内的能力现状</strong>，从而明确<strong>难点问题（在业务上的难点与挑战是什么？在技术上的难点和挑战是什么？）</strong>。比如也可以从业务出发，比如业务一年比一年难，之前的技术方案不满足业务要求，其中的难点是什么？</p></li><li><p><strong>难点问题的解决方案</strong>：主要围绕着上述难点问题，将问题一个一个进行解决，在做 PPT 时，需要注意问题解决方案的先后逻辑关系，不建议只是罗列问题以及问题的解决方案。可以围绕着一个技术\业务框架图，去介绍这些问题的解决方案以及方案在框架中的定位。</p></li></ol><blockquote><p>Notes:</p><p>这里要注意我们的解决方案可能是非常多的，但是不必全部都说出来，列举出来，因为答辩的时间是有限的，没有那么多时间去把所有的东西都说清楚，所以我们需要挑重点，说能体现我们能力的重点。</p></blockquote><ol start="4"><li><strong>项目成果</strong>：可以简单带过就可以，比如说你在<strong>数据建设</strong>上做了什么东西，达成了什么目标？在<strong>数据保障</strong>上做到了什么？在<strong>数据沉淀</strong>上做到了什么？在<strong>知识传播</strong>上又做了什么？</li></ol><h3 id="4-2-3-专业影响力及专业贡献"><a href="#4-2-3-专业影响力及专业贡献" class="headerlink" title="4.2.3.专业影响力及专业贡献"></a>4.2.3.专业影响力及专业贡献</h3><p>这里我们需要注意，因为所有的工作都需要有价值，有目标，以结果为导向，对我们之后的工作有帮助。所以在答辩的时候也需要从解决问题或者达成目标出发来说影响力及贡献，带着目的去做影响力的扩大和专业贡献，而不能简单地只说出有哪些影响力及贡献。</p><p>可以围绕着以下三个方面去说：</p><ol><li><strong>技术演进</strong>：推进了什么工具链的优化，帮助解决了什么问题，在什么场景应用，应用的广泛度，提高了多少人效，优化了多少性能（列举一些数字）</li><li><strong>知识沉淀</strong>：沉淀了什么方法论、什么文档、什么工具，帮助后续的工作有什么样的指导、建议、帮助（比如作为标准化的方案）。</li><li><strong>知识的传播、知识分享</strong>：在公司外部你有什么分享？然后达到了技术影响力的输出。在公司内部分享了东西？包括组内外的技术分享，交流组内的技术分享。指导了多少人干了什么事情，干了多少事情，从而能够帮助他人更高效、标准地上手什么样的开发等。</li></ol><h3 id="4-2-4-未来思考及规划"><a href="#4-2-4-未来思考及规划" class="headerlink" title="4.2.4.未来思考及规划"></a>4.2.4.未来思考及规划</h3><p>未来规划主要是<strong>站在目前自己的角色</strong>出发去说。不能说一些虚无缥缈的大东西。</p><p>思路可以是目前的痛点，未来的规划。</p><p>可以从以下三个角度去说：</p><ol><li><strong>从业务角度说</strong>：比如说现在业务遇到了什么样的难题，或者什么样的问题没有解决，未来你要怎么样去解决这些问题（比如怎样做到低成本或者更高效的？）</li><li><strong>从技术角度说</strong>：目前什么什么方案不成熟，然后后续要做什么样的优化（比如说目前什么什么技术研究深度、广度不够，后续要怎么进行深度和广度上的探索研究？）</li><li><strong>从个人影响力说</strong>：比如后续可以从公司外，公司内总内外怎么样去提升个人的影响力（比如说你可以分享一些技术，分享一些文章，开设一些课程？）</li></ol><h1 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h1><p>再次说明，晋升永远是有了下一个职级的能力才会有晋升机会。所以如果你想晋升，一定不能只停留满足现状。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-前言&quot;&gt;&lt;a href=&quot;#1-前言&quot; class=&quot;headerlink&quot; title=&quot;1.前言&quot;&gt;&lt;/a&gt;1.前言&lt;/h1&gt;&lt;p&gt;准备答辩内容及材料是一个非常耗时耗力的过程，因此有一些方法的沉淀的话，后续对自己以及帮你 review 材料的人都会更轻松一
      
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink sql 知其所以然（九）：1.13.2 flink tumble window</title>
    <link href="https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9Atumble-window-window-tvf/"/>
    <id>https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9Atumble-window-window-tvf/</id>
    <published>2021-11-13T06:25:58.000Z</published>
    <updated>2021-09-20T14:17:48.284Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>想啥呢，小宝贝，还不三连？？？（关注  +  点赞 + 再看）</strong>，对博主的肯定，会督促博主持续的输出更多的优质实战内容！！！</p></blockquote><h1 id="1-序篇-本文结构"><a href="#1-序篇-本文结构" class="headerlink" title="1.序篇-本文结构"></a>1.序篇-本文结构</h1><p>源码公众号后台回复<strong>1.13.2 tumble window 的奇妙解析之路</strong>获取。</p><p>针对 datastream api 大家都比较熟悉了，还是那句话，在 datastream 中，你写的代码逻辑是什么样的，它最终的执行方式就是什么样的。</p><p>但是对于 flink sql 的执行过程，大家还是不熟悉的。</p><p>此节就是窗口聚合章节的第二篇，上节介绍了 1.13 之前的 tumble window 实现，本节介绍 window tvf 下的 tumble window 案例给大家介绍其使用方式和原理。</p><p>本节依然从以下几个章节给大家详细介绍 flink sql 的能力。</p><ol><li>目标篇-本文能帮助大家了解 flink sql 什么？<ul><li>回顾上节的 flink sql 适用场景的结论</li></ul></li><li>概念篇-先聊聊常见的窗口聚合<ul><li>窗口竟然拖慢数据产出？</li><li>常用的窗口</li></ul></li><li>实战篇-简单的 tumble window 案例和运行原理<ul><li>先看一个 datastream 窗口案例</li><li>flink sql tumble window 的语义</li><li>tumble window 实际案例</li><li>GeneratedWatermarkGenerator - flink 1.12.1</li><li>BinaryRowDataKeySelector - flink 1.12.1</li><li>AggregateWindowOperator - flink 1.12.1</li></ul></li><li>总结与展望篇</li></ol><p><strong>先说说结论，以下这些结论已经在上节说过了，此处附上上节文章：</strong></p><ol><li><strong>场景问题</strong>：flink sql 很适合简单 ETL，以及基本全部场景下的聚合类指标（本节要介绍的 tumble window 就在聚合类指标的范畴之内）。</li><li><strong>语法问题</strong>：flink sql 语法其实是和其他 sql 语法基本一致的。基本不会产生语法问题阻碍使用 flink sql。但是本节要介绍的 tumble window 的语法就是略有不同的那部分。下面详细介绍。</li><li><strong>运行问题</strong>：查看 flink sql 任务时的一些技巧，以及其中一些可能会碰到的坑：<ul><li>去 flink webui 就能看到这个任务目前在做什么。包括算子名称都会给直接展示给我们目前哪个算子在干啥事情，在处理啥逻辑。</li><li>sql 的 watermark 类型必须要设置为 <code>TIMESTAMP(3)</code>。如果你的数据源时间戳类型是 13 位 bigint 类型时间戳，可以用 <code>ts AS TO_TIMESTAMP_LTZ(row_time, 3)</code> 将其转换为 <code>TIMESTAMP(3)</code> 类型。</li><li>事件时间逻辑中，sql api 和 datastream api 对于数据记录时间戳存储逻辑是不一样的。datastream api：每条记录的 rowtime 是放在 StreamRecord 中的时间戳字段中的。sql api：时间戳是每次都从数据中进行获取的。算子中会维护一个下标。可以按照下标从数据中获取时间戳。</li></ul></li></ol><h1 id="2-目标篇-本文能帮助大家了解-flink-sql-tumble-window-什么？"><a href="#2-目标篇-本文能帮助大家了解-flink-sql-tumble-window-什么？" class="headerlink" title="2.目标篇-本文能帮助大家了解 flink sql tumble window 什么？"></a>2.目标篇-本文能帮助大家了解 flink sql tumble window 什么？</h1><p>关于 flink sql tumble window 一般都会有以下问题。本文的目标也是为大家解答这些问题：</p><ol><li><strong>场景问题</strong>：场景问题就不必多说，datastream 在 tumble window 场景下的应用很多了，分钟级别聚合等常用场景</li><li><strong>语法问题</strong>：1.13.2 flink sql 写 tumble window 语法已经没有之前的特殊写法了。下文详细介绍。</li><li><strong>运行问题</strong>：使用一条简单的分钟级别同时在线案例的 tumble window sql 帮大家从 transformation、runtime 帮大家理解 tumble window 的整体运行机制。</li><li><strong>理解误区</strong>：既然是 sql 必然要遵循 sql 语义，sql tumble window 聚合是输入多条，产出一条数据。并不像 datastream 那样可以在窗口 udf 中做到多条输入，多条输出。</li></ol><p>在正式开始聊 tumble window 之前，先看看上节 flink sql 适用场景的结论。让大家先有 flink sql 的一个整体印象以及结论。</p><h2 id="2-1-回顾上节的-flink-sql-适用场景的结论"><a href="#2-1-回顾上节的-flink-sql-适用场景的结论" class="headerlink" title="2.1.回顾上节的 flink sql 适用场景的结论"></a>2.1.回顾上节的 flink sql 适用场景的结论</h2><p>不装了，我坦白了，flink sql 其实很适合干的活就是 dwd 清洗，dws 聚合。</p><p>此处主要针对实时数仓的场景来说。flink sql 能干 dwd 清洗，dws 聚合，基本上实时数仓的大多数场景都能给覆盖了。</p><p>flink sql 牛逼！！！</p><p>但是！！！</p><p>经过博主使用 flink sql 经验来看，并不是所有的 dwd，dws 聚合场景都适合 flink sql（截止发文阶段来说）！！！</p><p>其实这些目前不适合 flink sql 的场景总结下来就是在处理上比 datastream 还是会有一定的损失。</p><p>先总结下使用场景：<br><strong>1. dwd</strong>：简单的清洗、复杂的清洗、维度的扩充、各种 udf 的使用<br><strong>2. dws</strong>：各类聚合</p><p>然后分适合的场景和不适合的场景来说，因为只这一篇不能覆盖所有的内容，所以本文此处先大致给个结论，之后会结合具体的场景详细描述。</p><ul><li><strong>适合的场景：</strong><ol><li>简单的 dwd 清洗场景</li><li>全场景的 dws 聚合场景</li></ol></li><li><strong>目前不太适合的场景：</strong><ol><li>复杂的 dwd 清洗场景：举例比如使用了很多 udf 清洗，尤其是使用很多的 json 类解析清洗</li><li>关联维度场景：举例比如 datastream 中经常会有攒一批数据批量访问外部接口的场景，flink sql 目前对于这种场景虽然有 localcache、异步访问能力，但是依然还是一条一条访问外部缓存，这样相比批量访问还是会有性能差距。</li></ol></li></ul><h1 id="3-概念篇-先聊聊常见的窗口聚合"><a href="#3-概念篇-先聊聊常见的窗口聚合" class="headerlink" title="3.概念篇-先聊聊常见的窗口聚合"></a>3.概念篇-先聊聊常见的窗口聚合</h1><p>窗口聚合大家都在 datastream api 中很熟悉了，目前在实时数据处理的过程中，窗口计算可以说是最重要、最常用的一种计算方式了。</p><p>但是在抛出窗口概念之前，博主有几个关于窗口的小想法说一下。</p><h2 id="3-1-窗口竟然拖慢数据产出？"><a href="#3-1-窗口竟然拖慢数据产出？" class="headerlink" title="3.1.窗口竟然拖慢数据产出？"></a>3.1.窗口竟然拖慢数据产出？</h2><p>一个小想法。</p><p><strong>先抛结论：窗口会拖慢实时数据的产出，是在目前下游分析引擎能力有限的情况下的一种妥协方案。</strong></p><p>站在数据开发以及需求方的世界中，当然希望所有的数据都是<strong>实时来的，实时处理的，实时产出的，实时展现的</strong>。</p><p><strong>举个例子</strong>：如果你要满足一个一分钟窗口聚合的 pv，uv，或者其他聚合需求。</p><p><strong>olap 数据服务引擎</strong> 就可以满足上述的<strong>实时来的，实时处理的，实时产出的，实时展现的</strong>的场景。flink 消费处理明细数据，产出到 kafka，然后直接导入到 olap 引擎中。查询时直接用 olap 做聚合。这其中是没有任何窗口的概念的。<br>但是整个链路中，要保障端对端精确一次，要保障大数据量情况下 olap 引擎能够秒级查询返回，更何况有一些去重类指标的计算，等等场景。把这些压力都放在 olap 引擎的压力是很大的。</p><p>因此在 <strong>flink 数据计算引擎</strong>中就诞生了窗口的概念。我们可以直接在计算引擎中进行窗口聚合计算，然后等到窗口结束之后直接把结果数据产出。<br>这就出现了博主所说的窗口拖慢了实时数据产出的情况。而且窗口在处理不好的情况下可能会导致数据丢失。</p><p>关于上述两种情况的具体优劣选择，都由大家自行选择。上述只是引出博主一些想法。</p><h2 id="3-2-常用的窗口"><a href="#3-2-常用的窗口" class="headerlink" title="3.2.常用的窗口"></a>3.2.常用的窗口</h2><p>目前已知的窗口分为以下四种。</p><p><strong>1. Tumble Windows</strong><br><strong>2. Hop Windows</strong><br><strong>3. Cumulate Windows</strong><br><strong>4. Session Windows</strong></p><p>这些窗口的具体描述直接见官网，有详细的说明。此处不赘述。</p><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/sql/queries/window-agg/">https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/sql/queries/window-agg/</a></p><p>此处介绍下 flink 中常常会涉及到的两个容易混淆的概念就是：窗口 + key。这里来形象的说明下。</p><ul><li><p><strong>窗口：时间周期上面的划分</strong>。将无限流进行纵向切分，将无限流切分为一个一个的窗口，窗口相当于是无限流中的一段时间内的数据。</p></li><li><p><strong>key：数据类别上面的划分</strong>。将无限流进行横向划分，相同 key 的数据会被划分到一组中，这个 key 的数据也是一条无限流。</p></li></ul><p>如下图所示。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/1.png" alt="1"></p><h1 id="4-实战篇-简单的-tumble-window-案例和运行原理"><a href="#4-实战篇-简单的-tumble-window-案例和运行原理" class="headerlink" title="4.实战篇-简单的 tumble window 案例和运行原理"></a>4.实战篇-简单的 tumble window 案例和运行原理</h1><p>源码公众号后台回复<strong>1.13.2 tumble window 的奇妙解析之路</strong>获取。</p><h2 id="4-1-先看一个-datastream-窗口案例"><a href="#4-1-先看一个-datastream-窗口案例" class="headerlink" title="4.1.先看一个 datastream 窗口案例"></a>4.1.先看一个 datastream 窗口案例</h2><p>在介绍 sql tumble window 窗口算子执行案例之前，先看一个 datastream 中的窗口算子案例。其逻辑都是相通的。会对我们了解 sql tumble window 算子有帮助。</p><p>我们先看看 datastream 处理逻辑。</p><p>以下面这个为例。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">_04_TumbleWindowTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment env =</span><br><span class="line">                StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(<span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line"></span><br><span class="line">        env.addSource(<span class="keyword">new</span> UserDefinedSource())</span><br><span class="line">                .assignTimestampsAndWatermarks(<span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor&lt;Tuple4&lt;String, String, Integer, Long&gt;&gt;(Time.seconds(<span class="number">0</span>)) &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Tuple4&lt;String, String, Integer, Long&gt; element)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> element.f3;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .keyBy(<span class="keyword">new</span> KeySelector&lt;Tuple4&lt;String, String, Integer, Long&gt;, String&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> String <span class="title">getKey</span><span class="params">(Tuple4&lt;String, String, Integer, Long&gt; row)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> row.f0;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">                .sum(<span class="number">2</span>)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">&quot;1.12.1 DataStream TUMBLE WINDOW 案例&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UserDefinedSource</span> <span class="keyword">implements</span> <span class="title">SourceFunction</span>&lt;<span class="title">Tuple4</span>&lt;<span class="title">String</span>, <span class="title">String</span>, <span class="title">Integer</span>, <span class="title">Long</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isCancel;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Tuple4&lt;String, String, Integer, Long&gt;&gt; sourceContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> (!<span class="keyword">this</span>.isCancel) &#123;</span><br><span class="line"></span><br><span class="line">                sourceContext.collect(Tuple4.of(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="number">1</span>, System.currentTimeMillis()));</span><br><span class="line"></span><br><span class="line">                Thread.sleep(<span class="number">10L</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.isCancel = <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>datastream 生产的具体的 transformation 如下图：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/24.png" alt="24"></p><p>其中我们只关注最重要的 <code>WindowOperator</code> 算子。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/25.png" alt="25"></p><p>其中 <code>WindowOperator</code> 算子包含的重要属性如下图。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/26.png" alt="26"></p><p>来看看 <code>WindowOperator</code> 的执行逻辑。窗口执行的整体详细流程可以参考：<a href="http://wuchong.me/blog/2016/05/25/flink-internals-window-mechanism/">http://wuchong.me/blog/2016/05/25/flink-internals-window-mechanism/</a></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/23.png" alt="23"></p><h2 id="4-2-flink-sql-tumble-window-的语义"><a href="#4-2-flink-sql-tumble-window-的语义" class="headerlink" title="4.2.flink sql tumble window 的语义"></a>4.2.flink sql tumble window 的语义</h2><p>介绍到 tumble window 的语义，总要有对比的去介绍。这里的参照物就是 datastream api。</p><p>在 datastream api 中。tumble window 一般用作以下两种场景。</p><ol><li><strong>业务场景</strong>：使用 tumble window 很轻松的计算出窗口内的聚合数据。一般是多条输入数据，窗口结束时一条输出数据。</li><li><strong>优化场景</strong>：窗口聚合一批数据然后批量访问外部存储扩充维度、或者有一些自定义的处理逻辑。一般是多条输入数据，窗口结束时多条输出数据。</li></ol><p>但是在 sql api 中。tumble window 是聚合（group by）语义，聚合在 sql 标准中的数据处理逻辑是多条输入，在窗口触发时就输出一条数据的语义。而上面的常常用在 datastream 中的<strong>优化场景</strong>是多对多的场景。因此和 sql 语义不符合。所以 flink sql tumble window 一般都是用于计算聚合运算值来使用。</p><h2 id="4-3-tumble-window-实际案例"><a href="#4-3-tumble-window-实际案例" class="headerlink" title="4.3.tumble window 实际案例"></a>4.3.tumble window 实际案例</h2><p>滚动窗口的特性就是会将无限流进行纵向划分成一个一个的窗口，每个窗口都是相同的大小，并且不重叠。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/22.png" alt="22"></p><p>来，在介绍原理之前，总要先用起来，我们就以下面这个例子展开。</p><p><strong>1.（flink 1.13.2）场景：简单且常见的分维度分钟级别同时在线用户数、总销售额</strong></p><p>数据源表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    <span class="comment">-- 维度数据</span></span><br><span class="line">    dim STRING,</span><br><span class="line">    <span class="comment">-- 用户 id</span></span><br><span class="line">    user_id <span class="type">BIGINT</span>,</span><br><span class="line">    <span class="comment">-- 用户</span></span><br><span class="line">    price <span class="type">BIGINT</span>,</span><br><span class="line">    <span class="comment">-- 事件时间戳</span></span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    <span class="comment">-- watermark 设置</span></span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.dim.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><blockquote><p><strong>Notes - 关于 watermark 容易踩得坑：</strong><br>sql 的 watermark 类型必须要设置为 <code>TIMESTAMP(3)</code>。如果你的数据源时间戳类型是 13 位 bigint 类型时间戳，可以用 <code>ts AS TO_TIMESTAMP_LTZ(row_time, 3)</code> 将其转换为 <code>TIMESTAMP(3)</code> 类型。</p></blockquote><p>数据汇表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    dim STRING,</span><br><span class="line">    pv <span class="type">BIGINT</span>,</span><br><span class="line">    sum_price <span class="type">BIGINT</span>,</span><br><span class="line">    max_price <span class="type">BIGINT</span>,</span><br><span class="line">    min_price <span class="type">BIGINT</span>,</span><br><span class="line">    uv <span class="type">BIGINT</span>,</span><br><span class="line">    window_start <span class="type">bigint</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>数据处理逻辑：</p><p>可以看下下面语法，窗口聚合的写法有专门的 <code>tumble(row_time, interval &#39;1&#39; minute)</code> 写法，这就是与平常我们写的 hive sql，mysql 等不一样的地方。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> sink_table</span><br><span class="line"><span class="keyword">select</span> dim,</span><br><span class="line">   <span class="built_in">sum</span>(bucket_pv) <span class="keyword">as</span> pv,</span><br><span class="line">   <span class="built_in">sum</span>(bucket_sum_price) <span class="keyword">as</span> sum_price,</span><br><span class="line">   <span class="built_in">max</span>(bucket_max_price) <span class="keyword">as</span> max_price,</span><br><span class="line">   <span class="built_in">min</span>(bucket_min_price) <span class="keyword">as</span> min_price,</span><br><span class="line">   <span class="built_in">sum</span>(bucket_uv) <span class="keyword">as</span> uv,</span><br><span class="line">   <span class="built_in">max</span>(window_start) <span class="keyword">as</span> window_start</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line"> <span class="keyword">SELECT</span> dim,</span><br><span class="line">     UNIX_TIMESTAMP(<span class="built_in">CAST</span>(window_start <span class="keyword">AS</span> STRING)) <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">as</span> window_start, </span><br><span class="line">        window_end, </span><br><span class="line">        <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">as</span> bucket_pv,</span><br><span class="line">        <span class="built_in">sum</span>(price) <span class="keyword">as</span> bucket_sum_price,</span><br><span class="line">        <span class="built_in">max</span>(price) <span class="keyword">as</span> bucket_max_price,</span><br><span class="line">        <span class="built_in">min</span>(price) <span class="keyword">as</span> bucket_min_price,</span><br><span class="line">            <span class="comment">-- 计算 uv 数</span></span><br><span class="line">        <span class="built_in">count</span>(<span class="keyword">distinct</span> user_id) <span class="keyword">as</span> bucket_uv</span><br><span class="line"> <span class="keyword">FROM</span> <span class="keyword">TABLE</span>(TUMBLE(</span><br><span class="line"> <span class="keyword">TABLE</span> source_table</span><br><span class="line"> , DESCRIPTOR(row_time)</span><br><span class="line"> , <span class="type">INTERVAL</span> <span class="string">&#x27;60&#x27;</span> <span class="keyword">SECOND</span>))</span><br><span class="line"> <span class="keyword">GROUP</span> <span class="keyword">BY</span> window_start, </span><br><span class="line">    window_end,</span><br><span class="line">  dim,</span><br><span class="line">              <span class="comment">-- 按照用户 id 进行分桶，防止数据倾斜</span></span><br><span class="line">  <span class="built_in">mod</span>(user_id, <span class="number">1024</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> dim,</span><br><span class="line"> window_start</span><br></pre></td></tr></table></figure><p><strong>2.运行：可以看到，其实在 flink sql 任务中，其会把对应的处理逻辑给写到算子名称上面。</strong></p><blockquote><p><strong>Notes - 观察 flink sql 技巧 1：</strong><br>这个其实就是我们观察 flink sql 任务的第一个技巧。如果你想知道你的 flink 任务在干啥，第一反应是去 flink webui 看看这个任务目前在做什么。包括算子名称都会给直接展示给我们目前哪个算子在干啥事情，在处理啥逻辑</p></blockquote><p><strong>先看一下整个算子图，如下图。从左到右总共分为四个算子。</strong></p><ol><li>第一个算子就是数据源算子，分配 watermark</li><li>第二个算子就是在数据源算子的本地进行聚合，类似于 map-reduce map 阶段的 combiner 作用，先在本地进行聚合，然后将聚合结果发下去。</li><li>第三个算子就是第一层 group by 分桶聚合计算，将数据按照 user_id 分桶打散，然后聚合计算。</li><li>第四个算子就是第二层 group by 合桶计算。</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/3.png" alt="3"></p><p>整体描述一下：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/29.png" alt="29"></p><p>来看看每一个算子具体做了什么事情。</p><p><strong>第一个算子：</strong></p><ol><li>table scan 读取数据源</li><li>从数据源中获取对应的字段（包括源表定义的 rowtime）</li><li>分配 watermark（按照源表定义的 watermark 分配对应的 watermark）</li><li>将一些必要的字段抽取。比如 group by 中的字段。在 hash 时需要使用。</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/4.png" alt="4"></p><p><strong>第二个算子：</strong></p><ol><li>类似 map-reduce 的 combiner 本地聚合。这里的 combiner 的聚合粒度有两部组成，第一部分就是 group by 的 key，第二部分是 user_id。</li><li>将数据按照第一层 select 中的数据进行计算以及格式化</li></ol><blockquote><p><strong>Notes：</strong></p><ol><li>首先 local agg 的目的是在不影响数据正确性的情况下，减少输出到下游的数据量，提升任务性能。</li><li>其中 max，min，count 都能很好地利用本地 combiner 输出量，比如 max 就取 group by key 粒度的最大值即可</li><li>但是一旦涉及到 count(distinct)，只按照 group by key 粒度去处理数据，就会出现数据准确性问题，举例：比如两个 source 都来相同 id 的数据，在去重时，按照 group by key 去重就会导致这个 user_id 在两个算子上都计算一次，在下游算子聚合时就会将这两个结果都 +1，最后结果就算重复了。</li></ol></blockquote><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/5.png" alt="5"></p><p><strong>第三个算子：</strong></p><ol><li>窗口聚合分桶计算</li><li>将数据按照第一层 select 中的数据进行计算以及格式化</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/6.png" alt="6"></p><p><strong>第四个算子：</strong></p><ol><li>窗口聚合合桶计算</li><li>将数据按照第二层 select 中的数据进行计算以及格式化</li><li>将结果 sink 到输出表</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/7.png" alt="7"></p><p><strong>3.（flink 1.13.2）结果：</strong></p><figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">1&gt;</span><span class="bash"> +U[7, 36403, 1824202613, 99999, 2, 30498, 1632136920000]</span></span><br><span class="line"><span class="meta">2&gt;</span><span class="bash"> -U[a, 37001, 1857079208, 99999, 3, 30857, 1632136920000]</span></span><br><span class="line"><span class="meta">2&gt;</span><span class="bash"> +U[a, 37037, 1858977218, 99999, 3, 30886, 1632136920000]</span></span><br><span class="line"><span class="meta">1&gt;</span><span class="bash"> -U[7, 36403, 1824202613, 99999, 2, 30498, 1632136920000]</span></span><br><span class="line"><span class="meta">1&gt;</span><span class="bash"> +U[7, 36428, 1825407205, 99999, 2, 30523, 1632136920000]</span></span><br><span class="line"><span class="meta">1&gt;</span><span class="bash"> -U[2, 36970, 1848722634, 99999, 6, 30876, 1632136920000]</span></span><br><span class="line"><span class="meta">2&gt;</span><span class="bash"> -U[6, 36911, 1856162742, 99998, 2, 30801, 1632136920000]</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><p><strong>4.（flink 1.13.2）原理：</strong></p><p>关于 sql 开始运行的机制见上一节详述。</p><p>此处只介绍相比前一节新增内容。可以看到上述代码的具体 transformation 如下图。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/8.png" alt="8"></p><h2 id="4-4-LocalSlicingWindowAggOperator-flink-1-13-2"><a href="#4-4-LocalSlicingWindowAggOperator-flink-1-13-2" class="headerlink" title="4.4.LocalSlicingWindowAggOperator - flink 1.13.2"></a>4.4.LocalSlicingWindowAggOperator - flink 1.13.2</h2><h3 id="4-4-1-整体处理逻辑"><a href="#4-4-1-整体处理逻辑" class="headerlink" title="4.4.1.整体处理逻辑"></a>4.4.1.整体处理逻辑</h3><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/9.png" alt="9"></p><p>整体处理逻辑如下图。</p><p>这里处理每一条数据时，主要是把数据放入到 local buffer 中。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/1.png" alt="1"></p><p>涉及到 local combiner 处理计算时，就是第 3 点，跟进代码 <code>windowBuffer.advanceProgress(currentWatermark)</code>。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/12.png" alt="12"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/13.png" alt="13"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/14.png" alt="14"></p><p>这里看下具体 combine 流程。总共四步，如下图。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/15.png" alt="15"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/16.png" alt="16"></p><h3 id="4-4-2-local-agg-udf-逻辑"><a href="#4-4-2-local-agg-udf-逻辑" class="headerlink" title="4.4.2.local agg udf 逻辑"></a>4.4.2.local agg udf 逻辑</h3><p>其实 local agg 的处理逻辑很简单，基本和上节说的 1.12 实现一致。都是代码生成之后做 sum，count，count distinct 的计算。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/27.png" alt="27"></p><h2 id="4-5-SlicingWindowOperator-flink-1-12-1"><a href="#4-5-SlicingWindowOperator-flink-1-12-1" class="headerlink" title="4.5.SlicingWindowOperator - flink 1.12.1"></a>4.5.SlicingWindowOperator - flink 1.12.1</h2><h3 id="4-5-1-整体算子处理逻辑"><a href="#4-5-1-整体算子处理逻辑" class="headerlink" title="4.5.1.整体算子处理逻辑"></a>4.5.1.整体算子处理逻辑</h3><p>依然如下图：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/30.png" alt="30"></p><p>先看看 transformation 中包含什么内容：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/10.png" alt="10"></p><p>整体处理逻辑如下：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/17.png" alt="17"></p><p>也是在处理 watermark 时，进行聚合计算。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/18.png" alt="18"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/19.png" alt="19"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/20.png" alt="20"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/21.png" alt="21"></p><p><strong>这里有一个重点，就是 global agg udf 是执行 merge 操作进行聚合的。其逻辑就是将上游 combiner 的结果数据聚合。</strong></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/22.png" alt="22"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/23.png" alt="23"></p><p>在窗口触发时，将结果输出。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/24.png" alt="24"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/25.png" alt="25"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/26.png" alt="26"></p><h3 id="4-5-2-local-agg、global-agg-udf-逻辑"><a href="#4-5-2-local-agg、global-agg-udf-逻辑" class="headerlink" title="4.5.2.local agg、global agg udf 逻辑"></a>4.5.2.local agg、global agg udf 逻辑</h3><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/10_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A1.13tumblewindow%E8%AE%A1%E7%AE%97/28.png" alt="28"></p><p>其实 global agg 和 local agg 逻辑基本一致，这里不再赘述。</p><h1 id="5-总结与展望篇"><a href="#5-总结与展望篇" class="headerlink" title="5.总结与展望篇"></a>5.总结与展望篇</h1><p><strong>想啥呢，小宝贝，还不三连？？？（关注  +  点赞 + 再看）</strong></p><p>源码公众号后台回复<strong>1.13.2 tumble window 的奇妙解析之路</strong>获取。</p><p>本文主要介绍了 window tvf 实现的 tumble window 聚合类指标的常见场景案例以及其底层运行原理。</p><p>而且也介绍了在查看 flink sql 任务时的一些技巧：</p><ol><li>去 flink webui 就能看到这个任务目前在做什么。包括算子名称都会给直接展示给我们目前哪个算子在干啥事情，在处理啥逻辑。</li><li>sql 的 watermark 类型要设置为 <code>TIMESTAMP(3)</code>。如果你的数据源时间戳类型是 13 位 bigint 类型时间戳，可以用 <code>ts AS TO_TIMESTAMP_LTZ(row_time, 3)</code> 将其转换为 <code>TIMESTAMP(3)</code> 类型。</li><li>事件时间逻辑中，sql api 和 datastream api 对于数据记录时间戳存储逻辑是不一样的。datastream api：每条记录的 rowtime 是放在 StreamRecord 中的时间戳字段中的。sql api：时间戳是每次都从数据中进行获取的。算子中会维护一个下标。可以按照下标从数据中获取时间戳。</li></ol><p>希望大家能持续关注。支持博主。喜欢的请关注 + 点赞 + 再看。</p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink sql 知其所以然（十）：大家都用 cumulate window 计算累计指标啦</title>
    <link href="https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/11_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%EF%BC%89%EF%BC%9A%E5%91%A8%E6%9C%9F%E5%86%85%E7%B4%AF%E8%AE%A1%E6%8C%87%E6%A0%87%E7%94%A8%20cumulate%20window%20%E5%B0%B1%E5%A4%9F%E4%BA%86/"/>
    <id>https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/11_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%EF%BC%89%EF%BC%9A%E5%91%A8%E6%9C%9F%E5%86%85%E7%B4%AF%E8%AE%A1%E6%8C%87%E6%A0%87%E7%94%A8%20cumulate%20window%20%E5%B0%B1%E5%A4%9F%E4%BA%86/</id>
    <published>2021-11-13T06:25:58.000Z</published>
    <updated>2021-11-21T09:19:23.917Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>想啥呢，小宝贝，还不三连？？？（关注  +  点赞 + 再看）</strong>，对博主的肯定，会督促博主持续的输出更多的优质实战内容！！！</p></blockquote><h1 id="1-序篇"><a href="#1-序篇" class="headerlink" title="1.序篇"></a>1.序篇</h1><p>源码公众号后台回复<strong>1.13.2 cumulate window 的奇妙解析之路</strong>获取。</p><p>此节就是窗口聚合章节的第三篇，上节介绍了 1.13 window tvf tumble window 实现，本节主要介绍 1.13. window tvf 的一个重磅更新，即 cumulate window。</p><p>本节从以下几个章节给大家详细介绍 cumulate window 的能力。</p><ol><li>应用场景介绍</li><li>预期的效果</li><li>解决方案介绍</li><li>总结及展望篇</li></ol><h1 id="2-应用场景介绍"><a href="#2-应用场景介绍" class="headerlink" title="2.应用场景介绍"></a>2.应用场景介绍</h1><p>先来一个简单的小调查：在实时场景中，你见到过最多的指标需求场景是哪一种？</p><p>答案：博主相信，占比比较多的不是 PCU（即同时在线 PV，UV），而是周期内累计 PV，UV 指标（如每天累计到当前这一分钟的 Pv，UV）。因为这类指标是一段周期内的累计状态，对分析师来说更具统计分析价值，而且几乎所有的复合指标都是基于此类指标的统计（不然离线为啥都要一天的数据，而不要一分钟的数据呢）。</p><p>本文要介绍的就是周期内累计 PV，UV 指标在 flink 1.13 版本的最优解决方案。</p><h1 id="3-预期的效果"><a href="#3-预期的效果" class="headerlink" title="3.预期的效果"></a>3.预期的效果</h1><p>先来一个实际案例来看看在具体输入值的场景下，输出值应该长啥样。</p><p>指标：每天的截止当前分钟的累计 money（sum(money)），去重 id 数（count(distinct id)）。<strong>每天</strong>代表窗口大小为 1 天，<strong>分钟</strong>代表移动步长为分钟级别。</p><p>来一波输入数据：</p><table><thead><tr><th>time</th><th>id</th><th>money</th></tr></thead><tbody><tr><td>2021-11-01 00:01:00</td><td>A</td><td>3</td></tr><tr><td>2021-11-01 00:01:00</td><td>B</td><td>5</td></tr><tr><td>2021-11-01 00:01:00</td><td>A</td><td>7</td></tr><tr><td>2021-11-01 00:02:00</td><td>C</td><td>3</td></tr><tr><td>2021-11-01 00:03:00</td><td>C</td><td>10</td></tr></tbody></table><p>预期输出数据：</p><table><thead><tr><th>time</th><th>count distinct id</th><th>sum money</th></tr></thead><tbody><tr><td>2021-11-01 00:01:00</td><td>2</td><td>15</td></tr><tr><td>2021-11-01 00:02:00</td><td>3</td><td>18</td></tr><tr><td>2021-11-01 00:03:00</td><td>3</td><td>28</td></tr></tbody></table><p>转化为折线图长这样：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/12_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%EF%BC%89%EF%BC%9A1.13cumulatewindow/1.png" alt="当日累计"></p><p>可以看到，其特点就在于，每一分钟的输出结果都是当天零点累计到当前的结果。</p><h1 id="4-解决方案介绍"><a href="#4-解决方案介绍" class="headerlink" title="4.解决方案介绍"></a>4.解决方案介绍</h1><h2 id="4-1-1-13-之前"><a href="#4-1-1-13-之前" class="headerlink" title="4.1.1.13 之前"></a>4.1.1.13 之前</h2><p>可选的解决方案有两种</p><ol><li>tumble window（1天窗口） + early-fire（1分钟）</li><li>group by（1天） + minibatch（1分钟）</li></ol><p>但是上述两种解决方案产出的都是 retract 流，关于 retract 流存在的缺点见如下文章：</p><p>并且 tumble window + early-fire 的触发机制是基于处理时间而非事件时间，具体缺点见如下文章：</p><h2 id="4-2-1-13-及之后"><a href="#4-2-1-13-及之后" class="headerlink" title="4.2.1.13 及之后"></a>4.2.1.13 及之后</h2><p>诞生了 cumulate window 解法，具体见官网链接：</p><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/sql/queries/window-tvf/#cumulate">https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/sql/queries/window-tvf/#cumulate</a></p><p>如下官网文档所示，介绍 cumulate window 的第一句话就是 cumulate window 非常适合于之前使用 tumble window + early-fire 的场景。<br>可以说 cumulate window 就是在用户计算周期内累计 PV，UV 指标时，使用了 tumble window + early-fire 后发现这种方案存在了很多坑的情况下，而诞生的！</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/12_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%EF%BC%89%EF%BC%9A1.13cumulatewindow/2.png" alt="cumulate window"></p><p>其计算机制如下图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/12_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%EF%BC%89%EF%BC%9A1.13cumulatewindow/3.png" alt="cumulate window"></p><p>还是以刚刚的案例说明，以天为窗口，每分钟输出一次当天零点到当前分钟的累计值，在 cumulate window 中，其窗口划分规则如下：</p><p>[2021-11-01 00:00:00, 2021-11-01 00:01:00]<br>[2021-11-01 00:00:00, 2021-11-01 00:02:00]<br>[2021-11-01 00:00:00, 2021-11-01 00:03:00]<br>…<br>[2021-11-01 00:00:00, 2021-11-01 23:58:00]<br>[2021-11-01 00:00:00, 2021-11-01 23:59:00]</p><p>第一个 window 统计的是一个区间的数据；<br>第二个 window 统计的是第一区间和第二个区间的数据；<br>第三个 window 统计的是第一区间，第二个区间和第三个区间的数据。</p><p>那么以 cumulate window 实现上述的需求，具体的 SQL 如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> UNIX_TIMESTAMP(<span class="built_in">CAST</span>(window_end <span class="keyword">AS</span> STRING)) <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">as</span> window_end, </span><br><span class="line">      window_start, </span><br><span class="line">      <span class="built_in">sum</span>(money) <span class="keyword">as</span> sum_money,</span><br><span class="line">      <span class="built_in">count</span>(<span class="keyword">distinct</span> id) <span class="keyword">as</span> count_distinct_id</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">TABLE</span>(CUMULATE(</span><br><span class="line">         <span class="keyword">TABLE</span> source_table</span><br><span class="line">         , DESCRIPTOR(row_time)</span><br><span class="line">         , <span class="type">INTERVAL</span> <span class="string">&#x27;60&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">         , <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>))</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> window_start, </span><br><span class="line">        window_end</span><br></pre></td></tr></table></figure><p>其中 <code>CUMULATE(TABLE source_table, DESCRIPTOR(row_time), INTERVAL &#39;60&#39; SECOND, INTERVAL &#39;1&#39; DAY)</code> 中的<br><code>INTERVAL &#39;1&#39; DAY</code> 代表窗口大小为 1 天，<code>INTERVAL &#39;60&#39; SECOND</code>，窗口划分步长为 60s。</p><p>其中 <code>window_start, window_end</code> 字段是 cumulate window 自动生成的类型是 timestamp(3)。</p><p><code>window_start</code> 固定为窗口的开始时间。<code>window_end</code> 随着步长向前移动，变化。</p><p>最终结果如下。</p><p>输入数据：<br>row_time|id|money<br>-|-|-<br>2021-11-01 00:01:00 | A | 3<br>2021-11-01 00:01:00 | B | 5<br>2021-11-01 00:01:00 | A | 7<br>2021-11-01 00:02:00 | C | 3<br>2021-11-01 00:03:00 | C | 10</p><p>输出数据：<br>window_end|window_start|sum_money|count_distinct_id<br>-|-|-|-<br>2021-11-21T00:01 | 1635696000000 | 2 | 15<br>2021-11-21T00:02 | 1635696000000 | 3 | 18<br>2021-11-21T00:03 | 1635696000000 | 3 | 28</p><blockquote><p>Notes：<br>天级别窗口划分的时候一定要注意时区问题喔！<br><a href="https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/timezone/">https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/timezone/</a></p></blockquote><h2 id="4-3-cumulate-window-原理解析"><a href="#4-3-cumulate-window-原理解析" class="headerlink" title="4.3.cumulate window 原理解析"></a>4.3.cumulate window 原理解析</h2><p>首先 cumulate window 是一个窗口，其窗口计算的触发也是完全由 watermark 推动的。与 tumble window 一样。</p><p>cumulate window 维护了一个 slice state 和 merged state，slice state 就是每一分钟内窗口数据（叫做切片），merged state 的作用是当 watermark 推动到下一分钟时，这一分钟的 slice state 就会被 merge 到 merged stated 中。<br>从而就计算出当天累计到当前分钟的数据。</p><h2 id="4-4-cumulate-window-怎么解决-tumble-window-early-fire-的问题"><a href="#4-4-cumulate-window-怎么解决-tumble-window-early-fire-的问题" class="headerlink" title="4.4.cumulate window 怎么解决 tumble window + early-fire 的问题"></a>4.4.cumulate window 怎么解决 tumble window + early-fire 的问题</h2><ol><li>问题1：tumble window + early-fire 处理时间触发的问题。</li></ol><p>cumulate window 可以以事件时间推进进行触发。</p><ol start="2"><li>问题1：tumble window + early-fire retract 流问题。</li></ol><p>cumulate window 是 append 流。</p><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h2><p>源码公众号后台回复<strong>1.13.2 cumulate window 的奇妙解析之路</strong>获取。</p><p>本文主要介绍了 window tvf 实现的 cumulate window 聚合类指标的场景案例以及其运行原理：</p><ol><li>介绍了周期内累计 PV，UV 是我们最常用的指标场景质疑。</li><li>在 tumble window + early-fire 或者 groupby + minibatch 计算周期内累计 PV，UV 存在各种问题是，诞生了 cumulate window 帮我们解决了这些问题，并以一个案例进行说明。</li></ol>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink sql 知其所以然（十一）：去重不仅仅有 count distinct 还有强大的 deduplication</title>
    <link href="https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/12_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%EF%BC%9A%E5%8E%BB%E9%87%8D%E4%B8%8D%E4%BB%85%E4%BB%85%E6%9C%89%20count%20distinct%20%E8%BF%98%E6%9C%89%E5%BC%BA%E5%A4%A7%E7%9A%84%20deduplication/"/>
    <id>https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/12_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%EF%BC%9A%E5%8E%BB%E9%87%8D%E4%B8%8D%E4%BB%85%E4%BB%85%E6%9C%89%20count%20distinct%20%E8%BF%98%E6%9C%89%E5%BC%BA%E5%A4%A7%E7%9A%84%20deduplication/</id>
    <published>2021-11-13T06:25:58.000Z</published>
    <updated>2021-11-24T14:55:28.119Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>想啥呢，小宝贝，还不三连？？？（关注  +  点赞 + 再看）</strong>，对博主的肯定，会督促博主持续的输出更多的优质实战内容！！！</p></blockquote><h1 id="1-序篇"><a href="#1-序篇" class="headerlink" title="1.序篇"></a>1.序篇</h1><p>源码公众号后台回复<strong>1.13.2 deduplication 的奇妙解析之路</strong>获取。</p><p>下面即是文章目录，也对应到了本文的结论，小伙伴可以先看结论快速了解博主期望本文能给小伙伴们带来什么帮助：</p><ol><li>背景及应用场景介绍：博主期望你了解到，flink sql 的 deduplication 其实就是 row_number = 1，所以它可以在去重的同时，还能保留原始字段数据</li><li>来一个实战案例：博主以一个日志上报重复的场景，来引出下文要介绍的 flink sql deduplication 解决方案</li><li>基于 Deduplication 的解决方案及原理解析：博主期望你了解到，deduplication 中，当 row_number order by proctime（处理时间）去重的原理就是给每一个 partition key 维护一个 value state。<br>如果当前 value state 不为空，则说明 id 已经来过了，当前这条数据就不用下发了。<br>如果 value state 为空，则 id 还没还没来过，把 value state 标记之后，把当前数据下发。</li><li>总结及展望篇</li></ol><h1 id="2-背景及应用场景介绍"><a href="#2-背景及应用场景介绍" class="headerlink" title="2.背景及应用场景介绍"></a>2.背景及应用场景介绍</h1><p>你是否遇到过一下的场景：</p><ol><li>由于上游发过来的数据有重复或者日志源头数据有重复上报，导致下游计算 count，sum 时算多</li><li>想做到去重计算的同时，原始表的所有字段还能正常保留且下发</li></ol><p>那么你能想到哪些解决方案呢？</p><p>熟悉离线计算的小伙伴可能很快就能给出答案。没错，hive sql 中的 row_number = 1。flink sql 中也是提供了一模一样的功能，xdm，完美的解决这个问题。</p><p>下面开始正式篇章。</p><h1 id="3-来一个实战案例"><a href="#3-来一个实战案例" class="headerlink" title="3.来一个实战案例"></a>3.来一个实战案例</h1><p>先来一个实际案例来看看在具体输入值的场景下，输出值应该长啥样。</p><p>场景：埋点数据上报的的字段有 id（标识唯一一条日志），timestamp（事件时间戳），page（时间发生的当前页面），param1，param2，paramN…。<br>但是日志上报时由于一些机制导致日志上报重复，下游算多了，因此需要做一次去重，下游再去消费去过重的数据。</p><p>来一波输入数据：</p><table><thead><tr><th>id</th><th>timestamp</th><th>page</th><th>param1</th><th>param2</th><th>paramN</th></tr></thead><tbody><tr><td>1</td><td>2021-11-01 00:01:00</td><td>A</td><td>xxx1</td><td>xxx2</td><td>xxxN</td></tr><tr><td>1</td><td>2021-11-01 00:01:00</td><td>A</td><td>xxx1</td><td>xxx2</td><td>xxxN</td></tr><tr><td>2</td><td>2021-11-01 00:01:00</td><td>A</td><td>xxx3</td><td>xxx2</td><td>xxxN</td></tr><tr><td>2</td><td>2021-11-01 00:01:00</td><td>A</td><td>xxx3</td><td>xxx2</td><td>xxxN</td></tr><tr><td>3</td><td>2021-11-01 00:03:00</td><td>C</td><td>xxx5</td><td>xxx2</td><td>xxxN</td></tr></tbody></table><p>其中第二条和第四条是重复上报的数据，则预期输出数据如下：</p><table><thead><tr><th>id</th><th>timestamp</th><th>page</th><th>param1</th><th>param2</th><th>paramN</th></tr></thead><tbody><tr><td>1</td><td>2021-11-01 00:01:00</td><td>A</td><td>xxx1</td><td>xxx2</td><td>xxxN</td></tr><tr><td>2</td><td>2021-11-01 00:01:00</td><td>A</td><td>xxx3</td><td>xxx2</td><td>xxxN</td></tr><tr><td>3</td><td>2021-11-01 00:03:00</td><td>C</td><td>xxx5</td><td>xxx2</td><td>xxxN</td></tr></tbody></table><h1 id="4-基于-Deduplication-的解决方案及原理解析"><a href="#4-基于-Deduplication-的解决方案及原理解析" class="headerlink" title="4.基于 Deduplication 的解决方案及原理解析"></a>4.基于 Deduplication 的解决方案及原理解析</h1><h2 id="4-1-sql-写法"><a href="#4-1-sql-写法" class="headerlink" title="4.1.sql 写法"></a>4.1.sql 写法</h2><p>还是上面的案例，我们来看看最终的 sql 应该怎么写：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> id,</span><br><span class="line">       <span class="type">timestamp</span>,</span><br><span class="line">       page,</span><br><span class="line">       param1,</span><br><span class="line">       param2,</span><br><span class="line">       paramN</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">      <span class="keyword">SELECT</span></span><br><span class="line">          id,</span><br><span class="line">          <span class="type">timestamp</span>,</span><br><span class="line">          page,</span><br><span class="line">          param1,</span><br><span class="line">          param2,</span><br><span class="line">          paramN</span><br><span class="line">          <span class="comment">-- proctime 代表处理时间即 source 表中的 PROCTIME()</span></span><br><span class="line">          <span class="built_in">row_number</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> id <span class="keyword">order</span> <span class="keyword">by</span> proctime) <span class="keyword">as</span> rn</span><br><span class="line">      <span class="keyword">FROM</span> source_table</span><br><span class="line">)</span><br><span class="line"><span class="keyword">where</span> rn <span class="operator">=</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p>上面的 sql 应该很好理解。其中由于我们并不关心重复数据上报的时间前后，所以此处就直接使用 <code>order by proctime</code> 进行处理，按照数据来的前后时间去第一条。</p><h2 id="4-2-proctime-下-flink-生成的算子图及-sql-算子语义"><a href="#4-2-proctime-下-flink-生成的算子图及-sql-算子语义" class="headerlink" title="4.2.proctime 下 flink 生成的算子图及 sql 算子语义"></a>4.2.proctime 下 flink 生成的算子图及 sql 算子语义</h2><p>算子图如下所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/13_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%EF%BC%9A%E5%8E%BB%E9%87%8D%E4%B8%8D%E4%BB%85%E4%BB%85%E6%9C%89countdistinct%E8%BF%98%E6%9C%89%E5%BC%BA%E5%A4%A7%E7%9A%84deduplication/1.png" alt="deduplication"></p><ol><li><strong>source 算子</strong>：source 通过 keyby 的方式向 deduplication 算子发数据时，其中 keyby 的 key 就是 sql 中的 id</li><li><strong>deduplication 算子</strong>：deduplication 算子为每一个 partition key 都维护了一个 value state 用于去重。<br>每来一条数据时都从当前 partition key  的 value state 去获取 value，<br>如果不为空，则说明已经有数据来过了，当前这一条数据就是重复数据，就不往下游算子下发了，<br>如果为空，则说明之前没有数据来过，当前这一条数据就是第一条数据，则把当前的 value state 值设置为 true，往下游算子下发数据</li></ol><h2 id="4-3-proctime-下-deduplication-原理解析"><a href="#4-3-proctime-下-deduplication-原理解析" class="headerlink" title="4.3.proctime 下 deduplication 原理解析"></a>4.3.proctime 下 deduplication 原理解析</h2><p>具体的去重算子为 deduplication。我们通过 transformation 可以看到去重算子为下图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/13_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%EF%BC%9A%E5%8E%BB%E9%87%8D%E4%B8%8D%E4%BB%85%E4%BB%85%E6%9C%89countdistinct%E8%BF%98%E6%9C%89%E5%BC%BA%E5%A4%A7%E7%9A%84deduplication/2.png" alt="transformation"></p><p>上述的去重逻辑集中在 <code>org.apache.flink.table.runtime.operators.deduplicate.ProcTimeDeduplicateKeepFirstRowFunction</code> 的 <code>processFirstRowOnProcTime</code>，如下图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/13_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%EF%BC%9A%E5%8E%BB%E9%87%8D%E4%B8%8D%E4%BB%85%E4%BB%85%E6%9C%89countdistinct%E8%BF%98%E6%9C%89%E5%BC%BA%E5%A4%A7%E7%9A%84deduplication/3.png" alt="ProcTimeDeduplicateKeepFirstRowFunction"></p><h2 id="5-总结与展望"><a href="#5-总结与展望" class="headerlink" title="5.总结与展望"></a>5.总结与展望</h2><p>源码公众号后台回复<strong>1.13.2 deduplication 的奇妙解析之路</strong>获取。</p><p>本文主要介绍了 deduplication 的应用场景案例以及其运行原理，主要包含下面两部分：</p><ol><li>背景及应用场景介绍：博主期望你了解到，flink sql 的 deduplication 其实就是 row_number = 1，所以它可以在去重的同时，还能保留原始字段数据</li><li>来一个实战案例：博主以一个日志上报重复的场景，来引出下文要介绍的 flink sql deduplication 解决方案</li><li>基于 Deduplication 的解决方案及原理解析：博主期望你了解到，deduplication 中，当 row_number order by proctime（处理时间）去重的原理就是给每一个 partition key 维护一个 value state。<br>如果当前 value state 不为空，则说明 id 已经来过了，当前这条数据就不用下发了。<br>如果 value state 为空，则 id 还没还没来过，把 value state 标记之后，把当前数据下发。</li><li>总结及展望篇</li></ol>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink ck 恢复</title>
    <link href="https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/02_%E6%9C%AC%E5%9C%B0%E8%B0%83%E8%AF%95_flink_ck/"/>
    <id>https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/02_%E6%9C%AC%E5%9C%B0%E8%B0%83%E8%AF%95_flink_ck/</id>
    <published>2021-11-13T06:25:58.000Z</published>
    <updated>2022-02-03T15:22:02.486Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>想啥呢，小宝贝，还不三连？？？（关注  +  点赞 + 再看）</strong>，对博主的肯定，会督促博主持续的输出更多的优质实战内容！！！</p></blockquote><h1 id="1-序篇-本文结构"><a href="#1-序篇-本文结构" class="headerlink" title="1.序篇-本文结构"></a>1.序篇-本文结构</h1><p>源码公众号后台回复<strong>flink idea 本地调试状态重启</strong>获取。</p><ol><li>案例代码</li><li>本地任务启动指定 ck 存储路径</li><li>本地任务停止时，保留 ck</li><li>本地任务重启时，指定从 ck 重启</li></ol><h1 id="2-案例代码"><a href="#2-案例代码" class="headerlink" title="2.案例代码"></a>2.案例代码</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    dim <span class="type">BIGINT</span>,</span><br><span class="line">    user_id <span class="type">BIGINT</span>,</span><br><span class="line">    price <span class="type">BIGINT</span>,</span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.dim.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.dim.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;2&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    dim <span class="type">BIGINT</span>,</span><br><span class="line">    pv <span class="type">BIGINT</span>,</span><br><span class="line">    sum_price <span class="type">BIGINT</span>,</span><br><span class="line">    max_price <span class="type">BIGINT</span>,</span><br><span class="line">    min_price <span class="type">BIGINT</span>,</span><br><span class="line">    uv <span class="type">BIGINT</span>,</span><br><span class="line">    window_start <span class="type">bigint</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> sink_table</span><br><span class="line"><span class="keyword">select</span> dim,</span><br><span class="line">       <span class="built_in">sum</span>(bucket_pv) <span class="keyword">as</span> pv,</span><br><span class="line">       <span class="built_in">sum</span>(bucket_sum_price) <span class="keyword">as</span> sum_price,</span><br><span class="line">       <span class="built_in">max</span>(bucket_max_price) <span class="keyword">as</span> max_price,</span><br><span class="line">       <span class="built_in">min</span>(bucket_min_price) <span class="keyword">as</span> min_price,</span><br><span class="line">       <span class="built_in">sum</span>(bucket_uv) <span class="keyword">as</span> uv,</span><br><span class="line">       <span class="built_in">max</span>(window_start) <span class="keyword">as</span> window_start</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">     <span class="keyword">select</span> dim,</span><br><span class="line">            <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">as</span> bucket_pv,</span><br><span class="line">            <span class="built_in">sum</span>(price) <span class="keyword">as</span> bucket_sum_price,</span><br><span class="line">            <span class="built_in">max</span>(price) <span class="keyword">as</span> bucket_max_price,</span><br><span class="line">            <span class="built_in">min</span>(price) <span class="keyword">as</span> bucket_min_price,</span><br><span class="line">            <span class="built_in">count</span>(<span class="keyword">distinct</span> user_id) <span class="keyword">as</span> bucket_uv,</span><br><span class="line">            UNIX_TIMESTAMP(<span class="built_in">CAST</span>(tumble_start(row_time, <span class="type">interval</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>) <span class="keyword">AS</span> STRING)) <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">as</span> window_start</span><br><span class="line">     <span class="keyword">from</span> source_table</span><br><span class="line">     <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">            <span class="built_in">mod</span>(user_id, <span class="number">1024</span>),</span><br><span class="line">            dim,</span><br><span class="line">            tumble(row_time, <span class="type">interval</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> dim,</span><br><span class="line">         window_start</span><br></pre></td></tr></table></figure><h1 id="3-本地任务启动指定-ck-存储路径"><a href="#3-本地任务启动指定-ck-存储路径" class="headerlink" title="3.本地任务启动指定 ck 存储路径"></a>3.本地任务启动指定 ck 存储路径</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 状态后端设置</span></span><br><span class="line"><span class="comment">// 设置存储文件位置为 file:///Users/flink/checkpoints</span></span><br><span class="line">RocksDBStateBackend rocksDBStateBackend = <span class="keyword">new</span> RocksDBStateBackend(</span><br><span class="line">        <span class="string">&quot;file:///Users/flink/checkpoints&quot;</span>, ENABLE_INCREMENTAL_CHECKPOINT);</span><br><span class="line">rocksDBStateBackend.setNumberOfTransferThreads(NUMBER_OF_TRANSFER_THREADS);</span><br><span class="line">rocksDBStateBackend.setPredefinedOptions(PredefinedOptions.SPINNING_DISK_OPTIMIZED_HIGH_MEM);</span><br><span class="line">env.setStateBackend((StateBackend) rocksDBStateBackend);</span><br></pre></td></tr></table></figure><p>效果：</p><h1 id="4-本地任务启动停止时，保留-ck"><a href="#4-本地任务启动停止时，保留-ck" class="headerlink" title="4.本地任务启动停止时，保留 ck"></a>4.本地任务启动停止时，保留 ck</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ck 设置</span></span><br><span class="line">env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br></pre></td></tr></table></figure><p>效果：</p><h1 id="5-本地任务重启时，指定从-ck-重启"><a href="#5-本地任务重启时，指定从-ck-重启" class="headerlink" title="5.本地任务重启时，指定从 ck 重启"></a>5.本地任务重启时，指定从 ck 重启</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ck 设置</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">configuration.setString(<span class="string">&quot;execution.savepoint.path&quot;</span>, <span class="string">&quot;file:///Users/flink/checkpoints/ce2e1969c5088bf27daf35d4907659fd/chk-5&quot;</span>);</span><br><span class="line"></span><br><span class="line">StreamExecutionEnvironment env =</span><br><span class="line">      StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(configuration);</span><br></pre></td></tr></table></figure><p>效果：</p><h1 id="6-总结与展望篇"><a href="#6-总结与展望篇" class="headerlink" title="6.总结与展望篇"></a>6.总结与展望篇</h1><p><strong>想啥呢，小宝贝，还不三连？？？（关注  +  点赞 + 再看）</strong></p><p>源码公众号后台回复<strong>flink idea 本地调试状态重启</strong>获取。</p><p>希望大家能持续关注。支持博主。喜欢的请关注 + 点赞 + 再看。</p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink sql 知其所以然（八）：flink sql tumble window 的奇妙解析之路</title>
    <link href="https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88%20flink%20sql%20%E7%9A%84%20%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/"/>
    <id>https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88%20flink%20sql%20%E7%9A%84%20%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/</id>
    <published>2021-11-13T06:23:58.000Z</published>
    <updated>2021-09-11T16:54:59.268Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>感谢您的小爱心（<strong>关注  +  点赞 + 再看</strong>），对博主的肯定，会督促博主持续的输出更多的优质实战内容！！！</p></blockquote><h1 id="1-序篇-本文结构"><a href="#1-序篇-本文结构" class="headerlink" title="1.序篇-本文结构"></a>1.序篇-本文结构</h1><p>源码公众号后台回复<strong>flink sql tumble window 的奇妙解析之路</strong>获取。</p><p>针对 datastream api 大家都比较熟悉了，还是那句话，在 datastream 中，你写的代码逻辑是什么样的，它最终的执行方式就是什么样的。</p><p>但是对于 flink sql 的执行过程，大家还是不熟悉的。上节使用 ETL，group agg（sum，count等）简单聚合类 query 带大家走进一条 flink sql query 逻辑的世界。帮大家至少能够熟悉在 flink sql 程序运行时知道 flink 程序在干什么。</p><p>此节就是窗口聚合章节的第一篇，以一个最简单、最常用的分钟 tumble window 聚合案例给大家介绍其使用方式和原理。</p><p>由于 flink 1.13 引入了 window tvf，所以 1.13 和 1.12 及之前版本的实现不同。本节先介绍 flink 1.12 及之前的 tumble window 实现。这也是大家在引入 flink sql 能力时最常使用的。</p><p>本节依然从以下几个章节给大家详细介绍 flink sql 的能力。</p><ol><li>目标篇-本文能帮助大家了解 flink sql 什么？<ul><li>回顾上节的 flink sql 适用场景的结论</li></ul></li><li>概念篇-先聊聊常见的窗口聚合<ul><li>窗口竟然拖慢数据产出？</li><li>常用的窗口</li></ul></li><li>实战篇-简单的 tumble window 案例和运行原理<ul><li>先看一个 datastream 窗口案例</li><li>flink sql tumble window 的语义</li><li>tumble window 实际案例</li><li>GeneratedWatermarkGenerator - flink 1.12.1</li><li>BinaryRowDataKeySelector - flink 1.12.1</li><li>AggregateWindowOperator - flink 1.12.1</li></ul></li><li>总结与展望篇</li></ol><p><strong>先说说结论，以下这些结论已经在上节说过了，此处附上上节文章：</strong></p><ol><li><strong>场景问题</strong>：flink sql 很适合简单 ETL，以及基本全部场景下的聚合类指标（本节要介绍的 tumble window 就在聚合类指标的范畴之内）。</li><li><strong>语法问题</strong>：flink sql 语法其实是和其他 sql 语法基本一致的。基本不会产生语法问题阻碍使用 flink sql。但是本节要介绍的 tumble window 的语法就是略有不同的那部分。下面详细介绍。</li><li><strong>运行问题</strong>：查看 flink sql 任务时的一些技巧，以及其中一些可能会碰到的坑：<ul><li>去 flink webui 就能看到这个任务目前在做什么。包括算子名称都会给直接展示给我们目前哪个算子在干啥事情，在处理啥逻辑。</li><li>sql 的 watermark 类型要设置为 TIMESTAMP(3)。</li><li>事件时间逻辑中，sql api 和 datastream api 对于数据记录时间戳存储逻辑是不一样的。datastream api：每条记录的 rowtime 是放在 StreamRecord 中的时间戳字段中的。sql api：时间戳是每次都从数据中进行获取的。算子中会维护一个下标。可以按照下标从数据中获取时间戳。</li></ul></li></ol><h1 id="2-目标篇-本文能帮助大家了解-flink-sql-tumble-window-什么？"><a href="#2-目标篇-本文能帮助大家了解-flink-sql-tumble-window-什么？" class="headerlink" title="2.目标篇-本文能帮助大家了解 flink sql tumble window 什么？"></a>2.目标篇-本文能帮助大家了解 flink sql tumble window 什么？</h1><p>关于 flink sql tumble window 一般都会有以下问题。本文的目标也是为大家解答这些问题：</p><ol><li><strong>场景问题</strong>：场景问题就不必多说，datastream 在 tumble window 场景下的应用很多了，分钟级别聚合等常用场景</li><li><strong>语法问题</strong>：flink sql 写 tumble window 任务时是一种与 hive sql 中没有的语法。下文详细介绍。</li><li><strong>运行问题</strong>：使用一条简单的 tumble window sql 帮大家从 transformation、runtime 帮大家理解 tumble window 的整体运行机制。</li><li><strong>理解误区</strong>：既然是 sql 必然要遵循 sql 语义，sql tumble window 聚合是输入多条，产出一条数据。并不像 datastream 那样可以在窗口 udf 中做到多对多。</li></ol><p>在正式开始聊 tumble window 之前，先看看上节 flink sql 适用场景的结论。让大家先有 flink sql 的一个整体印象以及结论。</p><h2 id="2-1-回顾上节的-flink-sql-适用场景的结论"><a href="#2-1-回顾上节的-flink-sql-适用场景的结论" class="headerlink" title="2.1.回顾上节的 flink sql 适用场景的结论"></a>2.1.回顾上节的 flink sql 适用场景的结论</h2><p>不装了，我坦白了，flink sql 其实很适合干的活就是 dwd 清洗，dws 聚合。</p><p>此处主要针对实时数仓的场景来说。flink sql 能干 dwd 清洗，dws 聚合，基本上实时数仓的大多数场景都能给覆盖了。</p><p>flink sql 牛逼！！！</p><p>但是！！！</p><p>经过博主使用 flink sql 经验来看，并不是所有的 dwd，dws 聚合场景都适合 flink sql（截止发文阶段来说）！！！</p><p>其实这些目前不适合 flink sql 的场景总结下来就是在处理上比 datastream 还是会有一定的损失。</p><p>先总结下使用场景：<br><strong>1. dwd</strong>：简单的清洗、复杂的清洗、维度的扩充、各种 udf 的使用<br><strong>2. dws</strong>：各类聚合</p><p>然后分适合的场景和不适合的场景来说，因为只这一篇不能覆盖所有的内容，所以本文此处先大致给个结论，之后会结合具体的场景详细描述。</p><ul><li><strong>适合的场景：</strong><ol><li>简单的 dwd 清洗场景</li><li>全场景的 dws 聚合场景</li></ol></li><li><strong>目前不太适合的场景：</strong><ol><li>复杂的 dwd 清洗场景：举例比如使用了很多 udf 清洗，尤其是使用很多的 json 类解析清洗</li><li>关联维度场景：举例比如 datastream 中经常会有攒一批数据批量访问外部接口的场景，flink sql 目前对于这种场景虽然有 localcache、异步访问能力，但是依然还是一条一条访问外部缓存，这样相比批量访问还是会有性能差距。</li></ol></li></ul><h1 id="3-概念篇-先聊聊常见的窗口聚合"><a href="#3-概念篇-先聊聊常见的窗口聚合" class="headerlink" title="3.概念篇-先聊聊常见的窗口聚合"></a>3.概念篇-先聊聊常见的窗口聚合</h1><p>窗口聚合大家都在 datastream api 中很熟悉了，目前在实时数据处理的过程中，窗口计算可以说是最重要、最常用的一种计算方式了。</p><p>但是在抛出窗口概念之前，博主有几个关于窗口的小想法说一下。</p><h2 id="3-1-窗口竟然拖慢数据产出？"><a href="#3-1-窗口竟然拖慢数据产出？" class="headerlink" title="3.1.窗口竟然拖慢数据产出？"></a>3.1.窗口竟然拖慢数据产出？</h2><p>一个小想法。</p><p><strong>先抛结论：窗口会拖慢实时数据的产出，是在目前下游分析引擎能力有限的情况下的一种妥协方案。</strong></p><p>站在数据开发以及需求方的世界中，当然希望所有的数据都是<strong>实时来的，实时处理的，实时产出的，实时展现的</strong>。</p><p><strong>举个例子</strong>：如果你要满足一个一分钟窗口聚合的 pv，uv，或者其他聚合需求。</p><p><strong>olap 数据服务引擎</strong> 就可以满足上述的<strong>实时来的，实时处理的，实时产出的，实时展现的</strong>的场景。flink 消费处理明细数据，产出到 kafka，然后直接导入到 olap 引擎中。查询时直接用 olap 做聚合。这其中是没有任何窗口的概念的。<br>但是整个链路中，要保障端对端精确一次，要保障大数据量情况下 olap 引擎能够秒级查询返回，更何况有一些去重类指标的计算，等等场景。把这些压力都放在 olap 引擎的压力是很大的。</p><p>因此在 <strong>flink 数据计算引擎</strong>中就诞生了窗口的概念。我们可以直接在计算引擎中进行窗口聚合计算，然后等到窗口结束之后直接把结果数据产出。<br>这就出现了博主所说的窗口拖慢了实时数据产出的情况。而且窗口在处理不好的情况下可能会导致数据丢失。</p><p>关于上述两种情况的具体优劣选择，都由大家自行选择。上述只是引出博主一些想法。</p><h2 id="3-2-常用的窗口"><a href="#3-2-常用的窗口" class="headerlink" title="3.2.常用的窗口"></a>3.2.常用的窗口</h2><p>目前已知的窗口分为以下四种。</p><p><strong>1. Tumble Windows</strong><br><strong>2. Hop Windows</strong><br><strong>3. Cumulate Windows</strong><br><strong>4. Session Windows</strong></p><p>这些窗口的具体描述直接见官网，有详细的说明。此处不赘述。</p><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/sql/queries/window-agg/">https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/dev/table/sql/queries/window-agg/</a></p><p>此处介绍下 flink 中常常会涉及到的两个容易混淆的概念就是：窗口 + key。这里来形象的说明下。</p><ul><li><p><strong>窗口：时间周期上面的划分</strong>。将无限流进行纵向切分，将无限流切分为一个一个的窗口，窗口相当于是无限流中的一段时间内的数据。</p></li><li><p><strong>key：数据类别上面的划分</strong>。将无限流进行横向划分，相同 key 的数据会被划分到一组中，这个 key 的数据也是一条无限流。</p></li></ul><p>如下图所示。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/1.png" alt="1"></p><h1 id="4-实战篇-简单的-tumble-window-案例和运行原理"><a href="#4-实战篇-简单的-tumble-window-案例和运行原理" class="headerlink" title="4.实战篇-简单的 tumble window 案例和运行原理"></a>4.实战篇-简单的 tumble window 案例和运行原理</h1><p>源码公众号后台回复<strong>flink sql tumble window 的奇妙解析之路</strong>获取。</p><h2 id="4-1-先看一个-datastream-窗口案例"><a href="#4-1-先看一个-datastream-窗口案例" class="headerlink" title="4.1.先看一个 datastream 窗口案例"></a>4.1.先看一个 datastream 窗口案例</h2><p>在介绍 sql tumble window 窗口算子执行案例之前，先看一个 datastream 中的窗口算子案例。其逻辑都是相通的。会对我们了解 sql tumble window 算子有帮助。</p><p>我们先看看 datastream 处理逻辑。</p><p>以下面这个为例。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">_04_TumbleWindowTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment env =</span><br><span class="line">                StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(<span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line"></span><br><span class="line">        env.addSource(<span class="keyword">new</span> UserDefinedSource())</span><br><span class="line">                .assignTimestampsAndWatermarks(<span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor&lt;Tuple4&lt;String, String, Integer, Long&gt;&gt;(Time.seconds(<span class="number">0</span>)) &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Tuple4&lt;String, String, Integer, Long&gt; element)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> element.f3;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .keyBy(<span class="keyword">new</span> KeySelector&lt;Tuple4&lt;String, String, Integer, Long&gt;, String&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> String <span class="title">getKey</span><span class="params">(Tuple4&lt;String, String, Integer, Long&gt; row)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> row.f0;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">                .sum(<span class="number">2</span>)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">&quot;1.12.1 DataStream TUMBLE WINDOW 案例&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UserDefinedSource</span> <span class="keyword">implements</span> <span class="title">SourceFunction</span>&lt;<span class="title">Tuple4</span>&lt;<span class="title">String</span>, <span class="title">String</span>, <span class="title">Integer</span>, <span class="title">Long</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isCancel;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Tuple4&lt;String, String, Integer, Long&gt;&gt; sourceContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> (!<span class="keyword">this</span>.isCancel) &#123;</span><br><span class="line"></span><br><span class="line">                sourceContext.collect(Tuple4.of(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="number">1</span>, System.currentTimeMillis()));</span><br><span class="line"></span><br><span class="line">                Thread.sleep(<span class="number">10L</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.isCancel = <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>datastream 生产的具体的 transformation 如下图：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/24.png" alt="24"></p><p>其中我们只关注最重要的 <code>WindowOperator</code> 算子。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/25.png" alt="25"></p><p>其中 <code>WindowOperator</code> 算子包含的重要属性如下图。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/26.png" alt="26"></p><p>来看看 <code>WindowOperator</code> 的执行逻辑。窗口执行的整体详细流程可以参考：<a href="http://wuchong.me/blog/2016/05/25/flink-internals-window-mechanism/">http://wuchong.me/blog/2016/05/25/flink-internals-window-mechanism/</a></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/23.png" alt="23"></p><h2 id="4-2-flink-sql-tumble-window-的语义"><a href="#4-2-flink-sql-tumble-window-的语义" class="headerlink" title="4.2.flink sql tumble window 的语义"></a>4.2.flink sql tumble window 的语义</h2><p>介绍到 tumble window 的语义，总要有对比的去介绍。这里的参照物就是 datastream api。</p><p>在 datastream api 中。tumble window 一般用作以下两种场景。</p><ol><li><strong>业务场景</strong>：使用 tumble window 很轻松的计算出窗口内的聚合数据。一般是多条输入数据，窗口结束时一条输出数据。</li><li><strong>优化场景</strong>：窗口聚合一批数据然后批量访问外部存储扩充维度、或者有一些自定义的处理逻辑。一般是多条输入数据，窗口结束时多条输出数据。</li></ol><p>但是在 sql api 中。tumble window 是聚合（group by）语义，聚合在 sql 标准中的数据处理逻辑是多条输入，在窗口触发时就输出一条数据的语义。而上面的常常用在 datastream 中的<strong>优化场景</strong>是多对多的场景。因此和 sql 语义不符合。所以 flink sql tumble window 一般都是用于计算聚合运算值来使用。</p><h2 id="4-3-tumble-window-实际案例"><a href="#4-3-tumble-window-实际案例" class="headerlink" title="4.3.tumble window 实际案例"></a>4.3.tumble window 实际案例</h2><p>滚动窗口的特性就是会将无限流进行纵向划分成一个一个的窗口，每个窗口都是相同的大小，并且不重叠。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/22.png" alt="22"></p><p>本文主要介绍 flink 1.12 及之前版本的实现。下一篇文章介绍 flink 1.13 的实现。</p><p>来，在介绍原理之前，总要先用起来，我们就以下面这个例子展开。</p><p><strong>1.（flink 1.12.1）场景：简单且常见的分维度分钟级别同时在线用户数、总销售额</strong></p><p>数据源表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    <span class="comment">-- 维度数据</span></span><br><span class="line">    dim STRING,</span><br><span class="line">    <span class="comment">-- 用户 id</span></span><br><span class="line">    user_id <span class="type">BIGINT</span>,</span><br><span class="line">    <span class="comment">-- 用户</span></span><br><span class="line">    price <span class="type">BIGINT</span>,</span><br><span class="line">    <span class="comment">-- 事件时间戳</span></span><br><span class="line">    row_time <span class="keyword">AS</span> <span class="built_in">cast</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">as</span> <span class="type">timestamp</span>(<span class="number">3</span>)),</span><br><span class="line">    <span class="comment">-- watermark 设置</span></span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> row_time <span class="keyword">AS</span> row_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.dim.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;100000&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><blockquote><p><strong>Notes - 关于 watermark 容易踩得坑：</strong><br>sql 的 watermark 类型必须要设置为 <code>TIMESTAMP(3)</code>。</p></blockquote><p>数据汇表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    dim STRING,</span><br><span class="line">    pv <span class="type">BIGINT</span>,</span><br><span class="line">    sum_price <span class="type">BIGINT</span>,</span><br><span class="line">    max_price <span class="type">BIGINT</span>,</span><br><span class="line">    min_price <span class="type">BIGINT</span>,</span><br><span class="line">    uv <span class="type">BIGINT</span>,</span><br><span class="line">    window_start <span class="type">bigint</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>数据处理逻辑：</p><p>可以看下下面语法，窗口聚合的写法有专门的 <code>tumble(row_time, interval &#39;1&#39; minute)</code> 写法，这就是与平常我们写的 hive sql，mysql 等不一样的地方。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> sink_table</span><br><span class="line"><span class="keyword">select</span> dim,</span><br><span class="line">       <span class="built_in">sum</span>(bucket_pv) <span class="keyword">as</span> pv,</span><br><span class="line">       <span class="built_in">sum</span>(bucket_sum_price) <span class="keyword">as</span> sum_price,</span><br><span class="line">       <span class="built_in">max</span>(bucket_max_price) <span class="keyword">as</span> max_price,</span><br><span class="line">       <span class="built_in">min</span>(bucket_min_price) <span class="keyword">as</span> min_price,</span><br><span class="line">       <span class="built_in">sum</span>(bucket_uv) <span class="keyword">as</span> uv,</span><br><span class="line">       <span class="built_in">max</span>(window_start) <span class="keyword">as</span> window_start</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">     <span class="keyword">select</span> dim,</span><br><span class="line">            <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">as</span> bucket_pv,</span><br><span class="line">            <span class="built_in">sum</span>(price) <span class="keyword">as</span> bucket_sum_price,</span><br><span class="line">            <span class="built_in">max</span>(price) <span class="keyword">as</span> bucket_max_price,</span><br><span class="line">            <span class="built_in">min</span>(price) <span class="keyword">as</span> bucket_min_price,</span><br><span class="line">            <span class="comment">-- 计算 uv 数</span></span><br><span class="line">            <span class="built_in">count</span>(<span class="keyword">distinct</span> user_id) <span class="keyword">as</span> bucket_uv,</span><br><span class="line">            <span class="built_in">cast</span>(tumble_start(row_time, <span class="type">interval</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">minute</span>) <span class="keyword">as</span> <span class="type">bigint</span>) <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">as</span> window_start</span><br><span class="line">     <span class="keyword">from</span> source_table</span><br><span class="line">     <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">            <span class="comment">-- 按照用户 id 进行分桶，防止数据倾斜</span></span><br><span class="line">            <span class="built_in">mod</span>(user_id, <span class="number">1024</span>),</span><br><span class="line">            dim,</span><br><span class="line">            tumble(row_time, <span class="type">interval</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">minute</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> dim,</span><br><span class="line">         window_start</span><br></pre></td></tr></table></figure><p><strong>2.运行：可以看到，其实在 flink sql 任务中，其会把对应的处理逻辑给写到算子名称上面。</strong></p><blockquote><p><strong>Notes - 观察 flink sql 技巧 1：</strong><br>这个其实就是我们观察 flink sql 任务的第一个技巧。如果你想知道你的 flink 任务在干啥，第一反应是去 flink webui 看看这个任务目前在做什么。包括算子名称都会给直接展示给我们目前哪个算子在干啥事情，在处理啥逻辑</p></blockquote><p><strong>先看一下整个算子图，如下图。从左到右总共分为三个算子。</strong></p><ol><li>第一个算子就是数据源算子</li><li>第二个算子就是分了桶的窗口聚合算子，第一个算子和第二个算子之间 hash 传输就是按照 group key 进行 hash 传输</li><li>第三个算子就是外层进行合桶计算的算子，同样也是 hash 传输，将分桶的数据在一个算子中进行合并计算</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/5.png" alt="5"></p><p>来看看每一个算子具体做了什么事情。</p><p><strong>第一个算子：</strong></p><ol><li>table scan 读取数据源</li><li>从数据源中获取对应的字段（包括源表定义的 rowtime）</li><li>分配 watermark（按照源表定义的 watermark 分配对应的 watermark）</li><li>将一些必要的字段抽取。比如 group by 中的字段。在 hash 时需要使用。</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/6.png" alt="6"></p><p><strong>第二个算子：</strong></p><ol><li>窗口聚合，计算窗口聚合数据</li><li>将数据按照第一层 select 中的数据进行计算以及格式化</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/7.png" alt="7"></p><p><strong>第三个算子：</strong></p><ol><li>group 聚合合桶计算</li><li>将数据按照第二层 select 中的数据进行计算以及格式化</li><li>将数据 sink 写出</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/8.png" alt="8"></p><p><strong>3.（flink 1.12.1）结果：</strong></p><figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+I(9,1,32682,32682,32682,1,1631026440000)</span><br><span class="line">-U(9,1,32682,32682,32682,1,1631026440000)</span><br><span class="line">+U(9,2,115351,82669,32682,2,1631026440000)</span><br><span class="line">+I(2,1,76148,76148,76148,1,1631026440000)</span><br><span class="line">+I(8,1,79321,79321,79321,1,1631026440000)</span><br><span class="line">+I(a,1,85792,85792,85792,1,1631026440000)</span><br><span class="line">+I(0,1,12858,12858,12858,1,1631026440000)</span><br><span class="line">+I(5,1,36753,36753,36753,1,1631026440000)</span><br><span class="line">+I(3,1,19218,19218,19218,1,1631026440000)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p><strong>4.（flink 1.12.1）原理：</strong></p><p>关于 sql 开始运行的机制见上一节详述。</p><p>此处只介绍相比前一节新增内容。可以看到上述代码的具体 transformation 如下图。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/9.png" alt="9"></p><h2 id="4-4-GeneratedWatermarkGenerator-flink-1-12-1"><a href="#4-4-GeneratedWatermarkGenerator-flink-1-12-1" class="headerlink" title="4.4.GeneratedWatermarkGenerator - flink 1.12.1"></a>4.4.GeneratedWatermarkGenerator - flink 1.12.1</h2><p>按照顺序，首先看看 watermark 算子。同 datastream 的自定义 watermark 分配策略。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/10.png" alt="10"></p><p>watermark 生成的具体代码 <code>WatermarkGenerator$6</code>，主要获取 watermark 的逻辑在 <code>currentWatermark</code> 方法中。如下图。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/11.png" alt="11"></p><h2 id="4-5-BinaryRowDataKeySelector-flink-1-12-1"><a href="#4-5-BinaryRowDataKeySelector-flink-1-12-1" class="headerlink" title="4.5.BinaryRowDataKeySelector - flink 1.12.1"></a>4.5.BinaryRowDataKeySelector - flink 1.12.1</h2><p>接着就是 group by（同 datastream 中的 keyby）。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/12.png" alt="12"></p><p>group by key 生成的具体代码 <code>KeyProjection$19</code>，主要逻辑在 <code>apply</code> 方法中。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/13.png" alt="13"></p><p>下一个就是窗口聚合算子。</p><h2 id="4-6-AggregateWindowOperator-flink-1-12-1"><a href="#4-6-AggregateWindowOperator-flink-1-12-1" class="headerlink" title="4.6.AggregateWindowOperator - flink 1.12.1"></a>4.6.AggregateWindowOperator - flink 1.12.1</h2><p>兄弟们！！！兄弟们！！！兄弟们！！！</p><p>本节的重头戏来了。sql 窗口聚合算子解析搞起来了。</p><p>关于 <code>WatermarkGenerator</code> 和 <code>KeyProjection</code> 没有什么可以详细介绍的，都是输入一条数据，输出一条数据，逻辑很简单。</p><p>但是窗口聚合算子的计算逻辑相比上面两个算子复杂很多。窗口算子又承载了窗口聚合的主要逻辑，所以本文重点介绍窗口算子计算的逻辑。</p><p>先来看看 sql 窗口整体处理流程。其实与 datastream 处理流程基本一致，但只是少了 <code>Evictor</code>。如下图所示。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/40.png" alt="40"></p><p>接着来看看上述 sql 生成的窗口聚合算子 <code>AggregateWindowOperator</code>，截图中属性也很清晰。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/16.png" alt="16"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/14.png" alt="14"></p><p>具体生成的窗口聚合代码 <code>GroupingWindowAggsHandler$59</code>。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/41.png" alt="41"></p><p>计算逻辑 <code>GroupingWindowAggsHandler$59#accumulate</code>。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/42.png" alt="42"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/43.png" alt="43"></p><p><strong>上面那段都是在 flink 客户端初始化处理的。包括窗口算子的初始化等。</strong></p><p><strong>下面这段处理逻辑是在 flink TM 运行时开始执行的，包括窗口算子资源的初始化以及运行逻辑。就到了正式的数据处理环节了。</strong></p><p>窗口算子 Task 运行。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/27.png" alt="27"></p><p>窗口算子 Task 初始化。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/28.png" alt="28"></p><p>StreamTask 整体的处理流程。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/29.png" alt="29"></p><p>窗口算子 open 初始化。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/30.png" alt="30"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/31.png" alt="31"></p><p>窗口算子 open 初始化后的结果。如下图，对应的具体组件。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/32.png" alt="32"></p><p>初始化完成之后，开始处理具体数据。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/33.png" alt="33"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/34.png" alt="34"></p><p>循环 loop，一直 run 啊 run。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/35.png" alt="35"></p><p>判断记录的具体类型，然后执行不同的逻辑。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/36.png" alt="36"></p><p>来看看处理一条数据的 <code>processElement</code> 方法逻辑，进行 acc 处理。代码中的的 <code>windowAggregator</code> 就是之前代码生成的 <code>GroupingWindowAggsHandler$59</code>。</p><blockquote><p><strong>Notes：</strong><br>事件时间逻辑中，sql api 和 datastream api 对于数据记录时间戳存储逻辑是不一样的。<br>datastream api：每条记录的 rowtime 是放在 <code>StreamRecord</code> 中的时间戳字段中的。<br>sql api：时间戳是每次都从数据中进行获取的。算子中会维护一个下标。可以按照下标从数据中获取时间戳。</p></blockquote><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/39.png" alt="39"></p><p>来看看 watermark 到达并且触发窗口计算时，执行 <code>onEventTime</code> 逻辑。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/38.png" alt="38"></p><p>触发窗口计算时，<code>onEventTime -&gt; emitWindowResult</code>，产出具体数据。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/09_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88flinksql%E7%9A%84%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7/17.png" alt="17"></p><p>至此整个 sql tumble window 的处理逻辑也就很清楚了。和 datastream 基本上都是一致的。是不是整个逻辑就理清楚了。</p><h1 id="5-总结与展望篇"><a href="#5-总结与展望篇" class="headerlink" title="5.总结与展望篇"></a>5.总结与展望篇</h1><p>源码公众号后台回复<strong>flink sql tumble window 的奇妙解析之路</strong>获取。</p><p>本文主要介绍了 tumble window 聚合类指标的常见场景案例以及其底层运行原理。</p><p>而且也介绍了在查看 flink sql 任务时的一些技巧：</p><ol><li>去 flink webui 就能看到这个任务目前在做什么。包括算子名称都会给直接展示给我们目前哪个算子在干啥事情，在处理啥逻辑。</li><li>sql 的 watermark 类型要设置为 TIMESTAMP(3)。</li><li>事件时间逻辑中，sql api 和 datastream api 对于数据记录时间戳存储逻辑是不一样的。datastream api：每条记录的 rowtime 是放在 StreamRecord 中的时间戳字段中的。sql api：时间戳是每次都从数据中进行获取的。算子中会维护一个下标。可以按照下标从数据中获取时间戳。</li></ol><p>后续文章会基于一些最常见的案例以及原理层面介绍 1.13 版本的 flink sql tumble window（基于最新的 window tvf）。</p><p>希望大家能持续关注。支持博主。喜欢的请关注 + 点赞 + 再看。</p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink sql 知其所以然（七）：不会连最适合 flink sql 的 ETL 和 group agg 场景都没见过吧？</title>
    <link href="https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88%20flink%20sql%20%E7%9A%84%20ETL%20%E5%92%8C%20group%20agg%20%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7%EF%BC%9F/"/>
    <id>https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%8D%E4%BC%9A%E8%BF%9E%E6%9C%80%E9%80%82%E5%90%88%20flink%20sql%20%E7%9A%84%20ETL%20%E5%92%8C%20group%20agg%20%E5%9C%BA%E6%99%AF%E9%83%BD%E6%B2%A1%E8%A7%81%E8%BF%87%E5%90%A7%EF%BC%9F/</id>
    <published>2021-11-13T06:22:58.000Z</published>
    <updated>2021-09-05T14:56:56.497Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>感谢您的小爱心（<strong>关注  +  点赞 + 再看</strong>），对博主的肯定，会督促博主持续的输出更多的优质实战内容！！！</p></blockquote><h1 id="1-序篇-本文结构"><a href="#1-序篇-本文结构" class="headerlink" title="1.序篇-本文结构"></a>1.序篇-本文结构</h1><p>前面的章节铺垫了那么多，终于在本节走入一条 query 了。</p><p>针对 datastream api 大家都比较熟悉了，还是那句话，在 datastream 中，你写的代码逻辑是什么样的，它最终的执行方式就是什么样的。</p><p>但是对于 flink sql 的执行过程，大家还是不熟悉的。</p><p>因此本文通过以下章节使用 ETL，group agg（sum，count等）简单聚合类 query 带大家走进一条 flink sql query 逻辑的世界。帮大家至少能够熟悉在 flink sql 程序运行时知道 flink 程序在干什么。</p><ol><li>背景篇-大家不了解 flink sql 什么？</li><li>目标篇-本文能帮助大家了解 flink sql 什么？</li><li>实战篇-简单的 query 案例和运行原理</li><li>总结与展望篇</li></ol><p><strong>先说说结论：</strong></p><ol><li><strong>场景问题</strong>：flink sql 很适合简单 ETL，以及基本全部场景下的聚合类指标。</li><li><strong>语法问题</strong>：flink sql 语法其实是和其他 sql 语法基本一致的。基本不会产生语法问题阻碍使用 flink sql。</li><li><strong>运行问题</strong>：查看 flink sql 任务时的一些技巧：<ul><li>去 flink webui 看看这个任务目前在做什么。包括算子名称都会给直接展示给我们目前哪个算子在干啥事情，在处理啥逻辑。</li><li>如果你想知道你的 flink 任务执行了什么代码，就去看看 sql 最后转换成的 transformation 里面具体要执行哪些操作。flink sql 生成的代码也在里面。</li><li>如果你不确定线上任务执行原理，可以直接在本地尝试运行。</li></ul></li></ol><h1 id="2-背景篇-大家不了解-flink-sql-什么？"><a href="#2-背景篇-大家不了解-flink-sql-什么？" class="headerlink" title="2.背景篇-大家不了解 flink sql 什么？"></a>2.背景篇-大家不了解 flink sql 什么？</h1><p>首先从大家用 flink sql 的一个初衷和状态出发，想一下大家在开始上手 flink sql 时，是什么样的一个想法？</p><p>博主大概整理了下，在初步上手 flink sql，一般从入手到踩坑整个过程中，一般都会有以下几种问题或者想法：</p><ol><li><strong>场景问题</strong>：首先 flink sql 是用来提效的，那相比 datastream，哪些场景很适合 flink sql 去做？</li><li><strong>语法问题</strong>：我写 sql 时 flink sql 语法会不会和其他 sql 语法有不同？</li><li><strong>运行问题</strong>：我写了一条 sql，运行起来了，但是对我来说是黑盒的，我怎么知道这个任务正在执行什么操作？有没有什么好办法帮我去理解 flink sql 的运行机制？</li><li><strong>理解误区</strong>：在理解 flink sql 的运算机制上有哪些误区？</li><li><strong>坑</strong>：flink sql 一般都有啥坑？提前了解帮我们避免踩坑。</li></ol><p>就是上面这些想法，会让很多想在公司内部引入 flink sql 的同学望而却步。</p><h1 id="3-目标篇-本文能帮助大家了解-flink-sql-什么？"><a href="#3-目标篇-本文能帮助大家了解-flink-sql-什么？" class="headerlink" title="3.目标篇-本文能帮助大家了解 flink sql 什么？"></a>3.目标篇-本文能帮助大家了解 flink sql 什么？</h1><p>来看看本文的目标：</p><ol><li><strong>场景问题：帮大家理解哪些场景是非常适合 flink sql 的</strong></li><li><strong>语法问题：帮大家简单熟悉 flink sql 的语法</strong></li><li><strong>运行问题：使用一条简单的 query sql 看看其运行起来的过程，其运行的机制</strong></li><li>理解误区：运算机制上的常见误区</li><li>坑：看看 sql 一般会有啥坑</li></ol><p>由于一篇文章不能覆盖所有概念，本文主要介绍一些最简单的 ETL，聚合场景，主要集中于前三点。</p><p>后两点在后续系列文章中会按照场景详细展开。</p><h1 id="4-实战篇-简单的-query-案例和运行原理"><a href="#4-实战篇-简单的-query-案例和运行原理" class="headerlink" title="4.实战篇-简单的 query 案例和运行原理"></a>4.实战篇-简单的 query 案例和运行原理</h1><h2 id="4-1-场景问题：有哪些场景适合-flink-sql？"><a href="#4-1-场景问题：有哪些场景适合-flink-sql？" class="headerlink" title="4.1.场景问题：有哪些场景适合 flink sql？"></a>4.1.场景问题：有哪些场景适合 flink sql？</h2><p>不装了，我坦白了，flink sql 其实很适合干的活就是 dwd 清洗，dws 聚合。</p><p>此处主要针对实时数仓的场景来说。flink sql 能干 dwd 清洗，dws 聚合，基本上实时数仓的大多数场景都能给覆盖了。</p><p>flink sql 牛逼！！！</p><p>但是！！！</p><p>经过博主使用 flink sql 经验来看，并不是所有的 dwd，dws 聚合场景都适合 flink sql（截止发文阶段来说）！！！</p><p>其实这些目前不适合 flink sql 的场景总结下来就是在处理上比 datastream 还是会有一定的损失。</p><p>先总结下使用场景：<br><strong>1. dwd</strong>：简单的清洗、复杂的清洗、维度的扩充、各种 udf 的使用<br><strong>2. dws</strong>：各类聚合</p><p>然后分适合的场景和不适合的场景来说，因为只这一篇不能覆盖所有的内容，所以本文此处先大致给个结论，之后会结合具体的场景详细描述。</p><ul><li><strong>适合的场景：</strong><ol><li>简单的 dwd 清洗场景</li><li>全场景的 dws 聚合场景</li></ol></li><li><strong>目前不太适合的场景：</strong><ol><li>复杂的 dwd 清洗场景：举例比如使用了很多 udf 清洗，尤其是使用很多的 json 类解析清洗</li><li>关联维度场景：举例比如 datastream 中经常会有攒一批数据批量访问外部接口的场景，flink sql 目前对于这种场景虽然有 localcache、异步访问能力，但是依然还是一条一条访问外部缓存，这样相比批量访问还是会有性能差距。</li></ol></li></ul><h2 id="4-2-语法-运行问题"><a href="#4-2-语法-运行问题" class="headerlink" title="4.2.语法\运行问题"></a>4.2.语法\运行问题</h2><p>其实总结来说，对于接触过 sql 的同学来说，除了 flink sql 中窗口聚合类的写法来说，其他的 sql 语法都是相同的，很容易理解。</p><p>本节会针对具体的案例进行详细介绍。</p><h3 id="4-2-1-ETL"><a href="#4-2-1-ETL" class="headerlink" title="4.2.1.ETL"></a>4.2.1.ETL</h3><p>最简单的 ETL 类型任务。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> select_list <span class="keyword">FROM</span> table_expression [ <span class="keyword">WHERE</span> boolean_expression ]</span><br></pre></td></tr></table></figure><p><strong>1.场景：简单的 dwd 清洗过滤场景</strong></p><p>源码公众号后台回复<strong>不会连最适合 flink sql 的 ETL 和 group agg 场景都没见过吧</strong>获取。</p><p>数据源表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    order_number <span class="type">BIGINT</span>,</span><br><span class="line">    price        <span class="type">DECIMAL</span>(<span class="number">32</span>,<span class="number">2</span>)</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.order_number.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.order_number.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;11&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>数据汇表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    order_number <span class="type">BIGINT</span>,</span><br><span class="line">    price        <span class="type">DECIMAL</span>(<span class="number">32</span>,<span class="number">2</span>)</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>ETL 逻辑：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> sink_table</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> source_table</span><br><span class="line"><span class="keyword">where</span> order_number <span class="operator">=</span> <span class="number">10</span></span><br></pre></td></tr></table></figure><p><strong>2.运行：可以看到，其实在 flink sql 任务中，其会把对应的处理逻辑给写到算子名称上面。</strong></p><blockquote><p><strong>Notes - 观察 flink sql 技巧 1：</strong><br>这个其实就是我们观察 flink sql 任务的第一个技巧。如果你想知道你的 flink 任务在干啥，第一反应是去 flink webui 看看这个任务目前在做什么。包括算子名称都会给直接展示给我们目前哪个算子在干啥事情，在处理啥逻辑</p></blockquote><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/5.png" alt="5"></p><p><strong>3.结果</strong></p><figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">+I[10, 337546916355686018150362513408.00]</span><br><span class="line">+I[10, 734895198061906189720381030400.00]</span><br><span class="line">+I[10, 496632591763800912960818249728.00]</span><br><span class="line">+I[10, 495090465926828588045441171456.00]</span><br><span class="line">+I[10, 167305033642317182838130081792.00]</span><br><span class="line">+I[10, 409466913112794578407573684224.00]</span><br><span class="line">+I[10, 894352160414515330502514180096.00]</span><br><span class="line">+I[10, 680063350384451712068576346112.00]</span><br><span class="line">+I[10, 50807402446574997641386524672.00]</span><br><span class="line">+I[10, 646597093362022945955245981696.00]</span><br><span class="line">+I[10, 233317961584082024331537809408.00]</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p><strong>4.原理：</strong></p><p>先看一下一个 flink sql 任务的入口执行逻辑。</p><p>首先看看建表语句的执行和 query 语句执行的逻辑有什么不同。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/6.png" alt="6"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/7.png" alt="7"></p><p>可以发现执行到 <code>executeInternal</code> 时会针对具体的 <code>operation</code> 来执行不同的操作。</p><p>执行建表操作就是具体的 <code>CreateTableOperation</code> 时，会将表的信息保存到 <code>catalogManager</code>。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/8.png" alt="8"></p><p>执行 query 操作就是具体的 <code>ModifyOperation</code> 时，会将对应的逻辑转换成对应的 <code>Transformation</code>。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/9.png" alt="9"></p><p><code>Transformation</code> 中就包含了执行的整体逻辑以及对应要执行的 sql 代码内容。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/10.png" alt="10"></p><p>接下来我们详细看下对应的 transform 中包含了什么内容。</p><p>首先是最外层 <code>LegacySinkTransformation</code>，即 sink 算子，如图就是 print sink function。比较好理解。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/11.png" alt="11"></p><p>然后是中间层 <code>OneInputTransformation</code>，即 sql 中过滤和转换操作（<code>select * from source_table where order_number = 10</code>），如图是代码生成的具体过滤和转换逻辑。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/12.png" alt="12"></p><p>生成的代码就在 <code>GeneratedOperator</code> 中的 <code>code</code> 字段。我们将对应的 <code>code</code> 复制到一个新的文件夹中。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/13.png" alt="13"></p><p>这个算子是直接继承了 <code>OneInputStreamOperator</code> 进行直接执行逻辑，跳过了 datastream 那一层。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/14.png" alt="14"></p><p>我们来看看最重要的 <code>processElement</code> 逻辑，具体字段解释和执行逻辑如图所示。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/15.png" alt="15"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/16.png" alt="16"></p><blockquote><p><strong>Notes - 观察 flink sql 技巧 2：</strong><br>这个其实就是我们观察 flink sql 任务的第二个技巧。如果你想知道你的 flink 任务执行了什么代码，就去看看 sql 最后转换成的 transformation 里面具体要执行哪些操作。</p></blockquote><h3 id="4-2-2-去重场景"><a href="#4-2-2-去重场景" class="headerlink" title="4.2.2.去重场景"></a>4.2.2.去重场景</h3><p><strong>1.场景：最简单的去重场景</strong></p><p>源码公众号后台回复<strong>不会连最适合 flink sql 的 ETL 和 group agg 场景都没见过吧</strong>获取。</p><p>数据源：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    string_field STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.string_field.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;3&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>数据汇：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    string_field STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>数据处理：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> sink_table</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">distinct</span> string_field</span><br><span class="line"><span class="keyword">from</span> source_table</span><br></pre></td></tr></table></figure><p><strong>2.运行：可以看到，其实在 flink sql 任务中，其会把对应的处理逻辑给写到算子名称上面。</strong></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/17.png" alt="17"></p><p><strong>3.上面这个案例的结果：</strong></p><figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+I[cd3]</span><br><span class="line">+I[8fc]</span><br><span class="line">+I[b0c]</span><br><span class="line">+I[1d8]</span><br><span class="line">+I[e28]</span><br><span class="line">+I[c5f]</span><br><span class="line">+I[e7d]</span><br><span class="line">+I[dfa]</span><br><span class="line">+I[1fe]</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p><strong>4.原理：</strong></p><p>此处我们只关注和上面不同的逻辑。</p><p>第一个就是 <code>PartitionTransform</code> 中的 <code>KeyGroupStreamPartitioner</code>，就是对应的分区逻辑。来看看生成代码的逻辑。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/18.png" alt="18"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/19.png" alt="19"></p><p>其中做 shuffle 逻辑时，是按照 <code>string_field</code> 作为 key 进行 shuffle。</p><p>第二个就是 <code>OneInputTransformation</code> 中的 <code>KeyedProcessOperator</code>，就是对应的去重逻辑。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/20.png" alt="20"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/21.png" alt="21"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/22.png" alt="22"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/23.png" alt="23"></p><p>可以看到生成的 function 中只有这三段代码是业务逻辑代码，但是其中的 RowData 初始化大小都是 0。那么到底是哪里做的去重逻辑呢？</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/24.png" alt="24"></p><p>我们跟一下处理逻辑会发现。去重逻辑主要集中在 <code>GroupAggFunction#processElement</code>。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/25.png" alt="25"></p><h3 id="4-2-3-group-聚合场景"><a href="#4-2-3-group-聚合场景" class="headerlink" title="4.2.3.group 聚合场景"></a>4.2.3.group 聚合场景</h3><h4 id="4-2-3-1-简单聚合场景"><a href="#4-2-3-1-简单聚合场景" class="headerlink" title="4.2.3.1.简单聚合场景"></a>4.2.3.1.简单聚合场景</h4><p><strong>1.场景：最简单的聚合场景</strong></p><p>源码公众号后台回复<strong>不会连最适合 flink sql 的 ETL 和 group agg 场景都没见过吧</strong>获取。</p><p>count，sum，avg，max，min 等：</p><p>数据源：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    order_id STRING,</span><br><span class="line">    price <span class="type">BIGINT</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.order_id.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.price.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1000000&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>数据汇：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    order_id STRING,</span><br><span class="line">    count_result <span class="type">BIGINT</span>,</span><br><span class="line">    sum_result <span class="type">BIGINT</span>,</span><br><span class="line">    avg_result <span class="keyword">DOUBLE</span>,</span><br><span class="line">    min_result <span class="type">BIGINT</span>,</span><br><span class="line">    max_result <span class="type">BIGINT</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>数据处理逻辑：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> sink_table</span><br><span class="line"><span class="keyword">select</span> order_id,</span><br><span class="line">       <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">as</span> count_result,</span><br><span class="line">       <span class="built_in">sum</span>(price) <span class="keyword">as</span> sum_result,</span><br><span class="line">       <span class="built_in">avg</span>(price) <span class="keyword">as</span> avg_result,</span><br><span class="line">       <span class="built_in">min</span>(price) <span class="keyword">as</span> min_result,</span><br><span class="line">       <span class="built_in">max</span>(price) <span class="keyword">as</span> max_result</span><br><span class="line"><span class="keyword">from</span> source_table</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> order_id</span><br></pre></td></tr></table></figure><p><strong>2.运行：</strong></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/26.png" alt="26"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/27.png" alt="27"></p><p><strong>3.上面这个案例的结果：</strong></p><figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">+I[1, 1, 415300, 415300.0, 415300, 415300]</span><br><span class="line">+I[d, 1, 416878, 416878.0, 416878, 416878]</span><br><span class="line">+I[0, 1, 120837, 120837.0, 120837, 120837]</span><br><span class="line">+I[c, 1, 337749, 337749.0, 337749, 337749]</span><br><span class="line">+I[7, 1, 387053, 387053.0, 387053, 387053]</span><br><span class="line">+I[8, 1, 387042, 387042.0, 387042, 387042]</span><br><span class="line">+I[2, 1, 546317, 546317.0, 546317, 546317]</span><br><span class="line">+I[e, 1, 22131, 22131.0, 22131, 22131]</span><br><span class="line">+I[9, 1, 651731, 651731.0, 651731, 651731]</span><br><span class="line">-U[0, 1, 120837, 120837.0, 120837, 120837]</span><br><span class="line">+U[0, 2, 566664, 283332.0, 120837, 445827]</span><br><span class="line">+I[b, 1, 748659, 748659.0, 748659, 748659]</span><br><span class="line">-U[7, 1, 387053, 387053.0, 387053, 387053]</span><br><span class="line">+U[7, 2, 1058056, 529028.0, 387053, 671003]</span><br></pre></td></tr></table></figure><p><strong>4.原理：</strong></p><p>来瞅一眼 transformation。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/28.png" alt="28"></p><p>还是和之前的逻辑一样，跟一下 GroupAggFunction 的逻辑。如下图，有五个执行步骤执行计算。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/33.png" alt="33"></p><p>再看最终生成的 function 代码逻辑。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/31.png" alt="31"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/29.png" alt="29"></p><p>首先看看 count 怎么算的。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/30.png" alt="30"></p><p>sum 怎么算的。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/32.png" alt="32"></p><h4 id="4-2-3-2-去重聚合场景"><a href="#4-2-3-2-去重聚合场景" class="headerlink" title="4.2.3.2.去重聚合场景"></a>4.2.3.2.去重聚合场景</h4><p><strong>1.场景：去重聚合场景</strong></p><p>数据源：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> source_table (</span><br><span class="line">    dim STRING,</span><br><span class="line">    user_id <span class="type">BIGINT</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;datagen&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;rows-per-second&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.dim.length&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.min&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;fields.user_id.max&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1000000&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>数据汇：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    dim STRING,</span><br><span class="line">    uv <span class="type">BIGINT</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>数据处理：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> sink_table</span><br><span class="line"><span class="keyword">select</span> dim,</span><br><span class="line">       <span class="built_in">count</span>(<span class="keyword">distinct</span> user_id) <span class="keyword">as</span> uv</span><br><span class="line"><span class="keyword">from</span> source_table</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> dim</span><br></pre></td></tr></table></figure><p><strong>2.运行：</strong></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/35.png" alt="35"></p><p><strong>3.上面这个案例的结果：</strong></p><figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">+U[9, 3097]</span><br><span class="line">-U[a, 3054]</span><br><span class="line">+U[a, 3055]</span><br><span class="line">-U[8, 3030]</span><br><span class="line">+U[8, 3031]</span><br><span class="line">-U[4, 3137]</span><br><span class="line">+U[4, 3138]</span><br><span class="line">-U[6, 3139]</span><br><span class="line">+U[6, 3140]</span><br><span class="line">-U[0, 3082]</span><br><span class="line">+U[0, 3083]</span><br></pre></td></tr></table></figure><p><strong>4.原理：</strong></p><p>此处只看和之前的案例不一样的地方。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/36.png" alt="36"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/37.png" alt="37"></p><h4 id="4-2-3-3-语法糖"><a href="#4-2-3-3-语法糖" class="headerlink" title="4.2.3.3.语法糖"></a>4.2.3.3.语法糖</h4><p><strong>1.grouping sets</strong></p><p>多维计算。相当于语法糖，用户可以根据自己的场景去指定自己想要的维度组合。</p><p>数据汇：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    supplier_id STRING,</span><br><span class="line">    product_id STRING,</span><br><span class="line">    total <span class="type">BIGINT</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>数据处理逻辑：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">     supplier_id,</span><br><span class="line">     product_id,</span><br><span class="line">     <span class="built_in">COUNT</span>(<span class="operator">*</span>) <span class="keyword">AS</span> total</span><br><span class="line"><span class="keyword">FROM</span> (<span class="keyword">VALUES</span></span><br><span class="line">     (<span class="string">&#x27;supplier1&#x27;</span>, <span class="string">&#x27;product1&#x27;</span>, <span class="number">4</span>),</span><br><span class="line">     (<span class="string">&#x27;supplier1&#x27;</span>, <span class="string">&#x27;product2&#x27;</span>, <span class="number">3</span>),</span><br><span class="line">     (<span class="string">&#x27;supplier2&#x27;</span>, <span class="string">&#x27;product3&#x27;</span>, <span class="number">3</span>),</span><br><span class="line">     (<span class="string">&#x27;supplier2&#x27;</span>, <span class="string">&#x27;product4&#x27;</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">AS</span> Products(supplier_id, product_id, rating)</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">GROUPING</span> SETS ((supplier_id, product_id), (supplier_id), ())</span><br></pre></td></tr></table></figure><p>其结果等同于：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> sink_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">     supplier_id,</span><br><span class="line">     product_id,</span><br><span class="line">     <span class="built_in">COUNT</span>(<span class="operator">*</span>) <span class="keyword">AS</span> total</span><br><span class="line"><span class="keyword">FROM</span> (<span class="keyword">VALUES</span></span><br><span class="line">     (<span class="string">&#x27;supplier1&#x27;</span>, <span class="string">&#x27;product1&#x27;</span>, <span class="number">4</span>),</span><br><span class="line">     (<span class="string">&#x27;supplier1&#x27;</span>, <span class="string">&#x27;product2&#x27;</span>, <span class="number">3</span>),</span><br><span class="line">     (<span class="string">&#x27;supplier2&#x27;</span>, <span class="string">&#x27;product3&#x27;</span>, <span class="number">3</span>),</span><br><span class="line">     (<span class="string">&#x27;supplier2&#x27;</span>, <span class="string">&#x27;product4&#x27;</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">AS</span> Products(supplier_id, product_id, rating)</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> supplier_id, product_id</span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">     supplier_id,</span><br><span class="line">     <span class="built_in">cast</span>(<span class="keyword">null</span> <span class="keyword">as</span> string) <span class="keyword">as</span> product_id,</span><br><span class="line">     <span class="built_in">COUNT</span>(<span class="operator">*</span>) <span class="keyword">AS</span> total</span><br><span class="line"><span class="keyword">FROM</span> (<span class="keyword">VALUES</span></span><br><span class="line">     (<span class="string">&#x27;supplier1&#x27;</span>, <span class="string">&#x27;product1&#x27;</span>, <span class="number">4</span>),</span><br><span class="line">     (<span class="string">&#x27;supplier1&#x27;</span>, <span class="string">&#x27;product2&#x27;</span>, <span class="number">3</span>),</span><br><span class="line">     (<span class="string">&#x27;supplier2&#x27;</span>, <span class="string">&#x27;product3&#x27;</span>, <span class="number">3</span>),</span><br><span class="line">     (<span class="string">&#x27;supplier2&#x27;</span>, <span class="string">&#x27;product4&#x27;</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">AS</span> Products(supplier_id, product_id, rating)</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> supplier_id</span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">     <span class="built_in">cast</span>(<span class="keyword">null</span> <span class="keyword">as</span> string) <span class="keyword">AS</span> supplier_id,</span><br><span class="line">     <span class="built_in">cast</span>(<span class="keyword">null</span> <span class="keyword">as</span> string) <span class="keyword">AS</span> product_id,</span><br><span class="line">     <span class="built_in">COUNT</span>(<span class="operator">*</span>) <span class="keyword">AS</span> total</span><br><span class="line"><span class="keyword">FROM</span> (<span class="keyword">VALUES</span></span><br><span class="line">     (<span class="string">&#x27;supplier1&#x27;</span>, <span class="string">&#x27;product1&#x27;</span>, <span class="number">4</span>),</span><br><span class="line">     (<span class="string">&#x27;supplier1&#x27;</span>, <span class="string">&#x27;product2&#x27;</span>, <span class="number">3</span>),</span><br><span class="line">     (<span class="string">&#x27;supplier2&#x27;</span>, <span class="string">&#x27;product3&#x27;</span>, <span class="number">3</span>),</span><br><span class="line">     (<span class="string">&#x27;supplier2&#x27;</span>, <span class="string">&#x27;product4&#x27;</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">AS</span> Products(supplier_id, product_id, rating)</span><br></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">+I[supplier1, product1, 1]</span><br><span class="line">+I[supplier1, null, 1]</span><br><span class="line">+I[null, null, 1]</span><br><span class="line">+I[supplier1, product2, 1]</span><br><span class="line">-U[supplier1, null, 1]</span><br><span class="line">+U[supplier1, null, 2]</span><br><span class="line">-U[null, null, 1]</span><br><span class="line">+U[null, null, 2]</span><br><span class="line">+I[supplier2, product3, 1]</span><br><span class="line">+I[supplier2, null, 1]</span><br><span class="line">-U[null, null, 2]</span><br><span class="line">+U[null, null, 3]</span><br><span class="line">+I[supplier2, product4, 1]</span><br><span class="line">-U[supplier2, null, 1]</span><br><span class="line">+U[supplier2, null, 2]</span><br><span class="line">-U[null, null, 3]</span><br><span class="line">+U[null, null, 4]</span><br></pre></td></tr></table></figure><p>grouping sets 能帮助我们在多维场景下，减少很多冗余代码。关于 grouping sets 原理后面的系列文章会介绍。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/38.png" alt="38"></p><p><strong>2.rollup</strong></p><p>rollup 是上卷计算的一种简化写法。比如可以把 <code>GROUPING SETS ((supplier_id, product_id), (supplier_id), ())</code> 简化为 <code>ROLLUP (supplier_id, product_id)</code>。</p><p>数据汇：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    supplier_id STRING,</span><br><span class="line">    product_id STRING,</span><br><span class="line">    total <span class="type">BIGINT</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>数据处理逻辑：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> supplier_id, rating, <span class="built_in">COUNT</span>(<span class="operator">*</span>)</span><br><span class="line"><span class="keyword">FROM</span> (<span class="keyword">VALUES</span></span><br><span class="line">    (<span class="string">&#x27;supplier1&#x27;</span>, <span class="string">&#x27;product1&#x27;</span>, <span class="number">4</span>),</span><br><span class="line">    (<span class="string">&#x27;supplier1&#x27;</span>, <span class="string">&#x27;product2&#x27;</span>, <span class="number">3</span>),</span><br><span class="line">    (<span class="string">&#x27;supplier2&#x27;</span>, <span class="string">&#x27;product3&#x27;</span>, <span class="number">3</span>),</span><br><span class="line">    (<span class="string">&#x27;supplier2&#x27;</span>, <span class="string">&#x27;product4&#x27;</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">AS</span> Products(supplier_id, product_id, rating)</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">ROLLUP</span> (supplier_id, product_id)</span><br></pre></td></tr></table></figure><p>其结果等同于：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> supplier_id, rating, product_id, <span class="built_in">COUNT</span>(<span class="operator">*</span>)</span><br><span class="line"><span class="keyword">FROM</span> (<span class="keyword">VALUES</span></span><br><span class="line">    (<span class="string">&#x27;supplier1&#x27;</span>, <span class="string">&#x27;product1&#x27;</span>, <span class="number">4</span>),</span><br><span class="line">    (<span class="string">&#x27;supplier1&#x27;</span>, <span class="string">&#x27;product2&#x27;</span>, <span class="number">3</span>),</span><br><span class="line">    (<span class="string">&#x27;supplier2&#x27;</span>, <span class="string">&#x27;product3&#x27;</span>, <span class="number">3</span>),</span><br><span class="line">    (<span class="string">&#x27;supplier2&#x27;</span>, <span class="string">&#x27;product4&#x27;</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">AS</span> Products(supplier_id, product_id, rating)</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">GROUPING</span> <span class="keyword">SET</span> (</span><br><span class="line">    ( supplier_id, product_id ),</span><br><span class="line">    ( supplier_id             ),</span><br><span class="line">    (                         )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">+I[supplier1, product1, 1]</span><br><span class="line">+I[supplier1, null, 1]</span><br><span class="line">+I[null, null, 1]</span><br><span class="line">+I[supplier1, product2, 1]</span><br><span class="line">-U[supplier1, null, 1]</span><br><span class="line">+U[supplier1, null, 2]</span><br><span class="line">-U[null, null, 1]</span><br><span class="line">+U[null, null, 2]</span><br><span class="line">+I[supplier2, product3, 1]</span><br><span class="line">+I[supplier2, null, 1]</span><br><span class="line">-U[null, null, 2]</span><br><span class="line">+U[null, null, 3]</span><br><span class="line">+I[supplier2, product4, 1]</span><br><span class="line">-U[supplier2, null, 1]</span><br><span class="line">+U[supplier2, null, 2]</span><br><span class="line">-U[null, null, 3]</span><br><span class="line">+U[null, null, 4]</span><br></pre></td></tr></table></figure><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/39.png" alt="39"></p><p><strong>5.CUBE 计算</strong></p><p>源码公众号后台回复<strong>不会连最适合 flink sql 的 ETL 和 group agg 场景都没见过吧</strong>获取。</p><p>cube 相当于是一种覆盖了所有维度组合聚合计算。比如 group by a, b, c。其会将 a, b, c 三个维度的所有维度组合进行 group by。</p><p>数据汇：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sink_table (</span><br><span class="line">    supplier_id STRING,</span><br><span class="line">    product_id STRING,</span><br><span class="line">    total <span class="type">BIGINT</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>数据处理逻辑：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> supplier_id, rating, product_id, <span class="built_in">COUNT</span>(<span class="operator">*</span>)</span><br><span class="line"><span class="keyword">FROM</span> (<span class="keyword">VALUES</span></span><br><span class="line">    (<span class="string">&#x27;supplier1&#x27;</span>, <span class="string">&#x27;product1&#x27;</span>, <span class="number">4</span>),</span><br><span class="line">    (<span class="string">&#x27;supplier1&#x27;</span>, <span class="string">&#x27;product2&#x27;</span>, <span class="number">3</span>),</span><br><span class="line">    (<span class="string">&#x27;supplier2&#x27;</span>, <span class="string">&#x27;product3&#x27;</span>, <span class="number">3</span>),</span><br><span class="line">    (<span class="string">&#x27;supplier2&#x27;</span>, <span class="string">&#x27;product4&#x27;</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">AS</span> Products(supplier_id, product_id, rating)</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">CUBE</span> (supplier_id, product_id)</span><br></pre></td></tr></table></figure><p>它等同于</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> supplier_id, rating, product_id, <span class="built_in">COUNT</span>(<span class="operator">*</span>)</span><br><span class="line"><span class="keyword">FROM</span> (<span class="keyword">VALUES</span></span><br><span class="line">    (<span class="string">&#x27;supplier1&#x27;</span>, <span class="string">&#x27;product1&#x27;</span>, <span class="number">4</span>),</span><br><span class="line">    (<span class="string">&#x27;supplier1&#x27;</span>, <span class="string">&#x27;product2&#x27;</span>, <span class="number">3</span>),</span><br><span class="line">    (<span class="string">&#x27;supplier2&#x27;</span>, <span class="string">&#x27;product3&#x27;</span>, <span class="number">3</span>),</span><br><span class="line">    (<span class="string">&#x27;supplier2&#x27;</span>, <span class="string">&#x27;product4&#x27;</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">AS</span> Products(supplier_id, product_id, rating)</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">GROUPING</span> <span class="keyword">SET</span> (</span><br><span class="line">    ( supplier_id, product_id ),</span><br><span class="line">    ( supplier_id             ),</span><br><span class="line">    (              product_id ),</span><br><span class="line">    (                         )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">+I[supplier1, product1, 1]</span><br><span class="line">+I[supplier1, null, 1]</span><br><span class="line">+I[null, product1, 1]</span><br><span class="line">+I[null, null, 1]</span><br><span class="line">+I[supplier1, product2, 1]</span><br><span class="line">-U[supplier1, null, 1]</span><br><span class="line">+U[supplier1, null, 2]</span><br><span class="line">+I[null, product2, 1]</span><br><span class="line">-U[null, null, 1]</span><br><span class="line">+U[null, null, 2]</span><br><span class="line">+I[supplier2, product3, 1]</span><br><span class="line">+I[supplier2, null, 1]</span><br><span class="line">+I[null, product3, 1]</span><br><span class="line">-U[null, null, 2]</span><br><span class="line">+U[null, null, 3]</span><br><span class="line">+I[supplier2, product4, 1]</span><br><span class="line">-U[supplier2, null, 1]</span><br><span class="line">+U[supplier2, null, 2]</span><br><span class="line">+I[null, product4, 1]</span><br><span class="line">-U[null, null, 3]</span><br><span class="line">+U[null, null, 4]</span><br></pre></td></tr></table></figure><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/08_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%80%E6%9D%A1sqlquery/40.png" alt="40"></p><h1 id="5-总结与展望篇"><a href="#5-总结与展望篇" class="headerlink" title="5.总结与展望篇"></a>5.总结与展望篇</h1><p>本文主要介绍了 ETL，group agg 聚合类指标的一些常见场景案例以及其底层运行原理。我们可以发现 flink sql 的语法其实和 hive sql，mysql 啥的语法都是基本一致的。所以上手 flink sql 时，语法基本不会成为我们的障碍。</p><p>而且也介绍了在查看 flink sql 任务时的一些技巧：</p><ol><li>去 flink webui 看看这个任务目前在做什么。包括算子名称都会给直接展示给我们目前哪个算子在干啥事情，在处理啥逻辑。</li><li>如果你想知道你的 flink 任务执行了什么代码，就去看看 sql 最后转换成的 transformation 里面具体要执行哪些操作。</li></ol><p>后续文章会继续介绍 flink sql 窗口聚合，一些理解误区，和坑之类的案例。</p><p>希望大家能持续关注。支持博主。喜欢的请关注 + 点赞 + 再看。</p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>踩坑记| flink state 序列化 java enum 竟然岔劈了</title>
    <link href="https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/02_flink-datastream/01_%E8%B8%A9%E5%9D%91%E8%AE%B0|flinkstate%E5%BA%8F%E5%88%97%E5%8C%96javaenum%E7%AB%9F%E7%84%B6%E5%B2%94%E5%8A%88%E4%BA%86/"/>
    <id>https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/02_flink-datastream/01_%E8%B8%A9%E5%9D%91%E8%AE%B0|flinkstate%E5%BA%8F%E5%88%97%E5%8C%96javaenum%E7%AB%9F%E7%84%B6%E5%B2%94%E5%8A%88%E4%BA%86/</id>
    <published>2021-11-13T06:21:58.000Z</published>
    <updated>2021-08-24T15:26:15.026Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>感谢您的<strong>关注  +  点赞 + 再看</strong>，对博主的肯定，会督促博主持续的输出更多的优质实战内容！！！</p></blockquote><h1 id="1-序篇-先说结论"><a href="#1-序篇-先说结论" class="headerlink" title="1.序篇-先说结论"></a>1.序篇-先说结论</h1><p>本文主要记录博主在生产环境中踩的 flink 针对 java enum serde 时的坑。</p><p>结论：在 flink 程序中，如果状态中有存储 java enum，那么添加或者删除 enum 中的一个枚举值时，就有可能导致状态恢复异常，这里的异常可能不是在恢复过程中会实际抛出一个异常，而是有可能是 enum A 的值恢复给 enum B。</p><p>我从以下几个章节说明、解决这个问题，希望能抛砖引玉，带给大家一些启发。</p><ol><li>踩坑场景篇-这个坑是啥样的</li><li>问题排查篇-坑的排查过程</li><li>问题原理解析篇-导致问题的机制是什么</li><li>避坑篇-如何避免这种问题</li><li>总结篇</li></ol><h1 id="2-踩坑场景篇-这个坑是啥样的"><a href="#2-踩坑场景篇-这个坑是啥样的" class="headerlink" title="2.踩坑场景篇-这个坑是啥样的"></a>2.踩坑场景篇-这个坑是啥样的</h1><p>对任务做一个简单的过滤条件修改，任务重新上线之后，从 flink web ui 确认是从 savepoint 重启成功了，但是实际最终产出的数据上来看却像是没有从 savepoint 重启。</p><p>逻辑就是计算分维度的当天累计 pv。代码很简单，在后面会贴出来。</p><p>如下图：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/02_flink-datastream/01_%E8%B8%A9%E5%9D%91%E8%AE%B0%7Cflinkstate%E5%BA%8F%E5%88%97%E5%8C%96javaenum%E7%AB%9F%E7%84%B6%E5%B2%94%E5%8A%88%E4%BA%86/2.png" alt="2"></p><p>在 00:04 分重启时出现了当天累计 pv 出现了从零累计的情况。</p><p>但是预期正常的曲线应该张下面这样。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/02_flink-datastream/01_%E8%B8%A9%E5%9D%91%E8%AE%B0%7Cflinkstate%E5%BA%8F%E5%88%97%E5%8C%96javaenum%E7%AB%9F%E7%84%B6%E5%B2%94%E5%8A%88%E4%BA%86/1.png" alt="1"></p><p>任务是使用 DataStream 编写（基于 flink 1.13.1）。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SenerioTest</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env =</span><br><span class="line">                StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(<span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime);</span><br><span class="line"></span><br><span class="line">        env.addSource(<span class="keyword">new</span> SourceFunction&lt;SourceModel&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isCancel = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;SourceModel&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="comment">// 数据源</span></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">this</span>.isCancel = <span class="keyword">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .keyBy(<span class="keyword">new</span> KeySelector&lt;SourceModel, Long&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Long <span class="title">getKey</span><span class="params">(SourceModel value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> value.getUserId() % <span class="number">1000</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .timeWindow(Time.minutes(<span class="number">1</span>))</span><br><span class="line">        .aggregate(</span><br><span class="line">            <span class="keyword">new</span> AggregateFunction&lt;SourceModel, Map&lt;Tuple2&lt;DimNameEnum, String&gt;, Long&gt;, Map&lt;Tuple2&lt;DimNameEnum, String&gt;, Long&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="keyword">public</span> Map&lt;Tuple2&lt;DimNameEnum, String&gt;, Long&gt; createAccumulator() &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="keyword">public</span> Map&lt;Tuple2&lt;DimNameEnum, String&gt;, Long&gt; add(SourceModel value,</span><br><span class="line">                        Map&lt;Tuple2&lt;DimNameEnum, String&gt;, Long&gt; accumulator) &#123;</span><br><span class="line"></span><br><span class="line">                    Lists.newArrayList(Tuple2.of(DimNameEnum.province, value.getProvince())</span><br><span class="line">                            , Tuple2.of(DimNameEnum.age, value.getAge())</span><br><span class="line">                            , Tuple2.of(DimNameEnum.sex, value.getSex()))</span><br><span class="line">                            .forEach(t -&gt; &#123;</span><br><span class="line">                                Long l = accumulator.get(t);</span><br><span class="line"></span><br><span class="line">                                <span class="keyword">if</span> (<span class="keyword">null</span> == l) &#123;</span><br><span class="line">                                    accumulator.put(t, <span class="number">1L</span>);</span><br><span class="line">                                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                                    accumulator.put(t, l + <span class="number">1</span>);</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;);</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">return</span> accumulator;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="keyword">public</span> Map&lt;Tuple2&lt;DimNameEnum, String&gt;, Long&gt; getResult(</span><br><span class="line">                        Map&lt;Tuple2&lt;DimNameEnum, String&gt;, Long&gt; accumulator) &#123;</span><br><span class="line">                    <span class="keyword">return</span> accumulator;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="keyword">public</span> Map&lt;Tuple2&lt;DimNameEnum, String&gt;, Long&gt; merge(</span><br><span class="line">                        Map&lt;Tuple2&lt;DimNameEnum, String&gt;, Long&gt; a,</span><br><span class="line">                        Map&lt;Tuple2&lt;DimNameEnum, String&gt;, Long&gt; b) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="keyword">new</span> ProcessWindowFunction&lt;Map&lt;Tuple2&lt;DimNameEnum, String&gt;, Long&gt;, SinkModel, Long, TimeWindow&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">private</span> <span class="keyword">transient</span> ValueState&lt;Map&lt;Tuple2&lt;DimNameEnum, String&gt;, Long&gt;&gt; todayPv;</span><br><span class="line"></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                    <span class="keyword">this</span>.todayPv = getRuntimeContext().getState(<span class="keyword">new</span> ValueStateDescriptor&lt;Map&lt;Tuple2&lt;DimNameEnum, String&gt;, Long&gt;&gt;(</span><br><span class="line">                            <span class="string">&quot;todayPv&quot;</span>, TypeInformation.of(</span><br><span class="line">                            <span class="keyword">new</span> TypeHint&lt;Map&lt;Tuple2&lt;DimNameEnum, String&gt;, Long&gt;&gt;() &#123;</span><br><span class="line">                            &#125;)));</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Long aLong, Context context,</span></span></span><br><span class="line"><span class="function"><span class="params">                        Iterable&lt;Map&lt;Tuple2&lt;DimNameEnum, String&gt;, Long&gt;&gt; elements, Collector&lt;SinkModel&gt; out)</span></span></span><br><span class="line"><span class="function">                        <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    <span class="comment">// 将 elements 数据 merge 到 todayPv 中</span></span><br><span class="line">                    <span class="comment">// 每天零点将 state 清空重新累计</span></span><br><span class="line">                    <span class="comment">// 然后 out#collect 出去即可</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Data</span></span><br><span class="line">    <span class="meta">@Builder</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SourceModel</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">long</span> userId;</span><br><span class="line">        <span class="keyword">private</span> String province;</span><br><span class="line">        <span class="keyword">private</span> String age;</span><br><span class="line">        <span class="keyword">private</span> String sex;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">long</span> timestamp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Data</span></span><br><span class="line">    <span class="meta">@Builder</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SinkModel</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> String dimName;</span><br><span class="line">        <span class="keyword">private</span> String dimValue;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">long</span> timestamp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">enum</span> <span class="title">DimNameEnum</span> </span>&#123;</span><br><span class="line">        province,</span><br><span class="line">        age,</span><br><span class="line">        sex,</span><br><span class="line">        ;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="3-问题排查篇-坑的排查过程"><a href="#3-问题排查篇-坑的排查过程" class="headerlink" title="3.问题排查篇-坑的排查过程"></a>3.问题排查篇-坑的排查过程</h1><h2 id="3-1-愚蠢的怀疑引擎"><a href="#3-1-愚蠢的怀疑引擎" class="headerlink" title="3.1.愚蠢的怀疑引擎"></a>3.1.愚蠢的怀疑引擎</h2><p>首先怀疑是状态没有正常恢复。</p><p>但是查看 flink web ui 以及 tm 日志，都显示是从 savepoint 正常恢复了。</p><p>还怀疑是不是出现了 flink web ui 展示的内容和实际的执行不一致的情况。</p><p>但是发现任务的 ck 大小是正常的，复合预期的。</p><h2 id="3-2-老老实实打-log-吧"><a href="#3-2-老老实实打-log-吧" class="headerlink" title="3.2.老老实实打 log 吧"></a>3.2.老老实实打 log 吧</h2><p>既然能从 savepoint 正常恢复，那么就把状态值用 log 打出来看看到底发生了什么事情呗。</p><p>如下列代码，在 <code>ProcessWindowFunction</code> 中加上 log 日志。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">this</span>.todayPv.value()</span><br><span class="line">    .forEach(<span class="keyword">new</span> BiConsumer&lt;Tuple2&lt;DimNameEnum, String&gt;, Long&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(Tuple2&lt;DimNameEnum, String&gt; k,</span></span></span><br><span class="line"><span class="function"><span class="params">                Long v)</span> </span>&#123;</span><br><span class="line">            log.info(<span class="string">&quot;key 值：&#123;&#125;，value 值：&#123;&#125;&quot;</span>, k.toString(), v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><p>发现结果如下：</p><figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">key 值：(uv_type,男)，value 值：1000</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>发现状态中存储的 <code>DimNameEnum.province</code>，<code>DimNameEnum.age</code> 的数据都是正确的，但是缺缺少了 <code>DimNameEnum.sex</code>，多了 <code>(uv_type,男)</code> 这样的数据，于是查看代码，发现之前多加了一种枚举类型 <code>DimNameEnum.uv_type</code>。代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">enum</span> <span class="title">DimNameEnum</span> </span>&#123;</span><br><span class="line">    province,</span><br><span class="line">    age,</span><br><span class="line">    uv_type,</span><br><span class="line">    sex,</span><br><span class="line">    ;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>于是怀疑 flink 针对枚举值的 serde 不是按照枚举值名称来进行匹配的，而是按照枚举值下标来进行匹配的。因此就出现了 <code>DimNameEnum.uv_type</code> 将 <code>DimNameEnum.sex</code> 的位置占了的情况。</p><h1 id="4-问题原理解析篇-导致问题的机制是什么"><a href="#4-问题原理解析篇-导致问题的机制是什么" class="headerlink" title="4.问题原理解析篇-导致问题的机制是什么"></a>4.问题原理解析篇-导致问题的机制是什么</h1><p>来看看源码吧。</p><p>测试代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EnumsStateTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env =</span><br><span class="line">                StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(<span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime);</span><br><span class="line"></span><br><span class="line">        TypeInformation&lt;StateTestEnums&gt; t = TypeInformation.of(StateTestEnums.class);</span><br><span class="line"></span><br><span class="line">        EnumSerializer&lt;StateTestEnums&gt; e = (EnumSerializer&lt;StateTestEnums&gt;) t.createSerializer(env.getConfig());</span><br><span class="line"></span><br><span class="line">        DataOutputSerializer d = <span class="keyword">new</span> DataOutputSerializer(<span class="number">10000</span>);</span><br><span class="line"></span><br><span class="line">        e.serialize(StateTestEnums.A, d);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">enum</span> <span class="title">StateTestEnums</span> </span>&#123;</span><br><span class="line">        A,</span><br><span class="line">        B,</span><br><span class="line">        C</span><br><span class="line">        ;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>debug 结果如下：</p><p>首先看看对应的 <code>TypeInformation</code> 和 <code>TypeSerializer</code>。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/02_flink-datastream/01_%E8%B8%A9%E5%9D%91%E8%AE%B0%7Cflinkstate%E5%BA%8F%E5%88%97%E5%8C%96javaenum%E7%AB%9F%E7%84%B6%E5%B2%94%E5%8A%88%E4%BA%86/3.png" alt="3"></p><p>发现 enum 类型的序列化器是 <code>EnumSerializer</code>， 看看 <code>EnumSerializer</code> 的 serde 实现，如图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/02_flink-datastream/01_%E8%B8%A9%E5%9D%91%E8%AE%B0%7Cflinkstate%E5%BA%8F%E5%88%97%E5%8C%96javaenum%E7%AB%9F%E7%84%B6%E5%B2%94%E5%8A%88%E4%BA%86/4.png" alt="4"></p><p>最关键的两个变量：</p><ol><li>序列化时用 <code>valueToOrdinal</code></li><li>反序列化时用 <code>values</code></li></ol><p>从而印证了上面的说法。flink enum 序列化时使用的是枚举值下标进行 serde，因此一旦枚举值顺序发生改变，或者添加、删除一个枚举值，就会导致其他枚举值的下标出现错位的情况。从而导致数据错误。</p><h1 id="5-避坑篇-如何避免这种问题"><a href="#5-避坑篇-如何避免这种问题" class="headerlink" title="5.避坑篇-如何避免这种问题"></a>5.避坑篇-如何避免这种问题</h1><h2 id="5-1-枚举解决"><a href="#5-1-枚举解决" class="headerlink" title="5.1.枚举解决"></a>5.1.枚举解决</h2><p>在上述场景中，如果又想要把新枚举值加进去，又需要状态能够正常恢复，正常产出数据。</p><p>那么可以把新的枚举值在尾部添加，比如下面这样。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">enum</span> <span class="title">DimNameEnum</span> </span>&#123;</span><br><span class="line">    province,</span><br><span class="line">    age,</span><br><span class="line">    sex,</span><br><span class="line">    uv_type, <span class="comment">// 添加在尾部</span></span><br><span class="line">    ;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5-2-非枚举解决"><a href="#5-2-非枚举解决" class="headerlink" title="5.2.非枚举解决"></a>5.2.非枚举解决</h2><p>还有一种方法如标题，就是别用枚举值，直接用 string 就 vans 了。</p><h1 id="6-总结篇"><a href="#6-总结篇" class="headerlink" title="6.总结篇"></a>6.总结篇</h1><p>本文主要介绍了 flink 枚举值 serde 中的坑，当在 enum 中添加删除枚举值时，就有可能导致状态岔劈。随后给出了原因是由于 enum serde 器的实现导致的这种情况，最后给出了解决方案。</p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink sql 知其所以然（六）：flink sql 约会 calcite</title>
    <link href="https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84%20sql%20%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/"/>
    <id>https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flink%20sql%20%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84%20sql%20%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/</id>
    <published>2021-11-13T06:21:58.000Z</published>
    <updated>2021-09-02T16:05:42.560Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>感谢您的小爱心（<strong>关注  +  点赞 + 再看</strong>），对博主的肯定，会督促博主持续的输出更多的优质实战内容！！！</p></blockquote><h1 id="1-序篇-本文结构"><a href="#1-序篇-本文结构" class="headerlink" title="1.序篇-本文结构"></a>1.序篇-本文结构</h1><p>本文主要介绍 flink sql 与 calcite 之间的关系。flink sql 的解析主要依赖 calcite。</p><p>而博主通过此文抛砖引玉帮助大家理解 flink sql 在解析中是怎样依赖 calcite 的，以及 flink sql 解析的流程，sql parser 相关内容。希望对大家有所帮助。</p><p>本文通过以下几节进行介绍，对某个章节感兴趣的可以直接划到对应章节。</p><ol><li><p>背景篇-一条 flink sql 的执行过程</p><ul><li>发挥自己的想象力</li><li>看看 flink 的实现</li></ul></li><li><p>简介篇-calcite 扮演的角色</p><ul><li>calcite 是啥？</li><li>flink sql 为啥选择 calcite？</li></ul></li><li><p>案例篇-calcite 的能力、案例</p><ul><li>先用用 calcite</li><li>关系代数</li><li>calcite 必知的基础 model</li><li>calcite 的处理流程（以 flink sql 为例）</li><li>calcite 怎么做到这么通用？</li></ul></li><li><p>原理剖析篇-calcite 在 flink sql 中大展身手</p><ul><li>FlinkSqlParserImpl</li><li>FlinkSqlParserImpl 的生成</li></ul></li><li><p>总结与展望篇</p></li></ol><h1 id="2-背景篇-一条-flink-sql-的执行过程"><a href="#2-背景篇-一条-flink-sql-的执行过程" class="headerlink" title="2.背景篇-一条 flink sql 的执行过程"></a>2.背景篇-一条 flink sql 的执行过程</h1><p>本节先给大家大致描述<code>一条 flink sql 的执行过程</code>，不了解详细内容不要紧，主要先了解整个流程，有了全局视角之后，后续会详述细节。</p><p>在介绍一条 flink sql 的执行过程之前，先来看看 flink datastream 任务的执行过程，这对理解一条 flink sql 的执行过程有很大的帮助。</p><ul><li><p><strong>datastream</strong>：datastream 在使用时要在 flink datastream api 提供的各种 udf（比如 flatmap，keyedprocessfunction 等）中自定义处理逻辑，具体的业务执行逻辑都是敲代码、 java 文件写的，然后编译在 jvm 中执行，就和一个普通的 main 函数应用一模一样的流程。因为代码执行逻辑都是自己写的，所以这一部分相对好理解。</p></li><li><p><strong>sql</strong>：java 编译器不能识别和编译一条 sql 进行执行，那么一条 SQL 是咋执行的呢？</p></li></ul><h2 id="2-1-先发挥自己的想象力"><a href="#2-1-先发挥自己的想象力" class="headerlink" title="2.1.先发挥自己的想象力"></a>2.1.先发挥自己的想象力</h2><p>我们逆向思维进行考虑，如果想让一条 flink sql 按照我们的预期在 jvm 中执行，需要哪些过程。</p><ol><li><strong>整体来说</strong>：参考 datastream，如果 jvm 能执行 datastream java code 编译后的 class 文件，那么加一个 sql 解析层，能将 sql 逻辑解析为 datastream 的各种算子，然后编译执行不就 vans 了。</li><li><strong>sql parser</strong>：首先得有一个 sql parser 吧，得先能识别 sql 语法，将 sql 语法转化为 AST、具体的关系代数。</li><li><strong>关系代数到 datastream 算子的映射</strong>：sql 逻辑解析为 datastream，需要有一个解析的映射逻辑吧。sql 是基于关系代数的，可以维护一个 sql 中的每个关系代数到具体 datastream 接口的映射关系，有了这些映射关系我们就可以将 sql 映射成一段可执行的 datastream 代码。举个例子：其可以将：<ul><li>sql select xxx 解析为类似 datastream 中的 map</li><li>where xxx 解析为 filter</li><li>group by 解析成 keyby</li><li>sum（xx），count（xxx）可以解析为 datastream 中的 aggregate function</li><li>etc…</li></ul></li><li><strong>代码生成</strong>：有了 sql AST，sql 到 datasretam 算子的映射关系之后，就要进行具体的代码生成了。比如去解析 sql AST 中具体哪些字段用作 where 逻辑，哪些字段用作 group by，都需要生成对应具体的 datastream 代码。</li><li><strong>运行</strong>：经过上述流程之后，就可以将一个 sql 翻译成一个 datastream 作业了，happy 的执行。</li></ol><p>如下图所示，描绘了上述逻辑：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/12.png" alt="12"></p><p>那么这个和 flink 实际实现有啥异同呢？</p><p>flink 大致是这样做的，虽在 flink 本身的中间还有一些其他的流程，后来的版本也不是基于 datastream，但是整体的处理逻辑还是和上述一致的。</p><p>所以不了解整体流程的同学可以先按照上述流程进行理解。</p><p>按照 <code>博主的脑洞</code> 来总结一条 sql 的使命就是：<code>sql -&gt; AST -&gt; codegen(java code) -&gt; 让我们 run 起来好吗</code></p><h2 id="2-2-看看-flink-的实现"><a href="#2-2-看看-flink-的实现" class="headerlink" title="2.2.看看 flink 的实现"></a>2.2.看看 flink 的实现</h2><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/26.png" alt="26"></p><p>上面手绘可能看不清，下面这张图更清楚。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/28.png" alt="28"></p><p>标准的一条 flink sql 运行起来的流程如下：</p><blockquote><p>Notes：<br>刚开始对其中的 SqlNode，RelNode 概念可能比较模糊。先理解整个流程，后续会详细介绍这些概念。</p></blockquote><ol><li><p><strong>sql 解析阶段</strong>：calcite parser 解析（sql -&gt; AST，AST 即 SqlNode Tree）</p></li><li><p><strong>SqlNode 验证阶段</strong>：calcite validator 校验（SqlNode -&gt; SqlNode，语法、表达式、表信息）</p></li><li><p><strong>语义分析阶段</strong>：SqlNode 转换为 RelNode，RelNode 即 Logical Plan（SqlNode -&gt; RelNode）</p></li><li><p><strong>优化阶段</strong>：calcite optimizer 优化（RelNode -&gt; RelNode，剪枝、谓词下推等）</p></li><li><p><strong>物理计划生成阶段</strong>：Logical Plan 转换为 Physical Plan（等同于 RelNode 转换成 DataSet\DataStream API）</p></li><li><p><strong>后续的运行逻辑与 datastream 一致</strong></p></li></ol><p>可以发现 <code>flink 的实现</code> 比 <code>博主的脑洞</code> 整体主要框架上面是一致的。多出来的部分主要是 <strong>SqlNode 验证阶段</strong>，<strong>优化阶段</strong>。</p><h1 id="3-简介篇-calcite-在-flink-sql-中的角色"><a href="#3-简介篇-calcite-在-flink-sql-中的角色" class="headerlink" title="3.简介篇-calcite 在 flink sql 中的角色"></a>3.简介篇-calcite 在 flink sql 中的角色</h1><p>大致了解了 <code>一条 flink sql 的运行流程</code> 之后，我们来看看 calcite 这玩意到底在 flink 里干了些啥。</p><p>根据上文总结来说 calcite 在 flink sql 中担当了 <code>sql 解析、验证、优化</code>功能。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/30.png" alt="30"></p><p>看着 calcite 干了这么多事，那 calcite 是个啥东东，它的定位是啥？</p><h2 id="3-1-calcite-是啥？"><a href="#3-1-calcite-是啥？" class="headerlink" title="3.1.calcite 是啥？"></a>3.1.calcite 是啥？</h2><p>calcite 是一个动态数据的管理框架，它可以用来构建数据库系统的不同的解析的模块，但是它不包含数据存储数据处理等功能。</p><p>calcite 的目标是一种方案，适应所有的需求场景，希望能为不同计算平台和数据源提供统一的 sql 解析引擎，但是它只是提供查询引擎，而没有真正的去存储这些数据。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/61.png" alt="61"></p><p>下图是目前使用了 calcite 能力的其他组件，也可见官网 <a href="https://calcite.apache.org/docs/powered_by.html">https://calcite.apache.org/docs/powered_by.html</a> 。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/4.png" alt="4"></p><p>简单来说的话，可以先理解为 calcite 具有这几个功能（当然还有其他很牛逼的功能，感兴趣可以自查官网）。</p><ol><li><strong>自定义 sql 解析器</strong>：比如说我们新发明了一个引擎，然后我们要在这个引擎上来创造一套基于 sql 的接口，那么我们就可以使用直接 calcite，不用自己去写一套专门的 sql 的解析器，以及执行以及优化引擎，calcite 人都有。</li><li><strong>sql parser（extends SqlAbstractParserImpl）</strong>：将 sql 的各种关系代数解析为具体的 AST，这些 AST 都能对应到具体的 java model，在 java 的世界里面，对象很重要，有了这些对象（<code>SqlSelect</code>、<code>SqlNode</code>），就可以根据这些对象做具体逻辑处理了。举个例子，如下图，一条简单的 <code>select c,d from source where a = &#39;6&#39;</code> sql，经过 calcite 的解析之后，就可以得到 AST model（SqlNode）。<br>可以看到有 <code>SqlSelect</code>、<code>SqlIdentifier</code>、<code>SqlIdentifier</code>、<code>SqlCharStringLiteral</code>。</li><li><strong>sql validator（extends SqlValidatorImpl）</strong>：根据语法、表达式、表信息进行 SqlNode 正确性校验。</li><li><strong>sql optimizer</strong>：剪枝、谓词下推等优化</li></ol><p>上面的这些能力整体组成如下图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/29.png" alt="29"></p><p>实际使用 calcite 解析一条 sql，跑起来看看。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/2.png" alt="2"></p><h2 id="3-2-flink-sql-为什么选择-calcite？"><a href="#3-2-flink-sql-为什么选择-calcite？" class="headerlink" title="3.2.flink sql 为什么选择 calcite？"></a>3.2.flink sql 为什么选择 calcite？</h2><ol><li>不用重复造轮子。有限的精力应该放在有价值的事情上。</li><li>calcite 有针对 stream 表的解决方案。具体可见 <a href="https://calcite.apache.org/docs/stream.html。">https://calcite.apache.org/docs/stream.html。</a> </li></ol><h1 id="4-案例篇-calcite-的能力、案例"><a href="#4-案例篇-calcite-的能力、案例" class="headerlink" title="4.案例篇-calcite 的能力、案例"></a>4.案例篇-calcite 的能力、案例</h1><h2 id="4-1-先用用-calcite"><a href="#4-1-先用用-calcite" class="headerlink" title="4.1.先用用 calcite"></a>4.1.先用用 calcite</h2><p><code>重中之重，在了解原理之前，先跑起来是王道，也会帮助我们逐步理解。</code></p><p>官网已经有一个 csv 的案例了。感兴趣的可以直达 <a href="https://calcite.apache.org/docs/tutorial.html">https://calcite.apache.org/docs/tutorial.html</a> 。</p><p>跑完一个 csv demo，在详细了解 calcite 之前还需要了解下 sql，calcite 的支柱：关系代数。</p><h2 id="4-2-关系代数"><a href="#4-2-关系代数" class="headerlink" title="4.2.关系代数"></a>4.2.关系代数</h2><p>sql 是基于关系代数的查询语言，是关系代数在工程上的一种很好的实现方案。在工程中，关系代数难表达，但是 sql 就易于理解。关系代数和 sql 的关系如下。</p><ol><li><p>可以将一条 sql 解析为一个关系代数表达式的组合。在 sql 中的操作都可以转化成关系代数的表达式。</p></li><li><p>sql 的执行优化（所有的优化的前提都是优化前和优化后最终执行结果相同，即等价交换）是基于关系代数运算的。</p></li></ol><h3 id="4-2-1-常用关系代数"><a href="#4-2-1-常用关系代数" class="headerlink" title="4.2.1.常用关系代数"></a>4.2.1.常用关系代数</h3><p><strong>总结下，有哪些常用的关系代数：</strong></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/50.png" alt="50"></p><h3 id="4-2-2-sql-优化支柱之关系代数等价变换"><a href="#4-2-2-sql-优化支柱之关系代数等价变换" class="headerlink" title="4.2.2.sql 优化支柱之关系代数等价变换"></a>4.2.2.sql 优化支柱之关系代数等价变换</h3><p>关系代数等价变换是 calcite optimizer 的基础理论。</p><p>下面是一些等价变换的例子。</p><p>1.连接（<code>⋈</code>），笛卡尔积（<code>×</code>）的交换律</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/51.png" alt="51"></p><p>2.连接（<code>⋈</code>），笛卡尔积（<code>×</code>）的结合律<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/52.png" alt="52"></p><p>3.投影（<code>Π</code>）的串接定律<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/53.png" alt="53"></p><p>4.选择（<code>σ</code>）的串接定律<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/54.png" alt="54"></p><p>5.选择（<code>σ</code>）与投影（<code>Π</code>）的交换<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/55.png" alt="55"></p><p>6.选择（<code>σ</code>）与笛卡尔积（<code>×</code>）的交换<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/56.png" alt="56"></p><p>7.选择（<code>σ</code>）与并（<code>∪</code>）的交换<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/57.png" alt="57"></p><p>8.选择（<code>σ</code>）与差（<code>-</code>）的交换<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/58.png" alt="58"></p><p>9.投影（<code>Π</code>）与笛卡尔积（<code>×</code>）的交换<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/59.png" alt="59"></p><p>10.投影（<code>Π</code>）与并（<code>∪</code>）的交换<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/60.png" alt="60"></p><p><strong>然后看一个基于关系代数优化的实际 sql 案例：</strong></p><p>有三个关系 <code>A（a1,a2,a3,…）</code>、<code>B（b1,b2,b3, … ）</code>、<code>C（a1,b1,c1,c2, … ）</code></p><p>有一个查询请求如下：</p><p><code>SELECT A.a1 FROM A，B，C WHERE A.a1 = C.a1 AND B.b1 = C.b1 AND f(c1)</code></p><p>1.首先将 sql 转为关系代数的语法树。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/36.png" alt="36"></p><p>2.优化：选择（<code>σ</code>）的串接定律。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/47.png" alt="47"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/37.png" alt="37"></p><p>3.优化：选择（<code>σ</code>）与笛卡尔积（<code>×</code>）的交换。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/48.png" alt="48"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/38.png" alt="38"></p><p>4.优化：投影（<code>π</code>）与笛卡尔积（<code>×</code>）的交换。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/49.png" alt="49"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/39.png" alt="39"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/40.png" alt="40"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/41.png" alt="41"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/42.png" alt="42"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/43.png" alt="43"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/44.png" alt="44"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/45.png" alt="45"></p><p>关于关系代数我们就有了大致的了解。</p><p>除此之外，对于更深入了解 flink sql，calcite 而言，我们还需要了解一下在 calcite 代码体系中有哪些重要 model。</p><h2 id="4-3-calcite-必知的基础-model"><a href="#4-3-calcite-必知的基础-model" class="headerlink" title="4.3.calcite 必知的基础 model"></a>4.3.calcite 必知的基础 model</h2><p>calcite 中有两个最最基础、重要的 model 在我们理解 flink sql 解析流程时需要知道的。</p><ol><li><strong>SqlNode</strong>：sql 转化而成，可以理解为直观表达 sql 层次结构的的 model</li><li><strong>RelNode</strong>：SqlNode 转化而成，可以理解为将 SqlNode 转化为关系代数，表达关系代数层次结构的 model</li></ol><p>举个例子来说明下，下面这条 flink sql，经过解析之后的 <strong>SqlNode</strong>，<strong>RelNode</strong> 如下图：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="built_in">sum</span>(part_pv) <span class="keyword">as</span> pv,</span><br><span class="line">  window_start</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">as</span> part_pv,</span><br><span class="line">  <span class="built_in">cast</span>(tumble_start(rowtime, <span class="type">INTERVAL</span> <span class="string">&#x27;60&#x27;</span> <span class="keyword">SECOND</span>) <span class="keyword">as</span> <span class="type">bigint</span>) <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">as</span> window_start</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  source_db.source_table</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  tumble(rowtime, <span class="type">INTERVAL</span> <span class="string">&#x27;60&#x27;</span> <span class="keyword">SECOND</span>)</span><br><span class="line">  , <span class="built_in">mod</span>(id, <span class="number">1024</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  window_start</span><br></pre></td></tr></table></figure><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/62.png" alt="62"></p><p>可以看到 SqlNode 包含的内容是 sql 的层次结构，包括 <code>selectList</code>，<code>from</code>，<code>where</code>，<code>group by</code> 等。</p><p>RelNode 包含的是关系代数的层次结构，每一层都有一个 input 来承接。结合上面优化案例的树状结构一样。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/63.png" alt="63"></p><h2 id="4-4-calcite-的处理流程（以-flink-sql-为例）"><a href="#4-4-calcite-的处理流程（以-flink-sql-为例）" class="headerlink" title="4.4.calcite 的处理流程（以 flink sql 为例）"></a>4.4.calcite 的处理流程（以 flink sql 为例）</h2><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/29.png" alt="29"></p><p>如上图所示，此处我们结合上节介绍的 calcite 的 model，以及 flink sql 的实现来走一遍其处理流程：</p><ol><li><strong>sql 解析阶段（sql –&gt; SqlNode）</strong></li><li><strong>SqlNode 验证（SqlNode –&gt; SqlNode）</strong></li><li><strong>语义分析（SqlNode –&gt; RelNode）</strong></li><li><strong>优化阶段（RelNode –&gt; RelNode）</strong></li></ol><h3 id="4-4-1-flink-sql-demo"><a href="#4-4-1-flink-sql-demo" class="headerlink" title="4.4.1.flink sql demo"></a>4.4.1.flink sql demo</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="built_in">sum</span>(part_pv) <span class="keyword">as</span> pv,</span><br><span class="line">  window_start</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">as</span> part_pv,</span><br><span class="line">  <span class="built_in">cast</span>(tumble_start(rowtime, <span class="type">INTERVAL</span> <span class="string">&#x27;60&#x27;</span> <span class="keyword">SECOND</span>) <span class="keyword">as</span> <span class="type">bigint</span>) <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">as</span> window_start</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  source_db.source_table</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  tumble(rowtime, <span class="type">INTERVAL</span> <span class="string">&#x27;60&#x27;</span> <span class="keyword">SECOND</span>)</span><br><span class="line">  , <span class="built_in">mod</span>(id, <span class="number">1024</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  window_start</span><br></pre></td></tr></table></figure><p>其中前三步解析和转化，都在 在执行 <code>TableEnvironment#sqlQuery</code> 进行。</p><p>最后一步优化，在执行 sink 操作时进行，即在这个例子中是 <code>tEnv.toRetractStream(result, Row.class)</code>。</p><p>源码公众号后台回复<strong>flink sql 知其所以然（六）| flink sql 约会 calcite</strong>获取。</p><h3 id="4-4-2-sql-解析阶段（sql-–-gt-SqlNode）"><a href="#4-4-2-sql-解析阶段（sql-–-gt-SqlNode）" class="headerlink" title="4.4.2.sql 解析阶段（sql –&gt; SqlNode）"></a>4.4.2.sql 解析阶段（sql –&gt; SqlNode）</h3><p>sql 解析阶段使用 Sql Parser 将 sql 解析为 <code>SqlNode</code>。这一步在执行 <code>TableEnvironment#sqlQuery</code> 进行。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/64.png" alt="64"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/65.png" alt="65"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/66.png" alt="66"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/67.png" alt="67"></p><p>可以从上图看到 flink sql 具体实现类是 <code>FlinkSqlParserImpl</code>。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/68.png" alt="68"></p><p>具体 parse 得到 SqlNode 如上图。</p><h3 id="4-4-3-SqlNode-验证（SqlNode-–-gt-SqlNode）"><a href="#4-4-3-SqlNode-验证（SqlNode-–-gt-SqlNode）" class="headerlink" title="4.4.3.SqlNode 验证（SqlNode –&gt; SqlNode）"></a>4.4.3.SqlNode 验证（SqlNode –&gt; SqlNode）</h3><p>上面的第一步生产的 SqlNode 对象是一个未经验证的，这一步就是语法检查阶段，语法检查前需要知道元数据信息，这个检查会包括表名、字段名、函数名、数据类型的检查。进行语法检查的实现如下：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/69.png" alt="69"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/70.png" alt="70"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/76.png" alt="76"></p><p>可以从上图看到 flink sql 校验器的具体实现类是 <code>FlinkCalciteSqlValidator</code>，其中包含了元数据信息，从而可以进行元数据信息检查。</p><h3 id="4-4-4-语义分析（SqlNode-–-gt-RelNode）"><a href="#4-4-4-语义分析（SqlNode-–-gt-RelNode）" class="headerlink" title="4.4.4.语义分析（SqlNode –&gt; RelNode）"></a>4.4.4.语义分析（SqlNode –&gt; RelNode）</h3><p>这一步就是将 SqlNode 转换成 RelNode，也就是生成相应的关系代数层面的逻辑（这里一般都叫做逻辑计划：Logical Plan）。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/72.png" alt="72"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/73.png" alt="73"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/75.png" alt="75"></p><h3 id="4-4-5-优化阶段（RelNode-–-gt-RelNode）"><a href="#4-4-5-优化阶段（RelNode-–-gt-RelNode）" class="headerlink" title="4.4.5.优化阶段（RelNode –&gt; RelNode）"></a>4.4.5.优化阶段（RelNode –&gt; RelNode）</h3><p>这一步就是优化阶段。详细内容可以自己 debug 代码查看，此处不赘述。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/77.png" alt="77"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/78.png" alt="78"></p><h2 id="4-5-calcite-怎么做到这么通用？"><a href="#4-5-calcite-怎么做到这么通用？" class="headerlink" title="4.5.calcite 怎么做到这么通用？"></a>4.5.calcite 怎么做到这么通用？</h2><p>此处以 calcite parser 举例说明，其模块为什么这通用？其他的模块都是类似的方式。</p><p><code>先说结论</code>：因为 calcite parser 模块提供了接口，具体的 parse 逻辑、规则是可以根据用户自定义进行配置的。大家可以看下图，博主画出了一张图进行详述。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/5.png" alt="5"></p><p>如上图，引擎 sql 解析器的生成是有一个输入的，就是 <code>用户自定义语法分析规则变量</code>，具体引擎的 sql 解析器其实也是根据用户自定义的 <code>解析规则</code> 去生成的 <code>解析器</code>。其 <code>解析器</code> 的动态生成依赖 <code>javacc</code> 这样的组件。calcite 提供的是统一的 sql AST 模型、优化模型接口等，而具体的解析实现交给了用户自己去决定。</p><p><code>javacc</code> 会根据 calcite 中定义的 <code>Parser.jj</code> 文件，生成具体的 sql parser 代码（如上图），这个 sql parser 的能力就是将 sql 转换成 AST （SqlNode）。关于 calcite 能力的更详细内容见 <a href="https://matt33.com/2019/03/07/apache-calcite-process-flow/">https://matt33.com/2019/03/07/apache-calcite-process-flow/</a> 。</p><p>上图涉及到的文件大家可以下载 <code>calcite 源码</code> <a href="https://github.com/apache/calcite.git">https://github.com/apache/calcite.git</a> 之后，切换到 <code>core</code> module 之后查看。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/31.png" alt="31"></p><h3 id="4-5-1-javacc-是啥？"><a href="#4-5-1-javacc-是啥？" class="headerlink" title="4.5.1.javacc 是啥？"></a>4.5.1.javacc 是啥？</h3><p><code>javacc</code> 是一个用 java 开发的最受欢迎的语法分析生成器。这个分析生成器工具可以读取上下文无关且有着特殊意义的语法并把它转换成可以识别且匹配该语法的 java 程序。它是 100% 的纯 java 代码，可以在多种平台上运行。</p><p>简单解释 <code>javacc</code> 就是它是一个通用的语法分析生产器，用户可以使用 <code>javacc</code> 任意定义一套 DSL 及解析器。</p><p>举个例子，如果哪天你觉得 sql 也不够简洁通用，你可以使用 <code>javacc</code> 自己定义一套更简洁的 <code>user-define-ql</code>。然后使用 javacc 作为你的 <code>user-define-ql</code> 的解析器。是不是很流批，可以自己去搞编译器了。</p><h3 id="4-5-2-跑跑-javacc"><a href="#4-5-2-跑跑-javacc" class="headerlink" title="4.5.2.跑跑 javacc"></a>4.5.2.跑跑 javacc</h3><p>这里不介绍具体的 javacc 语法，直接以官网的 <code>Simple1.jj</code> 为案例。详细语法和功能可以参考官网（<a href="https://javacc.github.io/javacc/）">https://javacc.github.io/javacc/）</a> 或者一下博客。</p><ol><li><a href="https://www.cnblogs.com/Gavin_Liu/archive/2009/03/07/1405029.html">https://www.cnblogs.com/Gavin_Liu/archive/2009/03/07/1405029.html</a></li><li><a href="https://www.yangguo.info/2014/12/13/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86-Javacc%E4%BD%BF%E7%94%A8/">https://www.yangguo.info/2014/12/13/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86-Javacc%E4%BD%BF%E7%94%A8/</a></li><li><a href="https://www.engr.mun.ca/~theo/JavaCC-Tutorial/javacc-tutorial.pdf">https://www.engr.mun.ca/~theo/JavaCC-Tutorial/javacc-tutorial.pdf</a></li></ol><p><code>Simple1.jj</code> 是用于识别一系列的 <code>&#123;相同数量的花括号&#125;</code>，之后跟着 0 个或多个行终结符。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/7.png" alt="7"></p><p>下面是合法的字符串例子：</p><p><code>&#123;&#125;</code>，<code>&#123;&#123;&#123;&#123;&#123;&#125;&#125;&#125;&#125;&#125;</code>，etc.</p><p>下面是不合法的字符串例子：</p><p><code>&#123;&#123;&#123;&#123;`，`&#123;&#125;&#123;&#125;`，`&#123;&#125;&#125;</code>，<code>&#123;&#123;&#125;&#123;&#125;&#125;</code>，etc.</p><p>接下来让我们实际将 <code>Simple1.jj</code> 编译生成具体的规则代码。</p><p>在 pom 中加入 javacc build 插件：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- This must be run AFTER the fmpp-maven-plugin --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.codehaus.mojo<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>javacc-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">phase</span>&gt;</span>generate-sources<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>javacc<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">goal</span>&gt;</span>javacc<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">sourceDirectory</span>&gt;</span>$&#123;project.build.directory&#125;/generated-sources/<span class="tag">&lt;/<span class="name">sourceDirectory</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">includes</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">include</span>&gt;</span>**/Simple1.jj<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">includes</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!-- This must be kept synced with Apache Calcite. --&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">lookAhead</span>&gt;</span>1<span class="tag">&lt;/<span class="name">lookAhead</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">isStatic</span>&gt;</span>false<span class="tag">&lt;/<span class="name">isStatic</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">outputDirectory</span>&gt;</span>$&#123;project.build.directory&#125;/generated-sources/<span class="tag">&lt;/<span class="name">outputDirectory</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在 compile 之后，就会在 <code>generated-sources</code> 下生成代码：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/8.png" alt="8"></p><p>然后把代码 copy 到 Sources 路径下：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/33.png" alt="33"></p><p>执行下代码，可以看到 <code>&#123;&#125;</code>，<code>&#123;&#123;&#125;&#125;</code> 都可以校验通过，一旦出现不符合规则的 <code>&#123;&#123;</code> 输入，就会抛出异常。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/9.png" alt="9"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/10.png" alt="10"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/11.png" alt="11"></p><p>关于 javacc 基本上就了解个大概了。</p><p>感兴趣的可以尝试自定义一个编译器。</p><h3 id="4-5-3-fmpp-是啥？"><a href="#4-5-3-fmpp-是啥？" class="headerlink" title="4.5.3.fmpp 是啥？"></a>4.5.3.fmpp 是啥？</h3><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/5.png" alt="5"></p><p>fmpp 就是一个基于 freemarker 的模板生产器。用户可以统一管理自己的变量，然后用 <code>ftl 模板 + 变量</code> 生成对应的最终文件。在 calcite 中使用 fmpp 作为变量 + 模板的统一管理器。然后基于 fmpp 来生成对应的 <code>Parser.jj</code> 文件。</p><h1 id="5-原理剖析篇-calcite-在-flink-sql-中大展身手"><a href="#5-原理剖析篇-calcite-在-flink-sql-中大展身手" class="headerlink" title="5.原理剖析篇-calcite 在 flink sql 中大展身手"></a>5.原理剖析篇-calcite 在 flink sql 中大展身手</h1><p>博主画了一张图，包含了其中重要组件之间的依赖关系。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/3.png" alt="3"></p><p>你没猜错，还是上面那些流程，<code>fmpp（Parser.jj 模板生成）</code> -&gt; <code>javacc（Parser 生成）</code> -&gt; <code>calcite</code>。</p><p>在介绍 Parser 生成流程之前，先看看 flink 最终生成的 Parser：<code>FlinkSqlParserImpl</code> （此处使用 Blink Planner）。</p><h2 id="5-1-FlinkSqlParserImpl"><a href="#5-1-FlinkSqlParserImpl" class="headerlink" title="5.1.FlinkSqlParserImpl"></a>5.1.FlinkSqlParserImpl</h2><p>以下面这个案例出发（代码基于 flink 1.13.1 版本）：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ParserTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        env.setParallelism(<span class="number">10</span>);</span><br><span class="line"></span><br><span class="line">        EnvironmentSettings settings = EnvironmentSettings</span><br><span class="line">                .newInstance()</span><br><span class="line">                .useBlinkPlanner()</span><br><span class="line">                .inStreamingMode()</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, settings);</span><br><span class="line"></span><br><span class="line">        DataStream&lt;Tuple3&lt;String, Long, Long&gt;&gt; tuple3DataStream =</span><br><span class="line">                env.fromCollection(Arrays.asList(</span><br><span class="line">                        Tuple3.of(<span class="string">&quot;2&quot;</span>, <span class="number">1L</span>, <span class="number">1627254000000L</span>),</span><br><span class="line">                        Tuple3.of(<span class="string">&quot;2&quot;</span>, <span class="number">1L</span>, <span class="number">1627218000000L</span> + <span class="number">5000L</span>),</span><br><span class="line">                        Tuple3.of(<span class="string">&quot;2&quot;</span>, <span class="number">101L</span>, <span class="number">1627218000000L</span> + <span class="number">6000L</span>),</span><br><span class="line">                        Tuple3.of(<span class="string">&quot;2&quot;</span>, <span class="number">201L</span>, <span class="number">1627218000000L</span> + <span class="number">7000L</span>),</span><br><span class="line">                        Tuple3.of(<span class="string">&quot;2&quot;</span>, <span class="number">301L</span>, <span class="number">1627218000000L</span> + <span class="number">7000L</span>),</span><br><span class="line">                        Tuple3.of(<span class="string">&quot;2&quot;</span>, <span class="number">301L</span>, <span class="number">1627218000000L</span> + <span class="number">7000L</span>),</span><br><span class="line">                        Tuple3.of(<span class="string">&quot;2&quot;</span>, <span class="number">301L</span>, <span class="number">1627218000000L</span> + <span class="number">7000L</span>),</span><br><span class="line">                        Tuple3.of(<span class="string">&quot;2&quot;</span>, <span class="number">301L</span>, <span class="number">1627218000000L</span> + <span class="number">7000L</span>),</span><br><span class="line">                        Tuple3.of(<span class="string">&quot;2&quot;</span>, <span class="number">301L</span>, <span class="number">1627218000000L</span> + <span class="number">7000L</span>),</span><br><span class="line">                        Tuple3.of(<span class="string">&quot;2&quot;</span>, <span class="number">301L</span>, <span class="number">1627218000000L</span> + <span class="number">86400000</span> + <span class="number">7000L</span>)))</span><br><span class="line">                        .assignTimestampsAndWatermarks(</span><br><span class="line">                                <span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor&lt;Tuple3&lt;String, Long, Long&gt;&gt;(Time.seconds(<span class="number">0L</span>)) &#123;</span><br><span class="line">                                    <span class="meta">@Override</span></span><br><span class="line">                                    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Tuple3&lt;String, Long, Long&gt; element)</span> </span>&#123;</span><br><span class="line">                                        <span class="keyword">return</span> element.f2;</span><br><span class="line">                                    &#125;</span><br><span class="line">                                &#125;);</span><br><span class="line"></span><br><span class="line">        tEnv.registerFunction(<span class="string">&quot;mod&quot;</span>, <span class="keyword">new</span> Mod_UDF());</span><br><span class="line"></span><br><span class="line">        tEnv.registerFunction(<span class="string">&quot;status_mapper&quot;</span>, <span class="keyword">new</span> StatusMapper_UDF());</span><br><span class="line"></span><br><span class="line">        tEnv.createTemporaryView(<span class="string">&quot;source_db.source_table&quot;</span>, tuple3DataStream,</span><br><span class="line">                <span class="string">&quot;status, id, timestamp, rowtime.rowtime&quot;</span>);</span><br><span class="line"></span><br><span class="line">        String sql = <span class="string">&quot;SELECT\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  count(1),\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  cast(tumble_start(rowtime, INTERVAL &#x27;1&#x27; DAY) as string)\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;FROM\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  source_db.source_table\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;GROUP BY\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  tumble(rowtime, INTERVAL &#x27;1&#x27; DAY)&quot;</span>;</span><br><span class="line"></span><br><span class="line">        Table result = tEnv.sqlQuery(sql);</span><br><span class="line"></span><br><span class="line">        tEnv.toAppendStream(result, Row.class).print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>debug 过程如之前分析 sql -&gt; SqlNode 过程所示，如下图直接定位到 SqlParser：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/21.png" alt="21"></p><p>如上图可以看到具体的 Parser 就是 <code>FlinkSqlParserImpl</code>。</p><p>定位到具体的代码如下图所示（<code>flink-table-palnner-blink-2.11-1.13.1.jar</code>）。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/34.png" alt="34"></p><p>最终 parse 的结果 SqlNode 如下图。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/22.png" alt="22"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/24.png" alt="24"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/25.png" alt="25"></p><p>再来看看 <code>FlinkSqlParserImpl</code> 是怎么使用 calcite 生成的。</p><p>具体到 flink 中的实现，位于源码中的 <code>flink-table</code>.<code>flink-sql-parser</code> 模块（源码基于 flink 1.13.1）。</p><p>flink 是依赖 maven 插件实现的上面的整体流程。</p><h2 id="5-2-FlinkSqlParserImpl-的生成"><a href="#5-2-FlinkSqlParserImpl-的生成" class="headerlink" title="5.2.FlinkSqlParserImpl 的生成"></a>5.2.FlinkSqlParserImpl 的生成</h2><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/14.png" alt="14"></p><p>接下来看看整个 Parser 生成流程。</p><h3 id="5-2-1-flink-引入-calcite"><a href="#5-2-1-flink-引入-calcite" class="headerlink" title="5.2.1.flink 引入 calcite"></a>5.2.1.flink 引入 calcite</h3><p>使用 <code>maven-dependency-plugin</code> 将 calcite 解压到 flink 项目 build 目录下。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Extract parser grammar template from calcite-core.jar and put</span></span><br><span class="line"><span class="comment">         it under $&#123;project.build.directory&#125; where all freemarker templates are. --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-dependency-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>unpack-parser-template<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">phase</span>&gt;</span>initialize<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">goal</span>&gt;</span>unpack<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactItems</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">artifactItem</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.calcite<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>calcite-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">type</span>&gt;</span>jar<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">overWrite</span>&gt;</span>true<span class="tag">&lt;/<span class="name">overWrite</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">outputDirectory</span>&gt;</span>$&#123;project.build.directory&#125;/<span class="tag">&lt;/<span class="name">outputDirectory</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">includes</span>&gt;</span>**/Parser.jj<span class="tag">&lt;/<span class="name">includes</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">artifactItem</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">artifactItems</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br></pre></td></tr></table></figure><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/15.png" alt="15"></p><h3 id="5-2-2-fmpp-生成-Parser-jj"><a href="#5-2-2-fmpp-生成-Parser-jj" class="headerlink" title="5.2.2.fmpp 生成 Parser.jj"></a>5.2.2.fmpp 生成 Parser.jj</h3><p>使用 <code>maven-resources-plugin</code> 将 <code>Parser.jj</code> 代码生成。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-resources-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>copy-fmpp-resources<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">phase</span>&gt;</span>initialize<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">goal</span>&gt;</span>copy-resources<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">outputDirectory</span>&gt;</span>$&#123;project.build.directory&#125;/codegen<span class="tag">&lt;/<span class="name">outputDirectory</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">resources</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">resource</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">directory</span>&gt;</span>src/main/codegen<span class="tag">&lt;/<span class="name">directory</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">filtering</span>&gt;</span>false<span class="tag">&lt;/<span class="name">filtering</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">resource</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">resources</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.googlecode.fmpp-maven-plugin<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fmpp-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.freemarker<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>freemarker<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.28<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>generate-fmpp-sources<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">phase</span>&gt;</span>generate-sources<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">goal</span>&gt;</span>generate<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">cfgFile</span>&gt;</span>$&#123;project.build.directory&#125;/codegen/config.fmpp<span class="tag">&lt;/<span class="name">cfgFile</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">outputDirectory</span>&gt;</span>target/generated-sources<span class="tag">&lt;/<span class="name">outputDirectory</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">templateDirectory</span>&gt;</span>$&#123;project.build.directory&#125;/codegen/templates<span class="tag">&lt;/<span class="name">templateDirectory</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br></pre></td></tr></table></figure><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/16.png" alt="16"></p><h3 id="5-2-3-javacc-生成-parser"><a href="#5-2-3-javacc-生成-parser" class="headerlink" title="5.2.3.javacc 生成 parser"></a>5.2.3.javacc 生成 parser</h3><p>使用 javacc 将根据 <code>Parser.jj</code> 文件生成 Parser。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- This must be run AFTER the fmpp-maven-plugin --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.codehaus.mojo<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>javacc-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">phase</span>&gt;</span>generate-sources<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>javacc<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">goal</span>&gt;</span>javacc<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">sourceDirectory</span>&gt;</span>$&#123;project.build.directory&#125;/generated-sources/<span class="tag">&lt;/<span class="name">sourceDirectory</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">includes</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">include</span>&gt;</span>**/Parser.jj<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">includes</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!-- This must be kept synced with Apache Calcite. --&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">lookAhead</span>&gt;</span>1<span class="tag">&lt;/<span class="name">lookAhead</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">isStatic</span>&gt;</span>false<span class="tag">&lt;/<span class="name">isStatic</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">outputDirectory</span>&gt;</span>$&#123;project.build.directory&#125;/generated-sources/<span class="tag">&lt;/<span class="name">outputDirectory</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br></pre></td></tr></table></figure><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/17.png" alt="17"></p><h3 id="5-2-4-看看-Parser"><a href="#5-2-4-看看-Parser" class="headerlink" title="5.2.4.看看 Parser"></a>5.2.4.看看 Parser</h3><p>最终生成的 <code>Parser</code> 就是 <code>FlinkSqlParserImpl</code>。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/18.png" alt="18"></p><h3 id="5-2-5-blink-planner-引入-flink-sql-parser"><a href="#5-2-5-blink-planner-引入-flink-sql-parser" class="headerlink" title="5.2.5.blink planner 引入 flink-sql-parser"></a>5.2.5.blink planner 引入 flink-sql-parser</h3><p>blink planner（<code>flink-table-planner-blink</code>） 在打包时将 <code>flink-sql-parser</code>、<code>flink-sql-parser-hive</code> 打包进去。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/07_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%B5%81%E6%89%B9%E5%93%84%E5%93%84%E7%9A%84sql%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E5%99%A8/35.png" alt="35"></p><h1 id="6-总结与展望篇"><a href="#6-总结与展望篇" class="headerlink" title="6.总结与展望篇"></a>6.总结与展望篇</h1><p>本文主要介绍了 flink sql 与 calcite 之间的依赖关系，以及 flink sql parser 的生成过程。如果觉得对你理解 flink sql 解析有帮助，帮忙点个小爱心（关注 + 点赞 + 再看）三连吧。</p><h1 id="7-参考文献"><a href="#7-参考文献" class="headerlink" title="7.参考文献"></a>7.参考文献</h1><p><a href="https://www.slideshare.net/JordanHalterman/introduction-to-apache-calcite">https://www.slideshare.net/JordanHalterman/introduction-to-apache-calcite</a></p><p><a href="https://arxiv.org/pdf/1802.10233.pdf">https://arxiv.org/pdf/1802.10233.pdf</a></p><p><a href="https://changbo.tech/blog/7dec2e4.html">https://changbo.tech/blog/7dec2e4.html</a></p><p><a href="http://www.liaojiayi.com/calcite/">http://www.liaojiayi.com/calcite/</a></p><p><a href="https://www.zhihu.com/column/c_1110245426124554240">https://www.zhihu.com/column/c_1110245426124554240</a></p><p><a href="https://blog.csdn.net/QuinnNorris/article/details/70739094">https://blog.csdn.net/QuinnNorris/article/details/70739094</a></p><p><a href="https://www.pianshen.com/article/72171186489/">https://www.pianshen.com/article/72171186489/</a></p><p><a href="https://matt33.com/2019/03/07/apache-calcite-process-flow/">https://matt33.com/2019/03/07/apache-calcite-process-flow/</a></p><p><a href="https://www.jianshu.com/p/edf503a2a1e7">https://www.jianshu.com/p/edf503a2a1e7</a></p><p><a href="https://blog.csdn.net/u013007900/article/details/78978271">https://blog.csdn.net/u013007900/article/details/78978271</a></p><p><a href="https://blog.csdn.net/u013007900/article/details/78993101">https://blog.csdn.net/u013007900/article/details/78993101</a></p><p><a href="http://www.ptbird.cn/optimization-of-relational-algebraic-expression.html">http://www.ptbird.cn/optimization-of-relational-algebraic-expression.html</a></p><p><a href="https://book.51cto.com/art/201306/400084.htm">https://book.51cto.com/art/201306/400084.htm</a></p><p><a href="https://book.51cto.com/art/201306/400085.htm">https://book.51cto.com/art/201306/400085.htm</a></p><p><a href="https://miaowenting.site/2019/11/10/Flink-SQL-with-Calcite/">https://miaowenting.site/2019/11/10/Flink-SQL-with-Calcite/</a></p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink sql 知其所以然（五）| 自定义 protobuf format</title>
    <link href="https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/06_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89protobuf_format/"/>
    <id>https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/06_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89protobuf_format/</id>
    <published>2021-11-13T06:21:57.000Z</published>
    <updated>2021-08-25T12:48:49.341Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>感谢您的<strong>关注  +  点赞 + 再看</strong>，对博主的肯定，会督促博主持续的输出更多的优质实战内容！！！</p></blockquote><h1 id="1-序篇-本文结构"><a href="#1-序篇-本文结构" class="headerlink" title="1.序篇-本文结构"></a>1.序篇-本文结构</h1><p><code>protobuf</code> 作为目前各大公司中最广泛使用的高效的协议数据交换格式工具库，会大量作为流式数据传输的序列化方式，所以在 flink sql 中如果能实现 <code>protobuf</code> 的 <code>format</code> 会非常有用（<strong>目前社区已经有对应的实现，不过目前还没有 merge，预计在 1.14 系列版本中能 release</strong>）。</p><p><code>issue</code> 见：<a href="https://issues.apache.org/jira/browse/FLINK-18202?filter=-4&amp;jql=project%20%3D%20FLINK%20AND%20issuetype%20%3D%20%22New%20Feature%22%20AND%20text%20~%20protobuf%20order%20by%20created%20DESC">https://issues.apache.org/jira/browse/FLINK-18202?filter=-4&amp;jql=project%20%3D%20FLINK%20AND%20issuetype%20%3D%20%22New%20Feature%22%20AND%20text%20~%20protobuf%20order%20by%20created%20DESC</a></p><p><code>pr</code> 见：<a href="https://github.com/apache/flink/pull/14376">https://github.com/apache/flink/pull/14376</a></p><p>这一节主要介绍 flink sql 中怎么自定义实现 <code>format</code>，其中以最常使用的 <code>protobuf</code> 作为案例来介绍。</p><ol><li>背景篇-为啥需要 protobuf format</li><li>目标篇-protobuf format 预期效果</li><li>难点剖析篇-此框架建设的难点、目前有哪些实现</li><li>维表实现篇-实现的过程</li><li>总结与展望篇</li></ol><p>如果想在本地直接测试下：</p><ol><li>在公众号后台回复<ul><li><strong>flink sql 知其所以然（五）| sql 自定义 protobuf format</strong>获取源码（源码基于 1.13.1 实现）</li><li><strong>flink sql 知其所以然（五）| sql 自定义 protobuf format</strong>获取源码（源码基于 1.13.1 实现）</li><li><strong>flink sql 知其所以然（五）| sql 自定义 protobuf format</strong>获取源码（源码基于 1.13.1 实现）</li></ul></li><li>执行源码包中的 <code>flink.examples.sql._05.format.formats.SocketWriteTest</code> 测试类来制造 protobuf 数据</li><li>然后执行源码包中的 <code>flink.examples.sql._05.format.formats.ProtobufFormatTest</code> 测试类来消费 protobuf 数据，并且打印在 console 中，然后就可以在 console 中看到结果。</li></ol><h1 id="2-背景篇-为啥需要-protobuf-format"><a href="#2-背景篇-为啥需要-protobuf-format" class="headerlink" title="2.背景篇-为啥需要 protobuf format"></a>2.背景篇-为啥需要 protobuf format</h1><p>关于为什么选择 <code>protobuf</code> 可以看这篇文章，写的很详细：</p><p><a href="http://hengyunabc.github.io/thinking-about-grpc-protobuf/?utm_source=tuicool&amp;utm_medium=referral">http://hengyunabc.github.io/thinking-about-grpc-protobuf/?utm_source=tuicool&amp;utm_medium=referral</a></p><p>在实时计算的领域中，为了可读性会选择 <code>json</code>，为了效率以及一些已经依赖了 <code>grpc</code> 的公司会选择 <code>protobuf</code> 来做数据序列化，那么自然而然，日志的序列化方式也会选择 <code>protobuf</code>。</p><p>而官方目前已经 release 的版本中是没有提供 flink sql api 的 <code>protobuf format</code> 的。如下图，基于 1.13 版本。</p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/connectors/table/overview/">https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/connectors/table/overview/</a></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/06_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89protobuf_format/1.png" alt="1"></p><p>因此本文在介绍怎样自定义一个 format 的同时，实现一个 protobuf format 来给大家使用。</p><h1 id="3-目标篇-protobuf-format-预期效果"><a href="#3-目标篇-protobuf-format-预期效果" class="headerlink" title="3.目标篇-protobuf format 预期效果"></a>3.目标篇-protobuf format 预期效果</h1><p>预期效果是先实现几种最基本的数据类型，包括 protobuf 中的 <code>message</code>（自定义 model）、<code>map</code>（映射）、<code>repeated</code>（列表）、其他基本数据类型等，这些都是我们最常使用的类型。</p><p>预期 protobuf message 定义如下：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/06_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89protobuf_format/2.png" alt="2"></p><p>测试数据源数据如下，博主把 protobuf 的数据转换为 json，以方便展示，如下图：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/06_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89protobuf_format/3.png" alt="3"></p><p>预期 flink sql：</p><p>数据源表 DDL：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> protobuf_source (</span><br><span class="line">    name STRING</span><br><span class="line">  , names <span class="keyword">ARRAY</span><span class="operator">&lt;</span>STRING<span class="operator">&gt;</span></span><br><span class="line">  , si_map MAP<span class="operator">&lt;</span>STRING, <span class="type">INT</span><span class="operator">&gt;</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;socket&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;hostname&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;port&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;9999&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;protobuf&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;protobuf.class-name&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;flink.examples.sql._04.format.formats.protobuf.Test&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>数据汇表 DDL：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> print_sink (</span><br><span class="line">  name STRING</span><br><span class="line">  , names <span class="keyword">ARRAY</span><span class="operator">&lt;</span>STRING<span class="operator">&gt;</span></span><br><span class="line">  , si_map MAP<span class="operator">&lt;</span>STRING, <span class="type">INT</span><span class="operator">&gt;</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>Transform 执行逻辑：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> print_sink</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> protobuf_source</span><br></pre></td></tr></table></figure><p>下面是我在本地跑的结果：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/06_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89protobuf_format/9.png" alt="9"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/06_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89protobuf_format/10.png" alt="10"></p><p>可以看到打印的结果，数据是正确的被反序列化读入，并且最终输出到 console。</p><h1 id="4-难点剖析篇-目前有哪些实现"><a href="#4-难点剖析篇-目前有哪些实现" class="headerlink" title="4.难点剖析篇-目前有哪些实现"></a>4.难点剖析篇-目前有哪些实现</h1><p>目前业界可以参考的实现如下：<a href="https://github.com/maosuhan/flink-pb，">https://github.com/maosuhan/flink-pb，</a> 也就是这位哥们负责目前 flink protobuf 的 format。</p><p>这种实现的具体使用方式如下：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/06_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89protobuf_format/7.png" alt="7"></p><p>其实现有几个特点：</p><ol><li><strong>复杂性</strong>：用户需要在 flink sql 程序运行时，将对应的 protobuf java 文件引入 classpath，这个特点是复合 flink 这样的通用框架的特点的。但是如果需要在各个公司场景要做一个流式处理平台的场景下，各个 protobuf sdk 可能都位于不同的 jar 包中，那么其 jar 包管理可能是一个比较大的问题。</li><li><strong>高效 serde</strong>：一般很多场景下为了通用化 serde protobuf message，可能会选择 DynamicMessage 来处理 protobuf message，但是其 serde 性能相比原生 java code 的性能比较差。因为特点 1 引入了 protobuf 的 java class，所以其 serde function 可以基于 codegen 实现，而这将极大提高 serde 效率，效率提高就代表着省钱啊，可以吹逼的。</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/06_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89protobuf_format/8.png" alt="8"></p><blockquote><p>Notes：</p><p>当然博主针对第一点也有一些想法，比如怎样做到不依赖 protobuf java 文件，只依赖 protobuf 的 message 定义即可或者只依赖其 descriptor。<br>目前博主的想法如下：</p><ol><li>flink 程序在客户端获取到对应的 protobuf message 定义</li><li>然后根据这个定义恢复出 proto 文件</li><li>客户端本地执行 protoc 将此文件编译为 java 文件</li><li>客户端本地动态将此 java 文件编译并 load 到 jvm 中</li><li>使用 codegen 然后动态生成执行代码</li></ol><p>一气呵成！！！</p><p>具体实现其实可以参考：<a href="https://stackoverflow.com/questions/28381659/how-to-compile-protocol-buffers-schema-at-runtime">https://stackoverflow.com/questions/28381659/how-to-compile-protocol-buffers-schema-at-runtime</a></p></blockquote><h1 id="5-实现篇-实现的过程"><a href="#5-实现篇-实现的过程" class="headerlink" title="5.实现篇-实现的过程"></a>5.实现篇-实现的过程</h1><h2 id="5-1-flink-format-工作原理"><a href="#5-1-flink-format-工作原理" class="headerlink" title="5.1.flink format 工作原理"></a>5.1.flink format 工作原理</h2><p>其实上节已经详细描述了 flink sql 对于 source\sink\format 的加载机制。</p><ol><li>通过 SPI 机制加载所有的 source\sink\format 工厂 <code>Factory</code></li><li>过滤出 DeserializationFormatFactory\SerializationFormatFactory + format 标识的 format 工厂类</li><li>通过 format 工厂类创建出对应的 format</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/06_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89protobuf_format/12.png" alt="12"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/06_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89protobuf_format/11.png" alt="11"></p><p>如图 serde format 是通过 <code>TableFactoryHelper.discoverDecodingFormat</code> 和 <code>TableFactoryHelper.discoverEncodingFormat</code> 创建的</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// either implement your custom validation logic here ...</span></span><br><span class="line">        <span class="comment">// or use the provided helper utility</span></span><br><span class="line"><span class="keyword">final</span> FactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(<span class="keyword">this</span>, context);</span><br><span class="line"></span><br><span class="line"><span class="comment">// discover a suitable decoding format</span></span><br><span class="line"><span class="keyword">final</span> DecodingFormat&lt;DeserializationSchema&lt;RowData&gt;&gt; decodingFormat = helper.discoverDecodingFormat(</span><br><span class="line">        DeserializationFormatFactory.class,</span><br><span class="line">        FactoryUtil.FORMAT);</span><br></pre></td></tr></table></figure><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/06_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89protobuf_format/16.png" alt="16"></p><p>所有通过 SPI 的 source\sink\formt 插件都继承自 <code>Factory</code>。</p><p>整体创建 format 方法的调用链如下图。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/06_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89protobuf_format/13.png" alt="13"></p><h2 id="5-2-flink-protobuf-format-实现"><a href="#5-2-flink-protobuf-format-实现" class="headerlink" title="5.2.flink protobuf format 实现"></a>5.2.flink protobuf format 实现</h2><p>最终实现如下，涉及到了几个实现类：</p><ol><li><code>ProtobufFormatFactory</code></li><li><code>ProtobufOptions</code></li><li><code>ProtobufRowDataDeserializationSchema</code></li><li><code>ProtobufToRowDataConverters</code></li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/06_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89protobuf_format/14.png" alt="14"></p><p>具体流程：</p><ol><li>定义 SPI 的工厂类 <code>ProtobufFormatFactory implements DeserializationFormatFactory</code>，并且在 resource\META-INF 下创建 SPI 的插件文件</li><li>实现 <code>ProtobufFormatFactory#factoryIdentifier</code> 标识 <code>protobuf</code></li><li>实现 <code>ProtobufFormatFactory#createDecodingFormat</code> 来创建对应的 <code>DecodingFormat&lt;DeserializationSchema&lt;RowData&gt;&gt;</code>，<code>DecodingFormat</code> 是用来封装具体的反序列化器的，实现 <code>DecodingFormat&lt;DeserializationSchema&lt;RowData&gt;&gt;#createRuntimeDecoder</code>，返回 <code>ProtobufRowDataDeserializationSchema</code></li><li>定义 <code>ProtobufRowDataDeserializationSchema implements DeserializationSchema&lt;RowData&gt;</code>，这个就是具体的反序列化器，其实与 datastream api 相同</li><li>实现 <code>ProtobufRowDataDeserializationSchema#deserialize</code> 方法，与 datastream 相同，这个方法就是将 <code>byte[]</code> 序列化为 <code>RowData</code> 的具体逻辑</li><li>注意这里还实现了一个类 <code>ProtobufToRowDataConverters</code>，其作用就是在客户端创建出具体的将  <code>byte[]</code> 序列化为 <code>RowData</code> 的具体工具类，其会根据用户定义的表字段类型动态生成数据转换的 converter 类（策略模式：<a href="https://www.runoob.com/design-pattern/strategy-pattern.html），相当于表的">https://www.runoob.com/design-pattern/strategy-pattern.html），相当于表的</a> schema 确定之后，其 converter 也会确定</li></ol><p>上述实现类的具体关系如下：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/06_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89protobuf_format/19.png" alt="19"></p><p>介绍完流程，进入具体实现方案细节：</p><p><code>ProtobufFormatFactory</code> 主要创建 format 的逻辑：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProtobufFormatFactory</span> <span class="keyword">implements</span> <span class="title">DeserializationFormatFactory</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String IDENTIFIER = <span class="string">&quot;protobuf&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> DecodingFormat&lt;DeserializationSchema&lt;RowData&gt;&gt; createDecodingFormat(Context context,</span><br><span class="line">            ReadableConfig formatOptions) &#123;</span><br><span class="line"></span><br><span class="line">        FactoryUtil.validateFactoryOptions(<span class="keyword">this</span>, formatOptions);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.获取到 protobuf 的 class 全路径</span></span><br><span class="line">        <span class="keyword">final</span> String className = formatOptions.get(PROTOBUF_CLASS_NAME);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 2.load class</span></span><br><span class="line">            Class&lt;GeneratedMessageV3&gt; protobufV3 =</span><br><span class="line">                    (Class&lt;GeneratedMessageV3&gt;) <span class="keyword">this</span>.getClass().getClassLoader().loadClass(className);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 3.创建 DecodingFormat</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> DecodingFormat&lt;DeserializationSchema&lt;RowData&gt;&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> DeserializationSchema&lt;RowData&gt; <span class="title">createRuntimeDecoder</span><span class="params">(DynamicTableSource.Context context,</span></span></span><br><span class="line"><span class="function"><span class="params">                        DataType physicalDataType)</span> </span>&#123;</span><br><span class="line">                    <span class="comment">// 4.获取到 table schema rowtype</span></span><br><span class="line">                    <span class="keyword">final</span> RowType rowType = (RowType) physicalDataType.getLogicalType();</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 5.创建对应的 DeserializationSchema 作为反序列化器</span></span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">new</span> ProtobufRowDataDeserializationSchema(</span><br><span class="line">                            protobufV3</span><br><span class="line">                            , <span class="keyword">true</span></span><br><span class="line">                            , rowType);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> ChangelogMode <span class="title">getChangelogMode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">return</span> ChangelogMode.insertOnly();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (ClassNotFoundException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">factoryIdentifier</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> IDENTIFIER;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>resources\META-INF 文件：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/06_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89protobuf_format/17.png" alt="17"></p><p><code>ProtobufRowDataDeserializationSchema</code> 主要实现反序列化的逻辑：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProtobufRowDataDeserializationSchema</span> <span class="keyword">extends</span> <span class="title">AbstractDeserializationSchema</span>&lt;<span class="title">RowData</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> ProtobufToRowDataConverters.ProtobufToRowDataConverter runtimeConverter;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ProtobufRowDataDeserializationSchema</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            Class&lt;? extends GeneratedMessageV3&gt; messageClazz</span></span></span><br><span class="line"><span class="function"><span class="params">            , <span class="keyword">boolean</span> ignoreParseErrors</span></span></span><br><span class="line"><span class="function"><span class="params">            , RowType expectedResultType)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.ignoreParseErrors = ignoreParseErrors;</span><br><span class="line">        Preconditions.checkNotNull(messageClazz, <span class="string">&quot;Protobuf message class must not be null.&quot;</span>);</span><br><span class="line">        <span class="keyword">this</span>.messageClazz = messageClazz;</span><br><span class="line">        <span class="keyword">this</span>.descriptorBytes = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">this</span>.descriptor = ProtobufUtils.getDescriptor(messageClazz);</span><br><span class="line">        <span class="keyword">this</span>.defaultInstance = ProtobufUtils.getDefaultInstance(messageClazz);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// protobuf 本身的 schema</span></span><br><span class="line">        <span class="keyword">this</span>.protobufOriginalRowType = (RowType) ProtobufSchemaConverter.convertToRowDataTypeInfo(messageClazz);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.expectedResultType = expectedResultType;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.根据 table schema 动态创建出对应的反序列化器</span></span><br><span class="line">        <span class="keyword">this</span>.runtimeConverter = <span class="keyword">new</span> ProtobufToRowDataConverters(<span class="keyword">false</span>)</span><br><span class="line">                .createRowDataConverterByLogicalType(<span class="keyword">this</span>.descriptor, <span class="keyword">this</span>.expectedResultType);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RowData <span class="title">deserialize</span><span class="params">(<span class="keyword">byte</span>[] bytes)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (bytes == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 2.将 bytes 反序列化为 protobuf message</span></span><br><span class="line">            Message message = <span class="keyword">this</span>.defaultInstance</span><br><span class="line">                    .newBuilderForType()</span><br><span class="line">                    .mergeFrom(bytes)</span><br><span class="line">                    .build();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 3.反序列化逻辑，从 protobuf message 中获取字段转换为 RowData</span></span><br><span class="line">            <span class="keyword">return</span> (RowData) runtimeConverter.convert(message);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">            <span class="keyword">if</span> (ignoreParseErrors) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IOException(</span><br><span class="line">                    format(<span class="string">&quot;Failed to deserialize Protobuf &#x27;%s&#x27;.&quot;</span>, <span class="keyword">new</span> String(bytes)), t);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>可以注意到上述反序列化的主要逻辑就集中在 <code>runtimeConverter</code> 上，即 <code>ProtobufToRowDataConverters.ProtobufToRowDataConverter</code>。</p><p><code>ProtobufToRowDataConverters.ProtobufToRowDataConverter</code> 就是在 <code>ProtobufToRowDataConverters</code> 中定义的。</p><p><code>ProtobufToRowDataConverters.ProtobufToRowDataConverter</code> 其实就是一个 convertor 接口：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@FunctionalInterface</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ProtobufToRowDataConverter</span> <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="function">Object <span class="title">convert</span><span class="params">(Object object)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其作用就是将 protobuf message 中的每一个字段转换成为 <code>RowData</code> 中的每一个字段。</p><p><code>ProtobufToRowDataConverters</code> 中就定义了具体转换逻辑，如截图所示，每一个 LogicalType 都定义了 protobuf message 字段转换为 flink 数据类型的逻辑：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/06_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89protobuf_format/18.png" alt="18"></p><p>源码公众号后台回复<strong>flink sql 知其所以然（五）| 自定义 protobuf format</strong>获取。</p><h1 id="6-总结与展望篇"><a href="#6-总结与展望篇" class="headerlink" title="6.总结与展望篇"></a>6.总结与展望篇</h1><h2 id="6-1-总结"><a href="#6-1-总结" class="headerlink" title="6.1.总结"></a>6.1.总结</h2><p>本文主要是针对 flink sql protobuf format 进行了原理解释以及对应的实现。<br>如果你正好需要这么一个 format，直接公众号后台回复<strong>flink sql 知其所以然（五）| 自定义 protobuf format</strong>获取源码吧。</p><h2 id="6-2-展望"><a href="#6-2-展望" class="headerlink" title="6.2.展望"></a>6.2.展望</h2><p>当然上述只是 protobuf format 一个基础的实现，用于生产环境还有很多方面可以去扩展的。</p><ol><li>性能优化、通用化：protobuf java class 本地 codegen 来提高任务性能</li><li>数据质量：异常 AOP，alert 等</li></ol>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink sql 知其所以然（四）| sql api 类型系统</title>
    <link href="https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/05_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9Asqlapi%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F/"/>
    <id>https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/05_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9Asqlapi%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F/</id>
    <published>2021-11-13T06:21:56.000Z</published>
    <updated>2021-08-21T15:33:16.715Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ashiamd.github.io/docsify-notes/#/study/BigData/Flink/%E5%B0%9A%E7%A1%85%E8%B0%B7Flink%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E6%88%98-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0?id=%E4%BB%A3%E7%A0%813-cep%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">https://ashiamd.github.io/docsify-notes/#/study/BigData/Flink/%E5%B0%9A%E7%A1%85%E8%B0%B7Flink%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E6%88%98-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0?id=%E4%BB%A3%E7%A0%813-cep%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0</a></p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/table/types/">https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/table/types/</a></p><blockquote><p>感谢您的<strong>关注  +  点赞 + 再看</strong>，对博主的肯定，会督促博主持续的输出更多的优质实战内容！！！</p></blockquote><h1 id="1-序篇-先说结论"><a href="#1-序篇-先说结论" class="headerlink" title="1.序篇-先说结论"></a>1.序篇-先说结论</h1><p>protobuf 作为目前各大公司中最广泛使用的高效的协议数据交换格式工具库，会大量作为流式数据传输的序列化方式，所以在 flink sql 中如果能实现 protobuf 的 format 会非常有用（<strong>目前社区已经有对应的实现，不过目前还没有 merge，预计在 1.14 系列版本中能 release</strong>）。</p><p>这一节原本是介绍 flink sql 中怎么自定义实现 protobuf format 类型，但是 format 的实现过程中涉及到了 flink sql 类型系统的知识，所以此节先讲解 flink sql 类型系统的内容作为铺垫。以帮助能更好的理解 flink sql 的类型系统。</p><p>flink sql 类型系统并不是一开始就是目前这样的 <code>LogicalType</code> 体系，其最开始也是复用了 datastream 的 <code>TypeInformation</code>，后来才由 <code>TypeInformation</code> 转变为了 <code>LogicalType</code>，因此本节分为以下几个小节，来说明 flink sql api 类型的转变原因、过程以及新类型系统设计。</p><ol><li>背景篇</li><li>目标篇-预期效果是什么</li><li>框架设计篇-具体方案实现</li><li>总结篇</li></ol><h1 id="2-背景篇"><a href="#2-背景篇" class="headerlink" title="2.背景篇"></a>2.背景篇</h1><p>熟悉 DataStream API 的同学都知道，DataStream API 的类型系统 TypeInformation 体系。所以初期 SQL API 的类型系统也是完全由 TypeInformation 实现的。但是随着 SQL API 的 feature 增强，用户越来越多的使用 SQL API 之后，发现 TypeInformation 作为 SQL API 的类型系统还是有一些缺陷的。</p><p>具体我们参考 <code>Flip-37</code>：<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-37%3A+Rework+of+the+Table+API+Type+System。">https://cwiki.apache.org/confluence/display/FLINK/FLIP-37%3A+Rework+of+the+Table+API+Type+System。</a></p><p><code>Flip-65</code>：<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-65%3A+New+type+inference+for+Table+API+UDFs">https://cwiki.apache.org/confluence/display/FLINK/FLIP-65%3A+New+type+inference+for+Table+API+UDFs</a></p><p><code>issue</code>：<a href="https://issues.apache.org/jira/browse/FLINK-12251">https://issues.apache.org/jira/browse/FLINK-12251</a></p><p>比如一些用户反馈有以下问题：</p><p><a href="https://docs.google.com/document/d/1zKSY1z0lvtQdfOgwcLnCMSRHew3weeJ6QfQjSD0zWas/edit#heading=h.64s92ad5mb1">https://docs.google.com/document/d/1zKSY1z0lvtQdfOgwcLnCMSRHew3weeJ6QfQjSD0zWas/edit#heading=h.64s92ad5mb1</a></p><p>在 <code>Flip-37</code> 中介绍到：</p><p><strong>1. TypeInformation 不能和 SQL 类型系统很好的集成，并且不同实现语言也会对其类型信息产生影响。</strong></p><p><strong>2. TypeInformation 与 SQL 类型系统不一致。</strong></p><p><strong>3. 不能为 DECIMAL 等定义精度和小数位数。</strong></p><p><strong>4. 不支持 CHAR/VARCHAR 之间的差异（FLINK-10257、FLINK-9559）。</strong></p><p><strong>5. 物理类型和逻辑类型是紧密耦合的。</strong></p><p><code>flink sql 类型系统设计文档</code>：<a href="https://docs.google.com/document/d/1a9HUb6OaBIoj9IRfbILcMFPrOL7ALeZ3rVI66dvA2_U/edit#heading=h.5qoorezffk0t">https://docs.google.com/document/d/1a9HUb6OaBIoj9IRfbILcMFPrOL7ALeZ3rVI66dvA2_U/edit#heading=h.5qoorezffk0t</a></p><h2 id="2-1-序列化器受执行环境影响"><a href="#2-1-序列化器受执行环境影响" class="headerlink" title="2.1.序列化器受执行环境影响"></a>2.1.序列化器受执行环境影响</h2><p>怎么理解不同语言的环境会对类型信息产生影响，直接来看一下下面这个例子（基于 <code>flink 1.8</code>）：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.<span class="type">TableFunction</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">SimpleUser</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">TableFunc0</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>[<span class="type">SimpleUser</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// make sure input element&#x27;s format is &quot;&lt;string&amp;gt#&lt;int&gt;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(user: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (user.contains(<span class="string">&quot;#&quot;</span>)) &#123;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> splits = user.split(<span class="string">&quot;#&quot;</span>)</span><br><span class="line"></span><br><span class="line">      collect(<span class="type">SimpleUser</span>(splits(<span class="number">0</span>), splits(<span class="number">1</span>).toInt))</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>TableFunc0</code> 出参（SimpleUser）的 <code>TypeInformation</code> 不仅取决于<code>出参</code>本身，还取决于使用的<code>表环境</code>，而且最终的序列化器也是不同的，这里以 java 环境和 scala 环境做比较：</p><h3 id="2-1-1-java-环境"><a href="#2-1-1-java-环境" class="headerlink" title="2.1.1.java 环境"></a>2.1.1.java 环境</h3><p>在 java 环境中，使用 <code>org.apache.flink.table.api.java.StreamTableEnvironment#registerFunction</code> 注册函数。</p><p>Java 类型提取是通过基于反射的 <code>TypeExtractor</code> 提取 <code>TypeInformation</code>。</p><p>示例代码如下（基于 flink 1.8 版本）：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JavaEnvTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment sEnv = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">// create a TableEnvironment for streaming queries</span></span><br><span class="line">        StreamTableEnvironment sTableEnv = StreamTableEnvironment.create(sEnv);</span><br><span class="line"></span><br><span class="line">        sTableEnv.registerFunction(<span class="string">&quot;table1&quot;</span>, <span class="keyword">new</span> TableFunc0());</span><br><span class="line"></span><br><span class="line">        TableSqlFunction tableSqlFunction =</span><br><span class="line">                (TableSqlFunction) sTableEnv</span><br><span class="line">                        .getFunctionCatalog()</span><br><span class="line">                        .getSqlOperatorTable()</span><br><span class="line">                        .getOperatorList()</span><br><span class="line">                        .get(<span class="number">170</span>);</span><br><span class="line"></span><br><span class="line">        TypeSerializer&lt;?&gt; t = tableSqlFunction.getRowTypeInfo().createSerializer(sEnv.getConfig());</span><br><span class="line"></span><br><span class="line">        sEnv.execute();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/05_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9Asqlapi%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F/1.png" alt="1"></p><p>java 环境，可以看到，最终使用的是 <code>Kryo 序列化器</code>。</p><h3 id="2-1-2-scala-环境"><a href="#2-1-2-scala-环境" class="headerlink" title="2.1.2.scala 环境"></a>2.1.2.scala 环境</h3><p>在 scala 环境中，使用 <code>org.apache.flink.table.api.scala.StreamTableEnvironment#registerFunction</code> 注册函数。</p><p>使用 Scala 类型提取堆栈并通过使用 <code>Scala 宏</code>提取 <code>TypeInformation</code>。</p><p>示例代码如下（基于 flink 1.8 版本）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">object ScalaEnv &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">    val env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; create a TableEnvironment</span><br><span class="line">    val tableEnv &#x3D; StreamTableEnvironment.create(env)</span><br><span class="line"></span><br><span class="line">    tableEnv.registerFunction(&quot;hashCode&quot;, new TableFunc0())</span><br><span class="line"></span><br><span class="line">    val config &#x3D; env.getConfig</span><br><span class="line"></span><br><span class="line">    val function &#x3D; new TableFunc0()</span><br><span class="line"></span><br><span class="line">    registerFunction(config, function)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; execute</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def registerFunction[T: TypeInformation](config: ExecutionConfig, tf: TableFunction[T]): Unit &#x3D; &#123;</span><br><span class="line">    val typeInfo: TypeInformation[_] &#x3D; if (tf.getResultType !&#x3D; null) &#123;</span><br><span class="line">      tf.getResultType</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      implicitly[TypeInformation[T]]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val ty &#x3D; typeInfo.createSerializer(config)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/05_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9Asqlapi%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F/2.png" alt="2"></p><p>scala 环境，最终使用的是 <code>Case Class 序列化器</code>。</p><p>但是逻辑上同一个 sql 的 model 的序列化方式只应该与 model 本身有关，不应该与不同语言的 env 有关。不同的 env 的 model 序列化器都应该相同。</p><h2 id="2-2-类型系统不一致"><a href="#2-2-类型系统不一致" class="headerlink" title="2.2.类型系统不一致"></a>2.2.类型系统不一致</h2><p><code>SQL</code> 类型系统与 <code>TypeInformation</code> 系统不一致。如下图</p><p><code>TypeInformation</code> 类型系统的组成，熟悉 datastream 的同学应该都见过：<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/05_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9Asqlapi%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F/11.png" alt="11"></p><p>但是标准的 sql 类型系统的组成应该是由如下组成这样：<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/05_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9Asqlapi%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F/8.png" alt="8"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/05_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9Asqlapi%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F/9.png" alt="9"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/05_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9Asqlapi%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F/10.png" alt="10"></p><p>可见 <code>TypeInformation</code> 类型系统与标准 SQL 类型系统的对应关系是不太一致的，这也就导致了 flink sql 与 <code>TypeInformation</code> 不能很好的集成。</p><h2 id="2-3-TypeInformation-类型信息与序列化器绑定"><a href="#2-3-TypeInformation-类型信息与序列化器绑定" class="headerlink" title="2.3.TypeInformation 类型信息与序列化器绑定"></a>2.3.TypeInformation 类型信息与序列化器绑定</h2><p>如图 <code>TypeInformation</code> 的具体实现类需要实现 <code>TypeInformation&lt;T&gt;#createSerializer</code>，来指定类型信息的具体序列化器。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/05_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9Asqlapi%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F/3.png" alt="3"></p><p>举例，旧类型系统中，flink sql api 中是使用 <code>CRow</code> 进行的内部数据的流转， <code>CRowTypeInfo</code> 如下图，其序列化器固定为 <code>CRowSerializer</code>：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/05_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9Asqlapi%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F/19.png" alt="19"></p><p>再来一个例子， <code>ListTypeInfo</code> 的序列化器固定为 <code>ListSerializer</code>。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/05_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9Asqlapi%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F/4.png" alt="4"></p><p>可以看到 <code>TypeInformation</code> 的类型体系中，一种 <code>TypeInformation</code> 就和一个 <code>TypeSerializer</code> 是绑定的。</p><h1 id="3-目标篇-预期效果是什么"><a href="#3-目标篇-预期效果是什么" class="headerlink" title="3.目标篇-预期效果是什么"></a>3.目标篇-预期效果是什么</h1><p>博主体感比较深的是：</p><p><strong>1. 统一以及标准化 SQL 类型系统</strong></p><p><strong>2. 逻辑类型与物理类型解耦</strong></p><p>然后来看看 flink 是怎么做这件事情的，下面的代码都基于 <code>flink 1.13.1</code>。</p><h1 id="4-框架设计篇-具体方案实现"><a href="#4-框架设计篇-具体方案实现" class="headerlink" title="4.框架设计篇-具体方案实现"></a>4.框架设计篇-具体方案实现</h1><p>先从最终最上层的角度出发，看看 flink sql 程序运行时数据载体的变化。</p><p><strong>1.old planner</strong>：</p><p>内部数据流的基本数据类型：<code>CRow</code> = <code>Row</code> + 标识（是否回撤数据）</p><p>类型信息：<code>CRowTypeInfo</code>，其类型系统使用的完全也是 <code>TypeInformation</code> 那一套</p><p>序列化器：<code>CRowSerializer</code> = <code>RowSerializer</code> + 标识序列化</p><p><strong>2.blink planner</strong>：</p><p>内部数据流的基本数据类型：<code>RowData</code></p><p>类型信息：<code>RowType</code>，基于 <code>LogicalType</code></p><p>序列化器：<code>RowDataSerializer</code></p><h2 id="4-1-统一以及标准化-SQL-类型系统"><a href="#4-1-统一以及标准化-SQL-类型系统" class="headerlink" title="4.1.统一以及标准化 SQL 类型系统"></a>4.1.统一以及标准化 SQL 类型系统</h2><p>先来重温下，SQL 标准类型：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/05_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9Asqlapi%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F/8.png" alt="8"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/05_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9Asqlapi%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F/9.png" alt="9"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/05_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9Asqlapi%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F/10.png" alt="10"></p><p>然后开看看，flink sql 的类型系统设计，代码位于 <code>flink-table-common</code> 模块：</p><p>新的类型系统是基于 <code>LogicalTypeFamily</code>，<code>LogicalTypeRoot</code>，<code>LogicalType</code> 进行实现的：</p><p><code>LogicalTypeFamily</code>：<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/05_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9Asqlapi%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F/14.png" alt="14"></p><p><code>LogicalTypeRoot</code>：<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/05_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9Asqlapi%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F/15.png" alt="15"></p><p><code>LogicalType</code>：<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/05_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9Asqlapi%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F/18.png" alt="18"></p><p>具体 <code>LogicalType</code> 的各类实现类如下图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/05_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9Asqlapi%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F/12.png" alt="12"></p><p>可以发现其设计（枚举信息、实现等）都是与 SQL 标准进行了对齐的。</p><p>具体类型详情可以参考官方文档，这里不过多赘述。<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/table/types/">https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/table/types/</a></p><h2 id="4-2-逻辑类型与物理类型解耦"><a href="#4-2-逻辑类型与物理类型解耦" class="headerlink" title="4.2.逻辑类型与物理类型解耦"></a>4.2.逻辑类型与物理类型解耦</h2><p>解耦这部分的实现比较好理解，博主通过两种方式来解释其解耦方式：</p><h3 id="4-2-1-看看解耦的具体实现"><a href="#4-2-1-看看解耦的具体实现" class="headerlink" title="4.2.1.看看解耦的具体实现"></a>4.2.1.看看解耦的具体实现</h3><p>博主画了一张图来比较下 <code>TypeInformation</code> 与 <code>LogicalType</code>，如下图。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/05_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9Asqlapi%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F/20.png" alt="20"></p><ul><li><strong>datastream\old planner</strong>：如左图所示，都是基于 <code>TypeInformation</code> 体系，一种 <code>TypeInformation</code> 就和一个 <code>TypeSerializer</code> 是绑定的。</li><li><strong>blink planner</strong>：如右图所示，都是基于 <code>LogicalType</code> 体系，但是与 <code>TypeSerializer</code> 通过中间的一层映射层进行解耦，这层映射层是 blink planner 独有的，<strong>当然如果你也能自定义一个 planner，你也可以自定义对应的映射方式</strong>。</li></ul><p><code>LogicalType</code> 只包含类型信息，关于具体的序列化器是在不同的 planner 中实现的。Blink Planner 是 <code>InternalSerializers</code>。</p><h3 id="4-2-2-看看包的划分"><a href="#4-2-2-看看包的划分" class="headerlink" title="4.2.2.看看包的划分"></a>4.2.2.看看包的划分</h3><p>其实我们也可以通过这些具体实现类的在 flink 中所在的包也可以看出其解耦方式。如图所示。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/05_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9Asqlapi%E7%B1%BB%E5%9E%8B%E7%B3%BB%E7%BB%9F/21.png" alt="21"></p><ul><li><p><strong>datastream\old planner</strong>：如左图所示，其中的核心逻辑类型、序列化器都是在 <code>flink-core</code> 中实现的。都是基于以及复用了 <code>TypeInformation</code> 体系。</p></li><li><p><strong>blink planner</strong>：如右图所示，<code>LogicalType</code> 体系都是位于 <code>flink-table-common</code> 模块中，作为 sql 基础、标准的体系。而其中具体的序列化器是在 <code>flink-table-runtime-blink</code> 中的，可以说明不同的 planner 是有对应不同的实现的，从而实现了逻辑类型和物理序列化器的解耦。</p></li></ul><h1 id="5-总结篇"><a href="#5-总结篇" class="headerlink" title="5.总结篇"></a>5.总结篇</h1><p>本文主要介绍了 flink sql 类型系统的内容，从背景、目标以及最终的实现上做了一些思考和分析。</p><p>希望能抛砖引玉，让大家能在使用层面之上还能有一些更深层次的思考~</p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink sql 知其所以然（三）| sql 自定义 redis 数据汇表</title>
    <link href="https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/04_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E6%B1%87%E8%A1%A8/"/>
    <id>https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/04_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E6%B1%87%E8%A1%A8/</id>
    <published>2021-11-13T06:21:55.000Z</published>
    <updated>2021-08-16T15:50:08.570Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>感谢您的<strong>关注  +  点赞 + 再看</strong>，对博主的肯定，会督促博主持续的输出更多的优质实战内容！！！</p></blockquote><h1 id="1-序篇-本文结构"><a href="#1-序篇-本文结构" class="headerlink" title="1.序篇-本文结构"></a>1.序篇-本文结构</h1><ol><li>背景篇-为啥需要 redis 数据汇表</li><li>目标篇-redis 数据汇表预期效果</li><li>难点剖析篇-此框架建设的难点、目前有哪些实现</li><li>维表实现篇-实现的过程</li><li>总结与展望篇</li></ol><p>本文主要介绍了 flink sql redis 数据汇表的实现过程。</p><p>如果想在本地测试下：</p><ol><li>在公众号后台回复<ul><li><strong>flink sql 知其所以然（三）| sql 自定义 redis 数据汇表</strong>获取源码（源码基于 1.13.1 实现）</li><li><strong>flink sql 知其所以然（三）| sql 自定义 redis 数据汇表</strong>获取源码（源码基于 1.13.1 实现）</li><li><strong>flink sql 知其所以然（三）| sql 自定义 redis 数据汇表</strong>获取源码（源码基于 1.13.1 实现）</li></ul></li><li>在你的本地安装并启动 redis-server。</li><li>执行源码包中的 <code>flink.examples.sql._03.source_sink.RedisSinkTest</code> 测试类，然后使用 redis-cli 执行 <code>get a</code> 就可以看到结果了（目前只支持 kv，即 redis <code>set key value</code>）。</li></ol><p>如果想直接在集群环境使用：</p><ol><li>命令行执行 <code>mvn package -DskipTests=true</code> 打包</li><li>将生成的包 <code>flink-examples-0.0.1-SNAPSHOT.jar</code> 引入 flink lib 中即可，无需其它设置。</li></ol><h1 id="2-背景篇-为啥需要-redis-数据汇表"><a href="#2-背景篇-为啥需要-redis-数据汇表" class="headerlink" title="2.背景篇-为啥需要 redis 数据汇表"></a>2.背景篇-为啥需要 redis 数据汇表</h1><p>目前在实时计算的场景中，熟悉 datastream 的同学在很多场景下都会将结果数据写入到 redis 提供数据服务。</p><p>举个例子：</p><ol><li><strong>外存状态引擎</strong>：需要把历史所有的 id 存储下来，但是因为 id 会不断增多，仅仅使用 flink 内部状态引擎的话，状态会越来越大，很难去保障其稳定性。那么这时就会选择外部状态引擎，比如 redis。在我们使用 redis 存储所有设备 id 时，除了使用 redis 作为维表去访问 id 是否出现过，还需要将新增的 id 写入到 redis 中以供后续的去重。这时候就需要使用到 redis sink 表。</li><li><strong>数据服务引擎</strong>：在某些大促（双十一）的场景下需要将 flink 计算好的结果直接写入到 redis 中以提供高速数据服务引擎，直接提供给大屏查询使用。</li></ol><p>而官方是没有提供 flink sql api 的 redis sink connector 的。如下图，基于 1.13 版本。</p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/connectors/table/overview/">https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/connectors/table/overview/</a></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/04_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E6%B1%87%E8%A1%A8/1.png" alt="1"></p><p>阿里云 flink 是提供了这个能力的。</p><p><a href="https://www.alibabacloud.com/help/zh/faq-detail/118038.htm?spm=a2c63.q38357.a3.16.48fa711fo1gVUd">https://www.alibabacloud.com/help/zh/faq-detail/118038.htm?spm=a2c63.q38357.a3.16.48fa711fo1gVUd</a></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/04_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E6%B1%87%E8%A1%A8/2.png" alt="2"></p><p>因此本文在介绍怎样自定义一个 sql 数据汇表的同时，实现一个 sql redis sink connector 来给大家使用。</p><h1 id="3-目标篇-redis-数据汇表预期效果"><a href="#3-目标篇-redis-数据汇表预期效果" class="headerlink" title="3.目标篇-redis 数据汇表预期效果"></a>3.目标篇-redis 数据汇表预期效果</h1><p>redis 作为数据汇表在 datastream 中的最常用的数据结构有很多，基本上所有的数据结构都有可能使用到。<br>本文实现主要实现 kv 结构，其他结构大家可以拿到源码之后进行自定义实现。也就多加几行代码就完事了。</p><p>预期效果就如阿里云的 flink redis，redis <code>set key value</code> 的预期 flink sql：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> redis_sink_table (</span><br><span class="line">    key STRING, <span class="comment">-- redis key，第 1 列为 key</span></span><br><span class="line">    `<span class="keyword">value</span>` STRING <span class="comment">-- redis value，第 2 列为 value</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;redis&#x27;</span>, <span class="comment">-- 指定 connector 是 redis 类型的</span></span><br><span class="line">  <span class="string">&#x27;hostname&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;127.0.0.1&#x27;</span>, <span class="comment">-- redis server ip</span></span><br><span class="line">  <span class="string">&#x27;port&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;6379&#x27;</span>, <span class="comment">-- redis server 端口</span></span><br><span class="line">  <span class="string">&#x27;write.mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;string&#x27;</span> <span class="comment">-- 指定使用 redis `set key value`</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> redis_sink_table</span><br><span class="line"><span class="keyword">SELECT</span> o.f0 <span class="keyword">as</span> key, o.f1 <span class="keyword">as</span> <span class="keyword">value</span></span><br><span class="line"><span class="keyword">FROM</span> leftTable <span class="keyword">AS</span> o</span><br></pre></td></tr></table></figure><p>下面是我在本地跑的结果：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/04_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E6%B1%87%E8%A1%A8/3.png" alt="3"></p><p>首先看下我们的测试输入，<code>f0</code> 恒定为 <code>a</code>，<code>f1</code> 恒定为 <code>b</code>，并且每 10ms 写入一次：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/04_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E6%B1%87%E8%A1%A8/4.png" alt="4"></p><p>预期结果是 key 为 <code>a</code>，value 会为 <code>b</code>，实际结果也相同，使用 redis-cli 查询下，我删除掉也能在 10ms 后写入，所以查询时可以一直查得到：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/04_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E6%B1%87%E8%A1%A8/5.gif" alt="5"></p><h1 id="4-难点剖析篇-目前有哪些实现"><a href="#4-难点剖析篇-目前有哪些实现" class="headerlink" title="4.难点剖析篇-目前有哪些实现"></a>4.难点剖析篇-目前有哪些实现</h1><p>目前可以从网上搜到的实现、以及可以参考的实现有以下两个：</p><ol><li><a href="https://github.com/jeff-zou/flink-connector-redis。">https://github.com/jeff-zou/flink-connector-redis。</a> 但使用起来有比较多的限制，包括需要在建表时就指定 key-column，value-column 等，其实博主觉得没必要指定这些字段，这些都可以动态调整。其实现是对 apache-bahir-flink <a href="https://github.com/apache/bahir-flink">https://github.com/apache/bahir-flink</a> 的二次开发，但与 bahir 原生实现有割裂感，因为这个项目几乎参考 bahir redis connector 重新实现了一遍，接口与 bahir 不太相同。</li><li>阿里云实现 <a href="https://www.alibabacloud.com/help/zh/faq-detail/122722.htm?spm=a2c63.q38357.a3.7.a1227a53TBMuSY。">https://www.alibabacloud.com/help/zh/faq-detail/122722.htm?spm=a2c63.q38357.a3.7.a1227a53TBMuSY。</a> 阿里云的实现相对比较动态化，不需要在建表时就指定 hmap 等数据结构的 map key。</li></ol><p>因此博主在实现时，定了一个基调。</p><ol><li><strong>参考阿里云的 DDL 实现</strong></li><li><strong>高度复用性</strong>：复用 bahir 提供的 redis connnector</li><li><strong>简洁性</strong>：目前只实现 kv 结构，后续扩展可以给用户自己实现，扩展其实是非常简单的</li></ol><h1 id="5-实现篇-实现的过程"><a href="#5-实现篇-实现的过程" class="headerlink" title="5.实现篇-实现的过程"></a>5.实现篇-实现的过程</h1><p>在实现 redis 数据汇表之前，不得不谈谈 flink 数据汇表加载和使用机制。</p><h2 id="5-1-flink-数据汇表原理"><a href="#5-1-flink-数据汇表原理" class="headerlink" title="5.1.flink 数据汇表原理"></a>5.1.flink 数据汇表原理</h2><p>其实上节已经详细描述了 flink sql 对于 source\sink 的加载机制。</p><ol><li>通过 SPI 机制加载所有的 source\sink\format 工厂 <code>Factory</code></li><li>过滤出 DynamicTableSinkFactory + connector 标识的 sink 工厂类</li><li>通过 sink 工厂类创建出对应的 sink</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/04_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E6%B1%87%E8%A1%A8/7.png" alt="7"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/04_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E6%B1%87%E8%A1%A8/8.png" alt="8"></p><p>如图 source 和 sink 是通过 <code>FactoryUtil.createTableSource</code> 和 <code>FactoryUtil.createTableSink</code> 创建的</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/04_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E6%B1%87%E8%A1%A8/16.png" alt="16"></p><p>所有通过 SPI 的 source\sink\formt 插件都继承自 <code>Factory</code>。</p><p>整体创建 sink 方法的调用链如下图。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/04_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E6%B1%87%E8%A1%A8/10.png" alt="10"></p><h2 id="5-2-flink-数据汇表实现方案"><a href="#5-2-flink-数据汇表实现方案" class="headerlink" title="5.2.flink 数据汇表实现方案"></a>5.2.flink 数据汇表实现方案</h2><p>先看下博主的最终实现。</p><p>由于高度复用了 bahir redis connector，所以需要重点实现就只有两个类：</p><ol><li><code>RedisDynamicTableFactory</code></li><li><code>RedisDynamicTableSink</code></li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/04_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E6%B1%87%E8%A1%A8/6.png" alt="6"></p><p>具体流程：</p><ol><li>定义 SPI 的工厂类 <code>RedisDynamicTableFactory implements DynamicTableSinkFactory</code>，并且在 resource\META-INF 下创建 SPI 的插件文件</li><li>实现 factoryIdentifier 标识 <code>redis</code></li><li>实现 <code>RedisDynamicTableFactory#createDynamicTableSink</code> 来创建对应的 source <code>RedisDynamicTableSink</code></li><li>定义 <code>RedisDynamicTableSink implements DynamicTableSink</code></li><li>实现 <code>RedisDynamicTableFactory#getSinkRuntimeProvider</code> 方法，创建具体的维表 UDF  <code>RichSinkFunction&lt;T&gt;</code>，这里直接服用了 bahir redis 中的 <code>RedisSink&lt;IN&gt;</code></li></ol><p>介绍完流程，进入具体实现方案细节：</p><p><code>RedisDynamicTableFactory</code> 主要创建 sink 的逻辑：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RedisDynamicTableFactory</span> <span class="keyword">implements</span> <span class="title">DynamicTableSinkFactory</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">factoryIdentifier</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 标识 redis</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;redis&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> DynamicTableSink <span class="title">createDynamicTableSink</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// either implement your custom validation logic here ...</span></span><br><span class="line">        <span class="comment">// or use the provided helper utility</span></span><br><span class="line">        <span class="keyword">final</span> FactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(<span class="keyword">this</span>, context);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// validate all options</span></span><br><span class="line">        <span class="comment">// 所有 option 配置的校验，比如 write.mode 类参数</span></span><br><span class="line">        helper.validate();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// get the validated options</span></span><br><span class="line">        <span class="keyword">final</span> ReadableConfig options = helper.getOptions();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> RedisWriteOptions redisWriteOptions = RedisOptions.getRedisWriteOptions(options);</span><br><span class="line"></span><br><span class="line">        TableSchema schema = context.getCatalogTable().getSchema();</span><br><span class="line"></span><br><span class="line">        / 创建 RedisDynamicTableSink</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> RedisDynamicTableSink(schema.toPhysicalRowDataType()</span><br><span class="line">                , redisWriteOptions);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>resources\META-INF 文件：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/04_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E6%B1%87%E8%A1%A8/11.png" alt="11"></p><p><code>RedisDynamicTableSource</code> 主要创建 table udf 的逻辑：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RedisDynamicTableSink</span> <span class="keyword">implements</span> <span class="title">DynamicTableSink</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> SinkRuntimeProvider <span class="title">getSinkRuntimeProvider</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 初始化 redis 客户端配置</span></span><br><span class="line">        FlinkJedisConfigBase flinkJedisConfigBase = <span class="keyword">new</span> FlinkJedisPoolConfig.Builder()</span><br><span class="line">                .setHost(<span class="keyword">this</span>.redisWriteOptions.getHostname())</span><br><span class="line">                .setPort(<span class="keyword">this</span>.redisWriteOptions.getPort())</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        RedisMapper&lt;RowData&gt; redisMapper = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">switch</span> (<span class="keyword">this</span>.redisWriteOptions.getWriteMode()) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;string&quot;</span>:</span><br><span class="line">                <span class="comment">// redis key，value 序列化器</span></span><br><span class="line">                <span class="comment">// 从 RowData 转换成 redis 的 key value</span></span><br><span class="line">                redisMapper = <span class="keyword">new</span> SetRedisMapper();</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;其他类型 write mode 请自定义实现&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建 SinkFunction，注意！！！这里直接复用了 bahir 的实现</span></span><br><span class="line">        <span class="keyword">return</span> SinkFunctionProvider.of(<span class="keyword">new</span> RedisSink&lt;&gt;(</span><br><span class="line">                flinkJedisConfigBase</span><br><span class="line">                , redisMapper));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>RedisSink</code> 执行写入 redis 的主要流程，这里是 bahir 的实现：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RedisRowDataLookupFunction</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>&lt;<span class="title">RowData</span>&gt; </span>&#123;</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(IN input)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String key = redisSinkMapper.getKeyFromData(input);</span><br><span class="line">        String value = redisSinkMapper.getValueFromData(input);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 根据具体的命令执行具体写入 redis 的命令</span></span><br><span class="line">        <span class="keyword">switch</span> (redisCommand) &#123;</span><br><span class="line">            <span class="keyword">case</span> RPUSH:</span><br><span class="line">                <span class="keyword">this</span>.redisCommandsContainer.rpush(key, value);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> LPUSH:</span><br><span class="line">                <span class="keyword">this</span>.redisCommandsContainer.lpush(key, value);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> SADD:</span><br><span class="line">                <span class="keyword">this</span>.redisCommandsContainer.sadd(key, value);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> SET:</span><br><span class="line">                <span class="keyword">this</span>.redisCommandsContainer.set(key, value);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> PFADD:</span><br><span class="line">                <span class="keyword">this</span>.redisCommandsContainer.pfadd(key, value);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> PUBLISH:</span><br><span class="line">                <span class="keyword">this</span>.redisCommandsContainer.publish(key, value);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> ZADD:</span><br><span class="line">                <span class="keyword">this</span>.redisCommandsContainer.zadd(<span class="keyword">this</span>.additionalKey, value, key);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> ZREM:</span><br><span class="line">                <span class="keyword">this</span>.redisCommandsContainer.zrem(<span class="keyword">this</span>.additionalKey, key);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> HSET:</span><br><span class="line">                <span class="keyword">this</span>.redisCommandsContainer.hset(<span class="keyword">this</span>.additionalKey, key, value);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;Cannot process such data type: &quot;</span> + redisCommand);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 初始化 redis 执行器</span></span><br><span class="line">            <span class="keyword">this</span>.redisCommandsContainer = RedisCommandsContainerBuilder.build(<span class="keyword">this</span>.flinkJedisConfigBase);</span><br><span class="line">            <span class="keyword">this</span>.redisCommandsContainer.open();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            LOG.error(<span class="string">&quot;Redis has not been properly initialized: &quot;</span>, e);</span><br><span class="line">            <span class="keyword">throw</span> e;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="5-2-1-复用-bahir-connector"><a href="#5-2-1-复用-bahir-connector" class="headerlink" title="5.2.1.复用 bahir connector"></a>5.2.1.复用 bahir connector</h3><p>如图是 bahir redis connector 的实现。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/04_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E6%B1%87%E8%A1%A8/15.png" alt="15"></p><p>博主在实现过程中将能复用的都尽力复用。如图是最终实现目录。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/04_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E6%B1%87%E8%A1%A8/12.png" alt="12"></p><p>可以看到实现非常简单。</p><p>其中 <code>redis 客户端及其配置</code>、<code>redis 命令执行器</code> 和 <code>redis 命令定义器</code> 是直接复用了 bahir redis 的。<br>如果你想要在生产环境中进行使用，可以直接将两部分代码合并，成本很低。</p><p>源码公众号后台回复<strong>flink sql 知其所以然（三）| sql 自定义 redis 数据汇表</strong>获取。</p><h1 id="6-总结与展望篇"><a href="#6-总结与展望篇" class="headerlink" title="6.总结与展望篇"></a>6.总结与展望篇</h1><h2 id="6-1-总结"><a href="#6-1-总结" class="headerlink" title="6.1.总结"></a>6.1.总结</h2><p>本文主要是针对 flink sql redis 数据汇表进行了扩展以及实现，并且复用 bahir redis connector 的配置，具有良好的扩展性。<br>如果你正好需要这么一个 connector，直接公众号后台回复<strong>flink sql 知其所以然（三）| sql 自定义 redis 数据汇表</strong>获取源码吧。</p><h2 id="6-2-展望"><a href="#6-2-展望" class="headerlink" title="6.2.展望"></a>6.2.展望</h2><p>当然上述只是 redis 数据汇表一个基础的实现，用于生产环境还有很多方面可以去扩展的。</p><ol><li>jedis cluster 的扩展：目前 bahir datastream 中已经实现了，可以直接参考，扩展起来非常简单</li><li>异常 AOP，alert 等</li></ol>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink sql 知其所以然（二）| sql 自定义 redis 数据维表</title>
    <link href="https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/03_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E7%BB%B4%E8%A1%A8/"/>
    <id>https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/03_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E7%BB%B4%E8%A1%A8/</id>
    <published>2021-11-13T06:21:54.000Z</published>
    <updated>2021-08-15T10:59:00.384Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>感谢您的<strong>关注  +  点赞 + 再看</strong>，对博主的肯定，会督促博主持续的输出更多的优质实战内容！！！</p></blockquote><h1 id="1-序篇-本文结构"><a href="#1-序篇-本文结构" class="headerlink" title="1.序篇-本文结构"></a>1.序篇-本文结构</h1><ol><li>背景篇-为啥需要 redis 维表</li><li>目标篇-做 redis 维表的预期效果是什么</li><li>难点剖析篇-此框架建设的难点、目前有哪些实现</li><li>维表实现篇-维表实现的过程</li><li>总结与展望篇</li></ol><p>本文主要介绍了 flink sql redis 维表的实现过程。</p><p>如果想在本地测试下：</p><ol><li>在公众号后台回复<strong>flink sql 知其所以然（二）| sql 自定义 redis 数据维表</strong>获取源码（源码基于 1.13.1 实现）</li><li>在你的本地安装并打开 redis-server，然后使用 redis-cli 执行命令 <code>set a &quot;&#123;\&quot;score\&quot;:3,\&quot;name\&quot;:\&quot;namehhh\&quot;,\&quot;name1\&quot;:\&quot;namehhh112\&quot;&#125;&quot;</code></li><li>执行源码包中的 <code>flink.examples.sql._03.source_sink.RedisLookupTest</code> 测试类，就可以在 console 中看到结果。</li></ol><p>如果想直接在集群环境使用：</p><ol><li>命令行执行 <code>mvn package -DskipTests=true</code> 打包</li><li>将生成的包 <code>flink-examples-0.0.1-SNAPSHOT.jar</code> 引入 flink lib 中即可，无需其它设置。</li></ol><h1 id="2-背景篇-为啥需要-redis-维表"><a href="#2-背景篇-为啥需要-redis-维表" class="headerlink" title="2.背景篇-为啥需要 redis 维表"></a>2.背景篇-为啥需要 redis 维表</h1><h2 id="2-1-啥是维表？事实表？"><a href="#2-1-啥是维表？事实表？" class="headerlink" title="2.1.啥是维表？事实表？"></a>2.1.啥是维表？事实表？</h2><p>Dimension Table 概念多出现于数据仓库里面，维表与事实表相互对应。</p><p>给两个场景来看看：</p><p>比如需要统计分性别的 DAU：</p><ol><li>客户端上报的日志中（事实表）只有设备 id，只用这个事实表是没法统计出分性别的 DAU 的。</li><li>这时候就需要一张带有设备 id、性别映射的表（这就是维表）来提供性别数据。</li><li>然后使用事实表去 join 这张维表去获取到每一个设备 id 对应的性别，然后就可以统计出分性别的 DAU。相当于一个扩充维度的操作。</li></ol><p><a href="https://blog.csdn.net/weixin_47482194/article/details/105855116?spm=1001.2014.3001.5501">https://blog.csdn.net/weixin_47482194/article/details/105855116?spm=1001.2014.3001.5501</a></p><p>比如目前想要统计整体销售额：</p><ol><li>目前已有 “销售统计表”，是一个事实表，其中没有具体销售品项的金额。</li><li>“商品价格表” 可以用于提供具体销售品项的金额，这就是销售统计的一个维度表。</li></ol><p>事实数据和维度数据的识别必须依据具体的主题问题而定。“事实表” 用来存储事实的度量及指向各个维的外键值。维表用来保存该维的元数据。</p><p>参考：<a href="https://blog.csdn.net/lindan1984/article/details/96566626">https://blog.csdn.net/lindan1984/article/details/96566626</a></p><h2 id="2-2-为啥需要-redis-维表？"><a href="#2-2-为啥需要-redis-维表？" class="headerlink" title="2.2.为啥需要 redis 维表？"></a>2.2.为啥需要 redis 维表？</h2><p>目前在实时计算的场景中，熟悉 datastream 的同学大多数都使用过 mysql\Hbase\redis 作为维表引擎存储一些维度数据，然后在 datastream api 中调用 mysql\Hbase\redis 客户端去获取到维度数据进行维度扩充。</p><p>而 redis 作为 flink 实时场景中最常用的高速维表引擎，官方是没有提供 flink sql api 的 redis 维表 connector 的。如下图，基于 1.13 版本。</p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/connectors/table/overview/">https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/connectors/table/overview/</a></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/03_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E7%BB%B4%E8%A1%A8/1.png" alt="1"></p><p>阿里云 flink 是提供了这个能力的。但是这个需要使用阿里云的产品才能使用。有钱人可以直接上。</p><p><a href="https://www.alibabacloud.com/help/zh/faq-detail/122722.htm?spm=a2c63.q38357.a3.7.a1227a53TBMuSY">https://www.alibabacloud.com/help/zh/faq-detail/122722.htm?spm=a2c63.q38357.a3.7.a1227a53TBMuSY</a></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/03_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E7%BB%B4%E8%A1%A8/2.png" alt="2"></p><p>因此本文在介绍怎样自定义一个 sql 数据维表的同时，实现一个 sql redis 来给大家使用。</p><h1 id="3-目标篇-做-redis-维表预期效果是什么"><a href="#3-目标篇-做-redis-维表预期效果是什么" class="headerlink" title="3.目标篇-做 redis 维表预期效果是什么"></a>3.目标篇-做 redis 维表预期效果是什么</h1><p>redis 作为维表在 datastream 中的最常用的数据结构就是 kv、hmap 两种。本文实现主要实现 kv 结构，map 结构大家可以拿到源码之后进行自定义实现。也就多加几行代码就完事了。</p><p>预期效果就如阿里云的 flink redis：</p><p>下面是我在本地跑的结果，先看看 redis 中存储的数据，只有这一条数据，是 json 字符串：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/03_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E7%BB%B4%E8%A1%A8/9.png" alt="9"></p><p>下面是预期 flink sql：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> dimTable (</span><br><span class="line">    name STRING,</span><br><span class="line">    name1 STRING,</span><br><span class="line">    score <span class="type">BIGINT</span>  <span class="comment">-- redis 中存储数据的 schema</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;redis&#x27;</span>, <span class="comment">-- 指定 connector 是 redis 类型的</span></span><br><span class="line">    <span class="string">&#x27;hostname&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;127.0.0.1&#x27;</span>, <span class="comment">-- redis server ip</span></span><br><span class="line">    <span class="string">&#x27;port&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;6379&#x27;</span>, <span class="comment">-- redis server 端口</span></span><br><span class="line">    <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span> <span class="comment">-- 指定 format 解析格式</span></span><br><span class="line">    <span class="string">&#x27;lookup.cache.max-rows&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;500&#x27;</span>, <span class="comment">-- guava local cache 最大条目</span></span><br><span class="line">    <span class="string">&#x27;lookup.cache.ttl&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;3600&#x27;</span>, <span class="comment">-- guava local cache ttl</span></span><br><span class="line">    <span class="string">&#x27;lookup.max-retries&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;1&#x27;</span> <span class="comment">-- redis 命令执行失败后重复次数</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> o.f0, o.f1, c.name, c.name1, c.score</span><br><span class="line"><span class="keyword">FROM</span> leftTable <span class="keyword">AS</span> o</span><br><span class="line"><span class="comment">-- 维表 join</span></span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> dimTable <span class="keyword">FOR</span> <span class="built_in">SYSTEM_TIME</span> <span class="keyword">AS</span> <span class="keyword">OF</span> o.proctime <span class="keyword">AS</span> c</span><br><span class="line"><span class="keyword">ON</span> o.f0 <span class="operator">=</span> c.name</span><br></pre></td></tr></table></figure><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/03_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E7%BB%B4%E8%A1%A8/10.png" alt="10"></p><p>结果如下，后面三列就对应到 <code>c.name, c.name1, c.score</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">+I[a, b, namehhh, namehhh112, 3]</span><br><span class="line">+I[a, b, namehhh, namehhh112, 3]</span><br><span class="line">+I[a, b, namehhh, namehhh112, 3]</span><br><span class="line">+I[a, b, namehhh, namehhh112, 3]</span><br><span class="line">+I[a, b, namehhh, namehhh112, 3]</span><br><span class="line">+I[a, b, namehhh, namehhh112, 3]</span><br><span class="line">+I[a, b, namehhh, namehhh112, 3]</span><br><span class="line">+I[a, b, namehhh, namehhh112, 3]</span><br><span class="line">+I[a, b, namehhh, namehhh112, 3]</span><br></pre></td></tr></table></figure><h1 id="4-难点剖析篇-目前有哪些实现"><a href="#4-难点剖析篇-目前有哪些实现" class="headerlink" title="4.难点剖析篇-目前有哪些实现"></a>4.难点剖析篇-目前有哪些实现</h1><p>目前可以从网上搜到的实现、以及可以参考的实现有以下两个：</p><ol><li><a href="https://github.com/jeff-zou/flink-connector-redis。">https://github.com/jeff-zou/flink-connector-redis。</a> 但是其没有实现 flink sql redis 维表，只实现了 sink 表，并且使用起来有比较多的限制，包括需要在建表时就指定 key-column，value-column 等，其实博主觉得没必要指定这些字段，这些都可以动态调整。其实现是对 apache-bahir-flink <a href="https://github.com/apache/bahir-flink">https://github.com/apache/bahir-flink</a> 的二次开发，但与 bahir 原生实现有割裂感，因为这个项目几乎重新实现了一遍，接口也和 bahir 不同。</li><li>阿里云实现 <a href="https://www.alibabacloud.com/help/zh/faq-detail/122722.htm?spm=a2c63.q38357.a3.7.a1227a53TBMuSY。">https://www.alibabacloud.com/help/zh/faq-detail/122722.htm?spm=a2c63.q38357.a3.7.a1227a53TBMuSY。</a> 可以参考的只有用法和配置等。但是有些配置项也属于阿里自定义的。</li></ol><p>因此博主在实现时，就定了一个基调。</p><ol><li>复用 connector：复用 bahir 提供的 redis connnector</li><li>复用 format：复用 flink 目前的 format 机制，目前这个上述两个实现都没有做到</li><li>简洁性：实现 kv 结构。hget 封装一部分</li><li>维表 local cache：为避免高频率访问 redis，维表加了 local cache 作为缓存</li></ol><h1 id="5-维表实现篇-维表实现的过程"><a href="#5-维表实现篇-维表实现的过程" class="headerlink" title="5.维表实现篇-维表实现的过程"></a>5.维表实现篇-维表实现的过程</h1><p>在实现 redis 维表之前，不得不谈谈 flink 维表加载和使用机制。</p><h2 id="5-1-flink-维表原理"><a href="#5-1-flink-维表原理" class="headerlink" title="5.1.flink 维表原理"></a>5.1.flink 维表原理</h2><p>其实上节已经详细描述了 flink sql 对于 source\sink 的加载机制，维表属于 source 的中的 lookup 表，在具体 flink 程序运行的过程之中可以简单的理解为一个 map，在 map 中调用 redis-client 接口访问 redis 进行扩充维度的过程。</p><ol><li>通过 SPI 机制加载所有的 source\sink\format 工厂 <code>Factory</code></li><li>过滤出 DynamicTableSourceFactory + connector 标识的 source 工厂类</li><li>通过 source 工厂类创建出对应的 source</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/03_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E7%BB%B4%E8%A1%A8/7.png" alt="7"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/03_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E7%BB%B4%E8%A1%A8/5.png" alt="5"></p><p>如图 source 和 sink 是通过 <code>FactoryUtil.createTableSource</code> 和 <code>FactoryUtil.createTableSink</code> 创建的</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/03_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E7%BB%B4%E8%A1%A8/4.png" alt="4"></p><p>所有通过 SPI 的 source\sink\formt 插件都继承自 <code>Factory</code>。</p><p>整体创建 source 方法的调用链如下图。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/03_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E7%BB%B4%E8%A1%A8/6.png" alt="6"></p><h2 id="5-2-flink-维表实现方案"><a href="#5-2-flink-维表实现方案" class="headerlink" title="5.2.flink 维表实现方案"></a>5.2.flink 维表实现方案</h2><p>先看下博主的最终实现。</p><p>总重要的三个实现类：</p><ol><li><code>RedisDynamicTableFactory</code></li><li><code>RedisDynamicTableSource</code></li><li><code>RedisRowDataLookupFunction</code></li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/03_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E7%BB%B4%E8%A1%A8/8.png" alt="8"></p><p>具体流程：</p><ol><li>定义 SPI 的工厂类 <code>RedisDynamicTableFactory implements DynamicTableSourceFactory</code>，并且在 resource\META-INF 下创建 SPI 的插件文件</li><li>实现 factoryIdentifier 标识 <code>redis</code></li><li>实现 <code>RedisDynamicTableFactory#createDynamicTableSource</code> 来创建对应的 source <code>RedisDynamicTableSource</code></li><li>定义 <code>RedisDynamicTableSource implements LookupTableSource</code></li><li>实现 <code>RedisDynamicTableFactory#getLookupRuntimeProvider</code> 方法，创建具体的维表 UDF  <code>TableFunction&lt;T&gt;</code>，定义为 <code>RedisRowDataLookupFunction</code></li><li>实现 <code>RedisRowDataLookupFunction</code> 的 eval 方法，这个方法就是用于访问 redis 扩充维度的。</li></ol><p>介绍完流程，进入具体实现方案细节：</p><p><code>RedisDynamicTableFactory</code> 主要创建 source 的逻辑：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RedisDynamicTableFactory</span> <span class="keyword">implements</span> <span class="title">DynamicTableSourceFactory</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">factoryIdentifier</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 标识 redis</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;redis&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> DynamicTableSource <span class="title">createDynamicTableSource</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// either implement your custom validation logic here ...</span></span><br><span class="line">        <span class="comment">// or use the provided helper utility</span></span><br><span class="line">        <span class="keyword">final</span> FactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(<span class="keyword">this</span>, context);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// discover a suitable decoding format</span></span><br><span class="line">        <span class="comment">// format 实现</span></span><br><span class="line">        <span class="keyword">final</span> DecodingFormat&lt;DeserializationSchema&lt;RowData&gt;&gt; decodingFormat = helper.discoverDecodingFormat(</span><br><span class="line">                DeserializationFormatFactory.class,</span><br><span class="line">                FactoryUtil.FORMAT);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// validate all options</span></span><br><span class="line">        <span class="comment">// 所有 option 配置的校验，比如 cache 类参数</span></span><br><span class="line">        helper.validate();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// get the validated options</span></span><br><span class="line">        <span class="keyword">final</span> ReadableConfig options = helper.getOptions();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> RedisLookupOptions redisLookupOptions = RedisOptions.getRedisLookupOptions(options);</span><br><span class="line"></span><br><span class="line">        TableSchema schema = context.getCatalogTable().getSchema();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建 RedisDynamicTableSource</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> RedisDynamicTableSource(</span><br><span class="line">                schema.toPhysicalRowDataType()</span><br><span class="line">                , decodingFormat</span><br><span class="line">                , redisLookupOptions);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>resources\META-INF 文件：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/03_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E7%BB%B4%E8%A1%A8/13.png" alt="13"></p><p><code>RedisDynamicTableSource</code> 主要创建 table udf 的逻辑：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RedisDynamicTableSource</span> <span class="keyword">implements</span> <span class="title">LookupTableSource</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> LookupRuntimeProvider <span class="title">getLookupRuntimeProvider</span><span class="params">(LookupContext context)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 初始化 redis 客户端配置</span></span><br><span class="line">        FlinkJedisConfigBase flinkJedisConfigBase = <span class="keyword">new</span> FlinkJedisPoolConfig.Builder()</span><br><span class="line">                .setHost(<span class="keyword">this</span>.redisLookupOptions.getHostname())</span><br><span class="line">                .setPort(<span class="keyword">this</span>.redisLookupOptions.getPort())</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// redis key，value 序列化器</span></span><br><span class="line">        LookupRedisMapper lookupRedisMapper = <span class="keyword">new</span> LookupRedisMapper(</span><br><span class="line">                <span class="keyword">this</span>.createDeserialization(context, <span class="keyword">this</span>.decodingFormat, createValueFormatProjection(<span class="keyword">this</span>.physicalDataType)));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建 table udf</span></span><br><span class="line">        <span class="keyword">return</span> TableFunctionProvider.of(<span class="keyword">new</span> RedisRowDataLookupFunction(</span><br><span class="line">                flinkJedisConfigBase</span><br><span class="line">                , lookupRedisMapper</span><br><span class="line">                , <span class="keyword">this</span>.redisLookupOptions));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>RedisRowDataLookupFunction</code> table udf 执行维表关联的主要流程：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RedisRowDataLookupFunction</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>&lt;<span class="title">RowData</span>&gt; </span>&#123;</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 具体 redis 执行方法</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eval</span><span class="params">(Object... objects)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> retry = <span class="number">0</span>; retry &lt;= maxRetryTimes; retry++) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">// fetch result</span></span><br><span class="line">                <span class="keyword">this</span>.evaler.accept(objects);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                LOG.error(String.format(<span class="string">&quot;HBase lookup error, retry times = %d&quot;</span>, retry), e);</span><br><span class="line">                <span class="keyword">if</span> (retry &gt;= maxRetryTimes) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;Execution of Redis lookup failed.&quot;</span>, e);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    Thread.sleep(<span class="number">1000</span> * retry);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e1) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(e1);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(FunctionContext context)</span> </span>&#123;</span><br><span class="line">        LOG.info(<span class="string">&quot;start open ...&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// redis 命令执行器，初始化 redis 链接</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">this</span>.redisCommandsContainer =</span><br><span class="line">                    RedisCommandsContainerBuilder</span><br><span class="line">                            .build(<span class="keyword">this</span>.flinkJedisConfigBase);</span><br><span class="line">            <span class="keyword">this</span>.redisCommandsContainer.open();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            LOG.error(<span class="string">&quot;Redis has not been properly initialized: &quot;</span>, e);</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(e);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 初始化 local cache</span></span><br><span class="line">        <span class="keyword">this</span>.cache = cacheMaxSize &lt;= <span class="number">0</span> || cacheExpireMs &lt;= <span class="number">0</span> ? <span class="keyword">null</span> : CacheBuilder.newBuilder()</span><br><span class="line">                .recordStats()</span><br><span class="line">                .expireAfterWrite(cacheExpireMs, TimeUnit.MILLISECONDS)</span><br><span class="line">                .maximumSize(cacheMaxSize)</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (cache != <span class="keyword">null</span>) &#123;</span><br><span class="line">            context.getMetricGroup()</span><br><span class="line">                    .gauge(<span class="string">&quot;lookupCacheHitRate&quot;</span>, (Gauge&lt;Double&gt;) () -&gt; cache.stats().hitRate());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="keyword">this</span>.evaler = in -&gt; &#123;</span><br><span class="line">                RowData cacheRowData = cache.getIfPresent(in);</span><br><span class="line">                <span class="keyword">if</span> (cacheRowData != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    collect(cacheRowData);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// fetch result</span></span><br><span class="line">                    <span class="keyword">byte</span>[] key = lookupRedisMapper.serialize(in);</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">byte</span>[] value = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">switch</span> (redisCommand) &#123;</span><br><span class="line">                        <span class="keyword">case</span> GET:</span><br><span class="line">                            value = <span class="keyword">this</span>.redisCommandsContainer.get(key);</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                        <span class="keyword">case</span> HGET:</span><br><span class="line">                            value = <span class="keyword">this</span>.redisCommandsContainer.hget(key, <span class="keyword">this</span>.additionalKey.getBytes());</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                        <span class="keyword">default</span>:</span><br><span class="line">                            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;Cannot process such data type: &quot;</span> + redisCommand);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    RowData rowData = <span class="keyword">this</span>.lookupRedisMapper.deserialize(value);</span><br><span class="line"></span><br><span class="line">                    collect(rowData);</span><br><span class="line"></span><br><span class="line">                    cache.put(key, rowData);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="5-2-1-复用-bahir-connector"><a href="#5-2-1-复用-bahir-connector" class="headerlink" title="5.2.1.复用 bahir connector"></a>5.2.1.复用 bahir connector</h3><p>如图是 bahir redis connector 的实现。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/03_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E7%BB%B4%E8%A1%A8/11.png" alt="11"></p><p>博主在实现过程中将能复用的都尽力复用。如图是最终实现目录。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/03_flinksql%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9Asql%E8%87%AA%E5%AE%9A%E4%B9%89redis%E6%95%B0%E6%8D%AE%E7%BB%B4%E8%A1%A8/12.png" alt="12"></p><p>可以看到目录结构是与 bahir redis connector 一致的。</p><p>其中 <code>redis 客户端及其配置</code> 是直接复用了 bahir redis 的。由于 bahir redis 基本都是 sink 实现，某些实现没法继承复用，所以这里我单独开辟了目录，<code>redis 命令执行器</code> 和 <code>redis 命令定义器</code>，但是也基本和 bahir 一致。<br>如果你想要在生产环境中进行使用，可以直接将两部分代码合并，成本很低。</p><h3 id="5-2-2-复用-format"><a href="#5-2-2-复用-format" class="headerlink" title="5.2.2.复用 format"></a>5.2.2.复用 format</h3><p>博主直接复用了 flink 本身自带的 format 机制来作为维表反序列化机制。参考 HBase connector 实现将 cache 命中率添加到 metric 中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RedisDynamicTableFactory</span> <span class="keyword">implements</span> <span class="title">DynamicTableSourceFactory</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> DynamicTableSource <span class="title">createDynamicTableSource</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        <span class="comment">// discover a suitable decoding format</span></span><br><span class="line">        <span class="comment">// 复用 format 实现</span></span><br><span class="line">        <span class="keyword">final</span> DecodingFormat&lt;DeserializationSchema&lt;RowData&gt;&gt; decodingFormat = helper.discoverDecodingFormat(</span><br><span class="line">                DeserializationFormatFactory.class,</span><br><span class="line">                FactoryUtil.FORMAT);</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>format 同样也是 SPI 机制加载。</p><p>源码公众号后台回复<strong>flink sql 知其所以然（二）| sql 自定义 redis 数据维表</strong>获取。</p><h2 id="5-2-3-维表-local-cache"><a href="#5-2-3-维表-local-cache" class="headerlink" title="5.2.3.维表 local cache"></a>5.2.3.维表 local cache</h2><p>local cache 在初始化时可以指定 cache 大小，缓存时长等。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">this</span>.evaler = in -&gt; &#123;</span><br><span class="line">    RowData cacheRowData = cache.getIfPresent(in);</span><br><span class="line">    <span class="keyword">if</span> (cacheRowData != <span class="keyword">null</span>) &#123;</span><br><span class="line">        collect(cacheRowData);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// fetch result</span></span><br><span class="line">        <span class="keyword">byte</span>[] key = lookupRedisMapper.serialize(in);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">byte</span>[] value = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">switch</span> (redisCommand) &#123;</span><br><span class="line">            <span class="keyword">case</span> GET:</span><br><span class="line">                value = <span class="keyword">this</span>.redisCommandsContainer.get(key);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> HGET:</span><br><span class="line">                value = <span class="keyword">this</span>.redisCommandsContainer.hget(key, <span class="keyword">this</span>.additionalKey.getBytes());</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;Cannot process such data type: &quot;</span> + redisCommand);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        RowData rowData = <span class="keyword">this</span>.lookupRedisMapper.deserialize(value);</span><br><span class="line"></span><br><span class="line">        collect(rowData);</span><br><span class="line"></span><br><span class="line">        cache.put(key, rowData);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h1 id="6-总结与展望篇"><a href="#6-总结与展望篇" class="headerlink" title="6.总结与展望篇"></a>6.总结与展望篇</h1><h2 id="6-1-总结"><a href="#6-1-总结" class="headerlink" title="6.1.总结"></a>6.1.总结</h2><p>本文主要是针对 flink sql redis 维表进行了扩展以及实现，并且复用 bahir redis connector 的配置，具有良好的扩展性。<br>如果你正好需要这么一个 connector，直接公众号后台回复<strong>flink sql 知其所以然（二）| sql 自定义 redis 数据维表</strong>获取源码吧。</p><h2 id="6-2-展望"><a href="#6-2-展望" class="headerlink" title="6.2.展望"></a>6.2.展望</h2><p>当然上述只是 redis 维表一个基础的实现，用于生产环境还有很多方面可以去扩展的。</p><ol><li>jedis cluster 的扩展：目前 bahir datastream 中已经实现了，可以直接参考，扩展起来非常简单</li><li>aync lookup 维表的扩展：目前 hbase lookup 表已经实现了，可以直接参考实现</li><li>异常 AOP，alert 等</li></ol>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink sql 知其所以然（一）| source\sink 原理</title>
    <link href="https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/02_source_sink/"/>
    <id>https://yangyichao-mango.github.io/2021/11/13/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/02_source_sink/</id>
    <published>2021-11-13T06:21:53.000Z</published>
    <updated>2021-08-14T13:34:55.961Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>感谢您的<strong>关注  +  点赞 + 再看</strong>，对博主的肯定，会督促博主持续的输出更多的优质实战内容！！！</p></blockquote><h1 id="1-序篇-本文结构"><a href="#1-序篇-本文结构" class="headerlink" title="1.序篇-本文结构"></a>1.序篇-本文结构</h1><p>本文从以下五个小节介绍 flink sql source\sink\format 的概念、原理。</p><ol><li>背景篇-关于 sql</li><li>定义篇-sql source、sink</li><li>实战篇-sql source、sink 的用法</li><li>原理剖析篇-sql source、sink 是怎么跑起来的</li><li>总结与展望篇</li></ol><h1 id="2-背景篇-关于-sql"><a href="#2-背景篇-关于-sql" class="headerlink" title="2.背景篇-关于 sql"></a>2.背景篇-关于 sql</h1><p>关于 flink sql 的定位。</p><p>先聊聊使用 sql 的原因，总结来说就是一切从简。</p><ul><li>SQL 属于 DSL</li><li>SQL 易于理解</li><li>SQL 内置多种查询优化器</li><li>SQL 稳定的语言</li><li>SQL 易于管理</li><li>SQL 利于流批一体</li></ul><p>目前 1.13 版本的 SQL 已经集成了大量高效、易用的 feature。本系列教程也是基于 1.13.1。</p><h1 id="3-定义篇-sql-source、sink"><a href="#3-定义篇-sql-source、sink" class="headerlink" title="3.定义篇-sql source、sink"></a>3.定义篇-sql source、sink</h1><p>本文会简单介绍一些 flink sql 的 source、sink 的定义、使用方法，会着重切介绍其对应框架设计和实现。详细解析一下从一条 create table sql 到具体的算子层面的整个流程。</p><blockquote><p>Notes：<br>在 flink sql 中，source 有两种表，一种是数据源表，一种是数据维表。<br>数据源表就是有源源不断的数据的表。比如 mq。<br>数据维表就是用来给某些数据扩充维度使用的。比如 redis，mysql，一般都是做扩容维度的维表 join 使用。</p><p><strong>本节主要介绍数据源表，数据维表的整个流程和数据源表几乎一样。下文中的 source 默认都为数据源表。</strong></p></blockquote><p>首先在介绍 sql 之前，我们先来看看 datastream 中定义一个 source 需要的最基本的内容。</p><ol><li><strong>source、sink 的 connector 连接配置信息</strong>。比如 datastream api kafka connector 的 properties，topic 名称。</li><li><strong>source、sink 的序列化方式信息</strong>。比如 datastream api kafka connector 的 DeserializationSchema，SerializationSchema。</li><li><strong>source、sink 的字段信息</strong>。比如 datastream api kafka connector 的序列化或者反序列化出来的 Model 所包含的字段信息。</li><li><strong>source、sink 对象</strong>。比如 datastream api kafka connector source 对应的具体 java 对象。</li></ol><p>sql 中的 source、sink 所包含的基本点其实和 datastream 都是相同的，可以将 sql 中的一些语法给映射到 datastream 中来帮助快速理解 sql：</p><ol><li><strong>sql source、sink connector\properties</strong>。可以对应到 datastream api kafka connector 的 properties，topic 名称。</li><li><strong>sql source、sink format</strong>。可以对应到 datastream api kafka connector 的 DeserializationSchema，SerializationSchema。</li><li><strong>sql source、sink field</strong>。可以对应到 datastream api kafka connector 的序列化或者反序列化出来的 Model 所包含的字段信息。</li><li><strong>sql source、sink catalog_name、db_name、table_name</strong>。可以对应到 datastream api kafka connector source 对应的具体 java 对象。</li><li><strong>sql 本身的特性</strong>。比如某些场景下需要将 sql schema 持久化，会用到 hive catalog 等，这个可以说是 sql 目前比 datastream api 多的一个特性。但是仔细想想，其实 datastream 也能够拓展这样的能力，其实就是将某个 datastream 注册到外部存储中（可以，但对 datastream 来说没必要）。</li></ol><p>来看看官网的文档 create table schema 的描述，可以发现就是围绕着上面这五点展开的。 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/zh/docs/dev/table/sql/create/#create-table。">https://ci.apache.org/projects/flink/flink-docs-release-1.13/zh/docs/dev/table/sql/create/#create-table。</a></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [catalog_name.][db_name.]table_name</span><br><span class="line">  (</span><br><span class="line">    &#123; &lt;physical_column_definition&gt; | &lt;metadata_column_definition&gt; | &lt;computed_column_definition&gt; &#125;[ , ...n]</span><br><span class="line">    [ <span class="operator">&lt;</span>watermark_definition<span class="operator">&gt;</span> ]</span><br><span class="line">    [ <span class="operator">&lt;</span>table_constraint<span class="operator">&gt;</span> ][ , ...n]</span><br><span class="line">  )</span><br><span class="line">  [COMMENT table_comment]</span><br><span class="line">  [PARTITIONED <span class="keyword">BY</span> (partition_column_name1, partition_column_name2, ...)]</span><br><span class="line">  <span class="keyword">WITH</span> (key1<span class="operator">=</span>val1, key2<span class="operator">=</span>val2, ...)</span><br><span class="line">  [ <span class="keyword">LIKE</span> source_table [( <span class="operator">&lt;</span>like_options<span class="operator">&gt;</span> )] ]</span><br><span class="line">   </span><br><span class="line"><span class="operator">&lt;</span>physical_column_definition<span class="operator">&gt;</span>:</span><br><span class="line">  column_name column_type [ <span class="operator">&lt;</span>column_constraint<span class="operator">&gt;</span> ] [COMMENT column_comment]</span><br><span class="line">  </span><br><span class="line"><span class="operator">&lt;</span>column_constraint<span class="operator">&gt;</span>:</span><br><span class="line">  [<span class="keyword">CONSTRAINT</span> constraint_name] <span class="keyword">PRIMARY</span> KEY <span class="keyword">NOT</span> ENFORCED</span><br><span class="line"></span><br><span class="line"><span class="operator">&lt;</span>table_constraint<span class="operator">&gt;</span>:</span><br><span class="line">  [<span class="keyword">CONSTRAINT</span> constraint_name] <span class="keyword">PRIMARY</span> KEY (column_name, ...) <span class="keyword">NOT</span> ENFORCED</span><br><span class="line"></span><br><span class="line"><span class="operator">&lt;</span>metadata_column_definition<span class="operator">&gt;</span>:</span><br><span class="line">  column_name column_type METADATA [ <span class="keyword">FROM</span> metadata_key ] [ VIRTUAL ]</span><br><span class="line"></span><br><span class="line"><span class="operator">&lt;</span>computed_column_definition<span class="operator">&gt;</span>:</span><br><span class="line">  column_name <span class="keyword">AS</span> computed_column_expression [COMMENT column_comment]</span><br><span class="line"></span><br><span class="line"><span class="operator">&lt;</span>watermark_definition<span class="operator">&gt;</span>:</span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> rowtime_column_name <span class="keyword">AS</span> watermark_strategy_expression</span><br><span class="line"></span><br><span class="line"><span class="operator">&lt;</span>source_table<span class="operator">&gt;</span>:</span><br><span class="line">  [catalog_name.][db_name.]table_name</span><br><span class="line"></span><br><span class="line"><span class="operator">&lt;</span>like_options<span class="operator">&gt;</span>:</span><br><span class="line">&#123;</span><br><span class="line">   &#123; INCLUDING | EXCLUDING &#125; &#123; ALL | CONSTRAINTS | PARTITIONS &#125;</span><br><span class="line"> | &#123; INCLUDING | EXCLUDING | OVERWRITING &#125; &#123; GENERATED | OPTIONS | WATERMARKS &#125; </span><br><span class="line">&#125;[, ...]</span><br></pre></td></tr></table></figure><p>结合我们刚刚说的 sql source、sink 中主要包含 5 点解释一下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] <span class="comment">-- sql source、sink catalog_name、db_name、table_name</span></span><br><span class="line">(</span><br><span class="line">   <span class="comment">-- sql source、sink field 字段信息</span></span><br><span class="line">) <span class="keyword">WITH</span> </span><br><span class="line">( </span><br><span class="line">   <span class="comment">-- sql source、sink connector\properties 连接配置</span></span><br><span class="line">   <span class="comment">-- sql source、sink format</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>来个 kafka source 的例子：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> KafkaTable ( <span class="comment">-- sql source、sink catalog_name、db_name、table_name</span></span><br><span class="line">  `f0` STRING, <span class="comment">-- sql source、sink 的字段信息</span></span><br><span class="line">  `f1` STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>, <span class="comment">-- sql source、sink 的 connector 连接配置</span></span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;topic&#x27;</span>, <span class="comment">-- sql source、sink 的 connector 连接配置</span></span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>, <span class="comment">-- sql source、sink 的 connector 连接配置</span></span><br><span class="line">  <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>, <span class="comment">-- sql source、sink 的 connector 连接配置</span></span><br><span class="line">  <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span> <span class="comment">-- sql source、sink 的序列化方式信息</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>其对应的 datastream 写法如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;localhost:9092&quot;</span>);</span><br><span class="line">properties.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;testGroup&quot;</span>);</span><br><span class="line"></span><br><span class="line">DeserializationSchema&lt;Tuple2&lt;String, String&gt;&gt; d = <span class="keyword">new</span> AbstractDeserializationSchema&lt;Tuple2&lt;String, String&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Tuple2&lt;String, String&gt; <span class="title">deserialize</span><span class="params">(<span class="keyword">byte</span>[] message)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> json 解析为 tuple2 此处省略;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;String, String&gt;&gt; stream = env</span><br><span class="line">        .addSource(<span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(<span class="string">&quot;topic&quot;</span>, d, properties));</span><br></pre></td></tr></table></figure><p>将 sql source 和 datastream source 的组成部分互相映射起来可以得到下图，其中 datastream、sql 中颜色相同的属性互相对应：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/02_source_sink/2.png" alt="2"></p><p>可以看到，将所有的 sql 关系代数都映射到 datastream api 上，会有助于我们快速理解。</p><h1 id="4-实战篇-sql-source、sink-的用法"><a href="#4-实战篇-sql-source、sink-的用法" class="headerlink" title="4.实战篇-sql source、sink 的用法"></a>4.实战篇-sql source、sink 的用法</h1><p>直接见官网 Table API Connectors。已经描述的非常详细了，本文侧重原理，所以此处不多赘述。</p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/connectors/table/overview/">https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/connectors/table/overview/</a></p><p><a href="https://www.alibabacloud.com/help/zh/faq-list/62516.htm?spm=a2c63.p38356.b99.212.3c1a1442x9AY7m">https://www.alibabacloud.com/help/zh/faq-list/62516.htm?spm=a2c63.p38356.b99.212.3c1a1442x9AY7m</a></p><h1 id="5-原理剖析篇-sql-source、sink-是怎么跑起来的"><a href="#5-原理剖析篇-sql-source、sink-是怎么跑起来的" class="headerlink" title="5.原理剖析篇-sql source、sink 是怎么跑起来的"></a>5.原理剖析篇-sql source、sink 是怎么跑起来的</h1><p>关于 sql 具体工作原理可以参考 <a href="https://zhuanlan.zhihu.com/p/157265381。">https://zhuanlan.zhihu.com/p/157265381。</a></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/02_source_sink/3.jpeg" alt="3"></p><p>但是很多刚接触 flink sql 的读者看完这篇文章，会感觉到还没准备好就来了这么大一堆密集的信息。那么</p><ol><li><p>我到底应该从哪里看起呢？</p></li><li><p>能理解 sql 会映射到具体的算子执行。但是它具体是怎么对应到具体的算子上的呢？</p></li></ol><p>博主会从以下两个角度去帮大家理清楚整个流程。</p><ol><li>先抛开 flink sql、datastream 提供的能力来说，如果你在自己的一个程序中去接入一个数据源，你最关心的是哪些组件？</li></ol><p>答：消费一个数据源最重要的就是 connector（负责链接外部组件，消费数据） + serde（负责序列化成 flink 认识的变量形式）。</p><ol start="2"><li>结合第一个问题 + 一段简单的 flink sql 代码来看看 flink 是怎么去做这件事情的。</li></ol><p>代码（基于 1.13.1）：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaSourceTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(<span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        EnvironmentSettings settings = EnvironmentSettings</span><br><span class="line">                .newInstance()</span><br><span class="line">                .useBlinkPlanner()</span><br><span class="line">                .inStreamingMode().build();</span><br><span class="line"></span><br><span class="line">        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, settings);</span><br><span class="line"></span><br><span class="line">        tEnv.executeSql(</span><br><span class="line">                <span class="string">&quot;CREATE TABLE KafkaSourceTable (\n&quot;</span></span><br><span class="line">                        + <span class="string">&quot;  `f0` STRING,\n&quot;</span></span><br><span class="line">                        + <span class="string">&quot;  `f1` STRING\n&quot;</span></span><br><span class="line">                        + <span class="string">&quot;) WITH (\n&quot;</span></span><br><span class="line">                        + <span class="string">&quot;  &#x27;connector&#x27; = &#x27;kafka&#x27;,\n&quot;</span></span><br><span class="line">                        + <span class="string">&quot;  &#x27;topic&#x27; = &#x27;topic&#x27;,\n&quot;</span></span><br><span class="line">                        + <span class="string">&quot;  &#x27;properties.bootstrap.servers&#x27; = &#x27;localhost:9092&#x27;,\n&quot;</span></span><br><span class="line">                        + <span class="string">&quot;  &#x27;properties.group.id&#x27; = &#x27;testGroup&#x27;,\n&quot;</span></span><br><span class="line">                        + <span class="string">&quot;  &#x27;format&#x27; = &#x27;json&#x27;\n&quot;</span></span><br><span class="line">                        + <span class="string">&quot;)&quot;</span></span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        Table t = tEnv.sqlQuery(<span class="string">&quot;SELECT * FROM KafkaSourceTable&quot;</span>);</span><br><span class="line"></span><br><span class="line">        tEnv.toAppendStream(t, Row.class).print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到这段代码很简单，就是创建一个数据源表之后 select 数据 print。</p><p>通过上面这段 sql 映射出的 transformations 中发现，其实 flink 中最关键变量的也就是我们刚刚提出的第一个问题中的那两点：</p><ol><li><strong>sql source connector</strong> 是 <code>FlinkKafkaConsumer</code></li><li><strong>sql source format</strong> 是 <code>JsonRowDataDeserializationSchema</code></li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/02_source_sink/19.png" alt="19"></p><p>所以我们就可以从下面这三个方向（多出来的一个是配置信息）的问题去了解具体是怎么对应到具体的算子上的。</p><ol><li><strong>sql source connector</strong>：用户指定了 <code>connector = kafka</code>，flink 是怎么自动映射到 <code>FlinkKafkaConsumer</code> 的？</li><li><strong>sql source format</strong>：用户指定了 <code>format = json</code>，字段信息，flink 是怎么自动映射到 <code>JsonRowDataDeserializationSchema</code>，以及字段解析的？</li><li><strong>sql source properties</strong>：flink 是怎么自动将配置加载到 <code>FlinkKafkaConsumer</code> 中的？</li></ol><h2 id="5-1-connector-怎样映射到具体算子？"><a href="#5-1-connector-怎样映射到具体算子？" class="headerlink" title="5.1.connector 怎样映射到具体算子？"></a>5.1.connector 怎样映射到具体算子？</h2><p>引用官网图：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/02_source_sink/22.svg" alt="22"></p><blockquote><p>Notes：<br>其中 <code>LookupTableSource</code> 为数据维表。</p></blockquote><p>先说下结论，再跟一遍源码。</p><p>结论：</p><ol><li><strong>MetaData</strong>：将 sql create source table 转化为实际的 <code>CatalogTable</code>、翻译为 RelNode</li><li><strong>Planning</strong>：创建 RelNode 的过程中使用 SPI 将所有的 source（<code>DynamicTableSourceFactory</code>）\sink（<code>DynamicTableSinkFactory</code>） 工厂动态加载，获取到 connector = kafka，然后从所有 source 工厂中过滤出名称为 kafka + 继承自 <code>DynamicTableSourceFactory.class</code> 的工厂类 <code>KafkaDynamicTableFactory</code>，使用 <code>KafkaDynamicTableFactory</code> 创建出 <code>KafkaDynamicSource</code></li><li><strong>Runtime</strong>：<code>KafkaDynamicSource</code> 创建出 <code>FlinkKafkaConsumer</code>，负责 flink 程序实际运行。</li></ol><p>源码：</p><p>debug 代码，既然创建的是 <code>FlinkKafkaConsumer</code>，那我们就将断点打在 <code>FlinkKafkaConsumer</code> 的构造函数中。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/02_source_sink/5.png" alt="5"></p><p>如图可以发现当 debug 到当前断点时，已经进入 <code>FlinkKafkaConsumer</code> source 的创建阶段了，执行到这里的时候已经是完成了 sql connector 和具体实际 connector 的映射了。那么 connector 怎样映射到具体算子的过程呢？</p><p>我们往前回溯一下，定位到 <code>CatalogSourceTable</code> 中的 82 行（源码基于 1.13.1），发现 tableSource 已经是 <code>KafkaDynamicSource</code>，因此可以确定就是这一行代码将 connector = kafka 映射到 <code>FlinkKafkaConsumer</code> 的。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/02_source_sink/6.png" alt="6"></p><p>可以发现这段代码将包含了所有 sql create source table 中信息的 catalogTable 变量传入了。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/02_source_sink/7.png" alt="7"></p><p>进入这个方法后，可以看到是使用了 <code>FactoryUtil</code> 创建了 <code>DynamicTableSource</code>。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/02_source_sink/8.png" alt="8"></p><p>进入 <code>FactoryUtil.createTableSource</code> 后可以看到，就是最重要的两步操作。</p><ol><li>先获取 kafka 工厂对象。</li><li>使用 kafka 工厂对象创建出 kafka source。</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/02_source_sink/9.png" alt="9"></p><p>进入 <code>FactoryUtil.getDynamicTableFactory</code> 后：</p><ol><li>flink 是使用了 SPI 机制动态（SPI 机制天然插件化）的加载到了所有继承了 <code>Factory</code> 的工厂实例。通过截图可以看到有好多 source\sink\format Factory。关于 SPI 可以参考 <a href="https://www.jianshu.com/p/3a3edbcd8f24">https://www.jianshu.com/p/3a3edbcd8f24</a></li><li>通过 connector = kafka + <code>DynamicTableSourceFactory.class</code> 的标识去过滤出 <code>KafkaDynamicTableFactory</code>。</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/02_source_sink/10.png" alt="10"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/02_source_sink/11.png" alt="11"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/02_source_sink/12.png" alt="12"></p><p>然后 <code>KafkaDynamicTableFactory.createDynamicTableSource</code> 去创建对应的 source。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/02_source_sink/13.png" alt="13"></p><p>可以看到 <code>KafkaDynamicTableFactory.createDynamicTableSource</code> 中调用 <code>KafkaDynamicTableFactory.createKafkaTableSource</code> 来创建 <code>KafkaDynamicSource</code>。</p><p>基本上整个创建 Source 的流程就结束了。</p><h2 id="5-2-format-怎样映射到具体-serde？"><a href="#5-2-format-怎样映射到具体-serde？" class="headerlink" title="5.2.format 怎样映射到具体 serde？"></a>5.2.format 怎样映射到具体 serde？</h2><p>结论：</p><ol><li><strong>MetaData</strong>：和 connector 都一样</li><li><strong>Planning</strong>：format 是在创建 RelNode 的过程中，使用 <code>KafkaDynamicTableFactory</code> 创建出 <code>KafkaDynamicSource</code> 时，通过 SPI 去动态过滤出 format = json 并且继承自 <code>DeserializationFormatFactory.class</code> 的 format 工厂类 <code>JsonFormatFactory</code>。</li><li><strong>Runtime</strong>：<code>KafkaDynamicSource</code> 创建出 <code>FlinkKafkaConsumer</code> 时，实例化 serde 即 <code>JsonRowDataDeserializationSchema</code>，负责 flink 程序实际运行时的反序列化。</li></ol><p>源码：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/02_source_sink/15.png" alt="15"></p><p><code>KafkaDynamicTableFactory.createDynamicTableSource</code> 中获取反序列化 schema 定义。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/02_source_sink/16.png" alt="16"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/02_source_sink/17.png" alt="17"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/02_source_sink/18.png" alt="18"></p><ol><li>flink 是使用了 SPI 机制动态（SPI 机制天然插件化）的加载到了所有继承了 <code>Factory</code> 的 format 工厂实例。</li><li>通过 format = json 的标识并且继承自 <code>DeserializationFormatFactory.class</code> 去过滤出 <code>JsonFormatFactory</code>。</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/02_source_sink/20.png" alt="20"></p><h2 id="5-3-其他配置属性怎么加载？"><a href="#5-3-其他配置属性怎么加载？" class="headerlink" title="5.3.其他配置属性怎么加载？"></a>5.3.其他配置属性怎么加载？</h2><p>结论：</p><p>在 <code>KafkaDynamicTableFactory</code> 创建 <code>KafkaDynamicTable</code> 的过程中初始化。</p><p>源码：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/02_source_sink/14.png" alt="14"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/02_source_sink/21.png" alt="21"></p><h1 id="6-总结与展望篇"><a href="#6-总结与展望篇" class="headerlink" title="6.总结与展望篇"></a>6.总结与展望篇</h1><p>本文作为 flink sql 知其然系列的第一节，基于 1.13.1 版本 flink 介绍了 flink sql 的 source\sink\format 从 sql 变为可执行代码的原理。带大家过了一下源码。希望可以喜欢。</p><p>下节预告：flink sql 自定义 source\sink。</p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>踩坑记 | flink sql count_retract 还有这种坑！</title>
    <link href="https://yangyichao-mango.github.io/2021/11/12/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/03_%E8%B8%A9%E5%9D%91%E8%AE%B0_count_distinct_%E8%B4%9F%E5%80%BC/"/>
    <id>https://yangyichao-mango.github.io/2021/11/12/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/03_%E8%B8%A9%E5%9D%91%E8%AE%B0_count_distinct_%E8%B4%9F%E5%80%BC/</id>
    <published>2021-11-12T06:21:53.000Z</published>
    <updated>2021-07-31T06:03:19.219Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>感谢您的<strong>关注  +  点赞 + 再看</strong>，对博主的肯定，会督促博主持续的输出更多的优质实战内容！！！</p></blockquote><h1 id="1-序篇"><a href="#1-序篇" class="headerlink" title="1.序篇"></a>1.序篇</h1><p>通过本文你可了解到</p><ol><li>踩坑场景篇-这个坑是啥样的</li><li>问题排查篇-坑的排查过程</li><li>问题原理解析篇-导致问题的机制是什么</li><li>避坑篇-如何避免这种问题</li><li>展望篇-有什么机制可以根本避免这种情况</li></ol><p>先说下结论：在非窗口类 flink sql 任务中，会存在 retract 机制，即上游会向下游发送<strong>撤回消息（做减法）</strong>，<strong>最新的结果消息（做加法）</strong>两条消息来计算结果，保证结果正确性。</p><p>而如果我们在上下游中间使用了映射类 udf 改变了<strong>撤回消息（做减法）</strong>的一些字段值时，就可能会导致<strong>撤回消息（做减法）</strong>不能被正常处理，最终导致结果的错误。</p><h1 id="2-踩坑场景篇-这个坑是啥样的"><a href="#2-踩坑场景篇-这个坑是啥样的" class="headerlink" title="2.踩坑场景篇-这个坑是啥样的"></a>2.踩坑场景篇-这个坑是啥样的</h1><p>在介绍坑之前我们先介绍下我们的需求、实现方案的背景。</p><h2 id="2-1-背景"><a href="#2-1-背景" class="headerlink" title="2.1.背景"></a>2.1.背景</h2><p>在各类游戏中都会有一种场景，一个用户可以从 A 等级升级到 B 等级，用户可以不断的升级，但是一个用户同一时刻只会在同一个等级。需求指标就是当前分钟各个等级的用户数。</p><h2 id="2-2-预期效果"><a href="#2-2-预期效果" class="headerlink" title="2.2.预期效果"></a>2.2.预期效果</h2><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/03_%E8%B8%A9%E5%9D%91%E8%AE%B0_count_distinct_%E8%B4%9F%E5%80%BC/2.png" alt="2"></p><h2 id="2-3-解决思路"><a href="#2-3-解决思路" class="headerlink" title="2.3.解决思路"></a>2.3.解决思路</h2><ol><li>获取到当前所有用户的最新等级</li><li>一个用户同一时刻只会在一个等级，所以对每一个等级的用户做 count 操作</li></ol><h2 id="2-4-解决方案"><a href="#2-4-解决方案" class="headerlink" title="2.4.解决方案"></a>2.4.解决方案</h2><ol><li>获取到当前所有用户的最新等级：flink sql row_number() 就可以实现，按照数据的 rowtime 进行逆序排序就可以获取到用户当前最新的等级</li><li>对每一个等级的用户做 count 操作：对 row_number() 的后的明细结果进行 count 操作</li></ol><h3 id="2-4-1-sql"><a href="#2-4-1-sql" class="headerlink" title="2.4.1.sql"></a>2.4.1.sql</h3><p>具体实现 sql 如下，非常简单：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">WITH</span> detail_tmp <span class="keyword">AS</span> (</span><br><span class="line">  <span class="keyword">SELECT</span></span><br><span class="line">    等级,</span><br><span class="line">    id,</span><br><span class="line">    `<span class="type">timestamp</span>`</span><br><span class="line">  <span class="keyword">FROM</span></span><br><span class="line">    (</span><br><span class="line">      <span class="keyword">SELECT</span></span><br><span class="line">        等级,</span><br><span class="line">        id,</span><br><span class="line">        `<span class="type">timestamp</span>`,</span><br><span class="line">        <span class="comment">-- row_number 获取最新状态</span></span><br><span class="line">        <span class="built_in">row_number</span>() <span class="keyword">over</span>(</span><br><span class="line">          <span class="keyword">PARTITION</span> <span class="keyword">by</span> id</span><br><span class="line">          <span class="keyword">ORDER</span> <span class="keyword">BY</span></span><br><span class="line">            `<span class="type">timestamp</span>` <span class="keyword">DESC</span></span><br><span class="line">        ) <span class="keyword">AS</span> rn</span><br><span class="line">      <span class="keyword">FROM</span></span><br><span class="line">        source_db.source_table</span><br><span class="line">    )</span><br><span class="line">  <span class="keyword">WHERE</span></span><br><span class="line">    rn <span class="operator">=</span> <span class="number">1</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  DIM.中文等级 <span class="keyword">as</span> 等级,</span><br><span class="line">  <span class="built_in">sum</span>(part_uv) <span class="keyword">as</span> uv</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">SELECT</span></span><br><span class="line">      等级,</span><br><span class="line">      <span class="built_in">count</span>(id) <span class="keyword">as</span> part_uv</span><br><span class="line">    <span class="keyword">FROM</span></span><br><span class="line">      detail_tmp</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">      等级,</span><br><span class="line">      <span class="built_in">mod</span>(id, <span class="number">1024</span>)</span><br><span class="line">  )</span><br><span class="line"><span class="comment">-- 上游数据的等级名称是数字，需求方要求给转换成中文，所以这里加了一个 udf 映射</span></span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> <span class="keyword">LATERAL</span> <span class="keyword">TABLE</span>(等级中文映射_UDF(等级)) <span class="keyword">AS</span> DIM(中文等级) <span class="keyword">ON</span> <span class="literal">TRUE</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  DIM.中文等级</span><br></pre></td></tr></table></figure><h3 id="2-4-2-参数配置"><a href="#2-4-2-参数配置" class="headerlink" title="2.4.2.参数配置"></a>2.4.2.参数配置</h3><p>使用 minibatch 参数方式控制数据输出频率。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">table.exec.mini-batch.enabled :</span> <span class="literal">true</span></span><br><span class="line"><span class="string">--</span> <span class="string">设定</span> <span class="string">60s</span> <span class="string">的触发间隔</span></span><br><span class="line"><span class="attr">table.exec.mini-batch.allow-latency :</span> <span class="string">60s</span></span><br><span class="line"><span class="attr">table.exec.mini-batch.size :</span> <span class="number">10000000000</span></span><br></pre></td></tr></table></figure><p>任务 plan。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/03_%E8%B8%A9%E5%9D%91%E8%AE%B0_count_distinct_%E8%B4%9F%E5%80%BC/1.png" alt="1"></p><h2 id="2-5-问题场景"><a href="#2-5-问题场景" class="headerlink" title="2.5.问题场景"></a>2.5.问题场景</h2><p>这段 SQL 跑了 n 年都没有问题，但是有一天运营在配置【等级中文映射_UDF】时，不小心将一个等级的中文名给映射错了，虽然马上恢复了，但是当天的实时数据和离线数据对比后却发现，实时产出的数值比离线大很多！！！而之前都是保持一致的。</p><h1 id="3-问题排查篇-坑的排查过程"><a href="#3-问题排查篇-坑的排查过程" class="headerlink" title="3.问题排查篇-坑的排查过程"></a>3.问题排查篇-坑的排查过程</h1><p>首先我们想一下，这个指标是算 uv 的，运营将等级中文名配置错了，也应该是把原有等级的最终结果算少啊，怎么会算多呢？？？</p><p>然后我们将场景复现了下，来看看代码：</p><p>任务代码，大家可以直接 copy 到本地运行：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        EnvironmentSettings settings = EnvironmentSettings</span><br><span class="line">                .newInstance()</span><br><span class="line">                .useBlinkPlanner()</span><br><span class="line">                .inStreamingMode().build();</span><br><span class="line"></span><br><span class="line">        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, settings);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 模拟输入</span></span><br><span class="line">        DataStream&lt;Tuple3&lt;String, Long, Long&gt;&gt; tuple3DataStream =</span><br><span class="line">                env.fromCollection(Arrays.asList(</span><br><span class="line">                        Tuple3.of(<span class="string">&quot;2&quot;</span>, <span class="number">1L</span>, <span class="number">1627218000000L</span>),</span><br><span class="line">                        Tuple3.of(<span class="string">&quot;2&quot;</span>, <span class="number">101L</span>, <span class="number">1627218000000L</span> + <span class="number">6000L</span>),</span><br><span class="line">                        Tuple3.of(<span class="string">&quot;2&quot;</span>, <span class="number">201L</span>, <span class="number">1627218000000L</span> + <span class="number">7000L</span>),</span><br><span class="line">                        Tuple3.of(<span class="string">&quot;2&quot;</span>, <span class="number">301L</span>, <span class="number">1627218000000L</span> + <span class="number">7000L</span>)));</span><br><span class="line">        <span class="comment">// 分桶取模 udf</span></span><br><span class="line">        tEnv.registerFunction(<span class="string">&quot;mod&quot;</span>, <span class="keyword">new</span> Mod_UDF());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 中文映射 udf</span></span><br><span class="line">        tEnv.registerFunction(<span class="string">&quot;status_mapper&quot;</span>, <span class="keyword">new</span> StatusMapper_UDF());</span><br><span class="line"></span><br><span class="line">        tEnv.createTemporaryView(<span class="string">&quot;source_db.source_table&quot;</span>, tuple3DataStream,</span><br><span class="line">                <span class="string">&quot;status, id, timestamp&quot;</span>);</span><br><span class="line"></span><br><span class="line">        String sql = <span class="string">&quot;WITH detail_tmp AS (\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  SELECT\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    status,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    id,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    `timestamp`\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  FROM\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    (\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;      SELECT\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;        status,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;        id,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;        `timestamp`,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;        row_number() over(\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;          PARTITION by id\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;          ORDER BY\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;            `timestamp` DESC\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;        ) AS rn\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;      FROM source_db.source_table&quot;</span></span><br><span class="line">                + <span class="string">&quot;    )\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  WHERE\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    rn = 1\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;)\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;SELECT\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  DIM.status_new as status,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  sum(part_uv) as uv\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;FROM\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  (\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    SELECT\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;      status,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;      count(distinct id) as part_uv\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    FROM\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;      detail_tmp\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;    GROUP BY\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;      status,\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;      mod(id, 100)\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  )\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;LEFT JOIN LATERAL TABLE(status_mapper(status)) AS DIM(status_new) ON TRUE\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;GROUP BY\n&quot;</span></span><br><span class="line">                + <span class="string">&quot;  DIM.status_new&quot;</span>;</span><br><span class="line"></span><br><span class="line">        Table result = tEnv.sqlQuery(sql);</span><br><span class="line"></span><br><span class="line">        tEnv.toRetractStream(result, Row.class).print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>UDF 代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StatusMapper_UDF</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>&lt;<span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eval</span><span class="params">(String activityRound)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (activityRound == <span class="string">&quot;1&quot;</span>) &#123;</span><br><span class="line">            collector.collect(<span class="string">&quot;等级1&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (activityRound == <span class="string">&quot;2&quot;</span>) &#123;</span><br><span class="line">            collector.collect(<span class="string">&quot;等级2&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (activityRound == <span class="string">&quot;3&quot;</span>) &#123;</span><br><span class="line">            collector.collect(<span class="string">&quot;等级3&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在正确情况（模拟 UDF 没有任何变动的情况下）的输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(true,等级2,1)</span><br><span class="line">(false,等级2,1)</span><br><span class="line">(true,等级2,2)</span><br><span class="line">(false,等级2,2)</span><br><span class="line">(true,等级2,3)</span><br><span class="line">(false,等级2,3)</span><br><span class="line">(true,等级2,4)</span><br></pre></td></tr></table></figure><p>最终等级2 的 uv 数为 4，结果复合预期✅。</p><p>模拟下用户修改了 udf 配置之后，UDF 代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StatusMapper_UDF</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>&lt;<span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eval</span><span class="params">(String activityRound)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (i == <span class="number">5</span>) &#123;</span><br><span class="line">            collect(<span class="string">&quot;等级4&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (<span class="string">&quot;1&quot;</span>.equals(activityRound)) &#123;</span><br><span class="line">                collector.collect(<span class="string">&quot;等级1&quot;</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;2&quot;</span>.equals(activityRound)) &#123;</span><br><span class="line">                collector.collect(<span class="string">&quot;等级2&quot;</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;3&quot;</span>.equals(activityRound)) &#123;</span><br><span class="line">                collector.collect(<span class="string">&quot;等级3&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        i++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>得到的结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(true,等级2,1)</span><br><span class="line">(false,等级2,1)</span><br><span class="line">(true,等级2,2)</span><br><span class="line">(false,等级2,2)</span><br><span class="line">(true,等级2,3)</span><br><span class="line">(false,等级2,3)</span><br><span class="line">(true,等级2,7)</span><br></pre></td></tr></table></figure><p>最终等级2 的 uv 数为 7，很明显这是错误结果❌。</p><p>因此可以确定是由于这个 UDF 的处理逻辑变换而导致的结果出现错误。</p><p>下文就让我们来分析下其中缘由。</p><h1 id="问题原理解析篇-导致问题的机制是什么"><a href="#问题原理解析篇-导致问题的机制是什么" class="headerlink" title="问题原理解析篇-导致问题的机制是什么"></a>问题原理解析篇-导致问题的机制是什么</h1><p>我们首先来分析下上述 SQL，可以发现整个 flink sql 任务是使用了 unbounded + minibatch 实现的，在 minibatch 触发条件触发时，上游算子会将之前的结果撤回，然后将最新的结果发出。</p><p>这个任务的 execution plan 如图所示。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/03_%E8%B8%A9%E5%9D%91%E8%AE%B0_count_distinct_%E8%B4%9F%E5%80%BC/7.png" alt="7"></p><p>可以从算子图中的一些计算逻辑可以看到，整个任务都是基于 retract 机制运行（count_retract、sum_retract 等）。</p><p>而涉及到 udf 的核心逻辑是在 Operator(ID = 7)，和 Operator(ID = 12) 之间。当 Operator(ID = 7) GroupAggregate 结果发生改变之后，会发一条<strong>撤回消息（做减法）</strong>，一条<strong>最新的结果消息（做加法）</strong>到 Operator(ID = 12) GroupAggregate。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/03_%E8%B8%A9%E5%9D%91%E8%AE%B0_count_distinct_%E8%B4%9F%E5%80%BC/5.png" alt="5"></p><blockquote><p>Notes：<br>简单解释下上面说的<strong>撤回消息（做减法）</strong>，<strong>最新的结果消息（做加法）</strong>。举个算 count 的例子：当整个任务的第一条数据来之后，之前没有数据，所以不用撤回，结果就是 0（没有数据） + 1（第一条数据） = 1（结果），当第二条结果来之后，就要将上次发的 1 消息（可以理解为是整个任务的一个中间结果）撤回，将最新的结果 2 发下去。那么计算方法就是 1（上次的结果） - 1（撤回） + 2（当前最新的结果消息）= 2（结果）。</p></blockquote><p>通过算子图可以发现，【中文名称映射】UDF 是处于两个 GroupAggregate 之间的。也就是说 Operator(ID = 7) GroupAggregate 发出的<strong>撤回消息（做减法）</strong>，<strong>最新的结果消息（做加法）</strong>都会执行这个 UDF，那么就有可能<strong>撤回消息（做减法）</strong>中的某个作为下游 GroupAggregate 算子 key 的字段会被更改成其他值，那么这条消息就不会发到原来下游 GroupAggregate 算子的原始 key 中，那么原来的 key 的历史结果就撤回不了了。。。但是<strong>最新的结果消息（做加法）</strong>的字段没有被更改时，那么这个消息依然被发到了下游 GroupAggregate 算子，这就会导致没做减法，却做了加法，就会导致结果增加。</p><p>从这个角度出发，我们来分析下上面的 case，从内层发给外层的消息一条一条来分析。</p><p>内层消息怎么来看呢？其实就是将上面的 SQL 中的 left join 删除，重新跑一遍就可以得到结果，结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(true,等级2,1)</span><br><span class="line">(false,等级2,1)</span><br><span class="line">(true,等级2,2)</span><br><span class="line">(false,等级2,2)</span><br><span class="line">(true,等级2,3)</span><br><span class="line">(false,等级4,3)</span><br><span class="line">(true,等级2,4)</span><br></pre></td></tr></table></figure><p>来分析下内层消息发出之后对应到外层消息的操作：</p><table><thead><tr><th>内层</th><th>外层</th></tr></thead><tbody><tr><td>(true,等级2,1)</td><td>(true,等级2,1)</td></tr><tr><td>(false,等级2,1)</td><td>(false,等级2,1)</td></tr><tr><td>(true,等级2,2)</td><td>(true,等级2,2)</td></tr><tr><td>(false,等级2,2)</td><td>(false,等级2,2)</td></tr><tr><td>(true,等级2,3)</td><td>(true,等级2,3)</td></tr></tbody></table><p>前五条消息不会导致错误，不用详细说明。</p><table><thead><tr><th>内层</th><th>外层</th></tr></thead><tbody><tr><td>(true,等级2,1)</td><td>(true,等级2,1)</td></tr><tr><td>(false,等级2,1)</td><td>(false,等级2,1)</td></tr><tr><td>(true,等级2,2)</td><td>(true,等级2,2)</td></tr><tr><td>(false,等级2,2)</td><td>(false,等级2,2)</td></tr><tr><td>(true,等级2,3)</td><td>(true,等级2,3)</td></tr><tr><td>(false,等级4,3)</td><td></td></tr></tbody></table><p>第六条消息发出之后，经过 udf 的处理之后，中文名被映射成了【等级4】，而其再通过 hash partition 策略向下发送消息时，就不能将这条撤回消息发到原本 key 为【等级2】的算子中了，这条撤回消息也无法被处理了。</p><table><thead><tr><th>内层</th><th>外层</th></tr></thead><tbody><tr><td>(true,等级2,1)</td><td>(true,等级2,1)</td></tr><tr><td>(false,等级2,1)</td><td>(false,等级2,1)</td></tr><tr><td>(true,等级2,2)</td><td>(true,等级2,2)</td></tr><tr><td>(false,等级2,2)</td><td>(false,等级2,2)</td></tr><tr><td>(true,等级2,3)</td><td>(true,等级2,3)</td></tr><tr><td>(false,等级4,3)</td><td></td></tr><tr><td>(true,等级2,4)</td><td>(false,等级2,3) (true,等级2,7)</td></tr></tbody></table><p>第七条消息 (true,等级2,4) 发出后，外层 GroupAggregate 算子首先会将上次发出的记过撤回，即(false,等级2,3)，然后将(true,等级2,4)累加到当前的记过上，即 3（上次结果）+ 4（这次最新的结果）= 7（结果）。就导致了上述的错误结果。</p><p>定位到问题原因之后，我们来看看怎么避免上述错误。</p><h1 id="6-避坑篇-如何避免这种问题"><a href="#6-避坑篇-如何避免这种问题" class="headerlink" title="6.避坑篇-如何避免这种问题"></a>6.避坑篇-如何避免这种问题</h1><h2 id="6-1-从源头避免"><a href="#6-1-从源头避免" class="headerlink" title="6.1.从源头避免"></a>6.1.从源头避免</h2><p>udf 这种映射维度的 udf 尽量在上线前就固定下来，避免后续更改造成的数据错误。</p><h2 id="6-2-替换为-ScalarFunction-进行映射"><a href="#6-2-替换为-ScalarFunction-进行映射" class="headerlink" title="6.2.替换为 ScalarFunction 进行映射"></a>6.2.替换为 ScalarFunction 进行映射</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">WITH</span> detail_tmp <span class="keyword">AS</span> (</span><br><span class="line">  <span class="keyword">SELECT</span></span><br><span class="line">    status,</span><br><span class="line">    id,</span><br><span class="line">    `<span class="type">timestamp</span>`</span><br><span class="line">  <span class="keyword">FROM</span></span><br><span class="line">    (</span><br><span class="line">      <span class="keyword">SELECT</span></span><br><span class="line">        status,</span><br><span class="line">        id,</span><br><span class="line">        `<span class="type">timestamp</span>`,</span><br><span class="line">        <span class="built_in">row_number</span>() <span class="keyword">over</span>(</span><br><span class="line">          <span class="keyword">PARTITION</span> <span class="keyword">by</span> id</span><br><span class="line">          <span class="keyword">ORDER</span> <span class="keyword">BY</span></span><br><span class="line">            `<span class="type">timestamp</span>` <span class="keyword">DESC</span></span><br><span class="line">        ) <span class="keyword">AS</span> rn</span><br><span class="line">      <span class="keyword">FROM</span></span><br><span class="line">        (</span><br><span class="line">          <span class="keyword">SELECT</span></span><br><span class="line">            status,</span><br><span class="line">            id,</span><br><span class="line">            `<span class="type">timestamp</span>`</span><br><span class="line">          <span class="keyword">FROM</span></span><br><span class="line">            source_db.source_table</span><br><span class="line">        ) t1</span><br><span class="line">    ) t2</span><br><span class="line">  <span class="keyword">WHERE</span></span><br><span class="line">    rn <span class="operator">=</span> <span class="number">1</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="comment">-- 在此处进行中文名称映射</span></span><br><span class="line">  等级中文映射_UDF(status) <span class="keyword">as</span> status,</span><br><span class="line">  <span class="built_in">sum</span>(part_uv) <span class="keyword">as</span> uv</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">SELECT</span></span><br><span class="line">      status,</span><br><span class="line">      <span class="built_in">count</span>(<span class="keyword">distinct</span> id) <span class="keyword">as</span> part_uv</span><br><span class="line">    <span class="keyword">FROM</span></span><br><span class="line">      detail_tmp</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">      status,</span><br><span class="line">      <span class="built_in">mod</span>(id, <span class="number">100</span>)</span><br><span class="line">  )</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  status</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StatusMapper_UDF</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">eval</span><span class="params">(String activityRound)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (i == <span class="number">5</span>) &#123;</span><br><span class="line">            i++;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;等级4&quot;</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            i++;</span><br><span class="line">            <span class="keyword">if</span> (<span class="string">&quot;1&quot;</span>.equals(activityRound)) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">&quot;等级1&quot;</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;2&quot;</span>.equals(activityRound)) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">&quot;等级2&quot;</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;3&quot;</span>.equals(activityRound)) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">&quot;等级3&quot;</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;未知&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>还是刚刚的逻辑，刚刚的配方，我们先来看一下结果。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(true,等级2,1)</span><br><span class="line">(false,等级2,1)</span><br><span class="line">(true,等级2,2)</span><br><span class="line">(false,等级2,2)</span><br><span class="line">(true,等级2,3)</span><br><span class="line">(false,等级4,3)</span><br><span class="line">(true,等级2,4)</span><br></pre></td></tr></table></figure><p>发现虽然依然会有 <code>(false,等级4,3)</code> 这样的错误撤回数据（这是 udf 决定的，没法避免），但是我们可以发现最终的结果是 <code>(true,等级2,4)</code>，结果依然是正确的。</p><p>再来分析下问什么这种方式可以解决，如图 plan。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/03_%E8%B8%A9%E5%9D%91%E8%AE%B0_count_distinct_%E8%B4%9F%E5%80%BC/6.png" alt="6"></p><p>发现映射 udf 算子所处的位置不在两个 GroupAggregrate 之间了，因此在 retract 消息发送之后，不会被映射到错误其他 key 中，因此所有的 retract 消息都会正常处理。</p><h1 id="7-展望篇-有什么机制可以根本避免这种情况"><a href="#7-展望篇-有什么机制可以根本避免这种情况" class="headerlink" title="7.展望篇-有什么机制可以根本避免这种情况"></a>7.展望篇-有什么机制可以根本避免这种情况</h1><p>可以将<strong>撤回消息（做减法）</strong>，<strong>最新的结果消息（做加法）</strong>做成一个原子消息从上游发给下游，下游统一进行原子性处理，关联 udf 时，也只对 group key 关联一次即可。</p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都比较短小，不定期更新，从一些博主的脑洞想法出发抛砖引玉，提高小伙伴的姿♂势水平。本文介绍博主对流批一体来源以及未来发展方向的一些理解，阅读时长大概 2 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>揭秘字节跳动埋点数据实时动态处理引擎</title>
    <link href="https://yangyichao-mango.github.io/2021/11/12/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/05_%E6%8F%AD%E7%A7%98%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9F%8B%E7%82%B9%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%8A%A8%E6%80%81%E5%A4%84%E7%90%86%E5%BC%95%E6%93%8E/"/>
    <id>https://yangyichao-mango.github.io/2021/11/12/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/05_%E6%8F%AD%E7%A7%98%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9F%8B%E7%82%B9%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%8A%A8%E6%80%81%E5%A4%84%E7%90%86%E5%BC%95%E6%93%8E/</id>
    <published>2021-11-12T06:21:53.000Z</published>
    <updated>2021-08-05T12:32:20.232Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>感谢您的<strong>关注  +  点赞 + 再看</strong>，对博主的肯定，会督促博主持续的输出更多的优质实战内容！！！</p></blockquote><h1 id="1-序篇-先说结论"><a href="#1-序篇-先说结论" class="headerlink" title="1.序篇-先说结论"></a>1.序篇-先说结论</h1><p>宝贝们，还记得前几天博主去的火山引擎大数据场嘛，其中比较令大家感兴趣的就是最后一讲，字节一站式埋点平台的 flink 标准化清洗及拆流任务。</p><p>其中大家感觉比较流啤的就是的就是字节做到了：</p><ol><li>不重启任务可以上下线新的拆流及清洗规则，所有的规则变更都不需要涉及到任务的重启。</li><li>清洗 udf，rpc 接口热加载</li></ol><p>总的来说就是任务永不停，不可能停止的，好么，beiber。</p><blockquote><p>字节火山引擎 PPT。公众号回复 20210724 获取。</p></blockquote><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/05_%E6%8F%AD%E7%A7%98%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9F%8B%E7%82%B9%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%8A%A8%E6%80%81%E5%A4%84%E7%90%86%E5%BC%95%E6%93%8E/6.png" alt="6"></p><blockquote><p>本文博主就主要介绍第一点，即做到规则动态变化，可以做到动态添加一个 sink kafka topic，动态删除一个 sink kafka topic，而不重启任务。相信能抛砖引玉，给大家一些启发。</p></blockquote><p>本文从以下几个章节详细介绍框架实现：</p><ol><li>背景篇-为啥需要这么个框架</li><li>定义、目标篇-做这个框架的目标、预期效果是什么</li><li>难点剖析篇-此框架建设的难点、业界目前的实现</li><li>数据建设篇-框架具体方案设计</li><li>数据保障篇-框架的保障方案</li><li>总结与展望篇</li></ol><h1 id="2-背景篇-为啥需要这么个框架"><a href="#2-背景篇-为啥需要这么个框架" class="headerlink" title="2.背景篇-为啥需要这么个框架"></a>2.背景篇-为啥需要这么个框架</h1><p>首先来看看字节他们做这件事情的背景：</p><ol><li>任务重启造成数据的延迟：对于字节这种企业来说且每天都会新上线很多的埋点，把这些新的埋点拆流条件加入 flink 任务就要重启，但是字节客户端日志流量都是千万级别 qps 的，就意味着这个 flink 任务一旦重启耗时肯定是很长的，这对时延敏感的业务是不可接受的。</li><li>减少对于原始客户端日志的烟囱式消费，节约资源</li><li>统一标准化的埋点平台：用户能通过埋点平台用到正确的数据</li><li>与埋点平台联动的、统一化的、标准化的流式数据处理平台：用户能通过这个平台去获取想要的统一标准化的数据</li><li>数据的分级保障能力：Dump 日志，日志的产出需要分优先级进行保障（死保、尽力保…），用户能放心的用数据</li></ol><p>如图：<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/05_%E6%8F%AD%E7%A7%98%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9F%8B%E7%82%B9%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%8A%A8%E6%80%81%E5%A4%84%E7%90%86%E5%BC%95%E6%93%8E/2.png" alt="2"></p><p>因此诞生了这个框架。</p><h1 id="3-定义、目标篇-做这个框架的目标、预期效果是什么"><a href="#3-定义、目标篇-做这个框架的目标、预期效果是什么" class="headerlink" title="3.定义、目标篇-做这个框架的目标、预期效果是什么"></a>3.定义、目标篇-做这个框架的目标、预期效果是什么</h1><p>上述的痛点很多，本节就从最痛的<strong>任务重启的延迟</strong>角度出发解决问题，揭秘字节动态配置化的 flink 任务的实现。</p><p>预期效果如下：</p><ol><li><strong>即在任务不停止的情况下可以动态的上线一个动态规则、一个 sink kafka topic，上线某个、某类埋点对应的流数据的 kafka topic</strong></li></ol><p>如图左边是修改配置，添加了一个拆流规则以及对应 topic，右边这个规则 topic 就开始产出数据，对应的 console consumer 就消费到了复合规则的数据。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/05_%E6%8F%AD%E7%A7%98%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9F%8B%E7%82%B9%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%8A%A8%E6%80%81%E5%A4%84%E7%90%86%E5%BC%95%E6%93%8E/8.gif" alt="8"></p><ol start="2"><li><strong>即在任务不停止的情况下可以动态的下线一个动态规则、一个 sink kafka topic，下线某个、某类埋点对应的流数据的 kafka topic</strong></li></ol><p>如图左边是修改配置，删除了一个拆流规则以及对应 topic，右边这个规则 topic 就不产出数据了，对应的 console consumer 就没有新数据可以消费了。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/05_%E6%8F%AD%E7%A7%98%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9F%8B%E7%82%B9%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%8A%A8%E6%80%81%E5%A4%84%E7%90%86%E5%BC%95%E6%93%8E/9.gif" alt="9"></p><h1 id="4-难点剖析篇-此框架建设的难点、业界目前的实现"><a href="#4-难点剖析篇-此框架建设的难点、业界目前的实现" class="headerlink" title="4.难点剖析篇-此框架建设的难点、业界目前的实现"></a>4.难点剖析篇-此框架建设的难点、业界目前的实现</h1><p>首先带大家分析下，实现这个框架，最基本的模块都需要包含什么：</p><ol><li>flink 任务：本身就是一个 Map 任务，逻辑简单</li><li>动态上下线规则配置：肯定得有一个动态配置中心去告诉 flink 任务需要新上下线一个 kafka topic</li><li>动态规则过滤引擎：flink 任务监听到规则发生动态变化之后，要热更新规则，将新的规律规则应用起来。需要一个动态代码执行引擎</li><li>动态上下线 Kafka topic：目前大多数公司用的是 flink 自带的 kafka-connector，一旦涉及到需要添加一个下游，就需要添加一个 kafka producer operator，因为涉及到多加了一个 operator，那肯定得重启任务。需要动态添加删除 producer 的能力。</li></ol><h1 id="5-数据建设篇-框架具体方案设计"><a href="#5-数据建设篇-框架具体方案设计" class="headerlink" title="5.数据建设篇-框架具体方案设计"></a>5.数据建设篇-框架具体方案设计</h1><h2 id="5-1-方案设计"><a href="#5-1-方案设计" class="headerlink" title="5.1.方案设计"></a>5.1.方案设计</h2><h3 id="5-1-1-方案"><a href="#5-1-1-方案" class="headerlink" title="5.1.1.方案"></a>5.1.1.方案</h3><p>先说说方案选择的结论：</p><ol><li>flink 入口任务：Map 模型使用 ProcessFunction 底层算子</li><li>动态上下线规则配置：配置中心开源的有很多，这里为了实现轻量化，实现简单，使用 zookeeper 作为动态规则配置中心。当然如果对 zk 压力大，也可以使用广播配置实现。</li><li>动态规则引擎：规则引擎很多，比如常见的可以使用 JavaScript、Groovy、jython、mvel2、freemarker 等等，太多了。考虑到性能、易用性选用 janino 将动态规则动态编译出 class。然后作为动态规则引擎使用。后面会详述选用 janino 的原因。</li><li>动态上下线 Kafka topic：去除 flink-kafka-connector，直接在 ProcessFunction 中使用原生 kafka-clients 输出数据，维护一个 producer 池。</li></ol><p>整体方案架构图如图所示：<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/05_%E6%8F%AD%E7%A7%98%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9F%8B%E7%82%B9%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%8A%A8%E6%80%81%E5%A4%84%E7%90%86%E5%BC%95%E6%93%8E/3.png" alt="3"></p><p><strong>项目代码、在博主公众号回复【揭秘字节跳动埋点数据实时动态处理引擎】获取：</strong></p><h3 id="5-1-2-预期效果"><a href="#5-1-2-预期效果" class="headerlink" title="5.1.2.预期效果"></a>5.1.2.预期效果</h3><h4 id="5-1-2-1-上线配置"><a href="#5-1-2-1-上线配置" class="headerlink" title="5.1.2.1.上线配置"></a>5.1.2.1.上线配置</h4><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/05_%E6%8F%AD%E7%A7%98%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9F%8B%E7%82%B9%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%8A%A8%E6%80%81%E5%A4%84%E7%90%86%E5%BC%95%E6%93%8E/4.png" alt="4"></p><h4 id="5-1-2-2-下线配置"><a href="#5-1-2-2-下线配置" class="headerlink" title="5.1.2.2.下线配置"></a>5.1.2.2.下线配置</h4><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/05_%E6%8F%AD%E7%A7%98%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9F%8B%E7%82%B9%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%8A%A8%E6%80%81%E5%A4%84%E7%90%86%E5%BC%95%E6%93%8E/5.png" alt="5"></p><h2 id="5-2-具体实现"><a href="#5-2-具体实现" class="headerlink" title="5.2.具体实现"></a>5.2.具体实现</h2><p>整个任务的实现非常简单。</p><p>本地运行，可以参考下面两篇安装 zk 和 kafka。</p><ul><li>zk：<a href="https://www.jianshu.com/p/5491d16e6abd">https://www.jianshu.com/p/5491d16e6abd</a></li><li>kafka：<a href="https://www.jianshu.com/p/dd2578d47ff6">https://www.jianshu.com/p/dd2578d47ff6</a></li></ul><h3 id="5-2-1-flink-任务入口逻辑"><a href="#5-2-1-flink-任务入口逻辑" class="headerlink" title="5.2.1.flink 任务入口逻辑"></a>5.2.1.flink 任务入口逻辑</h3><p>首先来看看整个任务的入口逻辑，ProcessFunction 的功能很简单：</p><ol><li>针对数据源的每一条日志数据，遍历动态规则引擎池</li><li>只要这条数据满足某一条规则的条件，就将这条日志数据写出到规则对应的 topic 中</li></ol><p><strong>项目代码、在博主公众号回复【揭秘字节跳动埋点数据实时动态处理引擎】获取：</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"> env.addSource(<span class="keyword">new</span> UserDefinedSource())</span><br><span class="line">    .process(<span class="keyword">new</span> ProcessFunction&lt;ClientLogSource, ClientLogSink&gt;() &#123;</span><br><span class="line">        <span class="comment">// 动态规则配置中心</span></span><br><span class="line">        <span class="keyword">private</span> ZkBasedConfigCenter zkBasedConfigCenter;</span><br><span class="line">        <span class="comment">// kafka producer 管理中心</span></span><br><span class="line">        <span class="keyword">private</span> KafkaProducerCenter kafkaProducerCenter;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">super</span>.open(parameters);</span><br><span class="line">            <span class="keyword">this</span>.zkBasedConfigCenter = ZkBasedConfigCenter.getInstance();</span><br><span class="line">            <span class="keyword">this</span>.kafkaProducerCenter = KafkaProducerCenter.getInstance();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(ClientLogSource clientLogSource, Context context, Collector&lt;ClientLogSink&gt; collector)</span></span></span><br><span class="line"><span class="function">                <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// 遍历所有的动态规则</span></span><br><span class="line">            <span class="keyword">this</span>.zkBasedConfigCenter.getMap().forEach(<span class="keyword">new</span> BiConsumer&lt;Long, DynamicProducerRule&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(Long id, DynamicProducerRule dynamicProducerRule)</span> </span>&#123;</span><br><span class="line">                    <span class="comment">// 验证该条数据是否符合该条规则</span></span><br><span class="line">                    <span class="keyword">if</span> (dynamicProducerRule.eval(clientLogSource)) &#123;</span><br><span class="line">                        <span class="comment">// 将符合规则的数据发向对应规则的 topic 中</span></span><br><span class="line">                        kafkaProducerCenter.send(dynamicProducerRule.getTargetTopic(), clientLogSource.toString());</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">super</span>.close();</span><br><span class="line">            <span class="comment">// 关闭规则池</span></span><br><span class="line">            <span class="keyword">this</span>.zkBasedConfigCenter.close();</span><br><span class="line">            <span class="comment">// 关闭 producer 池</span></span><br><span class="line">            <span class="keyword">this</span>.kafkaProducerCenter.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure><h3 id="5-2-2-动态上下线规则配置"><a href="#5-2-2-动态上下线规则配置" class="headerlink" title="5.2.2.动态上下线规则配置"></a>5.2.2.动态上下线规则配置</h3><p>来看 flink ProcessFunction 中的核心点，第一部分就是 ZkBasedConfigCenter。其功能包含：</p><ol><li>任务启动时，初始化加载 zk 配置，初始化规则池，将规则池中的配置规则编译成 class 可执行规则</li><li>监听 zk 配置变更，将新增配置加入规则池，将下线配置从规则池删除</li></ol><h4 id="5-2-2-1-动态规则-schema-设计"><a href="#5-2-2-1-动态规则-schema-设计" class="headerlink" title="5.2.2.1.动态规则 schema 设计"></a>5.2.2.1.动态规则 schema 设计</h4><p>动态规则包含的内容与用户需求息息相关：</p><p>举例：用户需要将在首页上报 +  id &gt; 300 用户的客户端日志都写入 topic_id_bigger_than_300_and_main_page 的 kafka topic 中。</p><p>那么针对这个 flink 任务来说就有以下三项用户的输入：</p><ol><li>动态规则的过滤条件：即上游每一条数据过来之后检验这条数据是否满足规则条件。上面这个例子的条件就是 <code>clientLogSource.getId() &gt; 300 &amp;&amp; clientLogSource.getPage().equals(&quot;首页&quot;)</code>；其中 clientLogSource 是原始日志 model</li><li>动态规则要写入的 topic 名称：这条规则过滤出来的数据要写入哪个 topic。上面这个例子的 topic 就是 <code>topic_id_bigger_than_300_and_main_page</code></li><li>动态规则的唯一 id：唯一标识一个过滤规则的 id</li></ol><p>针对上述要求设计动态规则配置的 schema 如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;id-数值类型 string&quot;</span>: &#123;</span><br><span class="line"><span class="attr">&quot;condition-过滤条件&quot;</span>: <span class="string">&quot;1==1&quot;</span>,</span><br><span class="line"><span class="attr">&quot;targetTopic-目标 topic 名称&quot;</span>: <span class="string">&quot;tuzisir1&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="string">&quot;1&quot;</span>: &#123;</span><br><span class="line"><span class="attr">&quot;condition&quot;</span>: <span class="string">&quot;clientLogSource.getId() &gt; 300 &amp;&amp; clientLogSource.getPage().equals(\&quot;首页\&quot;)&quot;</span>,</span><br><span class="line"><span class="attr">&quot;targetTopic&quot;</span>: <span class="string">&quot;topic_id_bigger_than_300_and_main_page&quot;</span></span><br><span class="line">&#125;,</span><br><span class="line"><span class="attr">&quot;2&quot;</span>: &#123;</span><br><span class="line"><span class="attr">&quot;condition&quot;</span>: <span class="string">&quot;clientLogSource.getPage().equals(\&quot;个人主页\&quot;)&quot;</span>,</span><br><span class="line"><span class="attr">&quot;targetTopic&quot;</span>: <span class="string">&quot;topic_profile_page&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对应动态规则 model 设计如下：</p><p><strong>项目代码、在博主公众号回复【揭秘字节跳动埋点数据实时动态处理引擎】获取：</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DynamicProducerRule</span> <span class="keyword">implements</span> <span class="title">Evaluable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 具体过滤规则</span></span><br><span class="line">    <span class="keyword">private</span> String condition;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 具体写入 topic</span></span><br><span class="line">    <span class="keyword">private</span> String targetTopic;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用 janino 编译的规则过滤器</span></span><br><span class="line">    <span class="keyword">private</span> Evaluable evaluable;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">(Long id)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 使用 janino 初始化规则</span></span><br><span class="line">            Class&lt;Evaluable&gt; clazz = JaninoUtils.genCodeAndGetClazz(id, targetTopic, condition);</span><br><span class="line">            <span class="keyword">this</span>.evaluable = clazz.newInstance();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">eval</span><span class="params">(ClientLogSource clientLogSource)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.evaluable.eval(clientLogSource);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>重点在于 <strong>Evaluable</strong> 接口，动态生成代码就是继承了这个接口，用于执行过滤规则的基础接口。</p><p>代码动态生成下面会详细介绍。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Evaluable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 动态规则接口过滤方法</span></span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">eval</span><span class="params">(ClientLogSource clientLogSource)</span></span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="5-2-2-2-基于-zk-的动态配置中心"><a href="#5-2-2-2-基于-zk-的动态配置中心" class="headerlink" title="5.2.2.2.基于 zk 的动态配置中心"></a>5.2.2.2.基于 zk 的动态配置中心</h4><p>使用了 zk 作为动态配置中心，来动态监听规则配置以及更新规则池。</p><p><strong>项目代码、在博主公众号回复【揭秘字节跳动埋点数据实时动态处理引擎】获取：</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ZkBasedConfigCenter</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// zk config 变化监听器</span></span><br><span class="line">    <span class="keyword">private</span> TreeCache treeCache;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// zk 客户端</span></span><br><span class="line">    <span class="keyword">private</span> CuratorFramework zkClient;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">ZkBasedConfigCenter</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            open();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ！！！规则池！！！规则池！！！规则池</span></span><br><span class="line">    <span class="keyword">private</span> ConcurrentMap&lt;Long, DynamicProducerRule&gt; map = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 初始化规则</span></span><br><span class="line">        <span class="comment">// 初始化 zk config 监听器</span></span><br><span class="line">        <span class="comment">// 当有配置变更时</span></span><br><span class="line">        <span class="comment">// 调用 private void update(String json) 更新规则</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.treeCache.close();</span><br><span class="line">        <span class="keyword">this</span>.zkClient.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">(String json)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Map&lt;Long, DynamicProducerRule&gt;</span><br><span class="line">                result = getNewMap(json);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.将新增规则添加进规则池</span></span><br><span class="line">        <span class="comment">// 2.将下线规则从规则池删除</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> Map&lt;Long, DynamicProducerRule&gt; <span class="title">getNewMap</span><span class="params">(String json)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 将新规则解析，并使用 janino 进行初始化</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以使用一个固定路径的配置，如图博主使用的是 /kafka-config</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/05_%E6%8F%AD%E7%A7%98%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%9F%8B%E7%82%B9%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%8A%A8%E6%80%81%E5%A4%84%E7%90%86%E5%BC%95%E6%93%8E/7.gif" alt="7"></p><h3 id="5-2-3-动态规则引擎"><a href="#5-2-3-动态规则引擎" class="headerlink" title="5.2.3.动态规则引擎"></a>5.2.3.动态规则引擎</h3><p><strong>目前字节使用的引擎是 Groovy，但是博主常用 flink sql，sql 中的代码生成是使用 janino 做的，因此就比较了 janino 和 groovy 的性能差异，janino 编译出的原生 class 性能接近原生 class，是 Groovy 的 4 倍左右。其他的引擎不考虑，要么易用性差，要么性能差。</strong></p><blockquote><p>Notes：<br>性能这一点真的是很重要，1：4 的差距可以说是差别很大了。如果你的场景也是大流量，非常耗费性能的场景，建议直接入手 janino！！！</p></blockquote><p>来看看具体的 benchmark case 代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ClientLogSource 是原始日志</span></span><br><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">eval</span><span class="params">(flink.examples.datastream._01.bytedance.split.model.ClientLogSource clientLogSource)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> String.valueOf(clientLogSource.getId()).equals(<span class="string">&quot;1&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面这段代码，在博主 mac 本地执行，每次循环执行 5kw 次，总计执行 5 次 得出的结果如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">java:847</span> <span class="string">ms</span></span><br><span class="line"><span class="string">janino:745</span> <span class="string">ms</span></span><br><span class="line"><span class="string">groovy:4110</span> <span class="string">ms</span></span><br><span class="line"></span><br><span class="line"><span class="string">java:1097</span> <span class="string">ms</span></span><br><span class="line"><span class="string">janino:1170</span> <span class="string">ms</span></span><br><span class="line"><span class="string">groovy:4052</span> <span class="string">ms</span></span><br><span class="line"></span><br><span class="line"><span class="string">java:916</span> <span class="string">ms</span></span><br><span class="line"><span class="string">janino:1117</span> <span class="string">ms</span></span><br><span class="line"><span class="string">groovy:4311</span> <span class="string">ms</span></span><br><span class="line"></span><br><span class="line"><span class="string">java:915</span> <span class="string">ms</span></span><br><span class="line"><span class="string">janino:1112</span> <span class="string">ms</span></span><br><span class="line"><span class="string">groovy:4382</span> <span class="string">ms</span></span><br><span class="line"></span><br><span class="line"><span class="string">java:921</span> <span class="string">ms</span></span><br><span class="line"><span class="string">janino:1104</span> <span class="string">ms</span></span><br><span class="line"><span class="string">groovy:4321</span> <span class="string">ms</span></span><br></pre></td></tr></table></figure><p><strong>重复执行了很多次：java object : janino 编译原生 class ： groovy ：几乎都是 1：1：4 的耗时。所以此处我们选择性能更好的 janino。</strong></p><p><strong>项目代码、在博主公众号回复【揭秘字节跳动埋点数据实时动态处理引擎】获取：</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JaninoUtils</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Class&lt;Evaluable&gt; <span class="title">genCodeAndGetClazz</span><span class="params">(Long id, String topic, String condition)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 动态生成代码</span></span><br><span class="line">        <span class="comment">// 初始化 Class&lt;Evaluable&gt; 并返回</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="5-2-4-动态上下线-Kafka-topic"><a href="#5-2-4-动态上下线-Kafka-topic" class="headerlink" title="5.2.4.动态上下线 Kafka topic"></a>5.2.4.动态上下线 Kafka topic</h3><p>来看入口类中的第二个核心点，就是 KafkaProducerCenter。其功能包含：</p><ol><li>维护所有的 producer 池</li><li>提供消息发送接口</li></ol><p><strong>项目代码、在博主公众号回复【揭秘字节跳动埋点数据实时动态处理引擎】获取：</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaProducerCenter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// kafka producer 池</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ConcurrentMap&lt;String, Producer&lt;String, String&gt;&gt; producerConcurrentMap</span><br><span class="line">            = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> Producer&lt;String, String&gt; <span class="title">getProducer</span><span class="params">(String topicName)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果 kafka producer 池中有当前 topic 的 producer，则直接返回</span></span><br><span class="line">        <span class="comment">// 如果没有，则初始化一个新的 producer 然后返回</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">send</span><span class="params">(String topicName, String message)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(topicName,</span><br><span class="line">                <span class="string">&quot;&quot;</span>, message);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            RecordMetadata metadata = getProducer(topicName).send(record).get();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 关闭所有 producer 连接</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面就是所有的代码、逻辑实现方案。其实整体看下来是非常简单的。</p><h1 id="6-数据保障篇-框架的保障方案"><a href="#6-数据保障篇-框架的保障方案" class="headerlink" title="6. 数据保障篇-框架的保障方案"></a>6. 数据保障篇-框架的保障方案</h1><ol><li>配置中心挂了怎么办？</li></ol><p>为这个任务分配独立的队列资源，每当这个任务加载到最新配置时，都将配置在本地存储一份。当配置中心挂了的时候，还可以直接加载机器本地的配置，不至于什么都产出不了。</p><ol start="2"><li>怎么保障用户的配置是无误的？<ul><li>上线前审批：有专门的埋点管理人员进行逻辑验证及管理</li><li>上线前自动化测试：在埋点管理平台自动化验证逻辑正确性，保障上线到 flink 任务里的配置都是正确的</li><li>AOP 异常处理、报警：在环境中做 AOP 异常处理，将异常数据 dump 到专用异常 topic 中，也需要自动化把报警信息透出</li><li>结果验证：针对最终的结果需要有数据准确性验证机制</li></ul></li></ol><h1 id="7-总结与展望篇"><a href="#7-总结与展望篇" class="headerlink" title="7. 总结与展望篇"></a>7. 总结与展望篇</h1><h2 id="7-1-总结"><a href="#7-1-总结" class="headerlink" title="7.1.总结"></a>7.1.总结</h2><p>本文主要揭秘、实现了字节跳动埋点数据实时动态处理引擎。</p><h2 id="7-2-展望"><a href="#7-2-展望" class="headerlink" title="7.2.展望"></a>7.2.展望</h2><ol><li>本文主要实现了拆流的动态化，输出数据和输入数据完全相同，但是很多情况下，下游只需要其中的一些字段。因此之后还可以做到对于 sink message 字段、消息的个性化。比如可以加一个动态化的 Map 逻辑，将数据源中的 ClientLogSource 转化为任何用户想要的 Model。</li><li>目前过滤条件完全是 java 语法，之后可以扩展成为 sql 语法，提高可读性</li><li>函数、rpc 热加载</li></ol>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都比较短小，不定期更新，从一些博主的脑洞想法出发抛砖引玉，提高小伙伴的姿♂势水平。本文介绍博主对流批一体来源以及未来发展方向的一些理解，阅读时长大概 2 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>测试</title>
    <link href="https://yangyichao-mango.github.io/2021/10/12/wechat-blog/test/"/>
    <id>https://yangyichao-mango.github.io/2021/10/12/wechat-blog/test/</id>
    <published>2021-10-12T06:21:53.000Z</published>
    <updated>2021-07-18T02:58:09.837Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前沿-流批一体的一些理解"><a href="#前沿-流批一体的一些理解" class="headerlink" title="前沿 | 流批一体的一些理解"></a>前沿 | 流批一体的一些理解</h1><blockquote><p>每家数字化企业在目前遇到流批一体概念的时候，都会对这个概念抱有一些疑问，到底什么是流批一体？这个概念的来源？这个概念能为用户、开发人员以及企业带来什么样的好处？跟随着博主的理解和脑洞出发吧。</p></blockquote><iframe src="./a.html"></iframe>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都比较短小，不定期更新，从一些博主的脑洞想法出发抛砖引玉，提高小伙伴的姿♂势水平。本文介绍博主对流批一体来源以及未来发展方向的一些理解，阅读时长大概 2 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>实战 | flink sql 计算 top n - 1</title>
    <link href="https://yangyichao-mango.github.io/2021/10/12/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/"/>
    <id>https://yangyichao-mango.github.io/2021/10/12/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/</id>
    <published>2021-10-12T06:21:53.000Z</published>
    <updated>2021-07-22T14:45:19.573Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>感谢您的<strong>点赞</strong> + <strong>再看</strong>！！！</p></blockquote><h1 id="1-序篇"><a href="#1-序篇" class="headerlink" title="1.序篇"></a>1.序篇</h1><p>通过本文你可以 get 到：</p><ol><li>背景篇</li><li>定义篇-属于哪类特点的指标</li><li>数据应用篇-预期效果是怎样的</li><li>难点剖析篇-此类指标建设、保障的难点</li><li>数据建设篇-具体实现方案详述</li><li>数据服务篇-数据服务选型</li><li>数据保障篇-数据时效监控以及保障方案</li><li>效果篇-上述方案最终的效果</li><li>现状以及展望篇</li></ol><h1 id="2-背景篇"><a href="#2-背景篇" class="headerlink" title="2.背景篇"></a>2.背景篇</h1><p>根据微博目前站内词条消费情况，计算 top 50 消费热度词条，每分钟更新一次，并且按照列表展现给用户。</p><h1 id="3-定义篇-属于哪类特点的指标"><a href="#3-定义篇-属于哪类特点的指标" class="headerlink" title="3.定义篇-属于哪类特点的指标"></a>3.定义篇-属于哪类特点的指标</h1><p>这类指标可以统一划分到 topN 类别的指标中。即输入是具体词条消费日志，输出是词条消费排行榜。</p><h1 id="4-数据应用篇-预期效果是怎样的"><a href="#4-数据应用篇-预期效果是怎样的" class="headerlink" title="4.数据应用篇-预期效果是怎样的"></a>4.数据应用篇-预期效果是怎样的</h1><p>预期效果如下。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/weibo-topn.png" alt="1"></p><h1 id="5-难点剖析篇-此类指标建设、保障的难点"><a href="#5-难点剖析篇-此类指标建设、保障的难点" class="headerlink" title="5.难点剖析篇-此类指标建设、保障的难点"></a>5.难点剖析篇-此类指标建设、保障的难点</h1><h2 id="5-1-数据建设"><a href="#5-1-数据建设" class="headerlink" title="5.1.数据建设"></a>5.1.数据建设</h2><h3 id="5-1-1-难点"><a href="#5-1-1-难点" class="headerlink" title="5.1.1.难点"></a>5.1.1.难点</h3><ol><li>榜单类的指标有一个特点，就是客户端获取到的数据必须是同一分钟当时的词条消费热度，这就要求我们产出的每一条数据需要包含 topN 中的所有数据。这样才能保障用户获取到的数据的一致性。</li><li>flink 任务大状态：词条多，状态大；词条具有时效性，所以对于低热词条需要进行删除</li><li>flink 任务大流量、高性能：数据源是全站的词条消费流量，得扛得住突发流量的暴揍</li></ol><h3 id="5-1-2-业界方案调研"><a href="#5-1-2-业界方案调研" class="headerlink" title="5.1.2.业界方案调研"></a>5.1.2.业界方案调研</h3><h3 id="5-1-2-1-Flink-DataStream-api-实时计算-topN-热榜"><a href="#5-1-2-1-Flink-DataStream-api-实时计算-topN-热榜" class="headerlink" title="5.1.2.1.Flink DataStream api 实时计算 topN 热榜"></a>5.1.2.1.Flink DataStream api 实时计算 topN 热榜</h3><p><a href="https://zhuanlan.zhihu.com/p/356760456。" title="Flink datastream api 实时计算topN热榜">Flink DataStream api 实时计算topN热榜</a></p><ul><li>优点：可以按照用户自定义逻辑计算排名，基于 watermark 推动整个任务的计算，具备数据可回溯性。</li><li>缺点：开发成本高，而本期主要介绍 flink sql 的方案，这个方案可以供大家进行参考。</li><li><strong>结论：虽可实现，但并非 sql api 实现。</strong></li></ul><h3 id="5-1-2-2-Flink-SQL-api-实时计算-topN-热榜"><a href="#5-1-2-2-Flink-SQL-api-实时计算-topN-热榜" class="headerlink" title="5.1.2.2.Flink SQL api 实时计算 topN 热榜"></a>5.1.2.2.Flink SQL api 实时计算 topN 热榜</h3><p><a href="https://help.aliyun.com/document_detail/62508.html。" title="Flink sql TopN语句">Flink SQL TopN语句</a></p><p><a href="https://developer.aliyun.com/article/457445。" title="Flink SQL 功能解密系列 —— 流式 TopN 挑战与实现">Flink SQL 功能解密系列 —— 流式 TopN 挑战与实现</a></p><ul><li>优点：用户理解、开发成本低</li><li>缺点：只有排名发生变化的词条才会输出，排名未发生变化数据不会输出（后续会在<strong>数据建设</strong>模块进行解释），不能做到每一条数据包含目前 topN 的所有数据的需求。</li><li><strong>结论：不满足需求。</strong></li></ul><h3 id="5-1-2-3-结论"><a href="#5-1-2-3-结论" class="headerlink" title="5.1.2.3.结论"></a>5.1.2.3.结论</h3><p>我们需要制定自己的 flink sql 解决方案，以实现上述需求。这也是本节重点要讲述的内容，即在<strong>数据建设篇-具体实现方案详述</strong>详细展开。</p><h2 id="5-2-数据保障"><a href="#5-2-数据保障" class="headerlink" title="5.2.数据保障"></a>5.2.数据保障</h2><h3 id="5-2-1-难点"><a href="#5-2-1-难点" class="headerlink" title="5.2.1.难点"></a>5.2.1.难点</h3><ol><li>flink 任务高可用</li><li>榜单数据可回溯性</li></ol><h3 id="5-2-2-业界方案调研"><a href="#5-2-2-业界方案调研" class="headerlink" title="5.2.2.业界方案调研"></a>5.2.2.业界方案调研</h3><ol><li>flink 任务高可用：宕机之后快速恢复；有异地多活热备链路可随时切换</li><li>榜单数据可回溯性：任务失败之后，按照词条时间数据的进行回溯</li></ol><h2 id="5-3-数据服务保障"><a href="#5-3-数据服务保障" class="headerlink" title="5.3.数据服务保障"></a>5.3.数据服务保障</h2><h3 id="5-3-1-难点"><a href="#5-3-1-难点" class="headerlink" title="5.3.1.难点"></a>5.3.1.难点</h3><ol><li>数据服务引擎高可用</li><li>数据服务 server 高可用</li></ol><h3 id="5-3-2-业界方案调研"><a href="#5-3-2-业界方案调研" class="headerlink" title="5.3.2.业界方案调研"></a>5.3.2.业界方案调研</h3><ol><li>数据服务引擎高可用：数据服务引擎本身的高可用，异地双活实现</li><li>数据服务 server 高可用：异地双活实现；上游不更新数据，数据服务 server 模块也能查询出上一次的结果进行展示，至少不会什么数据都展示不了</li></ol><h1 id="6-数据建设篇-具体实现方案详述"><a href="#6-数据建设篇-具体实现方案详述" class="headerlink" title="6.数据建设篇-具体实现方案详述"></a>6.数据建设篇-具体实现方案详述</h1><h2 id="6-1-整体数据服务架构"><a href="#6-1-整体数据服务架构" class="headerlink" title="6.1.整体数据服务架构"></a>6.1.整体数据服务架构</h2><p>首先，我们最初的方案是如下图所示，单机房的服务端，但是很明显基本没有高可用保障。我们本文主要介绍 flink sql 方案，所以下文先介绍 flink sql，后文 6.6 介绍各种高可用、高性能优化及保障。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/topn-arch.png" alt="2"></p><h2 id="6-2-flink-方案设计"><a href="#6-2-flink-方案设计" class="headerlink" title="6.2.flink 方案设计"></a>6.2.flink 方案设计</h2><p>从本节开始，正式介绍 flink sql 相关的方案设计。</p><p>我们会从以下三个角度去介绍：</p><ol><li>数据源：了解数据源的 schema</li><li>数据汇：从数据应用角度出发设计数据汇的 schema</li><li>数据建设：从数据源、数据汇从而推导出我们要实现的 flink sql 方案</li></ol><h2 id="6-3-数据源"><a href="#6-3-数据源" class="headerlink" title="6.3.数据源"></a>6.3.数据源</h2><p>数据源即安装在各位的手机微博客户端上报的用户消费明细日志，即用户消费一次某个词条，就会上报一条对应的日志。</p><h3 id="6-3-1-schema"><a href="#6-3-1-schema" class="headerlink" title="6.3.1.schema"></a>6.3.1.schema</h3><table><thead><tr><th>字段名</th><th>备注</th></tr></thead><tbody><tr><td>user_id</td><td>消费词条的用户</td></tr><tr><td>热搜词条_name</td><td>消费词条名称</td></tr><tr><td>timestamp</td><td>消费词条时间戳</td></tr><tr><td>…</td><td>…</td></tr></tbody></table><h2 id="6-4-数据汇"><a href="#6-4-数据汇" class="headerlink" title="6.4 数据汇"></a>6.4 数据汇</h2><h3 id="6-4-1-schema"><a href="#6-4-1-schema" class="headerlink" title="6.4.1.schema"></a>6.4.1.schema</h3><p>最开始设计的 schema 如下：</p><table><thead><tr><th>字段名</th><th>字段类型</th><th>备注</th></tr></thead><tbody><tr><td>timestamp</td><td>bigint</td><td>当前分钟词条时间戳</td></tr><tr><td>热搜词条_name</td><td>string</td><td>词条名</td></tr><tr><td>rn</td><td>bigint</td><td>排名 1 - 50</td></tr></tbody></table><p>但是排名展示时，需要将这一分钟的前 50 名的数据全部查询到展示。而 flink 任务输出排名数据到外部存储时，保障前 50 名的词条数据事务性的输出（要么同时输出到数据服务中，要么一条也不输出）是一件比较复杂事情。所以我们索性将前 50 名的数据全部收集到同一条数据当中，时间戳最新的一条数据就是最新的结果数据。</p><p>重新设计的 schema 如下：</p><table><thead><tr><th>字段名</th><th>字段类型</th><th>备注</th></tr></thead><tbody><tr><td>timestamp</td><td>bigint</td><td>当前分钟词条时间戳</td></tr><tr><td>热搜榜单</td><td>string</td><td>热搜榜单，schema 如 {“排名第一的词条1” : “排名第一的词条消费量”, “排名第二的词条1” : “排名第二的词条消费量”,  “排名第三的词条1” : “排名第三的词条消费量”…} 前 50 名</td></tr></tbody></table><h2 id="6-5-数据建设"><a href="#6-5-数据建设" class="headerlink" title="6.5.数据建设"></a>6.5.数据建设</h2><h3 id="6-5-1-方案1-内层-rownum-外层自定义-udf"><a href="#6-5-1-方案1-内层-rownum-外层自定义-udf" class="headerlink" title="6.5.1.方案1 - 内层 rownum + 外层自定义 udf"></a>6.5.1.方案1 - 内层 rownum + 外层自定义 udf</h3><ol><li>从排名的角度出发，自然可以想到 <strong>rownum</strong> 进行排名（阿里云也有对应的实现案例）</li><li>最终要把排行榜合并到一条数据进行输出，那就必然会涉及到<strong>自定义 udf</strong> 将排名数据进行合并</li></ol><h4 id="6-5-1-1-sql"><a href="#6-5-1-1-sql" class="headerlink" title="6.5.1.1.sql"></a>6.5.1.1.sql</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span></span><br><span class="line">  target_db.target_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="built_in">max</span>(<span class="type">timestamp</span>) <span class="keyword">AS</span> <span class="type">timestamp</span>,</span><br><span class="line">  热搜_top50_json(热搜词条_name, cnt) <span class="keyword">AS</span> data <span class="comment">-- 外层 udaf 将所有数据进行 merge</span></span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">SELECT</span></span><br><span class="line">      热搜词条_name,</span><br><span class="line">      cnt,</span><br><span class="line">      <span class="type">timestamp</span>,</span><br><span class="line">      <span class="built_in">row_number</span>() <span class="keyword">over</span>(</span><br><span class="line">        <span class="keyword">PARTITION</span> <span class="keyword">by</span></span><br><span class="line">          热搜词条_name</span><br><span class="line">        <span class="keyword">ORDER</span> <span class="keyword">BY</span></span><br><span class="line">          cnt <span class="keyword">ASC</span></span><br><span class="line">      ) <span class="keyword">AS</span> rn <span class="comment">-- 内层 rownum 进行排名</span></span><br><span class="line">    <span class="keyword">FROM</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">SELECT</span></span><br><span class="line">          热搜词条_name,</span><br><span class="line">          <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">AS</span> cnt,</span><br><span class="line">          <span class="built_in">max</span>(<span class="type">timestamp</span>) <span class="keyword">AS</span> <span class="type">timestamp</span></span><br><span class="line">        <span class="keyword">FROM</span></span><br><span class="line">          source_db.source_table</span><br><span class="line">        <span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">          热搜词条_name</span><br><span class="line">        <span class="comment">-- 如果有热点词条导致数据倾斜，可以加一层打散层</span></span><br><span class="line">      )</span><br><span class="line">  )</span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">  rn <span class="operator">&lt;=</span> <span class="number">100</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  <span class="number">0</span>;</span><br></pre></td></tr></table></figure><h4 id="6-5-1-2-udf"><a href="#6-5-1-2-udf" class="headerlink" title="6.5.1.2.udf"></a>6.5.1.2.udf</h4><ul><li><p>udaf 开发参考： <a href="https://www.alibabacloud.com/help/zh/doc-detail/69553.htm?spm=a2c63.o282931.b99.244.4ad11889wWZiHL">https://www.alibabacloud.com/help/zh/doc-detail/69553.htm?spm=a2c63.o282931.b99.244.4ad11889wWZiHL</a></p></li><li><p>top50_udaf：作用是将已经经过上游处理的消费量排前 100 名词条拿到进行排序后，合并成一个 top50 排行榜 json 字符串产出。</p></li><li><p>Accumulator：由需求可以知道，当前 udaf 是为了计算前 50 名的消费词条，所以 Accumulator 应该存储截止当前时间按照消费 cnt 数排名的前 100 名的词条。我们由此就可以想到使用 <strong>最小堆</strong> 来当做 Accumulator，Accumulator 中只存储消费 cnt 前 100 的数据。</p></li><li><p>最小堆的实现：<a href="https://blog.csdn.net/jiutianhe/article/details/41441881">https://blog.csdn.net/jiutianhe/article/details/41441881</a></p></li></ul><p>topN 设计伪代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> 热搜<span class="title">_top50_json</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Map</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;, <span class="title">TopN</span>&lt;<span class="title">Pair</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> TopN&lt;Pair&lt;String, Long&gt;&gt; createAccumulator() &#123;</span><br><span class="line">        <span class="comment">// 创建 acc -&gt; 最小堆实现的 Top 50</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getValue</span><span class="params">(TopN&lt;Pair&lt;String, Long&gt;&gt; acc)</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">        <span class="comment">// 1.将最小堆 acc 中列表数据拿到</span></span><br><span class="line">        <span class="comment">// 2.然后将列表按照从大到小进行排序</span></span><br><span class="line">        <span class="comment">// 3.产出结果数据</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accumulate</span><span class="params">(TopN&lt;Pair&lt;String, Long&gt;&gt; acc, String 词条名称, <span class="keyword">long</span> cnt)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.获取到当前最小堆中的最小值</span></span><br><span class="line">        <span class="comment">// 如果当前词条的消费量 cnt 小于最小堆的堆顶</span></span><br><span class="line">        <span class="comment">// 则直接进行过滤</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 2.如果最小堆中不存在当前词条</span></span><br><span class="line">        <span class="comment">// 则直接将当前词条放入最小堆中</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 3.如果最小堆中已经存在当前词条存在</span></span><br><span class="line">        <span class="comment">// 那么将最小堆中这个词条的消费 cnt 与</span></span><br><span class="line">        <span class="comment">// 当前词条的 cnt 作比较，将大的那个放入最小堆中</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">retract</span><span class="params">(TopN&lt;Pair&lt;String, Long&gt;&gt; acc, String id, <span class="keyword">long</span> cnt)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 不需要实现 retract 方法</span></span><br><span class="line">        <span class="comment">// 由于 topn 具有特殊性：即我们只取每一个词条的最大值</span></span><br><span class="line">        <span class="comment">// 进行排名，所以可以不需要实现 retract 方法</span></span><br><span class="line">        <span class="comment">// 比较排名都在 accumulate 方法中已经实现完成</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Notes：</p><ul><li>上述 udf 最好设计成一个固定大小排行榜的 udf，比如一个 udf 实现类就只能用于处理一个固定大小的排行，防止用户进行】、<br>误用；</li><li>sql 内层计算的排行榜大小一定要比 sql 外层（聚合）排行榜大小大。举反例：假如内层计算前 30 名，外层计算前 50 名，内层 A 分桶第 31 名可能比 B 分桶第 1 名的值还大，但是 A 桶的第 31 名就不会被输出。反之则正确。</li></ul></blockquote><h4 id="6-5-1-3-flink-conf-yaml-参数配置"><a href="#6-5-1-3-flink-conf-yaml-参数配置" class="headerlink" title="6.5.1.3.flink-conf.yaml 参数配置"></a>6.5.1.3.flink-conf.yaml 参数配置</h4><p>由于上述 sql 是在无限流上的操作，所以上游数据每更新一次都会向下游发送一次 retract 消息以及最新的数据的消息进行计算。</p><p>那么就会存在这样一个问题，即 source qps 为 x 时，任务内的吞吐就为 x * n 倍，sink qps 也为 x，这会导致性能大幅下降的同时也会导致输出结果数据量非常大。</p><p>而我们只需要每分钟更新一次结果即可，所以可以使用 flink sql 自带的 minibatch 参数来控制输出结果的频次。</p><p>minibatch 具体参考可参考下面两篇文章：</p><ul><li><a href="https://www.alibabacloud.com/help/zh/doc-detail/182012.htm?spm=a2c63.p38356.b99.288.698a785cSiDhEG">https://www.alibabacloud.com/help/zh/doc-detail/182012.htm?spm=a2c63.p38356.b99.288.698a785cSiDhEG</a></li><li><a href="https://www.jianshu.com/p/aa2e94628e24">https://www.jianshu.com/p/aa2e94628e24</a></li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">table.exec.mini-batch.enabled :</span> <span class="literal">true</span></span><br><span class="line"><span class="string">--</span> <span class="string">minibatch</span> <span class="string">是下面两个任意一个符合条件就会起触发计算</span></span><br><span class="line"><span class="string">--</span> <span class="string">60s</span> <span class="string">一次</span></span><br><span class="line"><span class="attr">table.exec.mini-batch.allow-latency :</span> <span class="number">60</span> <span class="string">s</span></span><br><span class="line"><span class="string">--</span> <span class="string">数量达到</span> <span class="number">10000000000</span> <span class="string">触发一次</span></span><br><span class="line"><span class="string">--</span> <span class="string">设置为</span> <span class="number">10000000000</span> <span class="string">是为了让上面的</span> <span class="string">allow-latency</span> <span class="string">触发，每</span> <span class="string">60s</span> <span class="string">输出一次来满足我们的需求</span></span><br><span class="line"><span class="attr">table.exec.mini-batch.size :</span> <span class="number">10000000000</span></span><br></pre></td></tr></table></figure><p>状态过期，如果不设置的话，词条状态会越来越大，对非高热词条进行清除。</p><p><a href="http://apache-flink.147419.n8.nabble.com/Flink-sql-state-ttl-td10158.html">http://apache-flink.147419.n8.nabble.com/Flink-sql-state-ttl-td10158.html</a></p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">--</span> <span class="string">设置</span> <span class="number">1</span> <span class="string">天的</span> <span class="string">ttl，如果一天过后</span></span><br><span class="line"><span class="string">--</span> <span class="string">这个词条还没有更新，则直接删除</span></span><br><span class="line"><span class="attr">table.exec.state.ttl :</span> <span class="number">86400</span> <span class="string">s</span></span><br></pre></td></tr></table></figure><h3 id="6-5-2-方案2-自定义-udf"><a href="#6-5-2-方案2-自定义-udf" class="headerlink" title="6.5.2.方案2 - 自定义 udf"></a>6.5.2.方案2 - 自定义 udf</h3><ol><li><strong>自定义排名 udf</strong></li></ol><h4 id="6-5-2-1-sql"><a href="#6-5-2-1-sql" class="headerlink" title="6.5.2.1.sql"></a>6.5.2.1.sql</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> target_db.target_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="built_in">max</span>(<span class="type">timestamp</span>) <span class="keyword">AS</span> <span class="type">timestamp</span>,</span><br><span class="line">  <span class="comment">-- udf 计算每一个分桶的前 100 名列表</span></span><br><span class="line">  热搜_top50_json(<span class="built_in">cast</span>(热搜词条_name <span class="keyword">AS</span> string), cnt) <span class="keyword">AS</span> bucket_top100</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">SELECT</span></span><br><span class="line">      热搜词条_name <span class="keyword">AS</span> 热搜词条_name,</span><br><span class="line">      <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">AS</span> cnt,</span><br><span class="line">      <span class="built_in">max</span>(<span class="type">timestamp</span>) <span class="keyword">AS</span> <span class="type">timestamp</span></span><br><span class="line">    <span class="keyword">FROM</span></span><br><span class="line">      source_db.source_table</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">      热搜词条_name</span><br><span class="line">    <span class="comment">-- 如果有热点词条导致数据倾斜</span></span><br><span class="line">    <span class="comment">-- 可以加一层打散层</span></span><br><span class="line">  )</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  <span class="number">0</span></span><br><span class="line"><span class="comment">-- 由于这里是 group by 0</span></span><br><span class="line"><span class="comment">-- 所以可能会到导致热点，所以如果需要也可以加一层打散层</span></span><br><span class="line"><span class="comment">-- 在内部先算 top50，在外层将内部分桶的 top50 榜单进行 merge</span></span><br></pre></td></tr></table></figure><h4 id="6-5-2-2-udf"><a href="#6-5-2-2-udf" class="headerlink" title="6.5.2.2.udf"></a>6.5.2.2.udf</h4><p>此 udf 与 方案1 的 udf（见 6.5.1.2.udf） 完全相同。</p><h4 id="6-5-2-3-flink-conf-yaml-参数配置"><a href="#6-5-2-3-flink-conf-yaml-参数配置" class="headerlink" title="6.5.2.3.flink-conf.yaml 参数配置"></a>6.5.2.3.flink-conf.yaml 参数配置</h4><p>参数同 6.5.1.3 flink-conf.yaml 参数配置</p><h2 id="6-6-高可用、高性能"><a href="#6-6-高可用、高性能" class="headerlink" title="6.6.高可用、高性能"></a>6.6.高可用、高性能</h2><h3 id="6-6-1-整体高可用保障"><a href="#6-6-1-整体高可用保障" class="headerlink" title="6.6.1.整体高可用保障"></a>6.6.1.整体高可用保障</h3><p>异地双链路热备如下图：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/1.png" alt="2"></p><p>可能会发现图中有异地机房，但是我们目前只画出了 A 地区机房的数据链路，B 地区机房还没有画全，接着我们一步一步将这个图进行补全。</p><blockquote><p><strong>Notes：</strong></p><p><strong>异地双机房只是双链路的热备的一种案例。如果有同城双机房、双集群也可进行同样的服务部署。</strong></p><p><strong>为什么说异地机房的保障能力 &gt; 同城异地机房 &gt; 同城同机房双集群容灾能力？</strong></p><p><strong>同城同机房：只要这个机房挂了，即使你有两套链路也没救。</strong><br><strong>同城异地机房：很小几率情况会同城异地两个机房都挂了。。</strong><br><strong>异地机房：几乎不可能同时异地两个机房都被炸了。。。</strong></p></blockquote><h4 id="6-6-1-1-数据源日志高可用"><a href="#6-6-1-1-数据源日志高可用" class="headerlink" title="6.6.1.1.数据源日志高可用"></a>6.6.1.1.数据源日志高可用</h4><ul><li>数据源日志 server 服务高可用：异地机房，当一个机房挂了之后，在客户端可以自动将日志发送到另一个机房的 webserver</li><li>数据源日志 kafka 服务高可用：kafka 使用异地机房 topic，其实就是两个 topic，每个机房一个 topic，两个 topic 互为热备，producer 在向下游两个机房的 topic 写数据时，可以将 50% 的流量写入一个机房，另外 50% 的流量写入另一个机房，一旦一个机房的 kafka 集群宕机，则 producer 端可以自动将 100% 的流量切换到另一个机房的 kafka。</li></ul><p>正常情况下如图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/2.png" alt="2"></p><p>当发生 A 地机房 webserver 宕机时，客户端自动切换上报日志至 B 地机房 webserver。如下图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/3.png" alt="2"></p><p>kafka 也相同。如下图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/4.png" alt="2"></p><h4 id="6-6-1-2-flink-任务高可用"><a href="#6-6-1-2-flink-任务高可用" class="headerlink" title="6.6.1.2.flink 任务高可用"></a>6.6.1.2.flink 任务高可用</h4><p>flink 任务以 A 地机房做主链路，B 地机房启动相同的任务做热备双跑链路。</p><p>当 A 地机房 flink 任务宕机且无法恢复时，则 B 地机房的任务做热备替换。</p><p>正常情况下如图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/5.png" alt="2"></p><p>当 A 地机房 flink 任务宕机且无法恢复时，热备链路 flink 任务就可以顶上。如下图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/6.png" alt="2"></p><h4 id="6-6-1-3-数据服务高可用"><a href="#6-6-1-3-数据服务高可用" class="headerlink" title="6.6.1.3.数据服务高可用"></a>6.6.1.3.数据服务高可用</h4><p>正常情况如下：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/7.png" alt="2"></p><p>当 A 地 OLAP 或者 KV 存储挂了之后，webserver 可以自动切换至 B 地 OLAP 或者 KV 存储。如下图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/8.png" alt="2"></p><p>当 A 地 webserver 挂了之后，客户端可以自动拉取 B 地 webserver 数据，如下图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/9.png" alt="2"></p><h3 id="6-6-2-大流量、高性能"><a href="#6-6-2-大流量、高性能" class="headerlink" title="6.6.2.大流量、高性能"></a>6.6.2.大流量、高性能</h3><h4 id="6-6-2-1-数据源"><a href="#6-6-2-1-数据源" class="headerlink" title="6.6.2.1.数据源"></a>6.6.2.1.数据源</h4><ul><li>数据源、汇反序列化性能提升：静态反序列化性能 &gt; 动态反序列化性能。举例 ProtoBuf。可以在 source 端先进行代码生成，然后用生成好的代码去反序列化源消息的性能会远好于使用 ProtoBuf Dynamic Message。<a href="https://issues.apache.org/jira/browse/FLINK-18202?jql=project%20%3D%20FLINK%20AND%20text%20~%20%22protobuf%22%20ORDER%20BY%20created%20DESC。" title="flink 官方实现">flink 官方实现</a></li></ul><h3 id="6-6-3-缩减状态大小"><a href="#6-6-3-缩减状态大小" class="headerlink" title="6.6.3.缩减状态大小"></a>6.6.3.缩减状态大小</h3><ul><li>将状态中的 string 长度做映射之后变小</li><li>rocksdb 增量 checkpoint，减小任务做 checkpoint 的压力</li></ul><h1 id="7-数据服务篇-数据服务选型"><a href="#7-数据服务篇-数据服务选型" class="headerlink" title="7.数据服务篇-数据服务选型"></a>7.数据服务篇-数据服务选型</h1><h2 id="7-1-kv-存储"><a href="#7-1-kv-存储" class="headerlink" title="7.1.kv 存储"></a>7.1.kv 存储</h2><p>根据我们上述设计的数据汇 schema 来看，最适合存储引擎就是 kv 引擎，因为前端只需要展示最新的排行榜数据即可。所以我们可以使用 redis 等 kv 存储引擎来存储最新的数据。</p><h2 id="7-2-OLAP"><a href="#7-2-OLAP" class="headerlink" title="7.2.OLAP"></a>7.2.OLAP</h2><p>如果用户有需求需要记录上述数据的历史记录，我们也可以使用时序数据库或者 OLAP 引擎直接进行存储。</p><h1 id="8-数据保障篇-数据时效监控以及保障方案"><a href="#8-数据保障篇-数据时效监控以及保障方案" class="headerlink" title="8.数据保障篇-数据时效监控以及保障方案"></a>8.数据保障篇-数据时效监控以及保障方案</h1><h2 id="8-1-数据时效保障"><a href="#8-1-数据时效保障" class="headerlink" title="8.1.数据时效保障"></a>8.1.数据时效保障</h2><p>见下文。</p><h2 id="8-2-数据质量保障"><a href="#8-2-数据质量保障" class="headerlink" title="8.2.数据质量保障"></a>8.2.数据质量保障</h2><p>数据质量保障篇楼主正在 gang…</p><h1 id="9-效果篇-上述方案最终的效果"><a href="#9-效果篇-上述方案最终的效果" class="headerlink" title="9.效果篇-上述方案最终的效果"></a>9.效果篇-上述方案最终的效果</h1><h2 id="9-1-输出结果示例"><a href="#9-1-输出结果示例" class="headerlink" title="9.1.输出结果示例"></a>9.1.输出结果示例</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;黄子韬  杨紫是我哥们&quot;</span>: <span class="number">1672825</span>,</span><br><span class="line">  <span class="attr">&quot;延乔墓前的来信破防了&quot;</span>: <span class="number">1087416</span>,</span><br><span class="line">  <span class="attr">&quot;孟子义 张翰同学站起来&quot;</span>: <span class="number">747703</span></span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="9-2-应用产品示例"><a href="#9-2-应用产品示例" class="headerlink" title="9.2.应用产品示例"></a>9.2.应用产品示例</h2><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/weibo-topn.png" alt="1"></p><h1 id="10-现状以及展望篇"><a href="#10-现状以及展望篇" class="headerlink" title="10.现状以及展望篇"></a>10.现状以及展望篇</h1><ol><li>虽然上述 udf 是通用的 udf，但是是否能够脱离自定义 udf，直接计算出 top 50 的值？</li></ol><p>我目前的一个想法就是将结果 schema 拍平。举例：</p><table><thead><tr><th>字段名</th><th>字段类型</th><th>备注</th></tr></thead><tbody><tr><td>timestamp</td><td>bigint</td><td>当前分钟事件时间戳</td></tr><tr><td>热搜词条_1</td><td>string</td><td>第一名的热搜词条名称</td></tr><tr><td>热搜词条_2</td><td>string</td><td>第二名的热搜词条名称</td></tr><tr><td>热搜词条_3</td><td>string</td><td>第三名的热搜词条名称</td></tr><tr><td>热搜词条_4</td><td>string</td><td>第四名的热搜词条名称</td></tr><tr><td>热搜词条_5</td><td>string</td><td>第五名的热搜词条名称</td></tr><tr><td>…</td><td>…</td><td>…</td></tr><tr><td>热搜词条_n</td><td>string</td><td>第 n 名的词条名称</td></tr></tbody></table><p>每一次输出都将目前每一个排名的数据产出。但是目前在 flink sql 的实现思路上不太明了。</p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都比较短小，不定期更新，从一些博主的脑洞想法出发抛砖引玉，提高小伙伴的姿♂势水平。本文介绍博主对流批一体来源以及未来发展方向的一些理解，阅读时长大概 2 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>实战 | flink sql 计算 top n - 1</title>
    <link href="https://yangyichao-mango.github.io/2021/10/12/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn_code_pdf/"/>
    <id>https://yangyichao-mango.github.io/2021/10/12/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn_code_pdf/</id>
    <published>2021-10-12T06:21:53.000Z</published>
    <updated>2021-07-22T13:03:14.324Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>每家数字化企业在目前遇到流批一体概念的时候，都会对这个概念抱有一些疑问，到底什么是流批一体？这个概念的来源？这个概念能为用户、开发人员以及企业带来什么样的好处？跟随着博主的理解和脑洞出发吧。</p></blockquote><h1 id="1-序篇"><a href="#1-序篇" class="headerlink" title="1.序篇"></a>1.序篇</h1><p>通过本文你可以 get 到：</p><ol><li>背景篇</li><li>定义篇-属于哪类特点的指标</li><li>数据应用篇-预期效果是怎样的</li><li>难点剖析篇-此类指标建设、保障的难点</li><li>数据建设篇-具体实现方案详述</li><li>数据服务篇-数据服务选型</li><li>数据保障篇-数据时效监控以及保障方案</li><li>效果篇-上述方案最终的效果</li><li>现状以及展望篇</li></ol><h1 id="2-背景篇"><a href="#2-背景篇" class="headerlink" title="2.背景篇"></a>2.背景篇</h1><p>根据目前站内词条消费情况，计算 top 50 消费热度词条，每分钟更新一次，并且按照列表展现给用户。</p><p>其实就是类似于微博的热搜榜。</p><h1 id="3-定义篇-属于哪类特点的指标"><a href="#3-定义篇-属于哪类特点的指标" class="headerlink" title="3.定义篇-属于哪类特点的指标"></a>3.定义篇-属于哪类特点的指标</h1><p>这类指标可以统一划分到 topN 类别的指标中。即输入是具体词条消费日志，输出是词条消费排行榜。</p><h1 id="4-数据应用篇-预期效果是怎样的"><a href="#4-数据应用篇-预期效果是怎样的" class="headerlink" title="4.数据应用篇-预期效果是怎样的"></a>4.数据应用篇-预期效果是怎样的</h1><p>预期效果如下，即与微博热搜榜相同，都属于实时热点，一分钟更新一次。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/weibo-topn.png" alt="1"></p><h1 id="5-难点剖析篇-此类指标建设、保障的难点"><a href="#5-难点剖析篇-此类指标建设、保障的难点" class="headerlink" title="5.难点剖析篇-此类指标建设、保障的难点"></a>5.难点剖析篇-此类指标建设、保障的难点</h1><h2 id="5-1-数据建设"><a href="#5-1-数据建设" class="headerlink" title="5.1.数据建设"></a>5.1.数据建设</h2><h3 id="5-1-1-难点"><a href="#5-1-1-难点" class="headerlink" title="5.1.1.难点"></a>5.1.1.难点</h3><ol><li>榜单类的指标有一个特点，就是客户端获取到的数据必须是同一分钟当时的词条消费热度，这就要求我们产出的每一条数据需要包含 topN 中的所有数据。这样才能保障用户获取到的数据的一致性。</li><li>flink 任务大状态：词条多，状态大；词条具有时效性，所以对于低热词条需要进行删除</li><li>flink 任务大流量、高性能：数据源是全站的词条消费流量，得扛得住突发流量的暴揍</li></ol><h3 id="5-1-2-业界方案调研"><a href="#5-1-2-业界方案调研" class="headerlink" title="5.1.2.业界方案调研"></a>5.1.2.业界方案调研</h3><h3 id="5-1-2-1-Flink-DataStream-api-实时计算-topN-热榜"><a href="#5-1-2-1-Flink-DataStream-api-实时计算-topN-热榜" class="headerlink" title="5.1.2.1.Flink DataStream api 实时计算 topN 热榜"></a>5.1.2.1.Flink DataStream api 实时计算 topN 热榜</h3><p><a href="https://zhuanlan.zhihu.com/p/356760456。" title="Flink datastream api 实时计算topN热榜">Flink DataStream api 实时计算topN热榜</a></p><ul><li>优点：可以按照用户自定义逻辑计算排名，基于 watermark 推动整个任务的计算，具备数据可回溯性。</li><li>缺点：开发成本高，而本期主要介绍 flink sql 的方案，这个方案可以供大家进行参考。</li><li>结论：虽可实现，但并非 sql api 实现。</li></ul><h3 id="5-1-2-2-Flink-SQL-api-实时计算-topN-热榜"><a href="#5-1-2-2-Flink-SQL-api-实时计算-topN-热榜" class="headerlink" title="5.1.2.2.Flink SQL api 实时计算 topN 热榜"></a>5.1.2.2.Flink SQL api 实时计算 topN 热榜</h3><p><a href="https://help.aliyun.com/document_detail/62508.html。" title="Flink sql TopN语句">Flink SQL TopN语句</a></p><p><a href="https://developer.aliyun.com/article/457445。" title="Flink SQL 功能解密系列 —— 流式 TopN 挑战与实现">Flink SQL 功能解密系列 —— 流式 TopN 挑战与实现</a></p><ul><li>优点：用户理解、开发成本低</li><li>缺点：只有排名发生变化的词条才会输出，排名未发生变化数据不会输出（后续会在<strong>数据建设</strong>模块进行解释），不能做到每一条数据包含目前 topN 的所有数据的需求。</li><li>结论：不满足需求。</li></ul><h3 id="5-1-2-3-结论"><a href="#5-1-2-3-结论" class="headerlink" title="5.1.2.3.结论"></a>5.1.2.3.结论</h3><p>我们需要制定自己的 flink sql 解决方案，以实现上述需求。这也是本节重点要讲述的内容，即在<strong>数据建设篇-具体实现方案详述</strong>详细展开。</p><h2 id="5-2-数据保障"><a href="#5-2-数据保障" class="headerlink" title="5.2.数据保障"></a>5.2.数据保障</h2><h3 id="5-2-1-难点"><a href="#5-2-1-难点" class="headerlink" title="5.2.1.难点"></a>5.2.1.难点</h3><ol><li>flink 任务高可用</li><li>榜单数据可回溯性</li></ol><h3 id="5-2-2-业界方案调研"><a href="#5-2-2-业界方案调研" class="headerlink" title="5.2.2.业界方案调研"></a>5.2.2.业界方案调研</h3><ol><li>flink 任务高可用：宕机之后快速恢复；有异地多活热备链路可随时切换</li><li>榜单数据可回溯性：任务失败之后，按照词条时间数据的进行回溯</li></ol><h2 id="5-3-数据服务保障"><a href="#5-3-数据服务保障" class="headerlink" title="5.3.数据服务保障"></a>5.3.数据服务保障</h2><h3 id="5-3-1-难点"><a href="#5-3-1-难点" class="headerlink" title="5.3.1.难点"></a>5.3.1.难点</h3><ol><li>数据服务引擎高可用</li><li>数据服务 server 高可用</li></ol><h3 id="5-3-2-业界方案调研"><a href="#5-3-2-业界方案调研" class="headerlink" title="5.3.2.业界方案调研"></a>5.3.2.业界方案调研</h3><ol><li>数据服务引擎高可用：数据服务引擎本身的高可用，异地双活实现</li><li>数据服务 server 高可用：异地双活实现；上游不更新数据，数据服务 server 模块也能查询出上一次的结果进行展示，至少不会什么数据都展示不了</li></ol><h1 id="6-数据建设篇-具体实现方案详述"><a href="#6-数据建设篇-具体实现方案详述" class="headerlink" title="6.数据建设篇-具体实现方案详述"></a>6.数据建设篇-具体实现方案详述</h1><h2 id="6-1-整体数据服务架构"><a href="#6-1-整体数据服务架构" class="headerlink" title="6.1.整体数据服务架构"></a>6.1.整体数据服务架构</h2><p>首先，我们最初的方案是如下图所示，单机房的服务端，但是很明显基本没有高可用保障。我们本文主要介绍 flink sql 方案，所以下文先介绍 flink sql，后文 6.6 介绍各种高可用、高性能优化及保障。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/topn-arch.png" alt="2"></p><h2 id="6-2-flink-方案设计"><a href="#6-2-flink-方案设计" class="headerlink" title="6.2.flink 方案设计"></a>6.2.flink 方案设计</h2><p>从本节开始，正式介绍 flink sql 相关的方案设计。</p><p>我们会从以下三个角度去介绍：</p><ol><li>数据源：了解数据源的 schema</li><li>数据汇：从数据应用角度出发设计数据汇的 schema</li><li>数据建设：从数据源、数据汇从而推导出我们要实现的 flink sql 方案</li></ol><h2 id="6-3-数据源"><a href="#6-3-数据源" class="headerlink" title="6.3.数据源"></a>6.3.数据源</h2><p>数据源即安装在各位的手机微博客户端上报的用户消费明细日志，即用户消费一次某个词条，就会上报一条对应的日志。</p><h3 id="6-3-1-schema"><a href="#6-3-1-schema" class="headerlink" title="6.3.1.schema"></a>6.3.1.schema</h3><table><thead><tr><th>字段名</th><th>备注</th></tr></thead><tbody><tr><td>user_id</td><td>消费词条的用户</td></tr><tr><td>热搜词条_name</td><td>消费词条名称</td></tr><tr><td>timestamp</td><td>消费词条时间戳</td></tr><tr><td>…</td><td>…</td></tr></tbody></table><h2 id="6-4-数据汇"><a href="#6-4-数据汇" class="headerlink" title="6.4 数据汇"></a>6.4 数据汇</h2><h3 id="6-4-1-schema"><a href="#6-4-1-schema" class="headerlink" title="6.4.1.schema"></a>6.4.1.schema</h3><p>最开始设计的 schema 如下：</p><table><thead><tr><th>字段名</th><th>字段类型</th><th>备注</th></tr></thead><tbody><tr><td>timestamp</td><td>bigint</td><td>当前分钟词条时间戳</td></tr><tr><td>热搜词条_name</td><td>string</td><td>词条名</td></tr><tr><td>rn</td><td>bigint</td><td>排名 1 - 50</td></tr></tbody></table><p>但是排名展示时，需要将这一分钟的前 50 名的数据全部查询到展示。而 flink 任务输出排名数据到外部存储时，保障前 50 名的词条数据事务性的输出（要么同时输出到数据服务中，要么一条也不输出）是一件比较复杂事情。所以我们索性将前 50 名的数据全部收集到同一条数据当中，时间戳最新的一条数据就是最新的结果数据。</p><p>重新设计的 schema 如下：</p><table><thead><tr><th>字段名</th><th>字段类型</th><th>备注</th></tr></thead><tbody><tr><td>timestamp</td><td>bigint</td><td>当前分钟词条时间戳</td></tr><tr><td>top50_list</td><td>string</td><td>top50 榜单，scheam 如 {1 : “排名第一的词条1”, 2 : “排名第二的词条1”,  3 : “排名第三的词条1”…} 前 50 名</td></tr></tbody></table><h2 id="6-5-数据建设"><a href="#6-5-数据建设" class="headerlink" title="6.5.数据建设"></a>6.5.数据建设</h2><h3 id="6-5-1-方案1-内层-rownum-外层自定义-udf"><a href="#6-5-1-方案1-内层-rownum-外层自定义-udf" class="headerlink" title="6.5.1.方案1 - 内层 rownum + 外层自定义 udf"></a>6.5.1.方案1 - 内层 rownum + 外层自定义 udf</h3><ol><li>从排名的角度出发，自然可以想到 <strong>rownum</strong> 进行排名</li><li>最终要把排行榜合并到一条数据进行输出，那就必然会涉及到<strong>自定义 udf</strong> 将排名数据进行合并</li></ol><h4 id="6-5-1-1-sql"><a href="#6-5-1-1-sql" class="headerlink" title="6.5.1.1.sql"></a>6.5.1.1.sql</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span></span><br><span class="line">  target_db.target_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="built_in">max</span>(<span class="type">timestamp</span>) <span class="keyword">AS</span> <span class="type">timestamp</span>,</span><br><span class="line">  top50_udaf(热搜词条_name, cnt) <span class="keyword">AS</span> data <span class="comment">-- 外层 udaf 将所有数据进行 merge</span></span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">SELECT</span></span><br><span class="line">      热搜词条_name,</span><br><span class="line">      cnt,</span><br><span class="line">      <span class="type">timestamp</span>,</span><br><span class="line">      <span class="built_in">row_number</span>() <span class="keyword">over</span>(</span><br><span class="line">        <span class="keyword">PARTITION</span> <span class="keyword">by</span></span><br><span class="line">          热搜词条_name</span><br><span class="line">        <span class="keyword">ORDER</span> <span class="keyword">BY</span></span><br><span class="line">          cnt <span class="keyword">ASC</span></span><br><span class="line">      ) <span class="keyword">AS</span> rn <span class="comment">-- 内层 rownum 进行排名</span></span><br><span class="line">    <span class="keyword">FROM</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">SELECT</span></span><br><span class="line">          热搜词条_name,</span><br><span class="line">          <span class="built_in">sum</span>(cnt) <span class="keyword">AS</span> cnt,</span><br><span class="line">          <span class="built_in">max</span>(<span class="type">timestamp</span>) <span class="keyword">AS</span> <span class="type">timestamp</span></span><br><span class="line">        <span class="keyword">FROM</span></span><br><span class="line">          (</span><br><span class="line">            <span class="keyword">SELECT</span></span><br><span class="line">              热搜词条_name,</span><br><span class="line">              <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">AS</span> cnt,</span><br><span class="line">              <span class="built_in">max</span>(<span class="type">timestamp</span>) <span class="keyword">AS</span> <span class="type">timestamp</span></span><br><span class="line">            <span class="keyword">FROM</span></span><br><span class="line">              source_db.source_table</span><br><span class="line">            <span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">              热搜词条_name,</span><br><span class="line">              hash_mod_bucket(user_id, <span class="number">2048</span>)</span><br><span class="line">          )</span><br><span class="line">        <span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">          热搜词条_name</span><br><span class="line">      )</span><br><span class="line">  )</span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">  rn <span class="operator">&lt;=</span> <span class="number">100</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  <span class="number">0</span>;</span><br></pre></td></tr></table></figure><h4 id="6-5-1-2-udf"><a href="#6-5-1-2-udf" class="headerlink" title="6.5.1.2.udf"></a>6.5.1.2.udf</h4><ul><li><p>udaf 开发参考： <a href="https://www.alibabacloud.com/help/zh/doc-detail/69553.htm?spm=a2c63.o282931.b99.244.4ad11889wWZiHL">https://www.alibabacloud.com/help/zh/doc-detail/69553.htm?spm=a2c63.o282931.b99.244.4ad11889wWZiHL</a></p></li><li><p>top50_udaf：作用是将已经经过上游处理的消费量排前 100 名词条拿到进行排序后，合并成一个 top50 排行榜 json 字符串产出。</p></li><li><p>Accumulator：由需求可以知道，当前 udaf 是为了计算前 50 名的消费词条，所以 Accumulator 应该存储截止当前时间按照消费 cnt 数排名的前 100 名的词条。我们由此就可以想到使用 <strong>最小堆</strong> 来当做 Accumulator，Accumulator 中只存储消费 cnt 前 100 的数据。</p></li><li><p>最小堆的实现参考：<a href="https://blog.csdn.net/jiutianhe/article/details/41441881">https://blog.csdn.net/jiutianhe/article/details/41441881</a></p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TopN</span>&lt;<span class="title">T</span>&gt; <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> PriorityQueue&lt;T&gt; queue;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> maxSize;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Comparator&lt;T&gt; comparator;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> PriorityQueue&lt;T&gt; <span class="title">getQueue</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> queue;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">TopN</span><span class="params">(<span class="keyword">int</span> maxSize, Comparator&lt;T&gt; comparator)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (maxSize &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">this</span>.maxSize = maxSize;</span><br><span class="line">        <span class="keyword">this</span>.queue = <span class="keyword">new</span> PriorityQueue&lt;T&gt;(maxSize, comparator);</span><br><span class="line">        <span class="keyword">this</span>.comparator = comparator;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(T e)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (queue.size() &lt; <span class="keyword">this</span>.maxSize) &#123;</span><br><span class="line">            queue.add(e);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 如果队列已经超过 maxSize，则与最小堆堆顶的元素（最小值）进行比较判断</span></span><br><span class="line">            T peek = queue.peek();</span><br><span class="line">            <span class="keyword">if</span> (comparator.compare(e, peek) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                queue.poll();</span><br><span class="line">                queue.add(e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>topN 通用基类实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">TopN_UDAF</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Map</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;, <span class="title">TopN</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> topN;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">TopN_UDAF</span><span class="params">(<span class="keyword">int</span> topN)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.topN = topN;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> TopN&lt;Tuple2&lt;String, Long&gt;&gt; createAccumulator() &#123;</span><br><span class="line">        <span class="comment">// 创建 acc -&gt; 最小堆实现的 TopN</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> TopN&lt;&gt;(<span class="keyword">this</span>.topN, <span class="keyword">new</span> Comparator&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Tuple2&lt;String, Long&gt; o1, Tuple2&lt;String, Long&gt; o2)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (o1.equals(o2)) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> o1.f1.compareTo(o2.f1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Map&lt;String, Long&gt; <span class="title">getValue</span><span class="params">(TopN&lt;Tuple2&lt;String, Long&gt;&gt; acc)</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">        <span class="comment">// 将最小堆中数据拿到，然后排序之后放入结果 map 中传出</span></span><br><span class="line"></span><br><span class="line">        PriorityQueue&lt;Tuple2&lt;String, Long&gt;&gt; p = acc.getQueue();</span><br><span class="line"></span><br><span class="line">        List&lt;Tuple2&lt;String, Long&gt;&gt; topNList = p.stream()</span><br><span class="line">                .collect(Collectors.toList());</span><br><span class="line"></span><br><span class="line">        Collections.sort(topNList</span><br><span class="line">                , <span class="keyword">new</span> Comparator&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Tuple2&lt;String, Long&gt; o1, Tuple2&lt;String, Long&gt; o2)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">if</span> (o1.equals(o2)) &#123;</span><br><span class="line">                            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">return</span> o1.f1.compareTo(o2.f1);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        Map&lt;String, Long&gt; map = Maps.newLinkedHashMap();</span><br><span class="line"></span><br><span class="line">        top100List.forEach(<span class="keyword">new</span> Consumer&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(Tuple2&lt;String, Long&gt; t)</span> </span>&#123;</span><br><span class="line">                map.put(t.f0, t.f1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> map;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accumulate</span><span class="params">(TopN&lt;Tuple2&lt;String, Long&gt;&gt; acc, String id, <span class="keyword">long</span> cnt)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取到当前结果中的最小值，如果当前时间的消费 cnt 小于最小堆中的结果，直接进行过滤</span></span><br><span class="line">        <span class="keyword">long</span> min = CollectionUtils.isNotEmpty(acc.getQueue()) ? acc.getQueue().peek().f1 : <span class="number">0L</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (cnt &gt; min) &#123;</span><br><span class="line">            <span class="comment">// 1.如果最小堆中不存在当前词条，则直接将当前时间放入最小堆中；</span></span><br><span class="line">            <span class="comment">// 2.如果最小堆中已经存在当前词条存在，那么将消费 cnt 大的那个放入最小堆中</span></span><br><span class="line">            Tuple2&lt;String, Long&gt; t = Tuple2.of(id, cnt);</span><br><span class="line"></span><br><span class="line">            Map&lt;String, Long&gt; idCntMap = acc.getQueue()</span><br><span class="line">                    .stream()</span><br><span class="line">                    .collect(Collectors.toMap(tInner -&gt; tInner.f0, tInner -&gt; tInner.f1));</span><br><span class="line"></span><br><span class="line">            Long oldCnt = idCntMap.get(id);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">null</span> == oldCnt) &#123;</span><br><span class="line">                acc.add(t);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (cnt &gt; oldCnt) &#123;</span><br><span class="line">                    acc.getQueue().remove(Tuple2.of(id, oldCnt));</span><br><span class="line">                    acc.add(t);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">retract</span><span class="params">(TopN&lt;Tuple2&lt;String, Long&gt;&gt; acc, String id, <span class="keyword">long</span> cnt)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 由于 topn 具有特殊性：即我们只取每一个词条的最大值进行排名，所以可以不需要实现 retract 方法，比较排名都在 accumulate 方法中已经实现完成</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> TypeInformation&lt;TopN&lt;Tuple2&lt;String, Long&gt;&gt;&gt; getAccumulatorType() &#123;</span><br><span class="line">        <span class="keyword">return</span> TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;TopN&lt;Tuple2&lt;String, Long&gt;&gt;&gt;() &#123;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> TypeInformation&lt;Map&lt;String, Long&gt;&gt; getResultType() &#123;</span><br><span class="line">        <span class="keyword">return</span> TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Map&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>top100 实现类如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Top100_UDAF</span> <span class="keyword">extends</span> <span class="title">TopN_UDAF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Top100_UDAF</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(<span class="number">100</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>或者也可以通过 open 方法动态获取到 topN 的配置参数，具体方式实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(FunctionContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>.open(context);</span><br><span class="line">    <span class="keyword">this</span>.topN = Integer.parseInt(context.getJobParameter(<span class="string">&quot;TopN_UDAF.topN&quot;</span>, <span class="string">&quot;100&quot;</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Notes：</p><ul><li>上述 udf 最好设计成一个固定大小排行榜的 udf，比如一个 udf 实现类就只能用于处理一个固定大小的排行，防止用户误用；</li><li>sql 内层计算的排行榜大小一定要比 sql 外层（聚合）排行榜大小大。举反例：假如内层计算前 30 名，外层计算前 50 名，内层 A 分桶第 31 名可能比 B 分桶第 1 名的值还大，但是 A 桶的第 31 名就不会被输出。反之则正确。</li></ul></blockquote><h4 id="6-5-1-3-flink-conf-yaml-参数配置"><a href="#6-5-1-3-flink-conf-yaml-参数配置" class="headerlink" title="6.5.1.3.flink-conf.yaml 参数配置"></a>6.5.1.3.flink-conf.yaml 参数配置</h4><p>由于上述 sql 是在无限流上的操作，所以上游数据每更新一次都会向下游发送一次 retract 消息以及最新的数据的消息进行计算。</p><p>那么就会存在这样一个问题，即 source qps 为 x 时，任务内的吞吐就为 x * n 倍，sink qps 也为 x，这会导致性能大幅下降的同时也会导致输出结果数据量非常大。</p><p>而我们只需要每分钟更新一次结果即可，所以可以使用 flink sql 自带的 minibatch 参数来控制输出结果的频次。</p><p>minibatch 具体参考可参考下面两篇文章：</p><ul><li><a href="https://www.alibabacloud.com/help/zh/doc-detail/182012.htm?spm=a2c63.p38356.b99.288.698a785cSiDhEG">https://www.alibabacloud.com/help/zh/doc-detail/182012.htm?spm=a2c63.p38356.b99.288.698a785cSiDhEG</a></li><li><a href="https://www.jianshu.com/p/aa2e94628e24">https://www.jianshu.com/p/aa2e94628e24</a></li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">table.exec.mini-batch.enabled :</span> <span class="literal">true</span></span><br><span class="line"><span class="string">--</span> <span class="string">minibatch</span> <span class="string">是下面两个任意一个复合条件就会起触发计算</span></span><br><span class="line"><span class="string">--</span> <span class="string">60s</span> <span class="string">一次</span></span><br><span class="line"><span class="attr">table.exec.mini-batch.allow-latency :</span> <span class="number">60</span> <span class="string">s</span></span><br><span class="line"><span class="string">--</span> <span class="string">数量达到</span> <span class="number">10000000000</span> <span class="string">触发一次</span></span><br><span class="line"><span class="string">--</span> <span class="string">设置为</span> <span class="number">10000000000</span> <span class="string">是为了让上面的</span> <span class="string">allow-latency</span> <span class="string">触发，每</span> <span class="string">60s</span> <span class="string">输出一次来满足我们的需求</span></span><br><span class="line"><span class="attr">table.exec.mini-batch.size :</span> <span class="number">10000000000</span></span><br></pre></td></tr></table></figure><p>状态过期，如果不设置的话，词条状态会越来越大，对非高热词条进行清除。</p><p><a href="http://apache-flink.147419.n8.nabble.com/Flink-sql-state-ttl-td10158.html">http://apache-flink.147419.n8.nabble.com/Flink-sql-state-ttl-td10158.html</a></p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">--</span> <span class="string">设置</span> <span class="number">1</span> <span class="string">天的</span> <span class="string">ttl，如果一天过后这个词条还没有更新，则直接删除</span></span><br><span class="line"><span class="attr">table.exec.state.ttl :</span> <span class="number">86400</span> <span class="string">s</span></span><br></pre></td></tr></table></figure><h3 id="6-5-2-方案2-内外层自定义-udf"><a href="#6-5-2-方案2-内外层自定义-udf" class="headerlink" title="6.5.2.方案2 - 内外层自定义 udf"></a>6.5.2.方案2 - 内外层自定义 udf</h3><ol><li><strong>自定义内层排名 udf</strong></li><li><strong>自定义 udf</strong> 将排名数据进行合并</li></ol><h4 id="6-5-2-1-sql"><a href="#6-5-2-1-sql" class="headerlink" title="6.5.2.1.sql"></a>6.5.2.1.sql</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> target_db.target_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="comment">-- 计算整体的前 50 名列表</span></span><br><span class="line">  top50_aggr_udaf(bucket_top100) <span class="keyword">AS</span> top50_list,</span><br><span class="line">  <span class="built_in">max</span>(<span class="type">timestamp</span>) <span class="keyword">AS</span> <span class="type">timestamp</span></span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">SELECT</span></span><br><span class="line">      <span class="built_in">max</span>(<span class="type">timestamp</span>) <span class="keyword">AS</span> <span class="type">timestamp</span>,</span><br><span class="line">      <span class="comment">-- udf 计算每一个分桶的前 100 名列表</span></span><br><span class="line">      top100_udaf(<span class="built_in">cast</span>(热搜词条_name <span class="keyword">AS</span> string), cnt) <span class="keyword">AS</span> bucket_top100</span><br><span class="line">    <span class="keyword">FROM</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">SELECT</span> 热搜词条_name,</span><br><span class="line">          <span class="built_in">sum</span>(cnt) <span class="keyword">AS</span> cnt, <span class="comment">-- 计算消费 cnt 数</span></span><br><span class="line">          <span class="built_in">max</span>(<span class="type">timestamp</span>) <span class="keyword">AS</span> <span class="type">timestamp</span></span><br><span class="line">        <span class="keyword">FROM</span></span><br><span class="line">        (</span><br><span class="line">          <span class="keyword">SELECT</span></span><br><span class="line">            热搜词条_name <span class="keyword">AS</span> 热搜词条_name,</span><br><span class="line">            <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">AS</span> cnt,</span><br><span class="line">            <span class="built_in">max</span>(<span class="type">timestamp</span>) <span class="keyword">AS</span> <span class="type">timestamp</span></span><br><span class="line">          <span class="keyword">FROM</span></span><br><span class="line">            source_db.source_table</span><br><span class="line">          <span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">            热搜词条_name,</span><br><span class="line">            <span class="comment">-- 将数据打散，防止数据倾斜</span></span><br><span class="line">            hash_mod_bucket(user_id, <span class="number">2048</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">GROUP</span> <span class="keyword">BY</span> 热搜词条_name</span><br><span class="line">      )</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">      <span class="comment">-- 将数据打散，防止数据倾斜</span></span><br><span class="line">      hash_mod_bucket(热搜词条_name, <span class="number">2048</span>)</span><br><span class="line">  )</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  <span class="number">0</span>;</span><br></pre></td></tr></table></figure><h4 id="6-5-2-2-udf"><a href="#6-5-2-2-udf" class="headerlink" title="6.5.2.2.udf"></a>6.5.2.2.udf</h4><h5 id="6-5-2-2-1-top100-udaf"><a href="#6-5-2-2-1-top100-udaf" class="headerlink" title="6.5.2.2.1.top100_udaf"></a>6.5.2.2.1.top100_udaf</h5><p>此 udf 与 方案1 的 topN udf（见 4.2.1.2.udf） 完全相同。</p><h5 id="6-5-2-2-2-top50-aggr-udaf"><a href="#6-5-2-2-2-top50-aggr-udaf" class="headerlink" title="6.5.2.2.2.top50_aggr_udaf"></a>6.5.2.2.2.top50_aggr_udaf</h5><ul><li>Accumulator：依然同 top100_udaf 的实现，使用 <strong>最小堆</strong> 来当做 Accumulator；</li><li>accmulate：接受上游分桶中的数据，然后将每个桶中的词条获取到之后按照消费 cnt 与当前的 Accumulator 进行 merge 操作；</li><li>getValue：将 Accumulator 结果获取到之后放在 map 中并将结果输出；</li><li>retract：同上，也不需要实现，将各个分桶的结果做 merge 即可；</li></ul><p>基类实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">TopNAggr_UDAF</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>&lt;<span class="title">String</span>, <span class="title">TopN</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// topN</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> topN;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">TopNAggr_UDAF</span><span class="params">(<span class="keyword">int</span> topN)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.topN = topN;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> TopN&lt;Tuple2&lt;String, Long&gt;&gt; createAccumulator() &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> TopN&lt;&gt;(<span class="keyword">this</span>.topN, <span class="keyword">new</span> Comparator&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Tuple2&lt;String, Long&gt; o1, Tuple2&lt;String, Long&gt; o2)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (o1.equals(o2)) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> o1.f1.compareTo(o2.f1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getValue</span><span class="params">(TopN&lt;Tuple2&lt;String, Long&gt;&gt; acc)</span> </span>&#123;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 将结果从 acc 中拿到之后，其结果是乱序的，所以要把结果列表拿到之后进行再进行一遍排序</span></span><br><span class="line">        LinkedHashMap&lt;String, Map&lt;String, Long&gt;&gt; map = Maps.newLinkedHashMap();</span><br><span class="line"></span><br><span class="line">        List&lt;Tuple2&lt;String, Long&gt;&gt; topNList = acc</span><br><span class="line">                .getQueue()</span><br><span class="line">                .stream()</span><br><span class="line">                .collect(Collectors.toList());</span><br><span class="line"></span><br><span class="line">        Collections.sort(topNList</span><br><span class="line">                , <span class="keyword">new</span> Comparator&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Tuple2&lt;String, Long&gt; o1, Tuple2&lt;String, Long&gt; o2)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">if</span> (o1.equals(o2)) &#123;</span><br><span class="line">                            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">return</span> o2.f1.compareTo(o1.f1);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> i = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;String, Long&gt; t : topNList) &#123;</span><br><span class="line">            map.put(i + <span class="string">&quot;&quot;</span>, ImmutableMap.of(t.f0, t.f1));</span><br><span class="line">            i++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ObjectMapperUtils.toJSON(map);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accumulate</span><span class="params">(TopN&lt;Tuple2&lt;String, Long&gt;&gt; acc, Map&lt;String, Long&gt; map)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将 Accumulator 与分桶数据按照消费 cnt merge</span></span><br><span class="line">        Map&lt;String, Long&gt; idCntMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        acc.getQueue().forEach(<span class="keyword">new</span> Consumer&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(Tuple2&lt;String, Long&gt; t)</span> </span>&#123;</span><br><span class="line">                Long cnt = idCntMap.get(t.f0);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (<span class="keyword">null</span> == cnt) &#123;</span><br><span class="line">                    idCntMap.put(t.f0, t.f1);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    idCntMap.put(t.f0, cnt &gt; t.f1 ? cnt : t.f1);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        map.forEach(<span class="keyword">new</span> BiConsumer&lt;String, Long&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(String s, Long l)</span> </span>&#123;</span><br><span class="line">                Long cnt = idCntMap.get(s);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (<span class="keyword">null</span> == cnt) &#123;</span><br><span class="line">                    idCntMap.put(s, l);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    idCntMap.put(s, cnt &gt; l ? cnt : l);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        acc.getQueue().clear();</span><br><span class="line"></span><br><span class="line">        idCntMap.forEach(<span class="keyword">new</span> BiConsumer&lt;String, Long&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(String s, Long l)</span> </span>&#123;</span><br><span class="line">                acc.add(Tuple2.of(s, l));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">retract</span><span class="params">(TopN&lt;Tuple2&lt;String, Long&gt;&gt; acc, Map&lt;String, Long&gt; map)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 无需实现</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> TypeInformation&lt;TopN&lt;Tuple2&lt;String, Long&gt;&gt;&gt; getAccumulatorType() &#123;</span><br><span class="line">        <span class="keyword">return</span> TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;TopN&lt;Tuple2&lt;String, Long&gt;&gt;&gt;() &#123;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TypeInformation&lt;String&gt; <span class="title">getResultType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> TypeInformation.of(String.class);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>top50 实现类如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Top50Aggr_UDAF</span> <span class="keyword">extends</span> <span class="title">TopNAggr_UDAF</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Top50Aggr_UDAF</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(<span class="number">50</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>或者也可以通过 open 方法动态获取到 topN 的配置参数，具体方式实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(FunctionContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>.open(context);</span><br><span class="line">    <span class="keyword">this</span>.topN = Integer.parseInt(context.getJobParameter(<span class="string">&quot;TopNAggr_UDAF.topN&quot;</span>, <span class="string">&quot;50&quot;</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="6-5-2-3-flink-conf-yaml-参数配置"><a href="#6-5-2-3-flink-conf-yaml-参数配置" class="headerlink" title="6.5.2.3.flink-conf.yaml 参数配置"></a>6.5.2.3.flink-conf.yaml 参数配置</h4><p>参数同 4.2.1.3 flink-conf.yaml 参数配置</p><h3 id="6-5-3-方案3-cumulate-window"><a href="#6-5-3-方案3-cumulate-window" class="headerlink" title="6.5.3.方案3 - cumulate window"></a>6.5.3.方案3 - cumulate window</h3><p>如果你需要计算的是每天的排行榜，也可以使用 1.13 版本中最新的 cumulate window 进行计算。</p><h4 id="6-5-3-1-sql"><a href="#6-5-3-1-sql" class="headerlink" title="6.5.3.1.sql"></a>6.5.3.1.sql</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span></span><br><span class="line">  target_db.target_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  top50_aggr_udaf(bucket_top100, <span class="built_in">cast</span>(<span class="number">10</span> <span class="keyword">AS</span> <span class="type">bigint</span>)) <span class="keyword">AS</span> top50_list,</span><br><span class="line">  <span class="built_in">cast</span>(TUMBLE_START(<span class="type">timestamp</span>, <span class="type">INTERVAL</span> <span class="string">&#x27;60&#x27;</span> <span class="keyword">SECOND</span>) <span class="keyword">AS</span> <span class="type">bigint</span>) <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">AS</span> <span class="type">timestamp</span></span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">SELECT</span></span><br><span class="line">      top100_udaf(<span class="built_in">cast</span>(热搜词条_name <span class="keyword">AS</span> string), cnt) <span class="keyword">AS</span> bucket_top100,</span><br><span class="line">      TUMBLE_ROWTIME(<span class="type">timestamp</span>, <span class="type">INTERVAL</span> <span class="string">&#x27;60&#x27;</span> <span class="keyword">SECOND</span>) <span class="keyword">AS</span> <span class="type">timestamp</span></span><br><span class="line">    <span class="keyword">FROM</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">SELECT</span></span><br><span class="line">          热搜词条_name,</span><br><span class="line">          <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">AS</span> cnt,</span><br><span class="line">          cumulate_rowtime(<span class="type">timestamp</span>, <span class="type">INTERVAL</span> <span class="string">&#x27;60&#x27;</span> <span class="keyword">SECOND</span>, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>) <span class="keyword">AS</span> <span class="type">timestamp</span></span><br><span class="line">        <span class="keyword">FROM</span></span><br><span class="line">          source_db.source_table</span><br><span class="line">        <span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">          热搜词条_name,</span><br><span class="line">          CUMULATE(<span class="type">timestamp</span>, <span class="type">INTERVAL</span> <span class="string">&#x27;60&#x27;</span> <span class="keyword">SECOND</span>, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>)</span><br><span class="line">      )</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">      hash_mod_bucket(热搜词条_name),</span><br><span class="line">      TUMBLE(window_start, <span class="type">INTERVAL</span> <span class="string">&#x27;60&#x27;</span> <span class="keyword">SECOND</span>)</span><br><span class="line">  )</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  TUMBLE(window_start, <span class="type">INTERVAL</span> <span class="string">&#x27;60&#x27;</span> <span class="keyword">SECOND</span>)</span><br></pre></td></tr></table></figure><h4 id="6-5-3-2-udf"><a href="#6-5-3-2-udf" class="headerlink" title="6.5.3.2.udf"></a>6.5.3.2.udf</h4><h5 id="6-5-3-2-1-top100-udaf"><a href="#6-5-3-2-1-top100-udaf" class="headerlink" title="6.5.3.2.1.top100_udaf"></a>6.5.3.2.1.top100_udaf</h5><ul><li>Accumulator：由需求可以知道，当前 udaf 是为了计算前 100 名的消费词条，所以 Accumulator 应该存储截止当前时间按照消费 cnt 数排名的前 100 名的词条。我们由此就可以想到使用 <strong>最小堆</strong> 来当做 Accumulator，Accumulator 中只存储消费 cnt 前 100 的数据。</li><li>getValue：将 Accumulator 中的数据转换为 map 格式输出；</li><li>acumulate：将词条以及消费 cnt 存入 topN Accumulator 中；</li><li>merge：将所有 topN Accumulator 进行 merge 操作，取所有 topN 最大的；</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">TopN_UDAF</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Map</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;, <span class="title">TopN</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> topN;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">TopN_UDAF</span><span class="params">(<span class="keyword">int</span> topN)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.topN = topN;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> TopN&lt;Tuple2&lt;String, Long&gt;&gt; createAccumulator() &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> TopN&lt;&gt;(<span class="keyword">this</span>.topN, <span class="keyword">new</span> Comparator&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Tuple2&lt;String, Long&gt; o1, Tuple2&lt;String, Long&gt; o2)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">if</span> (o1.equals(o2)) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> o1.f1.compareTo(o2.f1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Map&lt;String, Long&gt; <span class="title">getValue</span><span class="params">(TopN&lt;Tuple2&lt;String, Long&gt;&gt; acc)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        PriorityQueue&lt;Tuple2&lt;String, Long&gt;&gt; p = acc.getQueue();</span><br><span class="line"></span><br><span class="line">        List&lt;Tuple2&lt;String, Long&gt;&gt; topNList = p.stream().collect(Collectors.toList());</span><br><span class="line"></span><br><span class="line">        Collections.sort(topNList</span><br><span class="line">                , <span class="keyword">new</span> Comparator&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Tuple2&lt;String, Long&gt; o1, Tuple2&lt;String, Long&gt; o2)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">if</span> (o1.equals(o2)) &#123;</span><br><span class="line">                            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">return</span> o1.f1.compareTo(o2.f1);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        Map&lt;String, Long&gt; map = Maps.newLinkedHashMap();</span><br><span class="line"></span><br><span class="line">        topNList.forEach(<span class="keyword">new</span> Consumer&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(Tuple2&lt;String, Long&gt; t)</span> </span>&#123;</span><br><span class="line">                map.put(t.f0, t.f1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> map;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accumulate</span><span class="params">(TopN&lt;Tuple2&lt;String, Long&gt;&gt; acc, String id, <span class="keyword">long</span> cnt)</span> </span>&#123;</span><br><span class="line">        Tuple2&lt;String, Long&gt; t = Tuple2.of(id, cnt);</span><br><span class="line">        acc.add(t);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">merge</span><span class="params">(TopN&lt;Tuple2&lt;String, Long&gt;&gt; acc, Iterable&lt;TopN&lt;Tuple2&lt;String, Long&gt;&gt;&gt; its)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        its.forEach(<span class="keyword">new</span> Consumer&lt;TopN&lt;Tuple2&lt;String, Long&gt;&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(TopN&lt;Tuple2&lt;String, Long&gt;&gt; topN)</span> </span>&#123;</span><br><span class="line">                topN.getQueue().iterator().forEachRemaining(<span class="keyword">new</span> Consumer&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(Tuple2&lt;String, Long&gt; t)</span> </span>&#123;</span><br><span class="line">                        acc.add(t);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> TypeInformation&lt;TopN&lt;Tuple2&lt;String, Long&gt;&gt;&gt; getAccumulatorType() &#123;</span><br><span class="line">        <span class="keyword">return</span> TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;TopN&lt;Tuple2&lt;String, Long&gt;&gt;&gt;() &#123;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> TypeInformation&lt;Map&lt;String, Long&gt;&gt; getResultType() &#123;</span><br><span class="line">        <span class="keyword">return</span> TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Map&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="6-5-3-2-2-top50Aggr-udaf"><a href="#6-5-3-2-2-top50Aggr-udaf" class="headerlink" title="6.5.3.2.2.top50Aggr_udaf"></a>6.5.3.2.2.top50Aggr_udaf</h5><ul><li>getValue：将 Accumulator 结果获取到之后，拿到 top50 之后放在 map 中并将结果输出；</li><li>acumulate：将内层桶的数据合并到当前 Acumulator 中，即将内层分桶的所有词条的数据以及消费 cnt 拿到之后进行累加；</li><li>merge：将 Accumulator 合并，相同词条的消费 cnt 相加；</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">TopNAggr_UDAF</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>&lt;<span class="title">String</span>, <span class="title">Map</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> topN;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">TopNAggr_UDAF</span><span class="params">(<span class="keyword">int</span> topN)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.topN = topN;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Map&lt;String, Long&gt; <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Maps.newHashMap();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getValue</span><span class="params">(Map&lt;String, Long&gt; acc)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        TopN&lt;Tuple2&lt;String, Long&gt;&gt; topN = <span class="keyword">new</span> TopN&lt;&gt;(<span class="keyword">this</span>.topN, <span class="keyword">new</span> Comparator&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Tuple2&lt;String, Long&gt; o1, Tuple2&lt;String, Long&gt; o2)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">if</span> (o1.equals(o2)) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> o1.f1.compareTo(o2.f1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        acc.forEach(<span class="keyword">new</span> BiConsumer&lt;String, Long&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(String s, Long l)</span> </span>&#123;</span><br><span class="line">                Tuple2&lt;String, Long&gt; t = Tuple2.of(s, l);</span><br><span class="line">                topN.add(t);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        LinkedHashMap&lt;String, Map&lt;String, Long&gt;&gt; map = Maps.newLinkedHashMap();</span><br><span class="line"></span><br><span class="line">        List&lt;Tuple2&lt;String, Long&gt;&gt; topNList = topN</span><br><span class="line">                .getQueue()</span><br><span class="line">                .stream()</span><br><span class="line">                .collect(Collectors.toList());</span><br><span class="line"></span><br><span class="line">        Collections.sort(topNList</span><br><span class="line">                , <span class="keyword">new</span> Comparator&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Tuple2&lt;String, Long&gt; o1, Tuple2&lt;String, Long&gt; o2)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">if</span> (o1.equals(o2)) &#123;</span><br><span class="line">                            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">return</span> o2.f1.compareTo(o1.f1);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> i = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;String, Long&gt; t : topNList) &#123;</span><br><span class="line">            map.put(i + <span class="string">&quot;&quot;</span>, ImmutableMap.of(t.f0, t.f1));</span><br><span class="line">            i++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ObjectMapperUtils.toJSON(map);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accumulate</span><span class="params">(Map&lt;String, Long&gt; acc, Map&lt;String, Long&gt; map)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        map.forEach(<span class="keyword">new</span> BiConsumer&lt;String, Long&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(String s, Long l)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                Long history = acc.get(s);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (<span class="keyword">null</span> != history) &#123;</span><br><span class="line">                    acc.put(s, l + history);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    acc.put(s, l);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">merge</span><span class="params">(Map&lt;String, Long&gt; acc, Iterable&lt;Map&lt;String, Long&gt;&gt; its)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        its.forEach(<span class="keyword">new</span> Consumer&lt;Map&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(Map&lt;String, Long&gt; map)</span> </span>&#123;</span><br><span class="line">                map.forEach(<span class="keyword">new</span> BiConsumer&lt;String, Long&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(String s, Long l)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                        Long history = acc.get(s);</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">if</span> (<span class="keyword">null</span> != history) &#123;</span><br><span class="line">                            acc.put(s, l + history);</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            acc.put(s, l);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> TypeInformation&lt;Map&lt;String, Long&gt;&gt; getAccumulatorType() &#123;</span><br><span class="line">        <span class="keyword">return</span> TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Map&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TypeInformation&lt;String&gt; <span class="title">getResultType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> TypeInformation.of(String.class);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="6-5-3-3-flink-conf-yaml-参数配置"><a href="#6-5-3-3-flink-conf-yaml-参数配置" class="headerlink" title="6.5.3.3.flink-conf.yaml 参数配置"></a>6.5.3.3.flink-conf.yaml 参数配置</h4><p>时间戳的时区设置，注意需要我们将时区设置为本地的时区。</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">--</span> <span class="string">北京时区</span></span><br><span class="line"><span class="attr">table.local-time-zone :</span> <span class="string">GMT+08:00</span></span><br></pre></td></tr></table></figure><h2 id="6-6-高可用、高性能"><a href="#6-6-高可用、高性能" class="headerlink" title="6.6.高可用、高性能"></a>6.6.高可用、高性能</h2><h3 id="6-6-1-整体高可用保障"><a href="#6-6-1-整体高可用保障" class="headerlink" title="6.6.1.整体高可用保障"></a>6.6.1.整体高可用保障</h3><p>异地双链路热备如下图：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/1.png" alt="2"></p><p>可能会发现图中有异地机房，但是我们目前只画出了 A 地区机房的数据链路，B 地区机房还没有画全，接着我们一步一步将这个图进行补全。</p><blockquote><p><strong>Notes：</strong></p><p><strong>异地双机房只是双链路的热备的一种案例。如果有同城双机房、双集群也可进行同样的服务部署。</strong></p><p><strong>为什么说异地机房的保障能力 &gt; 同城异地机房 &gt; 同城同机房双集群容灾能力？</strong></p><p><strong>同城同机房：只要这个机房挂了，即使你有两套链路也没救。</strong><br><strong>同城异地机房：很小几率情况会同城异地两个机房都挂了，除非整个城市被炸了。</strong><br><strong>异地机房：几乎不可能同时异地两个机房都被炸了。。。</strong></p></blockquote><h4 id="6-6-1-1-数据源日志高可用"><a href="#6-6-1-1-数据源日志高可用" class="headerlink" title="6.6.1.1.数据源日志高可用"></a>6.6.1.1.数据源日志高可用</h4><ul><li>数据源日志 server 服务高可用：异地机房，当一个机房挂了之后，在客户端可以自动将日志发送到另一个机房</li><li>数据源日志 kafka 服务高可用：kafka 使用异地机房，每个机房的 kafka 承担 50% 的流量，一旦一个机房的 kafka 集群宕机，则 producer 端可以自动将 100% 的流量切换到另一个机房的 kafka。</li></ul><p>正常情况下如图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/2.png" alt="2"></p><p>当发生 A 地机房 webserver 宕机时，客户端自动切换上报日志至 B 地机房 webserver。如下图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/3.png" alt="2"></p><p>kafka 也相同。如下图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/4.png" alt="2"></p><h4 id="6-6-1-2-flink-任务高可用"><a href="#6-6-1-2-flink-任务高可用" class="headerlink" title="6.6.1.2.flink 任务高可用"></a>6.6.1.2.flink 任务高可用</h4><p>flink 任务以 A 地机房做主链路，B 地机房启动相同的任务做热备双跑链路。</p><p>当 A 地机房 flink 任务宕机且无法恢复时，则 B 地机房的任务做热备替换。</p><p>正常情况下如图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/5.png" alt="2"></p><p>当 A 地机房 flink 任务宕机且无法恢复时，热备链路 flink 任务就可以顶上。如下图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/6.png" alt="2"></p><h4 id="6-6-1-3-数据服务高可用"><a href="#6-6-1-3-数据服务高可用" class="headerlink" title="6.6.1.3.数据服务高可用"></a>6.6.1.3.数据服务高可用</h4><p>正常情况如下：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/7.png" alt="2"></p><p>当 A 地 OLAP 或者 KV 存储挂了之后，webserver 可以自动切换至 B 地 OLAP 或者 KV 存储。如下图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/8.png" alt="2"></p><p>当 A 地 webserver 挂了之后，客户端可以自动拉取 B 地 webserver 数据，如下图所示：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/9.png" alt="2"></p><h3 id="6-6-2-大流量、高性能"><a href="#6-6-2-大流量、高性能" class="headerlink" title="6.6.2.大流量、高性能"></a>6.6.2.大流量、高性能</h3><h4 id="6-6-2-1-数据源"><a href="#6-6-2-1-数据源" class="headerlink" title="6.6.2.1.数据源"></a>6.6.2.1.数据源</h4><ul><li>任务数据源反序列化性能提升：静态反序列化性能 &gt; 动态反序列化性能。举例 ProtoBuf。可以在 source 端先进行代码生成，然后用生成好的代码去反序列化源消息的性能会远好于使用 ProtoBuf Dynamic Message。<a href="https://issues.apache.org/jira/browse/FLINK-18202?jql=project%20%3D%20FLINK%20AND%20text%20~%20%22protobuf%22%20ORDER%20BY%20created%20DESC。" title="flink 官方实现">flink 官方实现</a></li></ul><h3 id="6-6-3-缩减状态大小"><a href="#6-6-3-缩减状态大小" class="headerlink" title="6.6.3.缩减状态大小"></a>6.6.3.缩减状态大小</h3><ul><li>将状态中的 string 长度做映射之后变小</li><li>增量 checkpoint，减小任务做 checkpoint 的压力</li></ul><h1 id="7-数据服务篇-数据服务选型"><a href="#7-数据服务篇-数据服务选型" class="headerlink" title="7.数据服务篇-数据服务选型"></a>7.数据服务篇-数据服务选型</h1><h2 id="7-1-kv-存储"><a href="#7-1-kv-存储" class="headerlink" title="7.1.kv 存储"></a>7.1.kv 存储</h2><p>根据我们上述设计的数据汇 schema 来看，最适合存储引擎就是 kv 引擎，因为前端只需要展示最新的排行榜数据即可。所以我们可以使用 redis 等 kv 存储引擎来存储最新的数据。</p><h2 id="7-2-OLAP"><a href="#7-2-OLAP" class="headerlink" title="7.2.OLAP"></a>7.2.OLAP</h2><p>如果用户有需求需要记录上述数据的历史记录，我们也可以使用时序数据库或者 OLAP 引擎直接进行存储。</p><h1 id="8-数据保障篇-数据时效监控以及保障方案"><a href="#8-数据保障篇-数据时效监控以及保障方案" class="headerlink" title="8.数据保障篇-数据时效监控以及保障方案"></a>8.数据保障篇-数据时效监控以及保障方案</h1><h2 id="8-1-数据时效保障"><a href="#8-1-数据时效保障" class="headerlink" title="8.1.数据时效保障"></a>8.1.数据时效保障</h2><p>见下文。</p><h2 id="8-2-数据质量保障"><a href="#8-2-数据质量保障" class="headerlink" title="8.2.数据质量保障"></a>8.2.数据质量保障</h2><p>数据质量保障篇楼主正在 gang…</p><h1 id="9-效果篇-上述方案最终的效果"><a href="#9-效果篇-上述方案最终的效果" class="headerlink" title="9.效果篇-上述方案最终的效果"></a>9.效果篇-上述方案最终的效果</h1><h2 id="9-1-输出结果示例"><a href="#9-1-输出结果示例" class="headerlink" title="9.1.输出结果示例"></a>9.1.输出结果示例</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;1&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;黄子韬  杨紫是我哥们&quot;</span>: <span class="number">1672825</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;2&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;延乔墓前的来信破防了&quot;</span>: <span class="number">1087416</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;3&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;孟子义 张翰同学站起来&quot;</span>: <span class="number">747703</span></span><br><span class="line">  &#125;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="9-2-应用产品示例"><a href="#9-2-应用产品示例" class="headerlink" title="9.2.应用产品示例"></a>9.2.应用产品示例</h2><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/01_topn/weibo-topn.png" alt="1"></p><h1 id="10-现状以及展望篇"><a href="#10-现状以及展望篇" class="headerlink" title="10.现状以及展望篇"></a>10.现状以及展望篇</h1><ol><li>虽然上述 udf 是通用的 udf，但是是否能够脱离自定义 udf，直接计算出 top 50 的值？</li></ol><p>我目前的一个想法就是将结果 schema 拍平。举例：</p><table><thead><tr><th>字段名</th><th>字段类型</th><th>备注</th></tr></thead><tbody><tr><td>timestamp</td><td>bigint</td><td>当前分钟事件时间戳</td></tr><tr><td>热搜词条_1</td><td>string</td><td>第一名的热搜词条名称</td></tr><tr><td>热搜词条_2</td><td>string</td><td>第二名的热搜词条名称</td></tr><tr><td>热搜词条_3</td><td>string</td><td>第三名的热搜词条名称</td></tr><tr><td>热搜词条_4</td><td>string</td><td>第四名的热搜词条名称</td></tr><tr><td>热搜词条_5</td><td>string</td><td>第五名的热搜词条名称</td></tr><tr><td>…</td><td>…</td><td>…</td></tr><tr><td>热搜词条_n</td><td>string</td><td>第 n 名的词条名称</td></tr></tbody></table><p>每一次输出都将目前每一个排名的数据产出。但是目前在 flink sql 的实现思路上不太明显。</p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都比较短小，不定期更新，从一些博主的脑洞想法出发抛砖引玉，提高小伙伴的姿♂势水平。本文介绍博主对流批一体来源以及未来发展方向的一些理解，阅读时长大概 2 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>实战 | flink sql 计算多维 puv</title>
    <link href="https://yangyichao-mango.github.io/2021/10/12/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/02_dau-rownum/"/>
    <id>https://yangyichao-mango.github.io/2021/10/12/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/01_one-data/02_dau-rownum/</id>
    <published>2021-10-12T06:21:53.000Z</published>
    <updated>2021-07-05T13:15:45.647Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前沿-流批一体的一些理解"><a href="#前沿-流批一体的一些理解" class="headerlink" title="前沿 | 流批一体的一些理解"></a>前沿 | 流批一体的一些理解</h1><blockquote><p>每家数字化企业在目前遇到流批一体概念的时候，都会对这个概念抱有一些疑问，到底什么是流批一体？这个概念的来源？这个概念能为用户、开发人员以及企业带来什么样的好处？跟随着博主的理解和脑洞出发吧。</p></blockquote><p>通过本文你可以 get 到：</p><ul><li>1.</li><li>2.</li><li>3.</li><li>4.</li></ul><h1 id="1-背景"><a href="#1-背景" class="headerlink" title="1.背景"></a>1.背景</h1><p>多维 dau 相关指标。</p><h1 id="2-指标拆解"><a href="#2-指标拆解" class="headerlink" title="2.指标拆解"></a>2.指标拆解</h1><p>多维 dau。</p><h1 id="3-数据应用"><a href="#3-数据应用" class="headerlink" title="3.数据应用"></a>3.数据应用</h1><h2 id="3-1-应用层展示"><a href="#3-1-应用层展示" class="headerlink" title="3.1.应用层展示"></a>3.1.应用层展示</h2><p><img src="/blog-img/wechat-blog/01_data-warehouse/01_realtime-data-warehouse/02_data-construction/01_one-data/01_topn/weibo-topn.png" alt="1"></p><h2 id="3-2-数据服务架构"><a href="#3-2-数据服务架构" class="headerlink" title="3.2.数据服务架构"></a>3.2.数据服务架构</h2><p><img src="/blog-img/wechat-blog/01_data-warehouse/01_realtime-data-warehouse/02_data-construction/01_one-data/01_topn/topn-arch.png" alt="2"></p><h1 id="4-数据建设"><a href="#4-数据建设" class="headerlink" title="4.数据建设"></a>4.数据建设</h1><h2 id="4-1-数据源"><a href="#4-1-数据源" class="headerlink" title="4.1.数据源"></a>4.1.数据源</h2><p>数据源即安装在各位的手机微博客户端上报的用户消费明细日志，即用户消费一次某个事件，就会上报一条对应的日志。</p><h3 id="4-1-1-schema"><a href="#4-1-1-schema" class="headerlink" title="4.1.1.schema"></a>4.1.1.schema</h3><table><thead><tr><th>字段名</th><th>备注</th></tr></thead><tbody><tr><td>user_id</td><td>消费事件的用户</td></tr><tr><td>event_name</td><td>消费事件名称</td></tr><tr><td>timestamp</td><td>消费事件时间戳</td></tr><tr><td>…</td><td>…</td></tr></tbody></table><h2 id="4-2-数据汇"><a href="#4-2-数据汇" class="headerlink" title="4.2 数据汇"></a>4.2 数据汇</h2><h3 id="4-2-1-schema"><a href="#4-2-1-schema" class="headerlink" title="4.2.1.schema"></a>4.2.1.schema</h3><p>最开始设计的 schema 如下：</p><table><thead><tr><th>字段名</th><th>字段类型</th><th>备注</th></tr></thead><tbody><tr><td>timestamp</td><td>bigint</td><td>当前分钟事件时间戳</td></tr><tr><td>event_name</td><td>string</td><td>事件名</td></tr><tr><td>rn</td><td>bigint</td><td>排名 1 - 50</td></tr></tbody></table><p>但是排名展示时，需要将这一分钟前 50 名的数据全部查询到展示。而 flink 任务输出排名数据到外部存储时，保障前 50 名的事件数据事务性的输出是一件比较复杂事情。所以我们索性将前 50 名的数据全部收集到同一条数据当中，时间戳最新的一条数据就是最新的结果数据。</p><p>重新设计的 schema 如下：</p><table><thead><tr><th>字段名</th><th>字段类型</th><th>备注</th></tr></thead><tbody><tr><td>timestamp</td><td>bigint</td><td>当前分钟事件时间戳</td></tr><tr><td>top50_list</td><td>string</td><td>top50 榜单，scheam 如 {1 : “排名第一的事件1”, 2 : “排名第二的事件1”,  3 : “排名第三的事件1”…} 前 50 名</td></tr></tbody></table><h2 id="4-3-数据建设"><a href="#4-3-数据建设" class="headerlink" title="4.3.数据建设"></a>4.3.数据建设</h2><h3 id="4-3-1-方案1-内层-rownum-外层自定义-udf"><a href="#4-3-1-方案1-内层-rownum-外层自定义-udf" class="headerlink" title="4.3.1.方案1 - 内层 rownum + 外层自定义 udf"></a>4.3.1.方案1 - 内层 rownum + 外层自定义 udf</h3><h4 id="4-3-1-1-sql"><a href="#4-3-1-1-sql" class="headerlink" title="4.3.1.1.sql"></a>4.3.1.1.sql</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span></span><br><span class="line">  target_db.target_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="built_in">max</span>(<span class="type">timestamp</span>) <span class="keyword">AS</span> <span class="type">timestamp</span>,</span><br><span class="line">  top50_udaf(event_name, cnt) <span class="keyword">AS</span> data</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">SELECT</span></span><br><span class="line">      event_name,</span><br><span class="line">      cnt,</span><br><span class="line">      <span class="type">timestamp</span>,</span><br><span class="line">      <span class="built_in">row_number</span>() <span class="keyword">over</span>(</span><br><span class="line">        <span class="keyword">PARTITION</span> <span class="keyword">by</span></span><br><span class="line">          event_name</span><br><span class="line">        <span class="keyword">ORDER</span> <span class="keyword">BY</span></span><br><span class="line">          cnt <span class="keyword">ASC</span></span><br><span class="line">      ) <span class="keyword">AS</span> rn</span><br><span class="line">    <span class="keyword">FROM</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">SELECT</span></span><br><span class="line">          event_name,</span><br><span class="line">          <span class="built_in">sum</span>(cnt) <span class="keyword">AS</span> cnt,</span><br><span class="line">          <span class="built_in">max</span>(<span class="type">timestamp</span>) <span class="keyword">AS</span> <span class="type">timestamp</span></span><br><span class="line">        <span class="keyword">FROM</span></span><br><span class="line">          (</span><br><span class="line">            <span class="keyword">SELECT</span></span><br><span class="line">              event_name,</span><br><span class="line">              <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">AS</span> cnt,</span><br><span class="line">              <span class="built_in">max</span>(<span class="type">timestamp</span>) <span class="keyword">AS</span> <span class="type">timestamp</span></span><br><span class="line">            <span class="keyword">FROM</span></span><br><span class="line">              source_db.source_table</span><br><span class="line">            <span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">              event_name,</span><br><span class="line">              hash_mod_bucket(user_id, <span class="number">2048</span>)</span><br><span class="line">          )</span><br><span class="line">        <span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">          event_name</span><br><span class="line">      )</span><br><span class="line">  )</span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line">  rn <span class="operator">&lt;=</span> <span class="number">100</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  <span class="number">0</span>;</span><br></pre></td></tr></table></figure><h4 id="4-3-1-2-udf"><a href="#4-3-1-2-udf" class="headerlink" title="4.3.1.2.udf"></a>4.3.1.2.udf</h4><ul><li><p>udaf 开发参考： <a href="https://www.alibabacloud.com/help/zh/doc-detail/69553.htm?spm=a2c63.o282931.b99.244.4ad11889wWZiHL">https://www.alibabacloud.com/help/zh/doc-detail/69553.htm?spm=a2c63.o282931.b99.244.4ad11889wWZiHL</a></p></li><li><p>top50_udaf：作用是将已经经过上游处理的消费量排前 100 名事件拿到进行排序后，合并成一个排行榜 json 字符串产出。</p></li><li><p>Accumulator：由需求可以知道，当前 udaf 是为了计算前 50 名的消费事件，所以 Accumulator 应该存储截止当前时间按照消费 cnt 数排名的前 100 名的事件。我们由此就可以想到使用 <strong>最小堆</strong> 来当做 Accumulator，Accumulator 中只存储消费 cnt 前 100 的数据。</p></li><li><p>最小堆的实现参考：<a href="https://blog.csdn.net/jiutianhe/article/details/41441881">https://blog.csdn.net/jiutianhe/article/details/41441881</a></p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TopN</span>&lt;<span class="title">T</span>&gt; <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> PriorityQueue&lt;T&gt; queue;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> maxSize;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Comparator&lt;T&gt; comparator;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> PriorityQueue&lt;T&gt; <span class="title">getQueue</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> queue;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">TopN</span><span class="params">(<span class="keyword">int</span> maxSize, Comparator&lt;T&gt; comparator)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (maxSize &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">this</span>.maxSize = maxSize;</span><br><span class="line">        <span class="keyword">this</span>.queue = <span class="keyword">new</span> PriorityQueue&lt;T&gt;(maxSize, comparator);</span><br><span class="line">        <span class="keyword">this</span>.comparator = comparator;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(T e)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (queue.size() &lt; <span class="keyword">this</span>.maxSize) &#123;</span><br><span class="line">            queue.add(e);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 如果队列已经超过 maxSize，则与最小堆堆顶的元素（最小值）进行比较判断</span></span><br><span class="line">            T peek = queue.peek();</span><br><span class="line">            <span class="keyword">if</span> (comparator.compare(e, peek) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                queue.poll();</span><br><span class="line">                queue.add(e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>topN 通用基类实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">TopN_UDAF</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Map</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;, <span class="title">TopN</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> topN;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">TopN_UDAF</span><span class="params">(<span class="keyword">int</span> topN)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.topN = topN;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> TopN&lt;Tuple2&lt;String, Long&gt;&gt; createAccumulator() &#123;</span><br><span class="line">        <span class="comment">// 创建 acc -&gt; 最小堆实现的 TopN</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> TopN&lt;&gt;(<span class="keyword">this</span>.topN, <span class="keyword">new</span> Comparator&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Tuple2&lt;String, Long&gt; o1, Tuple2&lt;String, Long&gt; o2)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (o1.equals(o2)) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> o1.f1.compareTo(o2.f1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Map&lt;String, Long&gt; <span class="title">getValue</span><span class="params">(TopN&lt;Tuple2&lt;String, Long&gt;&gt; acc)</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">        <span class="comment">// 将最小堆中数据拿到，然后排序之后放入结果 map 中传出</span></span><br><span class="line"></span><br><span class="line">        PriorityQueue&lt;Tuple2&lt;String, Long&gt;&gt; p = acc.getQueue();</span><br><span class="line"></span><br><span class="line">        List&lt;Tuple2&lt;String, Long&gt;&gt; topNList = p.stream()</span><br><span class="line">                .collect(Collectors.toList());</span><br><span class="line"></span><br><span class="line">        Collections.sort(topNList</span><br><span class="line">                , <span class="keyword">new</span> Comparator&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Tuple2&lt;String, Long&gt; o1, Tuple2&lt;String, Long&gt; o2)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">if</span> (o1.equals(o2)) &#123;</span><br><span class="line">                            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">return</span> o1.f1.compareTo(o2.f1);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        Map&lt;String, Long&gt; map = Maps.newLinkedHashMap();</span><br><span class="line"></span><br><span class="line">        top100List.forEach(<span class="keyword">new</span> Consumer&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(Tuple2&lt;String, Long&gt; t)</span> </span>&#123;</span><br><span class="line">                map.put(t.f0, t.f1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> map;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accumulate</span><span class="params">(TopN&lt;Tuple2&lt;String, Long&gt;&gt; acc, String id, <span class="keyword">long</span> cnt)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取到当前结果中的最小值，如果当前时间的消费 cnt 小于最小堆中的结果，直接进行过滤</span></span><br><span class="line">        <span class="keyword">long</span> min = CollectionUtils.isNotEmpty(acc.getQueue()) ? acc.getQueue().peek().f1 : <span class="number">0L</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (cnt &gt; min) &#123;</span><br><span class="line">            <span class="comment">// 1.如果最小堆中不存在当前事件，则直接将当前时间放入最小堆中；</span></span><br><span class="line">            <span class="comment">// 2.如果最小堆中已经存在当前事件存在，那么将消费 cnt 大的那个放入最小堆中</span></span><br><span class="line">            Tuple2&lt;String, Long&gt; t = Tuple2.of(id, cnt);</span><br><span class="line"></span><br><span class="line">            Map&lt;String, Long&gt; idCntMap = acc.getQueue()</span><br><span class="line">                    .stream()</span><br><span class="line">                    .collect(Collectors.toMap(tInner -&gt; tInner.f0, tInner -&gt; tInner.f1));</span><br><span class="line"></span><br><span class="line">            Long oldCnt = idCntMap.get(id);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">null</span> == oldCnt) &#123;</span><br><span class="line">                acc.add(t);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (cnt &gt; oldCnt) &#123;</span><br><span class="line">                    acc.getQueue().remove(Tuple2.of(id, oldCnt));</span><br><span class="line">                    acc.add(t);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">retract</span><span class="params">(TopN&lt;Tuple2&lt;String, Long&gt;&gt; acc, String id, <span class="keyword">long</span> cnt)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 由于 topn 具有特殊性：即我们只取每一个事件的最大值进行排名，所以可以不需要实现 retract 方法，比较排名都在 accumulate 方法中已经实现完成</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> TypeInformation&lt;TopN&lt;Tuple2&lt;String, Long&gt;&gt;&gt; getAccumulatorType() &#123;</span><br><span class="line">        <span class="keyword">return</span> TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;TopN&lt;Tuple2&lt;String, Long&gt;&gt;&gt;() &#123;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> TypeInformation&lt;Map&lt;String, Long&gt;&gt; getResultType() &#123;</span><br><span class="line">        <span class="keyword">return</span> TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Map&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>top100 实现类如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Top100_UDAF</span> <span class="keyword">extends</span> <span class="title">TopN_UDAF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Top100_UDAF</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(<span class="number">100</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>或者也可以通过 open 方法动态获取到 topN 的配置参数，具体方式实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(FunctionContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>.open(context);</span><br><span class="line">    <span class="keyword">this</span>.topN = Integer.parseInt(context.getJobParameter(<span class="string">&quot;TopN_UDAF.topN&quot;</span>, <span class="string">&quot;100&quot;</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="4-3-1-3-flink-conf-yaml-参数配置"><a href="#4-3-1-3-flink-conf-yaml-参数配置" class="headerlink" title="4.3.1.3.flink-conf.yaml 参数配置"></a>4.3.1.3.flink-conf.yaml 参数配置</h4><p>由于上述 sql 是在 unbounded 流上的操作，所以上游数据每更新一次都会向下游发送一次 retract 消息以及最新的数据的消息进行计算。</p><p>举例即 source qps 为 x 时，任务内的吞吐就为 x * n 倍，sink qps 为 x，这样会导致性能大幅下降的同时也会导致输出结果数据量非常大。而我们只需要每分钟更新一次结果即可，所以可以使用 flink sql 自带的 minibatch 参数来控制输出结果的频次。</p><p>minibatch 具体参考可参考下面两篇文章：</p><ul><li><a href="https://www.alibabacloud.com/help/zh/doc-detail/182012.htm?spm=a2c63.p38356.b99.288.698a785cSiDhEG">https://www.alibabacloud.com/help/zh/doc-detail/182012.htm?spm=a2c63.p38356.b99.288.698a785cSiDhEG</a></li><li><a href="https://www.jianshu.com/p/aa2e94628e24">https://www.jianshu.com/p/aa2e94628e24</a></li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">table.exec.mini-batch.enabled :</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">table.exec.mini-batch.allow-latency :</span> <span class="number">60</span> <span class="string">s</span></span><br><span class="line"><span class="string">--</span> <span class="string">设置为</span> <span class="number">10000000000</span> <span class="string">是为了让上面的</span> <span class="string">allow-latency</span> <span class="string">起作用，每</span> <span class="string">60s</span> <span class="string">输出一次</span></span><br><span class="line"><span class="attr">table.exec.mini-batch.size :</span> <span class="number">10000000000</span></span><br></pre></td></tr></table></figure><p>状态过期，如果不设置的话，事件状态会越来越大。</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">--</span> <span class="string">设置</span> <span class="number">1</span> <span class="string">天的</span> <span class="string">ttl，如果一天过后这个事件还没有更新，则直接删除</span></span><br><span class="line"><span class="attr">table.exec.state.ttl :</span> <span class="number">86400</span> <span class="string">s</span></span><br></pre></td></tr></table></figure><h3 id="4-3-2-方案2-内外层自定义-udf"><a href="#4-3-2-方案2-内外层自定义-udf" class="headerlink" title="4.3.2.方案2 - 内外层自定义 udf"></a>4.3.2.方案2 - 内外层自定义 udf</h3><h4 id="4-3-2-1-sql"><a href="#4-3-2-1-sql" class="headerlink" title="4.3.2.1.sql"></a>4.3.2.1.sql</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> target_db.target_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="comment">-- 计算整体的前 50 名列表</span></span><br><span class="line">  top50_aggr_udaf(bucket_top100) <span class="keyword">AS</span> top50_list,</span><br><span class="line">  <span class="built_in">max</span>(<span class="type">timestamp</span>) <span class="keyword">AS</span> <span class="type">timestamp</span></span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">SELECT</span></span><br><span class="line">      <span class="built_in">max</span>(<span class="type">timestamp</span>) <span class="keyword">AS</span> <span class="type">timestamp</span>,</span><br><span class="line">      <span class="comment">-- udf 计算每一个分桶的前 100 名列表</span></span><br><span class="line">      top100_udaf(<span class="built_in">cast</span>(event_name <span class="keyword">AS</span> string), cnt) <span class="keyword">AS</span> bucket_top100</span><br><span class="line">    <span class="keyword">FROM</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">SELECT</span> event_name,</span><br><span class="line">          <span class="built_in">sum</span>(cnt) <span class="keyword">AS</span> cnt, <span class="comment">-- 计算消费 cnt 数</span></span><br><span class="line">          <span class="built_in">max</span>(<span class="type">timestamp</span>) <span class="keyword">AS</span> <span class="type">timestamp</span></span><br><span class="line">        <span class="keyword">FROM</span></span><br><span class="line">        (</span><br><span class="line">          <span class="keyword">SELECT</span></span><br><span class="line">            event_name <span class="keyword">AS</span> event_name,</span><br><span class="line">            <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">AS</span> cnt,</span><br><span class="line">            <span class="built_in">max</span>(<span class="type">timestamp</span>) <span class="keyword">AS</span> <span class="type">timestamp</span></span><br><span class="line">          <span class="keyword">FROM</span></span><br><span class="line">            source_db.source_table</span><br><span class="line">          <span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">            event_name,</span><br><span class="line">            <span class="comment">-- 将数据打散，防止数据倾斜</span></span><br><span class="line">            hash_mod_bucket(user_id, <span class="number">2048</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">GROUP</span> <span class="keyword">BY</span> event_name</span><br><span class="line">      )</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">      <span class="comment">-- 将数据打散，防止数据倾斜</span></span><br><span class="line">      hash_mod_bucket(event_name, <span class="number">2048</span>)</span><br><span class="line">  )</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  <span class="number">0</span>;</span><br></pre></td></tr></table></figure><h4 id="4-3-2-2-udf"><a href="#4-3-2-2-udf" class="headerlink" title="4.3.2.2.udf"></a>4.3.2.2.udf</h4><h5 id="4-3-2-2-1-top100-udaf"><a href="#4-3-2-2-1-top100-udaf" class="headerlink" title="4.3.2.2.1.top100_udaf"></a>4.3.2.2.1.top100_udaf</h5><p>此 udf 与 方案1 的 topN udf（见 4.2.1.2.udf） 完全相同。</p><h5 id="4-3-2-2-2-top50-aggr-udaf"><a href="#4-3-2-2-2-top50-aggr-udaf" class="headerlink" title="4.3.2.2.2.top50_aggr_udaf"></a>4.3.2.2.2.top50_aggr_udaf</h5><ul><li>Accumulator：依然同 top100_udaf 的实现，使用 <strong>最小堆</strong> 来当做 Accumulator；</li><li>accmulate：接受上游分桶中的数据，然后将每个桶中的事件获取到之后按照消费 cnt 与当前的 Accumulator 进行 merge 操作；</li><li>getValue：将 Accumulator 结果获取到之后放在 map 中并将结果输出；</li><li>retract：同上，也不需要实现，将各个分桶的结果做 merge 即可；</li></ul><p>基类实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">TopNAggr_UDAF</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>&lt;<span class="title">String</span>, <span class="title">TopN</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// topN</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> topN;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">TopNAggr_UDAF</span><span class="params">(<span class="keyword">int</span> topN)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.topN = topN;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> TopN&lt;Tuple2&lt;String, Long&gt;&gt; createAccumulator() &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> TopN&lt;&gt;(<span class="keyword">this</span>.topN, <span class="keyword">new</span> Comparator&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Tuple2&lt;String, Long&gt; o1, Tuple2&lt;String, Long&gt; o2)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (o1.equals(o2)) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> o1.f1.compareTo(o2.f1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getValue</span><span class="params">(TopN&lt;Tuple2&lt;String, Long&gt;&gt; acc)</span> </span>&#123;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 将结果从 acc 中拿到之后，其结果是乱序的，所以要把结果列表拿到之后进行再进行一遍排序</span></span><br><span class="line">        LinkedHashMap&lt;String, Map&lt;String, Long&gt;&gt; map = Maps.newLinkedHashMap();</span><br><span class="line"></span><br><span class="line">        List&lt;Tuple2&lt;String, Long&gt;&gt; topNList = acc</span><br><span class="line">                .getQueue()</span><br><span class="line">                .stream()</span><br><span class="line">                .collect(Collectors.toList());</span><br><span class="line"></span><br><span class="line">        Collections.sort(topNList</span><br><span class="line">                , <span class="keyword">new</span> Comparator&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Tuple2&lt;String, Long&gt; o1, Tuple2&lt;String, Long&gt; o2)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">if</span> (o1.equals(o2)) &#123;</span><br><span class="line">                            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">return</span> o2.f1.compareTo(o1.f1);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> i = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;String, Long&gt; t : topNList) &#123;</span><br><span class="line">            map.put(i + <span class="string">&quot;&quot;</span>, ImmutableMap.of(t.f0, t.f1));</span><br><span class="line">            i++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ObjectMapperUtils.toJSON(map);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accumulate</span><span class="params">(TopN&lt;Tuple2&lt;String, Long&gt;&gt; acc, Map&lt;String, Long&gt; map)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将 Accumulator 与分桶数据按照消费 cnt merge</span></span><br><span class="line">        Map&lt;String, Long&gt; idCntMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        acc.getQueue().forEach(<span class="keyword">new</span> Consumer&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(Tuple2&lt;String, Long&gt; t)</span> </span>&#123;</span><br><span class="line">                Long cnt = idCntMap.get(t.f0);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (<span class="keyword">null</span> == cnt) &#123;</span><br><span class="line">                    idCntMap.put(t.f0, t.f1);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    idCntMap.put(t.f0, cnt &gt; t.f1 ? cnt : t.f1);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        map.forEach(<span class="keyword">new</span> BiConsumer&lt;String, Long&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(String s, Long l)</span> </span>&#123;</span><br><span class="line">                Long cnt = idCntMap.get(s);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (<span class="keyword">null</span> == cnt) &#123;</span><br><span class="line">                    idCntMap.put(s, l);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    idCntMap.put(s, cnt &gt; l ? cnt : l);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        acc.getQueue().clear();</span><br><span class="line"></span><br><span class="line">        idCntMap.forEach(<span class="keyword">new</span> BiConsumer&lt;String, Long&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(String s, Long l)</span> </span>&#123;</span><br><span class="line">                acc.add(Tuple2.of(s, l));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">retract</span><span class="params">(TopN&lt;Tuple2&lt;String, Long&gt;&gt; acc, Map&lt;String, Long&gt; map)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 无需实现</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> TypeInformation&lt;TopN&lt;Tuple2&lt;String, Long&gt;&gt;&gt; getAccumulatorType() &#123;</span><br><span class="line">        <span class="keyword">return</span> TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;TopN&lt;Tuple2&lt;String, Long&gt;&gt;&gt;() &#123;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TypeInformation&lt;String&gt; <span class="title">getResultType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> TypeInformation.of(String.class);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>top50 实现类如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Top50Aggr_UDAF</span> <span class="keyword">extends</span> <span class="title">TopNAggr_UDAF</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Top50Aggr_UDAF</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(<span class="number">50</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>或者也可以通过 open 方法动态获取到 topN 的配置参数，具体方式实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(FunctionContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>.open(context);</span><br><span class="line">    <span class="keyword">this</span>.topN = Integer.parseInt(context.getJobParameter(<span class="string">&quot;TopNAggr_UDAF.topN&quot;</span>, <span class="string">&quot;50&quot;</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="4-3-2-3-flink-conf-yaml-参数配置"><a href="#4-3-2-3-flink-conf-yaml-参数配置" class="headerlink" title="4.3.2.3.flink-conf.yaml 参数配置"></a>4.3.2.3.flink-conf.yaml 参数配置</h4><p>参数同 4.2.1.3 flink-conf.yaml 参数配置</p><h3 id="4-3-3-方案3-cumulate-window"><a href="#4-3-3-方案3-cumulate-window" class="headerlink" title="4.3.3.方案3 - cumulate window"></a>4.3.3.方案3 - cumulate window</h3><h4 id="4-3-3-1-sql"><a href="#4-3-3-1-sql" class="headerlink" title="4.3.3.1.sql"></a>4.3.3.1.sql</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span></span><br><span class="line">  target_db.target_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  top50_aggr_udaf(bucket_top100, <span class="built_in">cast</span>(<span class="number">10</span> <span class="keyword">AS</span> <span class="type">bigint</span>)) <span class="keyword">AS</span> top50_list,</span><br><span class="line">  <span class="built_in">cast</span>(TUMBLE_START(<span class="type">timestamp</span>, <span class="type">INTERVAL</span> <span class="string">&#x27;60&#x27;</span> <span class="keyword">SECOND</span>) <span class="keyword">AS</span> <span class="type">bigint</span>) <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">AS</span> <span class="type">timestamp</span></span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">SELECT</span></span><br><span class="line">      top100_udaf(<span class="built_in">cast</span>(event_name <span class="keyword">AS</span> string), cnt) <span class="keyword">AS</span> bucket_top100,</span><br><span class="line">      TUMBLE_ROWTIME(<span class="type">timestamp</span>, <span class="type">INTERVAL</span> <span class="string">&#x27;60&#x27;</span> <span class="keyword">SECOND</span>) <span class="keyword">AS</span> <span class="type">timestamp</span></span><br><span class="line">    <span class="keyword">FROM</span></span><br><span class="line">      (</span><br><span class="line">        <span class="keyword">SELECT</span></span><br><span class="line">          event_name,</span><br><span class="line">          <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">AS</span> cnt,</span><br><span class="line">          cumulate_rowtime(<span class="type">timestamp</span>, <span class="type">INTERVAL</span> <span class="string">&#x27;60&#x27;</span> <span class="keyword">SECOND</span>, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>) <span class="keyword">AS</span> <span class="type">timestamp</span></span><br><span class="line">        <span class="keyword">FROM</span></span><br><span class="line">          source_db.source_table</span><br><span class="line">        <span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">          event_name,</span><br><span class="line">          CUMULATE(<span class="type">timestamp</span>, <span class="type">INTERVAL</span> <span class="string">&#x27;60&#x27;</span> <span class="keyword">SECOND</span>, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>)</span><br><span class="line">      )</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">      hash_mod_bucket(event_name),</span><br><span class="line">      TUMBLE(window_start, <span class="type">INTERVAL</span> <span class="string">&#x27;60&#x27;</span> <span class="keyword">SECOND</span>)</span><br><span class="line">  )</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  TUMBLE(window_start, <span class="type">INTERVAL</span> <span class="string">&#x27;60&#x27;</span> <span class="keyword">SECOND</span>)</span><br></pre></td></tr></table></figure><h3 id="4-3-3-2-udf"><a href="#4-3-3-2-udf" class="headerlink" title="4.3.3.2.udf"></a>4.3.3.2.udf</h3><h4 id="4-3-3-2-1-top100-udaf"><a href="#4-3-3-2-1-top100-udaf" class="headerlink" title="4.3.3.2.1.top100_udaf"></a>4.3.3.2.1.top100_udaf</h4><ul><li>Accumulator：由需求可以知道，当前 udaf 是为了计算前 100 名的消费事件，所以 Accumulator 应该存储截止当前时间按照消费 cnt 数排名的前 100 名的事件。我们由此就可以想到使用 <strong>最小堆</strong> 来当做 Accumulator，Accumulator 中只存储消费 cnt 前 100 的数据。</li><li>getValue：将 Accumulator 中的数据转换为 map 格式输出；</li><li>acumulate：将事件以及消费 cnt 存入 topN Accumulator 中；</li><li>merge：将所有 topN Accumulator 进行 merge 操作，取所有 topN 最大的；</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">TopN_UDAF</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Map</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;, <span class="title">TopN</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> topN;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">TopN_UDAF</span><span class="params">(<span class="keyword">int</span> topN)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.topN = topN;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> TopN&lt;Tuple2&lt;String, Long&gt;&gt; createAccumulator() &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> TopN&lt;&gt;(<span class="keyword">this</span>.topN, <span class="keyword">new</span> Comparator&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Tuple2&lt;String, Long&gt; o1, Tuple2&lt;String, Long&gt; o2)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">if</span> (o1.equals(o2)) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> o1.f1.compareTo(o2.f1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Map&lt;String, Long&gt; <span class="title">getValue</span><span class="params">(TopN&lt;Tuple2&lt;String, Long&gt;&gt; acc)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        PriorityQueue&lt;Tuple2&lt;String, Long&gt;&gt; p = acc.getQueue();</span><br><span class="line"></span><br><span class="line">        List&lt;Tuple2&lt;String, Long&gt;&gt; topNList = p.stream().collect(Collectors.toList());</span><br><span class="line"></span><br><span class="line">        Collections.sort(topNList</span><br><span class="line">                , <span class="keyword">new</span> Comparator&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Tuple2&lt;String, Long&gt; o1, Tuple2&lt;String, Long&gt; o2)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">if</span> (o1.equals(o2)) &#123;</span><br><span class="line">                            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">return</span> o1.f1.compareTo(o2.f1);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        Map&lt;String, Long&gt; map = Maps.newLinkedHashMap();</span><br><span class="line"></span><br><span class="line">        topNList.forEach(<span class="keyword">new</span> Consumer&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(Tuple2&lt;String, Long&gt; t)</span> </span>&#123;</span><br><span class="line">                map.put(t.f0, t.f1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> map;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accumulate</span><span class="params">(TopN&lt;Tuple2&lt;String, Long&gt;&gt; acc, String id, <span class="keyword">long</span> cnt)</span> </span>&#123;</span><br><span class="line">        Tuple2&lt;String, Long&gt; t = Tuple2.of(id, cnt);</span><br><span class="line">        acc.add(t);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">merge</span><span class="params">(TopN&lt;Tuple2&lt;String, Long&gt;&gt; acc, Iterable&lt;TopN&lt;Tuple2&lt;String, Long&gt;&gt;&gt; its)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        its.forEach(<span class="keyword">new</span> Consumer&lt;TopN&lt;Tuple2&lt;String, Long&gt;&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(TopN&lt;Tuple2&lt;String, Long&gt;&gt; topN)</span> </span>&#123;</span><br><span class="line">                topN.getQueue().iterator().forEachRemaining(<span class="keyword">new</span> Consumer&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(Tuple2&lt;String, Long&gt; t)</span> </span>&#123;</span><br><span class="line">                        acc.add(t);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> TypeInformation&lt;TopN&lt;Tuple2&lt;String, Long&gt;&gt;&gt; getAccumulatorType() &#123;</span><br><span class="line">        <span class="keyword">return</span> TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;TopN&lt;Tuple2&lt;String, Long&gt;&gt;&gt;() &#123;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> TypeInformation&lt;Map&lt;String, Long&gt;&gt; getResultType() &#123;</span><br><span class="line">        <span class="keyword">return</span> TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Map&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="4-3-3-2-2-top50Aggr-udaf"><a href="#4-3-3-2-2-top50Aggr-udaf" class="headerlink" title="4.3.3.2.2.top50Aggr_udaf"></a>4.3.3.2.2.top50Aggr_udaf</h4><ul><li>getValue：将 Accumulator 结果获取到之后，拿到 top50 之后放在 map 中并将结果输出；</li><li>acumulate：将内层桶的数据合并到当前 Acumulator 中，即将内层分桶的所有事件的数据以及消费 cnt 拿到之后进行累加；</li><li>merge：将 Accumulator 合并，相同事件的消费 cnt 相加；</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">TopNAggr_UDAF</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>&lt;<span class="title">String</span>, <span class="title">Map</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> topN;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">TopNAggr_UDAF</span><span class="params">(<span class="keyword">int</span> topN)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.topN = topN;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Map&lt;String, Long&gt; <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Maps.newHashMap();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getValue</span><span class="params">(Map&lt;String, Long&gt; acc)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        TopN&lt;Tuple2&lt;String, Long&gt;&gt; topN = <span class="keyword">new</span> TopN&lt;&gt;(<span class="keyword">this</span>.topN, <span class="keyword">new</span> Comparator&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Tuple2&lt;String, Long&gt; o1, Tuple2&lt;String, Long&gt; o2)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">if</span> (o1.equals(o2)) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> o1.f1.compareTo(o2.f1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        acc.forEach(<span class="keyword">new</span> BiConsumer&lt;String, Long&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(String s, Long l)</span> </span>&#123;</span><br><span class="line">                Tuple2&lt;String, Long&gt; t = Tuple2.of(s, l);</span><br><span class="line">                topN.add(t);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        LinkedHashMap&lt;String, Map&lt;String, Long&gt;&gt; map = Maps.newLinkedHashMap();</span><br><span class="line"></span><br><span class="line">        List&lt;Tuple2&lt;String, Long&gt;&gt; topNList = topN</span><br><span class="line">                .getQueue()</span><br><span class="line">                .stream()</span><br><span class="line">                .collect(Collectors.toList());</span><br><span class="line"></span><br><span class="line">        Collections.sort(topNList</span><br><span class="line">                , <span class="keyword">new</span> Comparator&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Tuple2&lt;String, Long&gt; o1, Tuple2&lt;String, Long&gt; o2)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">if</span> (o1.equals(o2)) &#123;</span><br><span class="line">                            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">return</span> o2.f1.compareTo(o1.f1);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> i = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;String, Long&gt; t : topNList) &#123;</span><br><span class="line">            map.put(i + <span class="string">&quot;&quot;</span>, ImmutableMap.of(t.f0, t.f1));</span><br><span class="line">            i++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ObjectMapperUtils.toJSON(map);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accumulate</span><span class="params">(Map&lt;String, Long&gt; acc, Map&lt;String, Long&gt; map)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        map.forEach(<span class="keyword">new</span> BiConsumer&lt;String, Long&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(String s, Long l)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                Long history = acc.get(s);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (<span class="keyword">null</span> != history) &#123;</span><br><span class="line">                    acc.put(s, l + history);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    acc.put(s, l);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">merge</span><span class="params">(Map&lt;String, Long&gt; acc, Iterable&lt;Map&lt;String, Long&gt;&gt; its)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        its.forEach(<span class="keyword">new</span> Consumer&lt;Map&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(Map&lt;String, Long&gt; map)</span> </span>&#123;</span><br><span class="line">                map.forEach(<span class="keyword">new</span> BiConsumer&lt;String, Long&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(String s, Long l)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                        Long history = acc.get(s);</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">if</span> (<span class="keyword">null</span> != history) &#123;</span><br><span class="line">                            acc.put(s, l + history);</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            acc.put(s, l);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> TypeInformation&lt;Map&lt;String, Long&gt;&gt; getAccumulatorType() &#123;</span><br><span class="line">        <span class="keyword">return</span> TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Map&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TypeInformation&lt;String&gt; <span class="title">getResultType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> TypeInformation.of(String.class);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="4-3-3-2-3-flink-conf-yaml-参数配置"><a href="#4-3-3-2-3-flink-conf-yaml-参数配置" class="headerlink" title="4.3.3.2.3.flink-conf.yaml 参数配置"></a>4.3.3.2.3.flink-conf.yaml 参数配置</h4><p>时间戳的时区设置，注意需要我们将时区设置为本地的时区。</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">--</span> <span class="string">北京时区</span></span><br><span class="line"><span class="attr">table.local-time-zone :</span> <span class="string">GMT+08:00</span></span><br></pre></td></tr></table></figure><h3 id="4-3-4-方案4-不使用自定义-UDF"><a href="#4-3-4-方案4-不使用自定义-UDF" class="headerlink" title="4.3.4.方案4 - 不使用自定义 UDF"></a>4.3.4.方案4 - 不使用自定义 UDF</h3><h3 id="4-3-5-UDF-设计注意项"><a href="#4-3-5-UDF-设计注意项" class="headerlink" title="4.3.5.UDF 设计注意项"></a>4.3.5.UDF 设计注意项</h3><ul><li>上述 udf 最好设计成一个固定大小排行榜的 udf，比如一个 udf 实现类就只能用于处理一个固定大小的排行，防止用户误用；</li><li>sql 内层计算的排行榜大小一定要比 sql 外层（聚合）排行榜大小大。举反例：假如内层计算前 30 名，外层计算前 50 名，内层 A 分桶第 31 名可能比 B 分桶第 1 名的值还大，但是 A 桶的第 31 名就不会被输出。反之则正确。</li></ul><h1 id="5-数据服务"><a href="#5-数据服务" class="headerlink" title="5.数据服务"></a>5.数据服务</h1><h2 id="5-1-kv-存储"><a href="#5-1-kv-存储" class="headerlink" title="5.1.kv 存储"></a>5.1.kv 存储</h2><p>根据我们上述设计的数据汇 schema 来看，最适合存储引擎就是 kv 引擎，因为前端只需要展示最新的排行榜数据即可。所以我们可以使用 redis 等 kv 存储引擎来存储最新的数据。</p><h2 id="5-2-OLAP"><a href="#5-2-OLAP" class="headerlink" title="5.2.OLAP"></a>5.2.OLAP</h2><p>如果用户有需求需要记录上述数据的历史记录，我们也可以使用时序数据库或者 OLAP 引擎直接进行存储。</p><h1 id="6-数据保障"><a href="#6-数据保障" class="headerlink" title="6.数据保障"></a>6.数据保障</h1><h2 id="6-1-数据时延保障"><a href="#6-1-数据时延保障" class="headerlink" title="6.1.数据时延保障"></a>6.1.数据时延保障</h2><h3 id="6-1-1-整体时延"><a href="#6-1-1-整体时延" class="headerlink" title="6.1.1.整体时延"></a>6.1.1.整体时延</h3><p>整体时延等于下面三个时延加和：</p><ul><li>数据源时延（日志 webserver 本地时间戳 - 数据时间戳）</li><li>数据处理时延（flink 任务的整体处理时延）</li><li>数据传输时延（数据在中间层写入消息队列以及读出消息队列的时延）</li></ul><p>整体时延 = 数据应用层服务端本地时间戳 - 结果数据的事件时间戳</p><h3 id="6-1-1-数据源时延"><a href="#6-1-1-数据源时延" class="headerlink" title="6.1.1.数据源时延"></a>6.1.1.数据源时延</h3><p>指数据本身上报就会存在的时延。</p><p>举例：从用户发生消费事件 -&gt; 产生消费日志 -&gt; 将日志上报到日志服务器，这期间存在的时延。</p><h4 id="6-1-1-1-监控方式"><a href="#6-1-1-1-监控方式" class="headerlink" title="6.1.1.1.监控方式"></a>6.1.1.1.监控方式</h4><p>在 ods -&gt; dwd 层处理过程中，添加处理任务系统当前时间 - 服务端时间戳的监控（P99 等）。</p><p>那么这里衍生出一个问题，为什么不用客户端时间戳呢？</p><p>因为客户端的软件版本、网络环境、机型、地区都是各种各样的，从而会导致上报上来的日志的客户端时间戳（用户操作时间戳）的准确性是参差不齐的。因此这里我们采用服务端时间戳（日志上报到服务端时，服务端的本地时间戳）来避免这种问题。</p><h3 id="6-1-2-数据处理时延"><a href="#6-1-2-数据处理时延" class="headerlink" title="6.1.2.数据处理时延"></a>6.1.2.数据处理时延</h3><p>指 flink 处理计算的耗时。</p><h4 id="6-1-2-1-监控方式"><a href="#6-1-2-1-监控方式" class="headerlink" title="6.1.2.1.监控方式"></a>6.1.2.1.监控方式</h4><p>flink 本身自带有 latency marker 机制（详见 <a href="https://cloud.tencent.com/developer/article/1549048">flink latency marker</a>），但是这个机制会有性能损耗，官方建议只在测试阶段进行使用。</p><p>这其实已经足够，因为我们在测试阶段就可以基本测试出，flink 任务处理计算的耗时情况。</p><p>或者也可以大致通过 flink 处理逻辑推算出处理延迟。</p><h2 id="6-2-数据质量保障"><a href="#6-2-数据质量保障" class="headerlink" title="6.2.数据质量保障"></a>6.2.数据质量保障</h2><h2 id="6-3-数据服务保障"><a href="#6-3-数据服务保障" class="headerlink" title="6.3.数据服务保障"></a>6.3.数据服务保障</h2>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都比较短小，不定期更新，从一些博主的脑洞想法出发抛砖引玉，提高小伙伴的姿♂势水平。本文介绍博主对流批一体来源以及未来发展方向的一些理解，阅读时长大概 2 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>管理 | 实时数仓数据质量保障</title>
    <link href="https://yangyichao-mango.github.io/2021/10/12/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/03_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86/01_dqc/01_%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F%E7%AE%A1%E7%90%86/01_quality_dqc/"/>
    <id>https://yangyichao-mango.github.io/2021/10/12/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/03_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86/01_dqc/01_%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F%E7%AE%A1%E7%90%86/01_quality_dqc/</id>
    <published>2021-10-12T06:21:53.000Z</published>
    <updated>2021-07-18T02:58:09.839Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前沿-流批一体的一些理解"><a href="#前沿-流批一体的一些理解" class="headerlink" title="前沿 | 流批一体的一些理解"></a>前沿 | 流批一体的一些理解</h1><blockquote><p>每家数字化企业在目前遇到流批一体概念的时候，都会对这个概念抱有一些疑问，到底什么是流批一体？这个概念的来源？这个概念能为用户、开发人员以及企业带来什么样的好处？跟随着博主的理解和脑洞出发吧。</p></blockquote><p>通过本文你可以 get 到：</p><ol><li>什么是数据质量保障、为什么要做数据质量保障</li><li>怎么去做数据时效保障</li></ol><h1 id="1-什么是数据质量保障、为什么要做数据质量保障"><a href="#1-什么是数据质量保障、为什么要做数据质量保障" class="headerlink" title="1.什么是数据质量保障、为什么要做数据质量保障"></a>1.什么是数据质量保障、为什么要做数据质量保障</h1><p>数据质量是一个数据开发人员的生命线，可以想象一下</p><ul><li>在业务侧，如果产出的数据是不准确的，甚至是错误的，那业务侧参考这个数据而做出的决策就绝对是错误的。这对依赖数据的决策方式毁灭性打击。</li><li>在数据加工链路侧，如果一个核心节点的数据产出是错误的，那么依赖这个数据的下游数据基本都是无效数据了。</li></ul><p>所以我称数据质量是数据的生命线。而数据质量保障就是来维护数据质量这条生命线的。</p><p>数据质量保障就是对上述数据质量有一个监控以及保障方案的体系的建设。</p><p>而对于现在大多数 lambda 架构的实时数仓来说，实时数据质量保障可以分为两块：</p><ul><li>实时数据自身同环比：其可以实时反映出数据波动的情况，快速发现数据质量问题，可以快速反映出目前实时数据波动的合理性；</li><li>实时离线数据校验：其可以反映出实时数据的准确性，由于实时数据处理过程中的乱序丢数，没有端对端一致性保障的情况下，实时离线数据校验就可以保障实时数据的一个准确性。</li></ul><h1 id="2-怎么去做数据时效保障"><a href="#2-怎么去做数据时效保障" class="headerlink" title="2.怎么去做数据时效保障"></a>2.怎么去做数据时效保障</h1>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都比较短小，不定期更新，从一些博主的脑洞想法出发抛砖引玉，提高小伙伴的姿♂势水平。本文介绍博主对流批一体来源以及未来发展方向的一些理解，阅读时长大概 2 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>管理 | 实时数仓数据时效保障</title>
    <link href="https://yangyichao-mango.github.io/2021/10/12/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/03_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86/01_dqc/02_%E6%95%B0%E6%8D%AE%E6%97%B6%E6%95%88%E7%AE%A1%E7%90%86/01_timeliness_dqc/"/>
    <id>https://yangyichao-mango.github.io/2021/10/12/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/03_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86/01_dqc/02_%E6%95%B0%E6%8D%AE%E6%97%B6%E6%95%88%E7%AE%A1%E7%90%86/01_timeliness_dqc/</id>
    <published>2021-10-12T06:21:53.000Z</published>
    <updated>2021-07-17T13:30:38.716Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>我要更快、更快更快！！！</p></blockquote><p>通过本文你可以 get 到：</p><ol><li>起因篇-为什么要做数据时效保障</li><li>定义篇-数据时效保障包含哪些内容</li><li>目标篇-时效性监控以及保障的目标</li><li>机制篇-怎么去做数据时效监控以及保障</li><li>效果篇-上述机制帮助用户暴露出过什么问题</li><li>现状以及展望篇</li></ol><h1 id="1-序篇"><a href="#1-序篇" class="headerlink" title="1.序篇"></a>1.序篇</h1><p>所有的数据建设都是为了用户更快、更方便、更放心的使用数据。</p><p>在用户使用实时数据的过程中，最影响用户体感的指标有两个：</p><ul><li>数据质量：实时数据产出的准确性。举个例子：实时数据在某些场景下不能保障端到端 exactly-once，因此实时与离线相同口径的数据会有 diff。而 1% 和 0.01% 的 diff 给用户的体验是完全不同的。</li><li>数据时效：实时数据产出的及时性。举个例子：延迟 1min 和 延迟 1ms 的用户体验也是完全不同的。</li></ul><p>而本文主要对数据时效保障进行解读。</p><p>懒癌患者福利，先说本文结论，通过以下两个指标就已经能监控和判定 90% 数据延迟、乱序问题了。</p><ul><li><strong>数据延迟监控：flink 消费上游的 lag（比如看消费 kafka lag 情况）</strong></li><li><strong>数据乱序监控：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/ops/metrics/#io。" title="由于迟到而丢弃的数据条目数">Task/Operator numLateRecordsDropped</a> 可以得到由于乱序导致窗口的丢数情况。</strong></li></ul><h1 id="2-起因篇-为什么要做数据时效保障"><a href="#2-起因篇-为什么要做数据时效保障" class="headerlink" title="2.起因篇-为什么要做数据时效保障"></a>2.起因篇-为什么要做数据时效保障</h1><p>要做一个东西时，我们首要分析的就是用户的痛点是什么，用户想要什么。从以下两个方面的分析入手。</p><ul><li>业务侧：首先从正向结果来看，业务侧能拿到第一手准确的实时数据，就能根据准确，快速的数据做出业务策略调整，扩大收益。但是正向结果是我们预期的目标，开发所要做的就是解决达成预期目标过程中的各种不稳定因素，这些不稳定因素就是负向结果。从负向结果看，一旦出现数据产出延迟，数据不准，就有可能让业务错失一个热点，产生巨大损失，两者之间的关系如下图；<br>因此从保障层面出发，这就要求更低的数据延迟、更小的数据乱序（某些对于数据乱序敏感的任务，产出的数据质量强依赖数据乱序情况）</li></ul><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/03_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86/01_dqc/02_%E6%95%B0%E6%8D%AE%E6%97%B6%E6%95%88%E7%AE%A1%E7%90%86/01_timeliness_dqc/1.png" alt="1"></p><ul><li>数据加工链路侧：从调研数据源阶段角度出发，DE 需要确定某些原始数据的延迟和乱序情况，确定数据源可用性，从而进行定制化的处理和优化；从保障数据汇结果时效性出发，某些实时数据加工链路是很长的，ods -&gt; dwd -&gt; dws -&gt; ads，当数据产出延迟时，DE 需要快速定位到问题任务进行处理，如下图。<br>数据加工时延越小，数据的乱序情况越小，说明整条处理链路的稳定性也越好，也就有能力提供更高的 SLA 保障；从以上角度出发，也需要我们对整个生产链路的数据延迟、乱序情况有一个全局视角的掌握。</li></ul><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/03_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86/01_dqc/02_%E6%95%B0%E6%8D%AE%E6%97%B6%E6%95%88%E7%AE%A1%E7%90%86/01_timeliness_dqc/2.png" alt="2"></p><p><strong>结论：数据时效保障就是对数据产出延迟、数据乱序的监控报警能力的构建、保障方案规范化的建设。</strong></p><h1 id="3-定义篇-数据时效保障包含哪些内容"><a href="#3-定义篇-数据时效保障包含哪些内容" class="headerlink" title="3.定义篇-数据时效保障包含哪些内容"></a>3.定义篇-数据时效保障包含哪些内容</h1><p>如上节场景分析，实时数据时效保障可分为两部分：</p><ol><li>数据时延监控、报警、保障：衡量实时数据产出的延迟情况，设定报警阈值，超过阈值触发报警。并且需要对数据产出延迟有一个全链路的视角，保障数据产出延迟在预期范围内；</li><li>数据乱序监控、报警、保障：乱序是实时任务处理中要关注的一个重要指标，如果数据源乱序非常严重的话，会影响窗口类任务产出的实时数据质量，所以我们也需要对齐进行监控以及保障。</li></ol><blockquote><p>Notes:<br>乱序的本质其实就是数据的延迟。乱序是一种特殊的延迟，数据延迟导致的一种结果。</p></blockquote><h1 id="4-目标篇-时效性监控以及保障的目标"><a href="#4-目标篇-时效性监控以及保障的目标" class="headerlink" title="4.目标篇-时效性监控以及保障的目标"></a>4.目标篇-时效性监控以及保障的目标</h1><ul><li>探查：了解数据源的延迟、乱序情况。针对数据源的延迟、乱序情况可以针对性优化。也对此能提出合理的 SLA 保障；</li><li>监控：针对具体延迟、乱序严重程度设定报警阈值，让开发可以快速感知问题；</li><li>定位：根据延迟、乱序报警快速定位数据延迟、乱序导致的质量问题；</li><li>恢复：问题解决完成之后，可以根据监控查看到实际的效果；</li></ul><h1 id="5-机制篇-怎么去做数据时效监控以及保障"><a href="#5-机制篇-怎么去做数据时效监控以及保障" class="headerlink" title="5.机制篇-怎么去做数据时效监控以及保障"></a>5.机制篇-怎么去做数据时效监控以及保障</h1><p>接下来我们<strong>对症（延迟、乱序情况）下药（监控、报警、保障措施）</strong>，先分析在数据生产、传输、加工的过程中哪些环节会导致数据的延迟以及乱序。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/03_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86/01_dqc/02_%E6%95%B0%E6%8D%AE%E6%97%B6%E6%95%88%E7%AE%A1%E7%90%86/01_timeliness_dqc/3.png" alt="3"></p><p>通过分析上述数据生产、传输、加工链路之后，我们可以发现能从<strong>数据源、数据处理任务</strong>两个不同的维度去分析会导致延迟、乱序的原因。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/03_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86/01_dqc/02_%E6%95%B0%E6%8D%AE%E6%97%B6%E6%95%88%E7%AE%A1%E7%90%86/01_timeliness_dqc/4.png" alt="4"></p><p><strong>数据源延迟乱序</strong>：属于数据源本身的属性，和下游消费的任务无关。</p><p><strong>数据加工延迟乱序</strong>：这是和具体的任务绑定。</p><p>其对应关系如下。</p><table><thead><tr><th>维度</th><th>数据源视角（与具体任务无关）</th><th>数据处理任务视角（与具体任务绑定）</th></tr></thead><tbody><tr><td>延迟</td><td>源日志上报的延迟</td><td>数据加工过程导致的延迟</td></tr><tr><td>乱序</td><td>源日志上报的乱序</td><td>数据加工过程中 shuffle 导致的乱序</td></tr></tbody></table><h2 id="5-1-数据时延监控"><a href="#5-1-数据时延监控" class="headerlink" title="5.1.数据时延监控"></a>5.1.数据时延监控</h2><h3 id="5-1-1-整体时延"><a href="#5-1-1-整体时延" class="headerlink" title="5.1.1.整体时延"></a>5.1.1.整体时延</h3><p>整体时延可以从以下两个角度出发进行计算。</p><ul><li>用户视角：只关心最终产出结果时延</li><li>开发视角：需要关心整个链路处理时延</li></ul><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/03_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86/01_dqc/02_%E6%95%B0%E6%8D%AE%E6%97%B6%E6%95%88%E7%AE%A1%E7%90%86/01_timeliness_dqc/5.png" alt="5"></p><h3 id="5-1-2-结果时延监控"><a href="#5-1-2-结果时延监控" class="headerlink" title="5.1.2.结果时延监控"></a>5.1.2.结果时延监控</h3><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/03_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86/01_dqc/02_%E6%95%B0%E6%8D%AE%E6%97%B6%E6%95%88%E7%AE%A1%E7%90%86/01_timeliness_dqc/6.png" alt="6"></p><h4 id="5-1-2-1-监控指标以及报警机制"><a href="#5-1-2-1-监控指标以及报警机制" class="headerlink" title="5.1.2.1.监控指标以及报警机制"></a>5.1.2.1.监控指标以及报警机制</h4><p>从用户体验角度直观的反映出数据的整体时延情况。</p><ul><li><p><strong>监控方式</strong>：有数据时效监控中心提供延迟监控 sdk。在看板的 web server 侧将数据时延上报到延迟监控 sdk 中。</p></li><li><p><strong>监控指标</strong>：计算 web-server-system-current-timestamp - message-event-timestamp 计算 P99 等指标。</p></li><li><p><strong>监控方式优点</strong>：能从用户体感角度出发，准确的刻画时延情况。</p></li><li><p><strong>监控方式缺点</strong>：对 web server 有埋点侵入性。</p></li><li><p><strong>报警机制</strong>：定时（比如 1min/次） check 监控指标的 P99 指标。</p></li><li><p><strong>报警阈值</strong>：判断监控指标的 P99 指标是否超过某个阈值（比如 5 min）。</p></li><li><p><strong>报警接收人</strong>：报警反馈给任务链路负责人。</p></li></ul><h3 id="5-1-3-链路时延监控"><a href="#5-1-3-链路时延监控" class="headerlink" title="5.1.3.链路时延监控"></a>5.1.3.链路时延监控</h3><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/03_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86/01_dqc/02_%E6%95%B0%E6%8D%AE%E6%97%B6%E6%95%88%E7%AE%A1%E7%90%86/01_timeliness_dqc/7.png" alt="7"></p><h4 id="5-1-3-1-数据源时延"><a href="#5-1-3-1-数据源时延" class="headerlink" title="5.1.3.1.数据源时延"></a>5.1.3.1.数据源时延</h4><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/03_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86/01_dqc/02_%E6%95%B0%E6%8D%AE%E6%97%B6%E6%95%88%E7%AE%A1%E7%90%86/01_timeliness_dqc/10.png" alt="8"></p><blockquote><p>这个时延和处理任务无关，单纯从指数据本身的属性，数据本身上报就存在的时延。</p></blockquote><p>举例：从用户发生消费事件一直到日志进入数据源存储引擎中（比如 kafka），这期间存在的时延。</p><h5 id="5-1-3-1-1-监控指标以及报警机制"><a href="#5-1-3-1-1-监控指标以及报警机制" class="headerlink" title="5.1.3.1.1.监控指标以及报警机制"></a>5.1.3.1.1.监控指标以及报警机制</h5><ul><li><p><strong>监控方式</strong>：单独有一个任务消费并处理数据源。需要保障这个任务任何时刻都不能有 lag，才能刻画出一个准确的数据源时延情况。</p></li><li><p><strong>监控指标</strong>：使用 system-current-timestamp - message-event-timestamp P99 等指标。</p></li><li><p><strong>监控方式优点</strong>：<strong>在数据源角度</strong>能准确的刻画出数据源事件时间时延情况。</p></li><li><p><strong>监控方式缺点</strong>：为了监控数据源乱序情况，需要单独启动一个任务耗费资源。不建议这种方式进行，如果要做，可以进行采样。而且会侵入用户代码，需要用户指定时间戳。</p></li><li><p><strong>报警机制</strong>：定时（比如 1min/次） check 监控指标的 P99 指标。</p></li><li><p><strong>报警阈值</strong>：判断监控指标的 P99 指标是否超过某个阈值（比如 5 min）。</p></li><li><p><strong>报警接收人</strong>：报警反馈给任务链路负责人。</p></li></ul><p>上面这种方式是站在数据源视角去精准的衡量出数据延迟情况的，但是很多时候我们只需要在下游任务视角去做这件事会更方便。比如：</p><ul><li><p><strong>监控方式</strong>：在下游任务处处理数据源时记录数据延迟情况。</p></li><li><p><strong>监控指标</strong>：使用任务本地 system-current-timestamp - message-event-timestamp P99 等指标。</p></li><li><p><strong>监控方式优点</strong>：节约资源。</p></li><li><p><strong>监控方式缺点</strong>：一旦下游任务消费有延迟，我们就不能准确的衡量出数据源的延迟情况了。而且会侵入用户代码，需要用户指定时间戳。</p></li><li><p><strong>报警机制</strong>：定时（比如 1min/次） check 监控指标的 P99 指标。</p></li><li><p><strong>报警阈值</strong>：判断监控指标的 P99 指标是否超过某个阈值（比如 180s）。</p></li><li><p><strong>报警接收人</strong>：报警反馈给任务链路负责人。</p></li></ul><blockquote><p>Notes：<br>这里衍生出一个问题，客户端日志数据一般会有以下两种时间戳：</p><ol><li>客户端时间戳：用户在客户端操作时的时间戳</li><li>服务端时间戳：客户端日志上报到服务端时，日志 server 打上的本地时间戳</li></ol><p>因为客户端的软件版本、网络环境、机型、地区的不同，会导致上报的日志<strong>客户端时间戳</strong>（用户操作时间戳）的准确性参差不齐（你可能会发现有历史、未来的时间戳）。因此事件时间都采用服务端时间戳（日志上报到服务端时，服务端的本地时间戳）来避免这种问题。</p><p>当我们采用服务端时间戳时，就基本会发现数据源的时延几乎为 0，因为数据处理链路和日志 server 都是 server 端，因此其之间的数据时延是非常小的，几乎可以忽略不计。</p></blockquote><h4 id="5-1-3-2-数据加工时延"><a href="#5-1-3-2-数据加工时延" class="headerlink" title="5.1.3.2.数据加工时延"></a>5.1.3.2.数据加工时延</h4><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/03_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86/01_dqc/02_%E6%95%B0%E6%8D%AE%E6%97%B6%E6%95%88%E7%AE%A1%E7%90%86/01_timeliness_dqc/11.png" alt="9"></p><p>用于衡量实时任务处理链路的时延。定位链路瓶颈问题。</p><h5 id="5-1-3-2-1-监控指标以及报警机制"><a href="#5-1-3-2-1-监控指标以及报警机制" class="headerlink" title="5.1.3.2.1.监控指标以及报警机制"></a>5.1.3.2.1.监控指标以及报警机制</h5><p>第一个就是 flink 消费数据源的延迟。比如 flink 任务性能不足，产生反压就会有大量 lag。</p><ul><li><p><strong>监控方式</strong>：在下游任务处处理数据源时记录数据延迟情况。</p></li><li><p><strong>监控指标</strong>：使用任务本地 system-current-timestamp - kafka-timestamp P99 等指标。</p></li><li><p><strong>监控方式优点</strong>：不侵入用户代码。</p></li><li><p><strong>监控方式缺点</strong>：可以衡量出任务消费时延情况。</p></li><li><p><strong>报警机制</strong>：定时（比如 1min/次） check 监控指标的 P99 指标。</p></li><li><p><strong>报警阈值</strong>：判断监控指标的 P99 指标是否超过某个阈值（常用 180s）。</p></li><li><p><strong>报警接收人</strong>：报警反馈给任务链路负责人。</p></li></ul><p>第二部分就是 flink 整个处理过程中的延迟情况。</p><ul><li><strong>监控方式</strong>：flink 本身自带有 latency marker 机制（详见 <a href="https://cloud.tencent.com/developer/article/1549048">flink latency marker</a>）。</li><li><strong>监控指标</strong>：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/ops/metrics/#end-to-end-latency-tracking">flink latency marker 官方文档</a>。</li><li><strong>监控方式优点</strong>：<strong>在下游消费任务的角度</strong>准确的刻画出整个 flink 任务加工时延。</li><li><strong>监控方式缺点</strong>：这个机制会有性能损耗，官方建议只在测试阶段进行使用。这其实已经足够，因为我们在测试阶段就可以基本测试出，flink 任务处理计算的耗时情况。</li></ul><h2 id="5-2-数据乱序监控"><a href="#5-2-数据乱序监控" class="headerlink" title="5.2.数据乱序监控"></a>5.2.数据乱序监控</h2><p>数据乱序监控主要是用来监控数据源、处理任务过程中操作的乱序对产出数据的影响。</p><h3 id="5-2-1-数据源乱序"><a href="#5-2-1-数据源乱序" class="headerlink" title="5.2.1.数据源乱序"></a>5.2.1.数据源乱序</h3><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/03_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86/01_dqc/02_%E6%95%B0%E6%8D%AE%E6%97%B6%E6%95%88%E7%AE%A1%E7%90%86/01_timeliness_dqc/8.png" alt="10"></p><p>指数据本身就存在的乱序，比如客户端网络上报存在的乱序，有的用户在偏远网络较差的地区，所以上报可能就会比很多用户延迟很多，这就造成了数据的乱序。</p><h4 id="5-2-1-1-监控指标以及报警机制"><a href="#5-2-1-1-监控指标以及报警机制" class="headerlink" title="5.2.1.1.监控指标以及报警机制"></a>5.2.1.1.监控指标以及报警机制</h4><ul><li><p><strong>监控方式</strong>：单独有一个任务消费并处理数据源。需要保障这个任务任何时刻都不能有 lag，才能刻画出一个准确的数据源时延情况。</p></li><li><p><strong>监控指标</strong>： 具体衡量乱序的指标类似于 watermark 分配方式。即为每一个 source consumer 维护一个 max(timestamp)，记为 max_ts，后续来的数据的时间戳记为 cur_tx，如果 cur_tx &gt; max_ts，则说明没有乱序，设置 max_tx = cur_ts，如果出现 cur_ts &lt; max_ts，则说明这条数据发生了乱序，计算出 abs(cur_ts - max_ts) 为具体乱序时长，最终计算乱序时长的 P99 等值。</p></li><li><p><strong>监控方式优点</strong>：<strong>在数据源角度</strong>能准确的刻画出数据源事件时间乱序情况。</p></li><li><p><strong>监控方式缺点</strong>：为了监控数据源乱序情况，需要单独启动一个任务耗费资源。不建议这种方式进行，如果要做，可以进行采样。</p></li><li><p><strong>报警机制</strong>：定时（比如 1min/次） check 监控指标的 P99 指标。</p></li><li><p><strong>报警阈值</strong>：判断监控指标的 P99 指标是否超过某个阈值（常用 180s）。</p></li><li><p><strong>报警接收人</strong>：报警反馈给任务负责人。</p></li></ul><p>上面这种方式是站在<strong>数据源视</strong>角去精准的衡量出数据乱序情况的，但是很多时候我们只需要在<strong>下游任务视角</strong>去做这件事会更方便。比如：</p><ul><li><strong>监控方式</strong>：在下游任务处处理数据源时记录数据乱序情况。</li><li><strong>监控指标</strong>：衡量指标同上。</li></ul><blockquote><p>Notes：<br>虽然数据源可能有乱序，但是这个乱序经过 flink 的一些策略处理后，乱序对计算数据的影响就会被消除。比如用户设置 watermark 时调大 max-out-of-orderness 以及设置 allow-lateness 的处理之后就会解决。</p></blockquote><h3 id="5-2-2-数据加工乱序"><a href="#5-2-2-数据加工乱序" class="headerlink" title="5.2.2.数据加工乱序"></a>5.2.2.数据加工乱序</h3><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/03_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86/01_dqc/02_%E6%95%B0%E6%8D%AE%E6%97%B6%E6%95%88%E7%AE%A1%E7%90%86/01_timeliness_dqc/9.png" alt="11"></p><p>单个任务消费上游数据后，内部做一些 rebalance shuffle 操作导致或者加剧数据乱序的情况。从而会导致一些开窗类的任务出现丢数的情况，导致最后数据计算出现误差。</p><p>举例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Model&gt; eventTimeResult = SourceFactory</span><br><span class="line">        .getSourceDataStream(xxx)</span><br><span class="line">        .uid(<span class="string">&quot;source&quot;</span>)</span><br><span class="line">        .rebalance() <span class="comment">// 这里 rebalance 之后会加剧数据乱序，从而可能会导致后续事件时间窗口丢数</span></span><br><span class="line">        .flatMap(xxx)</span><br><span class="line">        .assignTimestampsAndWatermarks(<span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor&lt;Model&gt;(Time.minutes(<span class="number">1L</span>)) &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Model model)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> model.getServerTimestamp();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .keyBy(KeySelectorFactory.getRemainderKeySelector(xxx))</span><br><span class="line">        .timeWindow(Time.seconds(xxx))</span><br><span class="line">        .process(xxx)</span><br><span class="line">        .uid(<span class="string">&quot;process-event-time&quot;</span>);</span><br></pre></td></tr></table></figure><h4 id="5-2-2-1-监控指标以及报警机制"><a href="#5-2-2-1-监控指标以及报警机制" class="headerlink" title="5.2.2.1.监控指标以及报警机制"></a>5.2.2.1.监控指标以及报警机制</h4><ul><li><p><strong>监控方式</strong>：我们关心的是乱序最终导致的丢数情况，所以监控丢数条目数即可。</p></li><li><p><strong>监控指标</strong>： <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/ops/metrics/#io。" title="由于迟到而丢弃的数据条目数">Task/Operator numLateRecordsDropped</a> 可以得到由于乱序导致窗口的丢数情况。</p></li><li><p><strong>监控方式优点</strong>：flink 自带此指标。</p></li><li><p><strong>报警机制</strong>：定时（比如 1min/次） check 监控指标的条目数。</p></li><li><p><strong>报警阈值</strong>：判断监控指标的条目数是否超过某个阈值（比如 5w 条）。</p></li><li><p><strong>报警接收人</strong>：报警反馈给任务负责人。</p></li></ul><h1 id="6-效果篇-上述机制帮助用户暴露出过什么问题"><a href="#6-效果篇-上述机制帮助用户暴露出过什么问题" class="headerlink" title="6.效果篇-上述机制帮助用户暴露出过什么问题"></a>6.效果篇-上述机制帮助用户暴露出过什么问题</h1><h2 id="6-1-数据源探查阶段"><a href="#6-1-数据源探查阶段" class="headerlink" title="6.1.数据源探查阶段"></a>6.1.数据源探查阶段</h2><p>在数据源探查阶段，通过快速启动数据源消费任务去探查数据源的延迟、乱序程度，确定数据源的可用性。比如发现数据源延迟常年在 5min 以上，那么我们向用户所能保障的数据时延也不会小于 5min。</p><h2 id="6-2-暴露延迟、乱序问题"><a href="#6-2-暴露延迟、乱序问题" class="headerlink" title="6.2.暴露延迟、乱序问题"></a>6.2.暴露延迟、乱序问题</h2><p><strong>通过我们的实践测试之后，我们发现报警和问题原因是符合 2-8 定律的，甚至比例达到了 2 - 9。即 90% 的问题都可以由 20% 的报警发现。</strong></p><h3 id="6-2-1-90-的时延问题是由于-flink-任务性能不足导致"><a href="#6-2-1-90-的时延问题是由于-flink-任务性能不足导致" class="headerlink" title="6.2.1.90% 的时延问题是由于 flink 任务性能不足导致"></a>6.2.1.90% 的时延问题是由于 flink 任务性能不足导致</h3><ul><li>报警项：flink 消费 kafka lag 延迟超过 180s</li><li>其他监控项辅助定位：flink 任务 cpu 使用率超过 100%；flink 任务 ygc 每分钟超过 20s</li></ul><h3 id="6-2-2-10-的时延问题是由于数据源延迟导致"><a href="#6-2-2-10-的时延问题是由于数据源延迟导致" class="headerlink" title="6.2.2.10% 的时延问题是由于数据源延迟导致"></a>6.2.2.10% 的时延问题是由于数据源延迟导致</h3><ul><li>报警项：flink 消费 kafka lag 延迟超过 180s；数据源时延超过 180s</li><li>其他监控项辅助定位：flink 任务 cpu 使用率正常，每分钟 ygc 时长正常</li></ul><h3 id="6-2-3-90-的乱序问题是由于数据源乱序导致"><a href="#6-2-3-90-的乱序问题是由于数据源乱序导致" class="headerlink" title="6.2.3.90% 的乱序问题是由于数据源乱序导致"></a>6.2.3.90% 的乱序问题是由于数据源乱序导致</h3><ul><li>报警项：flink 任务窗口算子丢数超过 xx 条；数据源乱序 P99 超过 180s（指 99% 的数据乱序情况不超过 180s）</li></ul><h3 id="6-2-4-10-的乱序问题是由于-flink-任务加工乱序导致"><a href="#6-2-4-10-的乱序问题是由于-flink-任务加工乱序导致" class="headerlink" title="6.2.4.10% 的乱序问题是由于 flink 任务加工乱序导致"></a>6.2.4.10% 的乱序问题是由于 flink 任务加工乱序导致</h3><ul><li>报警项：flink 任务窗口算子丢数超过 xx 条</li><li>他监控项辅助定位：数据源乱序 P99 处于合理范围；并且代码中有 rebalance 操作之后分配 watermark</li></ul><h2 id="6-3-确定延迟、乱序问题恢复情况"><a href="#6-3-确定延迟、乱序问题恢复情况" class="headerlink" title="6.3.确定延迟、乱序问题恢复情况"></a>6.3.确定延迟、乱序问题恢复情况</h2><p>当我们修复数据延迟、乱序问题之后，我们也需要观察任务的回复情况。上述监控也可以帮助观察问题的恢复情况。比如：延迟、乱序时长变小就说明用户的修复是有效的。</p><h1 id="7-现状以及展望篇"><a href="#7-现状以及展望篇" class="headerlink" title="7.现状以及展望篇"></a>7.现状以及展望篇</h1><h2 id="7-1-现状"><a href="#7-1-现状" class="headerlink" title="7.1.现状"></a>7.1.现状</h2><p>其实目前很多公司有 <strong>flink 消费 kafka lag 时延</strong>，<strong>Task/Operator numLateRecordsDropped</strong> 就已经足够用了。<br>全方位建设上述整个时延监控的成本还是很高的。</p><h2 id="7-2-展望"><a href="#7-2-展望" class="headerlink" title="7.2.展望"></a>7.2.展望</h2><h3 id="7-2-1-实时数据、任务血缘-时效性全景图"><a href="#7-2-1-实时数据、任务血缘-时效性全景图" class="headerlink" title="7.2.1.实时数据、任务血缘 + 时效性全景图"></a>7.2.1.实时数据、任务血缘 + 时效性全景图</h3><ul><li>需求：数仓的上下游链路是很长的，如果想更快快速定位整个数据链路中的时效性问题，就需要一个可视化整体链路时延全局图。</li><li>基础能力：需要实时数据、任务血缘（目前想要做到这一点，都已经比较难了，很多大厂的机制都不完善，甚至说没有）</li></ul><p>举例：从最终产出的一个 ads 层指标出发，逆推血缘，并展示出时效情况。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/03_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86/01_dqc/02_%E6%95%B0%E6%8D%AE%E6%97%B6%E6%95%88%E7%AE%A1%E7%90%86/01_timeliness_dqc/12.png" alt="12"></p><h3 id="7-2-2-实时时效性基线"><a href="#7-2-2-实时时效性基线" class="headerlink" title="7.2.2.实时时效性基线"></a>7.2.2.实时时效性基线</h3><h4 id="7-2-2-1-基线"><a href="#7-2-2-1-基线" class="headerlink" title="7.2.2.1.基线"></a>7.2.2.1.基线</h4><p>并且将时延超过阈值的链路使用醒目的颜色标注</p><ul><li>需求：不同的指标有不同的产出时延标准，有了 6.2.1 的基础能力之后，我们就可以根据具体时延要求设置时效性基线。比如设置最终指标产出时延不能超过 180s。那么基线就是 180s。只要整个链路的产出时延超过 180s 就报警。也可以对某一层的加工链路设置基线。</li></ul><p>举例：从最终产出的一个 ads 层指标出发，设置基线 180s，那么下图的任务就可以根据基线设定的任务，逆推计算出链路中时延过长的任务，直接报警。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/03_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86/01_dqc/02_%E6%95%B0%E6%8D%AE%E6%97%B6%E6%95%88%E7%AE%A1%E7%90%86/01_timeliness_dqc/13.png" alt="13"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/03_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86/01_dqc/02_%E6%95%B0%E6%8D%AE%E6%97%B6%E6%95%88%E7%AE%A1%E7%90%86/01_timeliness_dqc/14.png" alt="14"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/03_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86/01_dqc/02_%E6%95%B0%E6%8D%AE%E6%97%B6%E6%95%88%E7%AE%A1%E7%90%86/01_timeliness_dqc/15.png" alt="15"></p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都比较短小，不定期更新，从一些博主的脑洞想法出发抛砖引玉，提高小伙伴的姿♂势水平。本文介绍博主对流批一体来源以及未来发展方向的一些理解，阅读时长大概 2 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>火山引擎开发者社区 Meetup 数据技术专场小记</title>
    <link href="https://yangyichao-mango.github.io/2021/07/24/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    <id>https://yangyichao-mango.github.io/2021/07/24/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/</id>
    <published>2021-07-24T06:21:53.000Z</published>
    <updated>2021-07-24T16:05:46.789Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>现场回顾。PPT 公众号回复 20210724 获取。<br>感谢您的关注  +  点赞 + 再看，对博主的肯定，会督促博主持续的输出更多的优质实战内容！！！</p></blockquote><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/1.jpeg" alt="2"></p><h1 id="火山-abtest-引擎"><a href="#火山-abtest-引擎" class="headerlink" title="火山 abtest 引擎"></a>火山 abtest 引擎</h1><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/2.jpeg" alt="2"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/3.jpeg" alt="2"></p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ol><li>实时 abtest 的探索？以及一些应用场景？</li></ol><blockquote><p>有5分钟，1小时的指标，结合监控报警能力，可以在在某些效果很差的实验下进行及时止损。<br>其他情况下实时指标在 abtest 中的效果并不明显。</p></blockquote><ol start="2"><li>abtest 自动化归因能力？</li></ol><blockquote><p>目前在探索中，结合老虎机算法。但是归因能力还是需要更多业务的输入，以及用户消费数据的特点。</p></blockquote><h2 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h2><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/1_1.png" alt="2"></p><ol><li>智能化：智能化归因，和算法进行集成</li><li>场景化：深入用户场景</li><li>被集成：深入集成到各个业务系统中</li></ol><h1 id="clickhouse"><a href="#clickhouse" class="headerlink" title="clickhouse"></a>clickhouse</h1><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/9.jpeg" alt="2"></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/10.jpeg" alt="2"></p><blockquote><p>着重介绍了字节基于 RoaringBitMap 对于人群包圈定或者留存分析上面的优化应用。</p></blockquote><h2 id="人群包圈定流程"><a href="#人群包圈定流程" class="headerlink" title="人群包圈定流程"></a>人群包圈定流程</h2><ol><li>将画像维度下的 uid 集成到 bitmap 中</li><li>使用 bitmap 的各种 and，or 操作进行人群包圈定</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/2_1.png" alt="2"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/2_2.png" alt="2"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/2_3.png" alt="2"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/2_5.png" alt="2"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/2_6.png" alt="2"></p><h1 id="流批数据质量管控"><a href="#流批数据质量管控" class="headerlink" title="流批数据质量管控"></a>流批数据质量管控</h1><blockquote><p>字节跳动数据质量监控平台的能力。包括实时、离线数据的 dqc 能力。</p></blockquote><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/11.jpeg" alt="2"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/13.jpeg" alt="2"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/14.jpeg" alt="2"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/15.jpeg" alt="2"></p><ol><li>实时数据时效性监控从监控指标上面来分为延迟、乱序；从监控模块：任务，数据源，字节实现了哪些？以及是否有全链路的监控？</li></ol><blockquote><p>字节数据质量平台更多的是从数据源、数据表角度去监控数据表的数据时效、数据质量。</p><p>实时数据 dqc：目前主要是针对数据源角度，有延迟监控，没有乱序监控。但也只是针对单个的数据源有延迟、质量（和离线一直，包括实时数据空值等校验）监控，针对全链路，虽然目前建立了血缘数据，但是没有全链路监控。<br>针对任务角度的监控，全部都是集成在实时数据开发的 IDE 中，不属于 dqc 平台的范畴。</p><p>博主理解实时 dqc 更靠近业务侧，而非实时数仓侧。</p><p>当用户配置好了实时数据质量监控规则之后（延迟配置方式其实就是在 dqc 平台中配置对应 kafka 的时间戳字段进行延迟监控），其会自动生产 flink sql 任务。当然这个 flink sql 任务也会遇到延迟等问题，所以字节团队也会对这种关键任务配置任务层面的报警。</p><p>离线数据 dqc：离线数据是会存在全链路的 dqc 数据质量以及产出时效监控。</p></blockquote><ol start="2"><li>实时数据 dqc 目前有调研过算法吗（毕竟算法会比强阈值监控报警准确度高）？</li></ol><blockquote><p>目前也有自研时序算法来处理流量高峰期的误报过多的情况，但是目前跑下来发现效果没那么好。</p></blockquote><ol start="3"><li>离线数据时效性监控保障是怎样做的？</li></ol><blockquote><p>在字节总共分为两部分，第一部分是基线，第二部分是产出死保协议。这两种工具产品形态是两天，底层能力都是一套。</p><p>基线：会逆推进行预警，这个预警目前是会有两套算法进按照历史执行情况进行预估，两套算法相辅相成可以更准确的预估产出时效。</p><p>产出死保协议：此工具的能力是和基线一致的，类似于对于重要任务让各个任务节点的 DE 签署一个产出时间协议，保障对应的数据必须在某个时间点进行产出。如果没有按照这个时间点产出，那么你懂的。</p></blockquote><h1 id="埋点流量治理大规模实践"><a href="#埋点流量治理大规模实践" class="headerlink" title="埋点流量治理大规模实践"></a>埋点流量治理大规模实践</h1><blockquote><p>主要介绍了埋点流量治理的整套方法论，包括治理规范、工具链。</p></blockquote><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/16.jpeg" alt="2"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/17.jpeg" alt="2"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/18.jpeg" alt="2"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/19.jpeg" alt="2"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/20.jpeg" alt="2"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/21.jpeg" alt="2"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/22.jpeg" alt="2"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/02_meetup/01_%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E/01_20210724%E7%81%AB%E5%B1%B1%E5%BC%95%E6%93%8E%E5%A4%A7%E6%95%B0%E6%8D%AE/23.jpeg" alt="2"></p><ol><li><p>拓扑重构：重复消费 kafka 的任务合并到一个任务中，减少 kafka consumer。</p></li><li><p>将拆流任务和埋点平台进行绑定：拆流任务其实就是一个 mapper 任务，只有 map，这个 map 任务负责清洗、拆流，任务可以做到热加载，rpc 热加载，拆流策略热加载，udf 热加载，可以做到不重启任务就可以更改。<br>并且用户可以自定义产出数据类型（json、protobuf 等）、schema，任务会将热加载 schema 配置，动态生成 sink schema 代码，热 load 后写出。</p></li><li><p>针对客户端埋点延迟上报的情况，可以自定义配置上报时间以及策略。但是目前针对埋点丢失、重复、上报延迟也没有更好的方式去解决，只能是在客户端添加一些重试策略。</p></li></ol>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，帮助大家理解全局一致性快照。可能很多小伙伴都知道 flink 是使用 barrier 来做全局一致性快照，但是我提两个问题，为什么 flink 的 barrier 能够保证全局一致性快照的正确性？barrier 到底发挥了怎样的作用？小伙伴们能回答上来么，有想过背后的原因嘛，楼主通过本篇文章抛砖引玉，希望小伙伴们能够喜欢~
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yangyichao-mango.github.io/2021/07/10/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/05_place-holder/"/>
    <id>https://yangyichao-mango.github.io/2021/07/10/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/05_place-holder/</id>
    <published>2021-07-10T15:08:29.944Z</published>
    <updated>2021-07-10T15:08:29.944Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yangyichao-mango.github.io/2021/07/10/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/01_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BA%94%E7%94%A8/place-holder/"/>
    <id>https://yangyichao-mango.github.io/2021/07/10/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/01_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BA%94%E7%94%A8/place-holder/</id>
    <published>2021-07-10T15:08:29.943Z</published>
    <updated>2021-07-10T15:08:29.943Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yangyichao-mango.github.io/2021/07/10/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/04_data-sediment/place-holder/"/>
    <id>https://yangyichao-mango.github.io/2021/07/10/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/04_data-sediment/place-holder/</id>
    <published>2021-07-10T15:08:29.943Z</published>
    <updated>2021-07-10T15:08:29.943Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yangyichao-mango.github.io/2021/07/10/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/02_one-service/01_OLAP/place-holder/"/>
    <id>https://yangyichao-mango.github.io/2021/07/10/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/02_one-service/01_OLAP/place-holder/</id>
    <published>2021-07-10T15:08:29.943Z</published>
    <updated>2021-07-10T15:08:29.943Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yangyichao-mango.github.io/2021/07/10/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/02_%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/place-holder/"/>
    <id>https://yangyichao-mango.github.io/2021/07/10/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/02_%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/place-holder/</id>
    <published>2021-07-10T15:08:29.943Z</published>
    <updated>2021-07-10T15:08:29.943Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yangyichao-mango.github.io/2021/07/10/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/02_flink-datastream/place-holder/"/>
    <id>https://yangyichao-mango.github.io/2021/07/10/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/02_flink-datastream/place-holder/</id>
    <published>2021-07-10T15:08:29.943Z</published>
    <updated>2021-07-10T15:08:29.943Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yangyichao-mango.github.io/2021/06/29/wechat-blog/a/"/>
    <id>https://yangyichao-mango.github.io/2021/06/29/wechat-blog/a/</id>
    <published>2021-06-29T14:09:08.920Z</published>
    <updated>2021-06-29T14:29:26.589Z</updated>
    
    <content type="html"><![CDATA[<html lang="en"><head>    <title>Document</title>    <!-- 引入本地组件库 -->    <!--  <link rel="stylesheet" href="static/element-ui/index.css" >     <script src="static/element-ui/vue.js"></script>     <script src="static/element-ui/index.js"></script> -->    <!-- 引入CDN样式 -->    <link rel="stylesheet" href="https://unpkg.com/element-ui/lib/theme-chalk/index.css">    <script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>    <script src="https://unpkg.com/element-ui/lib/index.js"></script>    <style>        .not-active {            display: inline-block;            font-size: 12px;            margin: 5px 8px;        }        span {            margin: 0 2px;        }    </style></head><body><div id="app">    <!-- 待选标签 -->    <div v-for='(category, categoryIndex) in categories' :key="category.id">        <!-- 分类 -->        <span class="not-active">：</span>        <template>            <span v-if="category.count" class="not-active" @click="clearCategory(category, categoryIndex)"> 不限</span>            <my-tag v-else>不限</my-tag>        </template>        <!-- 标签 -->        <template v-for='(child, childIndex) in category.children'>            <my-tag v-if="child.active" :closable='true'                    @click-child='clickChild(category, categoryIndex, child, childIndex)'>                            </my-tag>            <span v-else class="not-active" @click='clickChild(category, categoryIndex, child, childIndex)'></span>        </template>    </div>    <!-- 已选标签 -->    <div v-if='conditions.length'>   <span class="not-active" @click="clearCondition">清空已选：<span>   <el-tag           v-for='(condition, index) in conditions'           :key="condition.id"           type="primary"           :closable="true"           size="small"           :disable-transitions="true"           @close='removeCondition(condition, index)'           @click='removeCondition(condition, index)'>       </el-tag>    </div>    <!--    <el-radio-group v-model="radio">-->    <!--        <el-radio :label="3">备选项</el-radio>-->    <!--        <el-radio :label="6">备选项</el-radio>-->    <!--        <el-radio :label="9">备选项</el-radio>-->    <!--    </el-radio-group>-->    <el-row>        <el-col :span="24"><div class="grid-content bg-purple-dark"><el-radio-group v-model="radio">            <el-radio :label="3">备选项1</el-radio>            <el-radio :label="6">备选项2</el-radio>            <el-radio :label="9">备选项3</el-radio>        </el-radio-group></div></el-col>        <el-col :span="12"><el-radio-group v-model="radio">            <el-radio :label="3">备选项1</el-radio>            <el-radio :label="6">备选项2</el-radio>            <el-radio :label="9">备选项3</el-radio>        </el-radio-group><el-col>    </el-row></div><script>    var categories = [{        name: '品牌',        count: 0,        children: [{            name: '联想',        }, {            name: '小米',        }, {            name: '苹果',        }, {            name: '东芝',        }]    }, {        name: 'CPU',        count: 0,        children: [{            name: 'intel i7 8700K',        }, {            name: 'intel i7 7700K',        }, {            name: 'intel i9 9270K',        }, {            name: 'intel i7 8700',        }, {            name: 'AMD 1600X',        }]    }, {        name: '内存',        count: 0,        children: [{            name: '七彩虹8G',        }, {            name: '七彩虹16G',        }, {            name: '金士顿8G',        }, {            name: '金士顿16G',        }]    }, {        name: '显卡',        count: 0,        children: [{            name: 'NVIDIA 1060 8G',        }, {            name: 'NVIDIA 1080Ti 16G',        }, {            name: 'NVIDIA 1080 8G',        }, {            name: 'NVIDIA 1060Ti 16G',        }]    }]</script><script>    // 简单封装一个公用组件    Vue.component('my-tag', {        template: "<el-tag v-bind='$attrs' v-on='$listeners' effect='dark' size='small' :disable-transitions='true' @click='clickChild' @close='clickChild'><slot></slot></el-tag>",        methods: {            clickChild() {                this.$emit("click-child")            }        }    });    var app = new Vue({        el: '#app',        data() {            return {                // categories, // 分类标签，可从外部加载配置                conditions: [], // 已选条件                radio: 3            }        },        watch: {            // 监听条件变化，按照请求接口拼装请求参数            conditions(val) {                let selectedCondition = {};                for (let categorie of this.categories) {                    let selected_list = [];                    for (let child of categorie.children) {                        if (child.active) {                            selected_list.push(child.name);                        }                    }                    selectedCondition[categorie.name] = selected_list.join("|")                }                console.log(selectedCondition);            }        },        methods: {            // 处理标签点击事件，未选中则选中，已选中则取消选中            clickChild(category, categoryIndex, child, childIndex) {                let uid = `${categoryIndex}-${childIndex}`                child.uid = uid;                console.log(uid)                // 取消选择                if (child.active === true) {                    category.count--;                    child.active = false;                    this.conditions.forEach((conditionChild, index) => {                        if (conditionChild.uid === child.uid) {                            this.conditions.splice(index, 1);                        }                    });                    // 选择                } else {                    category.count++;                    child.active = true;                    this.conditions.push(child);                }            },            // 清除已选整个类别标签            clearCategory(category, categoryIndex) {                category.count = 0;                // 可选列表均为未选中状态                category.children.forEach(child => {                    child.active = false;                })                // 清空该类已选元素                for (let index = this.conditions.length - 1; index >= 0; index--) {                    const conditionChild = this.conditions[index];                    if (conditionChild.uid.startsWith(categoryIndex)) {                        this.conditions.splice(index, 1);                    }                }            },            // 移除一个条件            removeCondition(condition, index) {                let categoryIndex = condition.uid.split("-")[0];                this.categories[categoryIndex].count--;                this.conditions.splice(index, 1)                condition.active = false;            },            // 清空所有条件            clearCondition() {                for (let i = this.conditions.length - 1; i >= 0; i--) {                    this.removeCondition(this.conditions[i], i);                }            }        }    });</script></body></html>]]></content>
    
    <summary type="html">
    
      
      
        &lt;html lang=&quot;en&quot;&gt;

&lt;head&gt;
    &lt;title&gt;Document&lt;/title&gt;

    &lt;!-- 引入本地组件库 --&gt;
    &lt;!--  &lt;link rel=&quot;stylesheet&quot; href=&quot;static/element-ui/index.cs
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>深入浅出 | flink minicluster 启动流程</title>
    <link href="https://yangyichao-mango.github.io/2021/04/12/wechat-blog/apache-flink:minicluster-start/"/>
    <id>https://yangyichao-mango.github.io/2021/04/12/wechat-blog/apache-flink:minicluster-start/</id>
    <published>2021-04-12T06:21:53.000Z</published>
    <updated>2021-04-17T05:47:05.079Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><ol><li><p>目录</p></li><li><p>flink 怎么判断是本地环境还是集群环境的？</p></li><li><p>flink 的接口都是声明式的，那么到底把我们的接口解析或者封装成什么样进行执行了？</p></li></ol><h1 id="flink-怎么判断是本地环境还是集群环境的？"><a href="#flink-怎么判断是本地环境还是集群环境的？" class="headerlink" title="flink 怎么判断是本地环境还是集群环境的？"></a>flink 怎么判断是本地环境还是集群环境的？</h1><h2 id="环境判断"><a href="#环境判断" class="headerlink" title="环境判断"></a>环境判断</h2><p><img src="/blog-img/apache-flink:minicluster-start/1.png" alt="1"></p><p>通过 debug StreamExecutionEnvironment.getExecutionEnvironment(); 可以定位到下面的代码</p><p><img src="/blog-img/apache-flink:minicluster-start/8.png" alt="8"></p><p>我们可以看注释，解释一下，基本流程如下：</p><ol><li>先尝试获取本地环境，即从 threadLocalContextEnvironmentFactory 获取环境配置</li><li>获取不到本地环境配置，就获取外部环境配置，即从 contextEnvironmentFactory 获取环境配置</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> StreamExecutionEnvironment <span class="title">getExecutionEnvironment</span><span class="params">(Configuration configuration)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Utils.resolveFactory(threadLocalContextEnvironmentFactory, contextEnvironmentFactory)</span><br><span class="line">            .map(factory -&gt; factory.createExecutionEnvironment(configuration))</span><br><span class="line">            .orElseGet(() -&gt; StreamExecutionEnvironment.createLocalEnvironment(configuration));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function">Optional&lt;T&gt; <span class="title">resolveFactory</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            ThreadLocal&lt;T&gt; threadLocalFactory, <span class="meta">@Nullable</span> T staticFactory)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> T localFactory = threadLocalFactory.get();</span><br><span class="line">    <span class="keyword">final</span> T factory = localFactory == <span class="keyword">null</span> ? staticFactory : localFactory;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Optional.ofNullable(factory);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>那么问题来了，当集群环境时，是如何将 StreamExecutionEnvironmentFactory 到 ThreadLocal 中？</strong></p><h2 id="集群环境"><a href="#集群环境" class="headerlink" title="集群环境"></a>集群环境</h2><p>通过 bin/flink run ….命令提交jar包到集群运行命令时，该脚本会调用org.apache.flink.client.cli.CliFrontend 来运行用户程序，如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Add HADOOP_CLASSPATH to allow the usage of Hadoop file systems</span></span><br><span class="line">exec $JAVA_RUN $JVM_ARGS $FLINK_ENV_JAVA_OPTS &quot;$&#123;log_setting[@]&#125;&quot; -classpath &quot;`manglePathList &quot;$CC_CLASSPATH:$INTERNAL_HADOOP_CLASSPATHS&quot;`&quot; org.apache.flink.client.cli.CliFrontend &quot;$@&quot;</span><br></pre></td></tr></table></figure><p>在CliFrontend中依次执行以下方法 main() -&gt;parseParameters() -&gt; run() -&gt;executeProgram()</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">executeProgram</span><span class="params">(<span class="keyword">final</span> Configuration configuration, <span class="keyword">final</span> PackagedProgram program)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> ProgramInvocationException </span>&#123;</span><br><span class="line">    ClientUtils.executeProgram(</span><br><span class="line">            <span class="keyword">new</span> DefaultExecutorServiceLoader(), configuration, program, <span class="keyword">false</span>, <span class="keyword">false</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在org.apache.flink.client.ClientUtils的executeProgram()中调用StreamContextEnvironment.setAsContext(…)，StreamContextEnvironment继承自StreamExecutionEnvironment。setAsContext()代码如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">StreamContextEnvironment.setAsContext(</span><br><span class="line">    executorServiceLoader,</span><br><span class="line">    configuration,</span><br><span class="line">    userCodeClassLoader,</span><br><span class="line">    enforceSingleJobExecution,</span><br><span class="line">    suppressSysout);</span><br></pre></td></tr></table></figure><p>创建生成运行环境的工厂类实例，在initializeContextEnvironment()方法中把实例放到StreamExecutionEnvironment类的静态属性threadLocalContextEnvironmentFactory 中，代码如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">initializeContextEnvironment</span><span class="params">(StreamExecutionEnvironmentFactory ctx)</span> </span>&#123;</span><br><span class="line">    contextEnvironmentFactory = ctx;</span><br><span class="line">    threadLocalContextEnvironmentFactory.set(contextEnvironmentFactory);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这样在用户程序StreamExecutionEnvironment.getExecutionEnvironment()时，获取到的运行环境就是StreamContextEnvironment类的setAsContext()方法中生成的<br>本地运行环境LocalStreamEnvironment和 独立集群、flink on yarn等运行环境StreamContextEnvironment 的主要区别在于，他们的成员属性 configuration 不同。LocalStreamEnvironment 中是创建的空键值对(new Configuration())，而StreamContextEnvironment 是通过CliFrontend 生成的Configuration对象。</p><h1 id="flink-的接口都是声明式的，那么到底把我们的接口解析或者封装成什么样进行执行了？"><a href="#flink-的接口都是声明式的，那么到底把我们的接口解析或者封装成什么样进行执行了？" class="headerlink" title="flink 的接口都是声明式的，那么到底把我们的接口解析或者封装成什么样进行执行了？"></a>flink 的接口都是声明式的，那么到底把我们的接口解析或者封装成什么样进行执行了？</h1><h2 id="声明式编程"><a href="#声明式编程" class="headerlink" title="声明式编程"></a>声明式编程</h2><p><a href="https://www.cnblogs.com/sirkevin/p/8283110.html">https://www.cnblogs.com/sirkevin/p/8283110.html</a></p><h2 id="flink-runtime-架构概览"><a href="#flink-runtime-架构概览" class="headerlink" title="flink runtime 架构概览"></a>flink runtime 架构概览</h2><p><img src="/blog-img/apache-flink:minicluster-start/9.svg" alt="9"></p><p>从上述整体的架构可以看出来，flink 的架构是区分客户端和服务端的，我们在客户端编写号代码，然后提交代码到服务端进行运行<br>而且 flink 的接口是声明式的，那么这个过程中肯定会涉及到代码的包装执行。具体代码进行了哪些包装，如下图 4 个图的流程就是 flink 从客户端提交代码到服务端具体执行的整个解析过程：</p><p><img src="/blog-img/apache-flink:minicluster-start/10.png" alt="10"></p><p><strong>1. StreamGraph：根据用户通过 Stream API 编写的代码生成的最初的图。</strong></p><ul><li>StreamNode：用来代表 operator 的类，并具有所有相关的属性，如并发度、入边和出边等。</li><li>StreamEdge：表示连接两个 StreamNode 的边。</li><li><strong><kbd><font color=red>是在客户端生成的，在 flink 中是一个具体的数据结构。</font></kbd></strong></li></ul><p><img src="/blog-img/apache-flink:minicluster-start/11.png" alt="11"></p><p><strong>2. JobGraph：StreamGraph 经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。</strong></p><ul><li>JobVertex：经过优化后符合条件的多个 StreamNode 可能会 chain 在一起生成一个 JobVertex，即一个 JobVertex 包含一个或多个 operator，JobVertex 的输入是 JobEdge，输出是 IntermediateDataSet。</li><li>IntermediateDataSet：表示 JobVertex 的输出，即经过 operator 处理产生的数据集。producer 是 JobVertex，consumer 是 JobEdge。</li><li>JobEdge：代表了 job graph 中的一条数据传输通道。source 是 IntermediateDataSet，target 是 JobVertex。即数据通过 JobEdge 由 IntermediateDataSet 传递给目标 JobVertex。</li><li><strong><kbd><font color=red>是在客户端生成的，在 flink 中是一个具体的数据结构。</font></kbd></strong></li></ul><p><img src="/blog-img/apache-flink:minicluster-start/12.png" alt="12"></p><p><strong>3. ExecutionGraph：JobManager 根据 JobGraph 生成 ExecutionGraph。ExecutionGraph 是 JobGraph 的并行化版本，是调度层最核心的数据结构。</strong></p><ul><li>ExecutionJobVertex：和 JobGraph 中的 JobVertex 一一对应。每一个 ExecutionJobVertex 都有和并发度一样多的 ExecutionVertex。</li><li>ExecutionVertex：表示 ExecutionJobVertex 的其中一个并发子任务，输入是 ExecutionEdge，输出是 IntermediateResultPartition。</li><li>IntermediateResult：和 JobGraph 中的 IntermediateDataSet 一一对应。一个 IntermediateResult 包含多个 IntermediateResultPartition，其个数等于该 operator 的并发度。</li><li>IntermediateResultPartition：表示 ExecutionVertex 的一个输出分区，producer 是 ExecutionVertex，consumer 是若干个 ExecutionEdge。</li><li>ExecutionEdge：表示 ExecutionVertex 的输入，source 是 IntermediateResultPartition，target 是 ExecutionVertex。source 和 target 都只能是一个。</li><li>Execution：是执行一个 ExecutionVertex 的一次尝试。当发生故障或者数据需要重算的情况下 ExecutionVertex 可能会有多个 ExecutionAttemptID。一个 Execution 通过 ExecutionAttemptID 来唯一标识。JM 和 TM 之间关于 task 的部署和 task status 的更新都是通过 ExecutionAttemptID 来确定消息接受者。</li><li><strong><kbd><font color=red>是在服务端生成的，在 flink 中是一个具体的数据结构。</font></kbd></strong></li></ul><p><img src="/blog-img/apache-flink:minicluster-start/13.png" alt="13"></p><p><strong>4. 物理执行图：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。</strong></p><ul><li>Task：Execution被调度后在分配的 TaskManager 中启动对应的 Task。Task 包裹了具有用户执行逻辑的 operator。</li><li>ResultPartition：代表由一个Task的生成的数据，和ExecutionGraph中的IntermediateResultPartition一一对应。</li><li>ResultSubpartition：是ResultPartition的一个子分区。每个ResultPartition包含多个ResultSubpartition，其数目要由下游消费 Task 数和 DistributionPattern 来决定。</li><li>InputGate：代表Task的输入封装，和JobGraph中JobEdge一一对应。每个InputGate消费了一个或多个的ResultPartition。</li><li>InputChannel：每个InputGate会包含一个以上的InputChannel，和ExecutionGraph中的ExecutionEdge一一对应，也和ResultSubpartition一对一地相连，即一个InputChannel接收一个ResultSubpartition的输出。</li><li><strong><kbd><font color=red>是在服务端生成的，不是具体的数据结构，而是实际的物理执行的一个图描述。</font></kbd></strong></li></ul><p>那么 Flink 为什么要设计这4张图呢，其目的是什么呢？Spark 中也有多张图，数据依赖图以及物理执行的 DAG。其目的都是一样的，就是解耦，每张图各司其职，每张图对应了 Job 不同的阶段，更方便做该阶段的事情。我们给出更完整的 Flink Graph 的层次图。<br><img src="/blog-img/apache-flink:minicluster-start/14.png" alt="14"></p><p>首先我们看到，JobGraph 之上除了 StreamGraph 还有 OptimizedPlan。OptimizedPlan 是由 Batch API 转换而来的。StreamGraph 是由 Stream API 转换而来的。为什么 API 不直接转换成 JobGraph？因为，Batch 和 Stream 的图结构和优化方法有很大的区别，比如 Batch 有很多执行前的预分析用来优化图的执行，而这种优化并不普适于 Stream，所以通过 OptimizedPlan 来做 Batch 的优化会更方便和清晰，也不会影响 Stream。JobGraph 的责任就是统一 Batch 和 Stream 的图，用来描述清楚一个拓扑图的结构，并且做了 chaining 的优化，chaining 是普适于 Batch 和 Stream 的，所以在这一层做掉。ExecutionGraph 的责任是方便调度和各个 tasks 状态的监控和跟踪，所以 ExecutionGraph 是并行化的 JobGraph。而“物理执行图”就是最终分布式在各个机器上运行着的tasks了。所以可以看到，这种解耦方式极大地方便了我们在各个层所做的工作，各个层之间是相互隔离的。</p><h2 id="如何生成-StreamGraph？"><a href="#如何生成-StreamGraph？" class="headerlink" title="如何生成 StreamGraph？"></a>如何生成 StreamGraph？</h2><blockquote><p>基于 1.12 源码。</p></blockquote><p>StreamGraph 是在客户端构造的，这意味着我们可以在本地通过调试观察 StreamGraph 的构造过程。<br>StreamGraph 生成的整个入口函数执行顺序如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 开始执行代码</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> JobExecutionResult <span class="title">execute</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> execute(getJobName());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> JobExecutionResult <span class="title">execute</span><span class="params">(String jobName)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Preconditions.checkNotNull(jobName, <span class="string">&quot;Streaming Job name should not be null.&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> execute(getStreamGraph(jobName));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 生成 StreamGraph</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamGraph <span class="title">getStreamGraph</span><span class="params">(String jobName, <span class="keyword">boolean</span> clearTransformations)</span> </span>&#123;</span><br><span class="line">    StreamGraph streamGraph = getStreamGraphGenerator().setJobName(jobName).generate();</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> streamGraph;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取 StreamGraphGenerator</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> StreamGraphGenerator <span class="title">getStreamGraphGenerator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> StreamGraphGenerator(transformations, config, checkpointCfg, getConfiguration())</span><br><span class="line">            .setRuntimeExecutionMode(executionMode)</span><br><span class="line">            .setStateBackend(defaultStateBackend)</span><br><span class="line">            .setChaining(isChainingEnabled)</span><br><span class="line">            .setUserArtifacts(cacheFile)</span><br><span class="line">            .setTimeCharacteristic(timeCharacteristic)</span><br><span class="line">            .setDefaultBufferTimeout(bufferTimeout);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始化 StreamGraphGenerator</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">StreamGraphGenerator</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        List&lt;Transformation&lt;?&gt;&gt; transformations,</span></span></span><br><span class="line"><span class="function"><span class="params">        ExecutionConfig executionConfig,</span></span></span><br><span class="line"><span class="function"><span class="params">        CheckpointConfig checkpointConfig,</span></span></span><br><span class="line"><span class="function"><span class="params">        ReadableConfig configuration)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.transformations = checkNotNull(transformations);</span><br><span class="line">    <span class="keyword">this</span>.executionConfig = checkNotNull(executionConfig);</span><br><span class="line">    <span class="keyword">this</span>.checkpointConfig = <span class="keyword">new</span> CheckpointConfig(checkpointConfig);</span><br><span class="line">    <span class="keyword">this</span>.configuration = checkNotNull(configuration);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 客户端开始执行</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> JobExecutionResult <span class="title">execute</span><span class="params">(StreamGraph streamGraph)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> JobClient jobClient = executeAsync(streamGraph);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到初始化 StreamGraphGenerator 需要传入四个参数，其中我们最需要关心的参数就是第一个 transformations；</p><h3 id="Transformation-是用来干啥的？"><a href="#Transformation-是用来干啥的？" class="headerlink" title="Transformation 是用来干啥的？"></a>Transformation 是用来干啥的？</h3><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/operators/#datastream-transformations">https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/operators/#datastream-transformations</a></p><p>Transformation 其实就是我们在代码中定义的各种转换操作，比如 map，filter，flatmap，keyby 等转换操作，经过 flink 的简单包装之后成为 Transformation，其之间的具体关系如下图：<br><img src="/blog-img/apache-flink:minicluster-start/16.png" alt="16"></p><p>经过这么多层的封装，肯定是为用户自定义算子增强或者填充了一些功能，具体是哪些功能呢？</p><p>举个例子：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">environment</span><br><span class="line">    .fromElements(Tuple2.of(<span class="number">1</span>, <span class="number">1</span>), Tuple2.of(<span class="number">2</span>, <span class="number">2</span>), Tuple2.of(<span class="number">1</span>, <span class="number">3</span>), Tuple2.of(<span class="number">2</span>, <span class="number">4</span>), Tuple2.of(<span class="number">3</span>, <span class="number">10</span>))</span><br><span class="line">    .map(t -&gt; t)</span><br><span class="line">    .keyBy(<span class="keyword">new</span> KeySelector&lt;Tuple2&lt;Integer, Integer&gt;, Integer&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(Tuple2&lt;Integer, Integer&gt; i)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> i.f0;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    .flatMap(<span class="keyword">new</span> CountWindowAverage())</span><br><span class="line">    .print();</span><br></pre></td></tr></table></figure><p>先 debug 到 <kbd>map(t -> t)</kbd>，然后我们来看 env 中的变量：<br><img src="/blog-img/apache-flink:minicluster-start/17.png" alt="17"></p><p>从这个变量来看就可以和上面的图映射上了，分别是 <kbd>OneInputTransformation -> StreamMap -> UserFunction</kbd>；<br>我们仔细看这些变量，来查看到底哪些增强了哪些功能。</p><p><strong>1. StreamMap</strong><br><img src="/blog-img/apache-flink:minicluster-start/20.png" alt="20"></p><ul><li>ChainingStrategy：flink 的优化策略中有本地性优化，就是会检查前一个算子和后一个算子是否能够 chain 在一起，这样的话就可以把数据的 shuffle 本地化，而 ChainingStrategy 就是这个优化的一个决定条件；</li></ul><p><strong>2. OneInputTransformation</strong><br><img src="/blog-img/apache-flink:minicluster-start/19.png" alt="19"></p><ul><li>input：负责将上游 Transformation 进行存储，之后可以利用这个上下游关系串联整个 DAG；</li><li>stateKeySelector：如果是 keyBy 后接 map，则 map 算子就会作用在具体 key 的数据上，这个就是对应的 KeySelector；</li><li>stateKeyType：同 stateKeySelector，存储的是具体的 key 的类型信息；</li><li>outputType：输出数据的类型信息；</li><li>typeUsed：TODO；</li><li>parallelism：算子并发度信息；</li><li>maxParallelism：算子最大并发度信息；</li><li>minResources，preferredResources：用户自定义细粒度的资源配置（<strong>这个 feature 还没有开放给用户</strong>）；</li><li>managedMemoryWeight：TODO</li><li>uid：状态恢复时的状态唯一标识，恢复时会根据状态中的 uid 和算子中的 uid 进行匹配，匹配不到状态就不能恢复；</li><li>userProvidedNodeHash：TODO</li><li>bufferTimeout：TODO</li><li>slotSharingGroup：可以进行 slot 分享的 group 标识；</li><li>coLocationGroupKey：TODO</li></ul><p>注意不同的 Transformation 做了不同的增强，常见的 Transformation 有以下几类，大家可以自己进行 debug 查看都有哪些增强。<br><img src="/blog-img/apache-flink:minicluster-start/15.png" alt="15"></p><p>另外，并不是每一个 Transformation 都会转换成 runtime 层中物理操作。有一些只是逻辑概念，比如 union、split/select、partition等。如下图所示的转换树，在运行时会优化成下方的操作图。</p><p><img src="/blog-img/apache-flink:minicluster-start/21.png" alt="21"><br>union、split/select、partition 中的信息会被写入到 Source –&gt; Map 的边中。通过源码也可以发现，UnionTransformation,SplitTransformation,SelectTransformation,PartitionTransformation 由于不包含具体的操作所以都没有 StreamOperator 成员变量，而其他 Transformation 的子类基本上都有。</p><p><font color=red><kbd>Transformation 的作用：</kbd></font></p><ul><li><font color=red><kbd>1：用户在编写时定义的上下游算子。即串联用户定义的原始 DAG 上下游信息。</kbd></font></li><li><font color=red><kbd>2：用户在编写时定义的输入输出类型。即尝试获取算子输入输出类型信息以及对应的序列化器。</kbd></font></li><li><font color=red><kbd>3：用户在编写时定义的算子并行度。即计算出每一个算子的并行度信息。</kbd></font></li></ul><h3 id="StreamGraph-是用来干啥的？"><a href="#StreamGraph-是用来干啥的？" class="headerlink" title="StreamGraph 是用来干啥的？"></a>StreamGraph 是用来干啥的？</h3><p>先来看看 StreamGraph 实例</p><p><img src="/blog-img/apache-flink:minicluster-start/26.png" alt="26"></p><h3 id="Transformation-gt-StreamGraph"><a href="#Transformation-gt-StreamGraph" class="headerlink" title="Transformation -&gt; StreamGraph"></a>Transformation -&gt; StreamGraph</h3><p>我们来看看 StreamGraph 构建完成之后最主要的内容包含哪些？</p><p><img src="/blog-img/apache-flink:minicluster-start/28.png" alt="28"></p><p>DataStream 上的每一个 Transformation 都对应了一个 StreamOperator，StreamOperator是运行时的具体实现，会决定UDF(User-Defined Funtion)的调用方式。下图所示为 StreamOperator 的类图（点击查看大图）：</p><p><img src="/blog-img/apache-flink:minicluster-start/22.png" alt="22"></p><p><img src="/blog-img/apache-flink:minicluster-start/23.png" alt="23"></p><p>可以发现，所有实现类都继承了AbstractStreamOperator。另外除了 project 操作，其他所有可以执行UDF代码的实现类都继承自AbstractUdfStreamOperator，该类是封装了UDF的StreamOperator。UDF就是实现了Function接口的类，如MapFunction,FilterFunction。</p><p>我们通过在 DataStream 上做了一系列的转换（map、filter等）得到了 Transformation 集合，然后通过 StreamGraphGenerator.generate 获得 StreamGraph，该方法的源码如下：</p><p><img src="/blog-img/apache-flink:minicluster-start/24.png" alt="24"></p><p>StreamGraphGenerator 中其实包含的最主要的内容就是各个类型 Transformation 的 Translator。如图所示；</p><p><img src="/blog-img/apache-flink:minicluster-start/25.png" alt="25"></p><p>每个 Translator 会将 Transformation 转换为对应的 StreamNode 或者 StreamEdge。</p><h2 id="如何生成-JobGraph？"><a href="#如何生成-JobGraph？" class="headerlink" title="如何生成 JobGraph？"></a>如何生成 JobGraph？</h2><p>我们来看看 JobGraph 构建完成之后最主要的内容包含哪些？</p><p><img src="/blog-img/apache-flink:minicluster-start/29.png" alt="29"></p><p>JobGraph 的相关数据结构主要在 org.apache.flink.runtime.jobgraph 包中。构造 JobGraph 的代码主要集中在 StreamingJobGraphGenerator 类中，入口函数是 StreamingJobGraphGenerator.createJobGraph()。我们首先来看下 StreamingJobGraphGenerator 的核心源码：</p><p>StreamNode 转成 JobVertex，StreamEdge 转成 JobEdge，JobEdge 和 JobVertex 之间创建 IntermediateDataSet 来连接。关键点在于将多个 SteamNode chain 成一个 JobVertex的过程，这部分源码比较绕，有兴趣的同学可以结合源码单步调试分析。下一章将会介绍 JobGraph 提交到 JobManager 后是如何转换成分布式化的 ExecutionGraph 的。</p><p>每个 JobVertex 都会对应一个可序列化的 StreamConfig, 用来发送给 JobManager 和 TaskManager。最后在 TaskManager 中起 Task 时,需要从这里面反序列化出所需要的配置信息, 其中就包括了含有用户代码的StreamOperator。</p><p>setChaining会对source调用createChain方法，该方法会递归调用下游节点，从而构建出node chains。createChain会分析当前节点的出边，根据Operator Chains中的chainable条件，将出边分成chainalbe和noChainable两类，并分别递归调用自身方法。之后会将StreamNode中的配置信息序列化到StreamConfig中。如果当前不是chain中的子节点，则会构建 JobVertex 和 JobEdge相连。如果是chain中的子节点，则会将StreamConfig添加到该chain的config集合中。一个node chains，除了 headOfChain node会生成对应的 JobVertex，其余的nodes都是以序列化的形式写入到StreamConfig中，并保存到headOfChain的 CHAINED_TASK_CONFIG 配置项中。直到部署时，才会取出并生成对应的ChainOperators，具体过程请见理解 Operator Chains。</p><h2 id="如何生成-StreamGraph？-1"><a href="#如何生成-StreamGraph？-1" class="headerlink" title="如何生成 StreamGraph？"></a>如何生成 StreamGraph？</h2><h1 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h1><ol><li><a href="http://wuchong.me/blog/2016/05/03/flink-internals-overview/">http://wuchong.me/blog/2016/05/03/flink-internals-overview/</a></li><li><a href="http://wuchong.me/blog/2016/05/04/flink-internal-how-to-build-streamgraph/">http://wuchong.me/blog/2016/05/04/flink-internal-how-to-build-streamgraph/</a></li><li><a href="https://outofmemory.cn/article-201939.html">https://outofmemory.cn/article-201939.html</a></li><li><a href="http://wuchong.me/blog/2016/05/10/flink-internals-how-to-build-jobgraph/">http://wuchong.me/blog/2016/05/10/flink-internals-how-to-build-jobgraph/</a></li></ol>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>深入浅出 | flink minicluster 启动流程</title>
    <link href="https://yangyichao-mango.github.io/2021/04/12/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/01_table-start/"/>
    <id>https://yangyichao-mango.github.io/2021/04/12/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/01_flink-sql/01_table-start/</id>
    <published>2021-04-12T06:21:53.000Z</published>
    <updated>2021-06-08T06:38:08.737Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><p>1.module</p><p>2.catalog</p><p>3.view 和 table 区别</p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>深入浅出 | flink 全局一致性快照（一）</title>
    <link href="https://yangyichao-mango.github.io/2021/03/12/wechat-blog/apache-flink:state-1/"/>
    <id>https://yangyichao-mango.github.io/2021/03/12/wechat-blog/apache-flink:state-1/</id>
    <published>2021-03-12T06:21:53.000Z</published>
    <updated>2021-04-05T12:59:40.400Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><ol><li><p>目录<br>先带大家串一遍本篇文章的思路</p></li><li><p>什么是状态？<br>发散思维的去思考状态，我们所理解的状态不仅仅只限于 flink 的状态，让大家了解到状态是一个无处不在的东西</p></li><li><p>什么是全局一致性快照？和状态有什么关系？<br>全局一致性快照的一些生活、工作中应用的例子</p></li><li><p>为什么需要一致性快照？全局一致性快照和 flink 的关系？<br>jvm GC，分布式应用做故障恢复（比如 flink），死锁检测等</p></li><li><p>全局一致性快照的分布式应用举例<br>通过一个简单分布式应用介绍一下全局一致性状态是每时每刻都存在的。时间轴上的每一个时刻都存在一个全局一致性快照（类似拍照片）。flink 做 cp，sp，类似于每隔固定的时间从时间轴上的一个点拿出来这个时间点对应的一个全局一致性状态</p></li><li><p>全局一致性快照的标准定义<br>假如说有两个事件，a 和 b，在绝对时间下，如果 a 发生在 b 之前，且 b 被包含在快照当中，那么则 a 事件或者其对快照产生的影响也被包含在快照当中</p></li><li><p>怎么实现全局一致性快照？<br>同步去做，包括时钟同步、Stop-the-world，但是这两种方法都不可接受；<br>既然同步无法做，那如果异步能做出相同的全局一致性状态也可以</p></li><li><p>分布式应用的全局一致性快照其 Process 状态和 Channel 状态到底需要记录什么？怎么记录 Channel 的状态？<br>不是必须要在同一时刻嘛，为啥还能异步去做？只要异步做出来的状态和同步做出来的状态效果一致也可以。并且详细分析了 process 和 channel 中的状态包括什么，以及记录 channel 状态的方法。</p></li><li><p>分布式应用全局一致性快照算法流程总结<br>通过第 8 章节的分析，总结出一套分布式应用的通用异步全局一致性快照算法</p></li><li><p>Chandy-Lamport 算法流程、例子<br>介绍 Chandy-Lamport 算法流程并以一个例子介绍其执行过程，并且说明和第 9 章节分析得出的方法之间的关系，两者之间互相是不冲突的</p></li><li><p>flink 实现的全局一致性快照介绍<br>大致介绍 flink 的全局一致性快照，并且也说明了和第 9 章节得出的方法之间的关系，两者之间也是不冲突的</p></li><li><p>分布式应用异步全局一致性快照方法、Chandy-Lamport 算法、flink 全局一致性快照之间的关系<br>Chandy-Lamport 算法、flink 全局一致性快照也满足第 8 章节的结论；并且 Chandy-Lamport 算法、flink 全局一致性快照是分布式应用异步全局一致性快照方法一种特殊形式</p></li><li><p>参考文章<br>本文编写过程中参考的文章</p></li></ol><h1 id="什么是状态？（了解状态）"><a href="#什么是状态？（了解状态）" class="headerlink" title="什么是状态？（了解状态）"></a>什么是状态？（了解状态）</h1><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>首先想让大家发散思维的去思考状态？我们所理解的状态不仅仅只限于 flink 的状态。让大家了解到状态是一个普遍存在的东西</p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>就是当前计算需要依赖到之前计算的结果，那么之前计算的结果就是状态</p><h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><ol><li><p>比如生活中的例子：为什么我知道我的面前放着一台电脑，因为眼睛接收到外界的图案，然后我的大脑接收到这个图案后，拿记忆中存储的图案进行对比，匹配得到这是电脑，那么记忆中存储的图案就是状态；还有比如日久生情，为什么感情会越来越深，因为今天的感情 = 今天积累的感情 + 以前积累的感情，以前积累的感情就是状态。这其中都存在状态</p></li><li><p>比如 web server 应用中的状态：打开 github 页面，列表展示了我的归属仓库。其中就是 web client 发了查询我的归属仓库请求，web server 接收到请求之后，然后去存储引擎中进行查询匹配返回。那么存储引擎中存储的内容就是状态<br><img src="/blog-img/apache-flink:state-1/1.png" alt="1"></p></li><li><p>比如 flink 应用中的状态：要做当天 uid 去重，就要存储所有的 uid；要获取当前最大值，那么历史最大值就是状态<br><img src="/blog-img/apache-flink:state-1/2.jpeg" alt="2"></p></li></ol><h1 id="什么是全局一致性快照？（了解全局一致性快照）"><a href="#什么是全局一致性快照？（了解全局一致性快照）" class="headerlink" title="什么是全局一致性快照？（了解全局一致性快照）"></a>什么是全局一致性快照？（了解全局一致性快照）</h1><ul><li><strong>全局：代表是一个分布式的</strong></li><li><strong>一致性快照：代表绝对时间的同一时刻的状态</strong></li><li><strong>相当于打开上帝视角，去观察同一时刻的应用所有的状态；这里的快照 = 状态，文章之后我可能会把这两个词混用，大家明白他们的意思一致即可</strong></li></ul><ol><li>比如生活中的例子：比如拍了一个照片，那么照片的内容就是当时的一个全局一致性快照；每一个首脑都是一个进程，所有的进程的状态在同一时刻的组合就是一个全局一致性快照</li></ol><p><img src="/blog-img/apache-flink:state-1/3.png" alt="3"></p><ol start="2"><li>比如分布式应用的例子：首先是一个分布式应用，它有多个进程分布在多个服务器上；其次，它在应用内部有自己的处理逻辑和状态；第三，应用间是可以互相通信的；第四，在这种分布式的应用，有内部状态，硬件可以通信的情况下；某一时刻的全局状态，就叫做全局的快照。</li></ol><p><strong>分布式应用某一时刻的全局一致性快照 = 各个 process 的本地状态 + channel 中正在传递的消息</strong></p><p><img src="/blog-img/apache-flink:state-1/4.png" alt="4"></p><p>介绍完了几个例子之后，我们来看看我们为什么需要一致性快照？</p><h1 id="为什么需要一致性快照？全局一致性快照和-flink-的关系？"><a href="#为什么需要一致性快照？全局一致性快照和-flink-的关系？" class="headerlink" title="为什么需要一致性快照？全局一致性快照和 flink 的关系？"></a>为什么需要一致性快照？全局一致性快照和 flink 的关系？</h1><br><p><img src="/blog-img/apache-flink:state-1/5.png" alt="5"></p><h2 id="实时案例"><a href="#实时案例" class="headerlink" title="实时案例"></a>实时案例</h2><ol><li>做检查点（全局一致性快照）用来故障恢复，<strong><font color=red>重点！！！重点！！！重点！！！就在于我们不必要从历史起点开始重跑所有的数据（其实这就是我们需要检查点的目的！！！）</font></strong>；因为<ul><li>流式应用的上游存储介质一般都不支持存储历史所有数据（比如上游为 kafka，kafka 不可能存储历史所有数据）</li><li>重跑时效性不能满足时效性要求（重跑历史数据的情况下，时效性是达不到要求的）</li></ul></li></ol><ol start="2"><li>可以做任务的死锁检测</li></ol><h2 id="全局一致性快照和-flink-的关系"><a href="#全局一致性快照和-flink-的关系" class="headerlink" title="全局一致性快照和 flink 的关系"></a>全局一致性快照和 flink 的关系</h2><p>flink 的 cp 和 sp 实际上就是全局一致性快照在分布式应用中 flink 的一个具体实现。</p><p><img src="/blog-img/apache-flink:state-1/6.png" alt="6"></p><h1 id="全局一致性快照的分布式应用案例？"><a href="#全局一致性快照的分布式应用案例？" class="headerlink" title="全局一致性快照的分布式应用案例？"></a>全局一致性快照的分布式应用案例？</h1><p>通过一个简单分布式应用介绍一下全局一致性状态是每时每刻都存在的。时间轴上的每一个时刻都存在一个全局一致性快照（可以用拍照片去类比）。</p><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>下面分布式应用的一个示例：</p><p><img src="/blog-img/apache-flink:state-1/7.jpeg" alt="7"><br><img src="/blog-img/apache-flink:state-1/8.jpeg" alt="8"><br><img src="/blog-img/apache-flink:state-1/9.jpeg" alt="9"><br><img src="/blog-img/apache-flink:state-1/10.jpeg" alt="10"></p><h2 id="每时每刻都存在全局一致性快照"><a href="#每时每刻都存在全局一致性快照" class="headerlink" title="每时每刻都存在全局一致性快照"></a>每时每刻都存在全局一致性快照</h2><p>上面这个只是四个时刻的四个快照，其实应用的每一个时刻都存在一个全局一致性快照。</p><p><img src="/blog-img/apache-flink:state-1/11.png" alt="11"></p><h1 id="全局一致性快照的标准定义"><a href="#全局一致性快照的标准定义" class="headerlink" title="全局一致性快照的标准定义"></a>全局一致性快照的标准定义</h1><h2 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h2><p>假如说有两个事件，a 和 b，在绝对时间下，如果 a 发生在 b 之前，且 b 被包含在快照当中，那么则 a 也被包含在快照当中。满足这个条件的全局快照，就称为全局一致性快照。</p><p><img src="/blog-img/apache-flink:state-1/12.png" alt="12"><br><img src="/blog-img/apache-flink:state-1/13.png" alt="13"></p><h2 id="楼主理解"><a href="#楼主理解" class="headerlink" title="楼主理解"></a>楼主理解</h2><p>就是如果将做了绝对时刻 T 的一个快照，那么这个绝对时刻 T 之前发生的所有事件以及其影响都会被包含在这个快照中</p><p>上文已经介绍了全局一致性快照的定义以及为什么我们需要全局一致性快照，那我们来放眼实际应用中，怎么才能做出一个满足生产实际要求的全局一致性快照呢？</p><h1 id="怎么实现全局一致性快照？"><a href="#怎么实现全局一致性快照？" class="headerlink" title="怎么实现全局一致性快照？"></a>怎么实现全局一致性快照？</h1><br>实现方式主要分为同步实现方式和异步实现方式两类。<h2 id="同步实现方式"><a href="#同步实现方式" class="headerlink" title="同步实现方式"></a>同步实现方式</h2><p><img src="/blog-img/apache-flink:state-1/14.jpeg" alt="14"></p><ul><li><a href="https://baike.baidu.com/item/NTP%E6%9C%8D%E5%8A%A1%E5%99%A8/8633994?fr=aladdin">NTP</a>: NTP服务器[Network Time Protocol（NTP）]是用来使计算机时间同步化的一种协议，它可以使计算机对其服务器或时钟源（如石英钟，GPS等等)做同步化，它可以提供高精准度的时间校正（LAN上与标准间差小于1毫秒，WAN上几十毫秒）<br>结论：无法实现</li><li><a href="https://www.jianshu.com/p/b210f9db19a3">Stop-The-World</a></li></ul><p><strong>结论：不满足需求，无法采用</strong></p><p><strong><font color=red>如果同步实现方式不满足需求，那么能使用异步方式做到同步相同的快照也是可以满足需求的。</font></strong></p><h2 id="异步实现方式"><a href="#异步实现方式" class="headerlink" title="异步实现方式"></a>异步实现方式</h2><ul><li><a href="https://www.microsoft.com/en-us/research/uploads/prod/2016/12/Determining-Global-States-of-a-Distributed-System.pdf?ranMID=24542&ranEAID=J84DHJLQkR4&ranSiteID=J84DHJLQkR4-mVoVymFnAblBx3zwyf98Pw&epi=J84DHJLQkR4-mVoVymFnAblBx3zwyf98Pw&irgwc=1&OCID=AID2000142_aff_7593_1243925&tduid=%28ir__1hs2uuow6wkfq3oxkk0sohzzwm2xpc33lxd0o6g200%29%287593%29%281243925%29%28J84DHJLQkR4-mVoVymFnAblBx3zwyf98Pw%29%28%29&irclickid=_1hs2uuow6wkfq3oxkk0sohzzwm2xpc33lxd0o6g200">Chandy-Lamport</a></li></ul><p><strong><font color=red>在介绍 Chandy-Lamport 算法之前，我们先介绍一些理论和数学上的概念铺垫铺垫，帮助我们理解 Chandy-Lamport 算法。</font></strong><br><strong><font color=red>这些概念主要就是介绍分布式应用的 Process 和 Channel 应该存储什么内容？以及怎样去存储这些内容？只有我们知道了要存储什么东西才好去设计和介绍具体算法嘛~</font></strong><br><strong>楼主也是看了很多博客才总结得到了这些条件！！！</strong></p><h1 id="分布式应用的全局一致性快照其-Process-状态和-Channel-状态记录了什么？怎么记录-Channel-的状态？"><a href="#分布式应用的全局一致性快照其-Process-状态和-Channel-状态记录了什么？怎么记录-Channel-的状态？" class="headerlink" title="分布式应用的全局一致性快照其 Process 状态和 Channel 状态记录了什么？怎么记录 Channel 的状态？"></a>分布式应用的全局一致性快照其 Process 状态和 Channel 状态记录了什么？怎么记录 Channel 的状态？</h1><h2 id="分布式应用要记录的状态"><a href="#分布式应用要记录的状态" class="headerlink" title="分布式应用要记录的状态"></a>分布式应用要记录的状态</h2><p>如下图案例 <strong>Single-Token conservation</strong>，是一个分布式应用，有 p 和 q 两个进程，p 可以通过 Channel pq（记为 Cpq） 向 q 发消息，q 可以通过 Channel qp（记为 Cqp） 向 p 发消息，其中有一个叫 token 的消息，在这个系统中一直不停的流转。<br><img src="/blog-img/apache-flink:state-1/15.png" alt="15"></p><p>如之前所述，分布式应用的全局一致性快照包含 Process 状态和 Channel 状态<br>那么上图 <strong>Single-Token conservation</strong> 示例中的全局一致性快照 <strong><kbd>S = S(p) + S(Cpq) + S(q) + S(Cqp)</kbd></strong></p><p>其中：</p><ul><li><kbd>S</kbd>：全局一致性快照</li><li><kbd>S(p)</kbd>：p 进程的状态</li><li><kbd>S(Cpq)</kbd>：p 进程到 q 进程的 Channel 状态</li><li><kbd>S(q)</kbd>：q 进程的状态</li><li><kbd>S(Cqp)</kbd>：q 进程到 p 进程的 Channel 状态</li></ul><p><img src="/blog-img/apache-flink:state-1/15.png" alt="15"></p><p><strong><font color=red>问题：</font></strong><br><strong><font color=red>这里就碰到了我们要分析的关键问题：做全局一致性快照时，其中 <kbd>S(p)</kbd>，<kbd>S(q)</kbd> 好理解，但是 <kbd>S(Cpq)</kbd>，<kbd>S(Cqp)</kbd> 到底记录了什么东西？应该记录什么东西？接下来详细讲讲我的理解</font></strong></p><h2 id="Process-状态应该记录什么内容？"><a href="#Process-状态应该记录什么内容？" class="headerlink" title="Process 状态应该记录什么内容？"></a>Process 状态应该记录什么内容？</h2><p>记录和用户业务需求相关的状态内容，用到了关于状态的地方，进行记录就好了，这部分是好理解的。<br>举例：uid 去重就存储历史所有的 uid 就可以了</p><h2 id="Channel-状态应该记录什么内容？"><a href="#Channel-状态应该记录什么内容？" class="headerlink" title="Channel 状态应该记录什么内容？"></a>Channel 状态应该记录什么内容？</h2><h3 id="Single-Token-conservation-的全局一致性快照"><a href="#Single-Token-conservation-的全局一致性快照" class="headerlink" title="Single-Token conservation 的全局一致性快照"></a>Single-Token conservation 的全局一致性快照</h3><p>token 在 p 时（对应第一张图），这时的全局一致性快照为：<br><kbd>S(token-in-p) = S(p-token-in-p) + S(Cpq-token-in-p) + S(q-token-in-p) + S(Cqp-token-in-p) </kbd></p><p>其中：</p><ul><li><kbd>S(token-in-p)</kbd>：token 在 p 时，做的全局一致性快照</li><li><kbd>S(p-token-in-p)</kbd>：token 在 p 时，p 进程的状态</li><li><kbd>S(Cpq-token-in-p)</kbd>：token 在 p 时，p 进程到 q 进程的 Channel 状态</li><li><kbd>S(q-token-in-p)</kbd>：token 在 p 时，q 进程的状态</li><li><kbd>S(Cqp-token-in-p)</kbd>：token 在 p 时，q 进程到 p 进程的 Channel 状态</li></ul><h3 id="提出问题"><a href="#提出问题" class="headerlink" title="提出问题"></a>提出问题</h3><p>注意，上述这个表达式其实是结论，这个结论是很好理解的，但是你有想过站在实际应用的角度去思考下面的问题吗？</p><ol><li><p>问题1：为什么第一张图的全局一致性状态是 Process 和 Channel 做的快照都有 token-in-p 呢？<br>根据之前的拍照片的类比，当前这个绝对时刻做快照时，token 在 p；那么所有的 process 和 channel 记录状态时，token 都应该在 p。</p></li><li><p>问题2：S(p-token-in-p) 好理解，在这个时刻 token 还没有从 p 发出去，p 做快照时肯定知道 token 还在 p；但是站在 Cpq 做状态的角度来说：</p></li></ol><ul><li>Cpq 做状态时，怎么保障 Cpq 知道 token in p？需要我们探索下有什么方法怎么让 Cpq 在做状态时知道 token in p？</li><li>站在实际在应用中实现的角度时，满足怎样的数学条件（我们要开始实现一个真实的全局一致性快照啦，肯定会涉及到一些数据知识，别急，往后看，用到的数学知识并不复杂）才能做出一个 S(Cpq) ？</li></ul><h3 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h3><h4 id="分析-S-Cpq-记录了什么内容？"><a href="#分析-S-Cpq-记录了什么内容？" class="headerlink" title="分析 S(Cpq) 记录了什么内容？"></a>分析 <kbd>S(Cpq)</kbd> 记录了什么内容？</h4><p>这里我们简单先理解下，<kbd>S(Cpq)</kbd> 其实就是在 <kbd>S(p)</kbd> 和 <kbd>S(q)</kbd> 自己的状态做成时，还在 Channel pq 之间发送的那些消息。</p><p>那么我们怎么用数学的方式理解 Cpq 记录的这些消息以及 Process 和 Channel 做状态时需要满足的条件呢？让我们往下看</p><h4 id="变量定义"><a href="#变量定义" class="headerlink" title="变量定义"></a>变量定义</h4><ul><li>n：在 p 的状态记录前，p 记录的 p 发往 Cpq 的 msg 数；</li><li>n′：在 Cpq 的状态记录前，Cpq 记录的 p 发往 Cpq 的 msg 数；</li><li>m：在 q 的状态记录前，q 记录的 q 从 Cpq 中接收到的 msg 数；</li><li>m′：在 Cpq 的状态记录前，Cpq 记录的 q 从 Cpq 中接收到的 msg 数；</li></ul><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p><strong><font color=red>Cpq 记录 <kbd>S(Cpq)</kbd> 时，必然会有 n = n’ ≥ m = m’；（注意这是充分必要条件喔~）</font></strong><br>即一个 Channel 要记录的状态是，它 sender 记录自己状态之前它所接收到的 msg 列表，再减去 receiver 记录自己状态之前它已经收到的 msg 列表，减去的之后的数据列表就是还在通道中的数据列表，这个列表是需要 Channel 作为状态记录下来的。<br>而如果 n′ = m′，那么 Channel c 中要记录的 msg 列表就是 empty 列表。如果 n′ &gt; m′，那么要记录的列表是 (m′+1),…n′ 号消息对应的 msg 列表。</p><h4 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h4><p>首先是 n = n’，利用反证法：如果 n != n’，则会有两种情况：</p><ol><li>n &gt; n’ 时：<ul><li>可能会出现 n = 10（p 记录状态前，p 记录 p 发往 Cpq msg 数为 10（msg 编号 1 - 10））；</li><li>n’ = 7（Cpq 记录状态前，Cpq 记录 p 发往 Cpq 的 msg 数为 7（msg 编号 1 - 7））；</li><li>那么假设 token 的编号为 9，就会出现 p 记录的状态为 <kbd>S(p-token-in-Cpq)</kbd>，Cpq 记录的状态为 <kbd>S(p-token-in-p)</kbd>，实际是不可能出现的；</li></ul></li><li>n &lt; n’ 时：<ul><li>可能会出现 n = 7（p 记录状态前，p 记录 p 发往 Cpq msg 数为 7（编号 1 - 7））；</li><li>n’ = 10（Cpq 记录状态前，Cpq 记录 p 发往 Cpq 的 msg 数为 10（编号 1 - 10））；</li><li>那么假设 token 的编号为 9，就会出现 p 记录的状态为 <kbd>S(p-token-in-p)</kbd>，Cpq 记录的状态为 <kbd>S(p-token-in-Cpq)</kbd>，实际是不可能出现的；</li></ul></li><li>n = n’ 时：保障了无论什么情况下，只要 p 做出 <kbd>S(p-token-in-p)</kbd> 的状态时，因为 n = n’，代表 p 没有把 token 发出去，Cpq 也没有接受到 token，就能让 Cpq 也做出 <kbd>S(Cpq-token-in-p)</kbd>；</li></ol><p>然后是 m = m’，同样利用反证法</p><ol><li>m &gt; m’ 时：<ul><li>可能会出现 n = n’ = m &gt; m’，q 记录状态前，Cpq 记录 q 从 Cpq 接收到的 msg 数为 10（编号 1 - 10，因为 n = n’ = m 也即 Cpq 记录的 p 发往 Cpq 的那些 msg）；</li><li>Cpq记录状态前，Cpq 记录的 q 从 Cpq 接收到的 msg 数为 7（编号 1 - 7）；</li><li>那么假设 token 的编号为 9，就会出现 Cpq 记录的状态为 <kbd>S(Cpq-token-in-Cpq)</kbd>，q 记录的状态为 <kbd>S(q-token-in-p)</kbd>，实际是不可能出现的；</li></ul></li></ol><p>最后是 n’ ≥ m’ and n ≥ m：在任何一种情况下，做全局一致性快照时，都会有 Cpq 下游接收到的 msg 数不可能超过 p 发送给 Channel 的 msg 条数，即：n’ ≥ m’ 以及 n ≥ m（也可使用反证法证明）</p><p><strong>分析到这里，上节提的两个问题也就被解决了。</strong></p><p><strong>为了帮大家更容易的理解全局一致性快照包含的内容，接下来我用伪代码描述一下，会比文字更好理解~</strong></p><h3 id="来段伪代码描述全局一致快照包含的内容"><a href="#来段伪代码描述全局一致快照包含的内容" class="headerlink" title="来段伪代码描述全局一致快照包含的内容"></a>来段伪代码描述全局一致快照包含的内容</h3><h4 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// S_all 即全局一致性快照</span></span><br><span class="line">S_all = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 假设总共有 x 个 process</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= x; i++) &#123;</span><br><span class="line"><span class="comment">// 第 i 个 process 的状态为 S_i，直接按照 += 写，勿喷</span></span><br><span class="line">  S_all += S_i;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 假设总共有 y 个 channel</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= y; i++) &#123;</span><br><span class="line">  <span class="comment">// 1.S_C_out_in_i：第 i 个 channel 的状态，in 代表第 i 个 channel 的输入，out 含义为第 i 个 channel 的输出</span></span><br><span class="line">  <span class="comment">// 2.m_out_in_i 和 n_out_in_i 其实就是上文中的 n 和 m</span></span><br><span class="line">  <span class="comment">// 2.m_out_in_i：第 i 个 channel 做快照前，发往 in process（下游）的消息个数</span></span><br><span class="line">  <span class="comment">// 3.n_out_in_i：第 i 个 channel 做快照前，接受到 out process（上游） 的消息个数</span></span><br><span class="line">  <span class="comment">// 4.需要注意，每一个 channel 的 m_out_in_i 和 n_out_in_i 都可能是不一样的，这里是伪代码所以直接按照下面的方式写了</span></span><br><span class="line">  S_C_out_in_i = Message[m_out_in_i + <span class="number">1</span>] + ... + Message[n_out_in_i];</span><br><span class="line">  </span><br><span class="line">  S_all += S_C_out_in_i;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 状态做完啦~</span></span><br></pre></td></tr></table></figure><h2 id="怎样去记录-S-Cpq-？"><a href="#怎样去记录-S-Cpq-？" class="headerlink" title="怎样去记录 S(Cpq)？"></a>怎样去记录 <kbd>S(Cpq)</kbd>？</h2><p>通过上面的分析，我们已经讨论得到了 <kbd>S(Cpq)</kbd> 都包含了什么内容，并且其之间要满足什么样的数学关系。但是在现实实际生活中，消息在 Channel 上乱飞时，我们是无法记录这些消息作为 Channel 的状态的。<br><strong><font color=red>但是这些消息终究会到达目的地，我们可以在消息的目的地去记录这些消息作为 Channel 的状态。即我们可以在 q 中记录 Channel pq 的 <kbd>S(Cpq)</kbd>，在 p 中记录 Channel pq 的 <kbd>S(Cqp)</kbd>。</font></strong></p><h3 id="伪代码-1"><a href="#伪代码-1" class="headerlink" title="伪代码"></a>伪代码</h3><p>顺便那么上面那段伪代码就可以简化为下面这样：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// S_all 即全局一致性快照</span></span><br><span class="line">S_all = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 假设总共有 x 个 process</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= x; i++) &#123;</span><br><span class="line">  <span class="comment">// S_i_all：第 i 个 process 要记录的状态</span></span><br><span class="line">  S_i_all = <span class="keyword">null</span>;</span><br><span class="line">  </span><br><span class="line"><span class="comment">// S_i：第 i 个 process 的状态</span></span><br><span class="line">  S_i_all += S_i; <span class="comment">// 【直接按照 += 写，勿喷】</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 第 i 个 process 总共有 y 个 input channel，即有 y 个上游 process，下文中 j 即指代第 j 个 channel，也代指 j channel 的上游 j process</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= y; j++) &#123;</span><br><span class="line">    <span class="comment">// 1.S_C_j_i：第 i 个 channel 的状态</span></span><br><span class="line">    <span class="comment">// 2.m_j_i 和 n_j_i 其实就是上文中的 n 和 m</span></span><br><span class="line">    <span class="comment">// 2.m_j_i：第 j 个 channel 做快照前，发往 i（下游）的消息个数</span></span><br><span class="line">    <span class="comment">// 3.n_j_i：第 j 个 channel 做快照前，接受到 j（上游） 的消息个数</span></span><br><span class="line">    <span class="comment">// 4.需要注意，每一个 channel 的 m_j_i 和 n_j_i 都可能是不一样的，这里是伪代码所以直接按照下面的方式写了</span></span><br><span class="line">    S_C_j_i = Message[m_j_i + <span class="number">1</span>] + ... + Message[n_j_i];</span><br><span class="line"></span><br><span class="line">    S_i_all += S_C_j_i;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  S_all += S_i_all;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 状态做完啦~</span></span><br></pre></td></tr></table></figure><h3 id="记录-S-Cpq-需要满足的条件"><a href="#记录-S-Cpq-需要满足的条件" class="headerlink" title="记录 S(Cpq) 需要满足的条件"></a>记录 <kbd>S(Cpq)</kbd> 需要满足的条件</h3><p><strong><font color=red>重点重点重点！！！</font></strong><br><strong><font color=red>分析上面的伪代码后，我们可以发现，要得到 S_all，其中只有一个变量在进程做快照时不知道的，那就是 n_j_i（即第 i 个 channel 做快照前，接受到 j（上游） 的消息个数），别忘了 n = n‘，即也可以定义为 j 做快照前，j 发往 channel 的消息个数。那么实际上这个值 j process 是知道的，就代表 i 进程需要知道 j 告诉他 n_j_i 的值是多少。重点来了，当 i process 做完快照之后，直接发一个 marker 下去，这个 marker 不会对计算有任何影响（即不会对状态产生任何影响），marker 只是一个标识，j process 做完自己的快照之后，直到接收到 marker 之间的消息就是Channel ij 的状态。i 就是通过 marker 来告诉 j process n_j_i 的值是多少的。（其他的变量为什么都知道就不详细分析了，很容易理解）</font></strong></p><h1 id="分布式应用全局一致性快照算法流程总结"><a href="#分布式应用全局一致性快照算法流程总结" class="headerlink" title="分布式应用全局一致性快照算法流程总结"></a>分布式应用全局一致性快照算法流程总结</h1><h2 id="算法流程总结"><a href="#算法流程总结" class="headerlink" title="算法流程总结"></a>算法流程总结</h2><ol><li><strong>发起快照</strong>：有一个 manager process（这个 manager 可以是所有 process 中的任意一个 process，也可以是一个单独的中央管理者）告诉所有的 process 说可以开始做状态了；</li><li><strong>执行快照</strong>：所有 process 就开始记录自己本地的状态（非所有 input channel）了，记录完本地状态，然后发 marker 给下游所有的 channel，然后开始记录上游所有 input channel 的消息（直到接收到上游所有的 marker）；</li><li><strong>执行快照</strong>：每个 process 对于每一个 input channel 来说，都将自己做完状态后直到收到 marker 之间的消息记录下来，作为这个 input channel 的状态；</li><li><strong>执行快照</strong>：当收到上游所有 marker 之后，这个 process 要记录的状态就全部得到了，然后告诉 manager process 说做完状态了；</li><li><strong>终止快照</strong>：manager process 接收到所有 process 做完的消息之后，就标记所有的状态以及完成了。</li></ol><h2 id="算法流程示例"><a href="#算法流程示例" class="headerlink" title="算法流程示例"></a>算法流程示例</h2><h3 id="发起快照"><a href="#发起快照" class="headerlink" title="发起快照"></a>发起快照</h3><ul><li>有一个 manager process（这个 manager 可以是所有 process 中的任意一个 process，也可以是一个单独的中央管理者）告诉所有的 process 说可以开始做状态了；<br><img src="/blog-img/apache-flink:state-1/41.png" alt="41"></li></ul><h3 id="执行快照"><a href="#执行快照" class="headerlink" title="执行快照"></a>执行快照</h3><ul><li>所有 process 就开始记录自己本地的状态（非所有 input channel）了，记录完本地状态，然后发 marker 给下游所有的 channel，然后开始记录上游所有 input channel 的消息（直到接收到上游所有的 marker）；<br><img src="/blog-img/apache-flink:state-1/42.png" alt="42"></li><li>每个 process 对于每一个 input channel 来说，都将自己做完状态后直到收到 marker 之间的消息记录下来，作为这个 input channel 的状态；<br><img src="/blog-img/apache-flink:state-1/43.png" alt="43"></li><li>当收到上游所有 marker 之后，这个 process 要记录的状态就全部得到了，然后告诉 manager process 说做完状态了；</li></ul><h3 id="终止快照"><a href="#终止快照" class="headerlink" title="终止快照"></a>终止快照</h3><ul><li>manager process 接收到所有 process 做完的消息之后，就标记所有的状态以及完成了。<br><img src="/blog-img/apache-flink:state-1/44.png" alt="44"></li></ul><h1 id="Chandy-Lamport-算法流程、示例"><a href="#Chandy-Lamport-算法流程、示例" class="headerlink" title="Chandy-Lamport 算法流程、示例"></a>Chandy-Lamport 算法流程、示例</h1><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p><img src="/blog-img/apache-flink:state-1/16.jpeg" alt="16"></p><h3 id="发起快照-1"><a href="#发起快照-1" class="headerlink" title="发起快照"></a>发起快照</h3><p><img src="/blog-img/apache-flink:state-1/17.png" alt="17"><br>解读：</p><ul><li>本次快照的起始点，先把起始点的快照给做了，然后发出 marker（<strong>这个 marker 消息是干啥用的呢？没错，就是我们之前分析的结论</strong>），开始记录 input channel</li></ul><h3 id="执行快照-1"><a href="#执行快照-1" class="headerlink" title="执行快照"></a>执行快照</h3><p><img src="/blog-img/apache-flink:state-1/18.jpeg" alt="18"><br><img src="/blog-img/apache-flink:state-1/19.jpeg" alt="19"><br>解读：</p><ul><li>Pi 记录本地快照，标记 Cki 为空：因为从 Cki 接收到了 marker，这时的状态是 Pk 刚刚做完快照，Pk 做完快照发往 Cki 的消息个数 = Pi 做完快照从 Cki 接收到的消息个数。即 n = n’ = m’ = m；即 Cki = [Empty]；</li><li>Pi 开始向所有 output channel 发 marker，开始记录除 Cki 之外的 input channel 消息，因为本地快照已经做完了；然后上游还有部分进程没有做完快照，为了记录除 Cki 之外的 input Channel 消息，</li></ul><p>解读：</p><ul><li>结合前一张图说的开始记录 input channel 消息，Pi 停止记录 Cki 的消息，同时将此前记录所有 Cki 收到的消息作为本次快照中的最终状态；n’ &gt; m’，在 Pi 这里记录了 Cki 的状态，即 Cki = [m‘ + 1, m’ + 2…n]</li></ul><h3 id="终止快照-1"><a href="#终止快照-1" class="headerlink" title="终止快照"></a>终止快照</h3><p><img src="/blog-img/apache-flink:state-1/20.jpeg" alt="20"></p><h2 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h2><p><img src="/blog-img/apache-flink:state-1/21.png" alt="21"><br><img src="/blog-img/apache-flink:state-1/22.png" alt="22"><br><img src="/blog-img/apache-flink:state-1/23.png" alt="23"><br><img src="/blog-img/apache-flink:state-1/24.png" alt="24"><br><img src="/blog-img/apache-flink:state-1/25.png" alt="25"><br><img src="/blog-img/apache-flink:state-1/26.png" alt="26"><br><img src="/blog-img/apache-flink:state-1/27.png" alt="27"><br><img src="/blog-img/apache-flink:state-1/28.png" alt="28"><br><img src="/blog-img/apache-flink:state-1/29.png" alt="29"></p><h2 id="Chandy-Lamport-与上节分布式应用全局一致性快照算法的异同"><a href="#Chandy-Lamport-与上节分布式应用全局一致性快照算法的异同" class="headerlink" title="Chandy-Lamport 与上节分布式应用全局一致性快照算法的异同"></a>Chandy-Lamport 与上节<a href="#分布式应用全局一致性快照算法流程总结">分布式应用全局一致性快照算法</a>的异同</h2><p><img src="/blog-img/apache-flink:state-1/47.png" alt="47"></p><p>Chandy-Lamport 就是上节<a href="#分布式应用全局一致性快照算法流程总结">分布式应用全局一致性快照算法</a>的其中一种特殊形式；<a href="#分布式应用全局一致性快照算法流程总结">分布式应用全局一致性快照算法</a>中说的是每个 process 在接收到 manager 做快照的消息之后就直接可以开始记录状态了，而 Chandy-Lamport 其实就是把这个 manager 的消息用接收到的第一个 marker 消息给代替了，用数学表达式表示就是接收到第一个 marker 的 channel 的 n = n’ = m = m’，剩余的 channel 满足 n = n’ ≥ m = m’，并且 Chandy-Lamport 算法也都满足第 8 章节介绍的各种条件。</p><h1 id="flink-实现的全局一致性快照介绍（flink-容错机制）"><a href="#flink-实现的全局一致性快照介绍（flink-容错机制）" class="headerlink" title="flink 实现的全局一致性快照介绍（flink 容错机制）"></a>flink 实现的全局一致性快照介绍（flink 容错机制）</h1><h2 id="Chandy-Lamport-与-Flink之间的关系"><a href="#Chandy-Lamport-与-Flink之间的关系" class="headerlink" title="Chandy-Lamport 与 Flink之间的关系"></a>Chandy-Lamport 与 Flink之间的关系</h2><p><a href="https://arxiv.org/pdf/1506.08603.pdf">flink 全局一致性快照论文</a></p><p>Flink 是分布式系统，所以 Flink 会采用全局一致性快照的方式形成检查点，来支持故障恢复。Flink 的异步全局一致性快照算法跟 Chandy-Lamport 算法的区别主要有以下几点：</p><ul><li>第一，Chandy-Lamport 支持强连通图，而 Flink支持弱连通图；</li><li>第二，Flink采用的是裁剪的（Tailored）Chandy-Lamport 异步快照算法；</li><li>第三，Flink的异步快照算法在DAG场景下不需要存储 Channel state，从而极大减少快照的存储空间。</li></ul><h2 id="flink-的容错机制"><a href="#flink-的容错机制" class="headerlink" title="flink 的容错机制"></a>flink 的容错机制</h2><p><img src="/blog-img/apache-flink:state-1/30.png" alt="30"></p><h2 id="端到端的Exactly-once"><a href="#端到端的Exactly-once" class="headerlink" title="端到端的Exactly once"></a>端到端的Exactly once</h2><p>Exactly once 的意思是，作业结果总是正确的，但是很可能产出多次；所以它的要求是需要有可重放的 source。<br>端到端的 Exactly once，是指作业结果正确且只会被产出一次，它的要求除了有可重放的 source 外，还要求有事务型的 sink 和可以接收幂等的产出结果。</p><h2 id="flink-的全局一致性快照"><a href="#flink-的全局一致性快照" class="headerlink" title="flink 的全局一致性快照"></a>flink 的全局一致性快照</h2><p><img src="/blog-img/apache-flink:state-1/31.png" alt="31"><br><img src="/blog-img/apache-flink:state-1/32.png" alt="32"><br><img src="/blog-img/apache-flink:state-1/33.png" alt="33"></p><h2 id="Barrier-对齐"><a href="#Barrier-对齐" class="headerlink" title="Barrier 对齐"></a>Barrier 对齐</h2><p><img src="/blog-img/apache-flink:state-1/34.png" alt="34"><br><img src="/blog-img/apache-flink:state-1/35.png" alt="35"><br><img src="/blog-img/apache-flink:state-1/36.png" alt="36"></p><h2 id="flink-的全局一致性快照与上节分布式应用全局一致性快照算法的异同"><a href="#flink-的全局一致性快照与上节分布式应用全局一致性快照算法的异同" class="headerlink" title="flink 的全局一致性快照与上节分布式应用全局一致性快照算法的异同"></a>flink 的全局一致性快照与上节<a href="#分布式应用全局一致性快照算法流程总结">分布式应用全局一致性快照算法</a>的异同</h2><p><img src="/blog-img/apache-flink:state-1/46.png" alt="46"></p><p>flink 的全局一致性快照就是上节<a href="#分布式应用全局一致性快照算法流程总结">分布式应用全局一致性快照算法</a>的其中一种特殊形式；<a href="#分布式应用全局一致性快照算法流程总结">分布式应用全局一致性快照算法</a>中说的是每个 process 在接收到 manager 做快照的消息之后就直接可以开始记录状态了，而 flink 其实就是将各个 process 开始做状态的时间点设为了接收到上游 input channel 所有的 barrier，这样一个好处就是由于各个 process 是接收到了上游所有 barrier 之后开始的，用数学表达式表示其实就满足了 n = n’ = m = m’，就没有必要存储 channel 中的状态了；并且 flink 算法也都满足第 8 章节介绍的各种条件。</p><h2 id="状态后端"><a href="#状态后端" class="headerlink" title="状态后端"></a>状态后端</h2><h3 id="JVM-Heap"><a href="#JVM-Heap" class="headerlink" title="JVM Heap"></a>JVM Heap</h3><p><img src="/blog-img/apache-flink:state-1/37.png" alt="37"><br>第一种，JVM Heap，它里面的数据是以Java对象形式存在的，读写也是以对象形式去完成的，所以速度很快。但是也存在两个弊端：第一个弊端，以对象方式存储所需的空间是磁盘上序列化压缩后的数据大小的很多倍，所以占用的内存空间很大；第二个弊端，虽然读写不用做序列化，但是在形成 snapshot 时需要做序列化，所以它的异步 snapshot 过程会比较慢。</p><h3 id="RocksDB"><a href="#RocksDB" class="headerlink" title="RocksDB"></a>RocksDB</h3><p><img src="/blog-img/apache-flink:state-1/38.png" alt="38"><br>第二种，RocksDB，这个类型在读写时就需要做序列化，所以它读写的速度比较慢。但是它有一个好处，基于LSM的数据结构在快照之后会形成 sst 文件，它的异步 checkpoint 过程就是文件拷贝的过程，CPU 消耗会比较低。</p><h1 id="分布式应用全局一致性快照算法、Chandy-Lamport-算法、flink-全局一致性快照之间的关系"><a href="#分布式应用全局一致性快照算法、Chandy-Lamport-算法、flink-全局一致性快照之间的关系" class="headerlink" title="分布式应用全局一致性快照算法、Chandy-Lamport 算法、flink 全局一致性快照之间的关系"></a><a href="#分布式应用全局一致性快照算法流程总结">分布式应用全局一致性快照算法</a>、Chandy-Lamport 算法、flink 全局一致性快照之间的关系</h1><br><p><img src="/blog-img/apache-flink:state-1/45.png" alt="45"></p><h1 id="参考的文章"><a href="#参考的文章" class="headerlink" title="参考的文章"></a>参考的文章</h1><ul><li><a href="https://developer.aliyun.com/article/667562">https://developer.aliyun.com/article/667562</a></li><li><a href="https://matt33.com/2019/10/27/paper-chandy-lamport/">https://matt33.com/2019/10/27/paper-chandy-lamport/</a></li><li><a href="https://developer.aliyun.com/article/448900">https://developer.aliyun.com/article/448900</a></li></ul>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，帮助大家理解全局一致性快照。可能很多小伙伴都知道 flink 是使用 barrier 来做全局一致性快照，但是我提两个问题，为什么 flink 的 barrier 能够保证全局一致性快照的正确性？barrier 到底发挥了怎样的作用？小伙伴们能回答上来么，有想过背后的原因嘛，楼主通过本篇文章抛砖引玉，希望小伙伴们能够喜欢~
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>深入浅出 | flink 全局一致性快照（二）</title>
    <link href="https://yangyichao-mango.github.io/2021/03/12/wechat-blog/apache-flink:state-2/"/>
    <id>https://yangyichao-mango.github.io/2021/03/12/wechat-blog/apache-flink:state-2/</id>
    <published>2021-03-12T06:21:53.000Z</published>
    <updated>2021-04-05T06:21:14.663Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><ol><li><p>目录<br>先带大家串一遍本篇文章的思路</p></li><li><p>什么是状态？<br>发散思维的去思考状态，我们所理解的状态不仅仅只限于 flink 的状态，让大家了解到状态是一个无处不在的东西</p></li><li><p>什么是全局一致性快照？和状态有什么关系？<br>全局一致性快照的一些生活、工作中应用的例子</p></li><li><p>为什么需要一致性快照？全局一致性快照和 flink 的关系？<br>jvm GC，分布式应用做故障恢复（比如 flink），死锁检测等</p></li><li><p>全局一致性快照的分布式应用举例<br>通过一个简单分布式应用介绍一下全局一致性状态是每时每刻都存在的。时间轴上的每一个时刻都存在一个全局一致性快照（类似拍照片）。flink 做 cp，sp，类似于每隔固定的时间从时间轴上的一个点拿出来这个时间点对应的一个全局一致性状态</p></li><li><p>全局一致性快照的标准定义<br>假如说有两个事件，a 和 b，在绝对时间下，如果 a 发生在 b 之前，且 b 被包含在快照当中，那么则 a 事件或者其对快照产生的影响也被包含在快照当中</p></li><li><p>怎么实现全局一致性快照？<br>同步去做，包括时钟同步、Stop-the-world，但是这两种方法都不可接受；<br>既然同步无法做，那如果异步能做出相同的全局一致性状态也可以</p></li><li><p>分布式应用的全局一致性快照其 Process 状态和 Channel 状态到底需要记录什么？怎么记录 Channel 的状态？<br>不是必须要在同一时刻嘛，为啥还能异步去做？只要异步做出来的状态和同步做出来的状态效果一致也可以。并且详细分析了 process 和 channel 中的状态包括什么，以及记录 channel 状态的方法。</p></li><li><p>Chandy-Lamport 算法流程、例子<br>介绍 Chandy-Lamport 算法流程并以一个例子介绍其执行过程</p></li><li><p>flink 实现的全局一致性快照介绍<br>大致介绍 flink 的全局一致性快照</p></li><li><p>参考文章<br>本文编写过程中参考的文章</p></li></ol><h1 id="什么是状态？（了解状态）"><a href="#什么是状态？（了解状态）" class="headerlink" title="什么是状态？（了解状态）"></a>什么是状态？（了解状态）</h1>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，深度解析 flink 中的全局一致性快照流程以及具体实现，抛砖引玉，提高小伙伴的姿♂势水平。阅读时长大概 20 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>深入浅出 | flink 全局一致性快照（一）</title>
    <link href="https://yangyichao-mango.github.io/2021/03/12/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_apache-flink:state-1/"/>
    <id>https://yangyichao-mango.github.io/2021/03/12/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_apache-flink:state-1/</id>
    <published>2021-03-12T06:21:53.000Z</published>
    <updated>2021-07-10T15:41:09.398Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列每篇文章都是从实际生产出发，帮助大家理解全局一致性快照。可能很多小伙伴都知道 flink 是使用 barrier 来做全局一致性快照，但是我提两个问题，为什么 flink 的 barrier 能够保证全局一致性快照的正确性？barrier 到底发挥了怎样的作用？小伙伴们能回答上来么，有想过背后的原因嘛，楼主通过本篇文章抛砖引玉，希望小伙伴们能够喜欢~</p></blockquote><h1 id="通过本文你可以-get-到"><a href="#通过本文你可以-get-到" class="headerlink" title="通过本文你可以 get 到"></a>通过本文你可以 get 到</h1><ol><li>通过本文你可以 get 到</li></ol><p>先带大家串一遍本篇文章的思路</p><ol start="2"><li>什么是状态？</li></ol><p>发散思维的去思考状态，我们所理解的状态不仅仅只限于 flink 的状态，让大家了解到状态是一个无处不在的东西</p><ol start="3"><li>什么是全局一致性快照？和状态有什么关系？</li></ol><p>全局一致性快照的一些生活、工作中应用的例子</p><ol start="4"><li>为什么需要一致性快照？全局一致性快照和 flink 的关系？</li></ol><p>jvm GC，分布式应用做故障恢复（比如 flink），死锁检测等</p><ol start="5"><li>全局一致性快照的分布式应用举例</li></ol><p>通过一个简单分布式应用介绍一下全局一致性状态是每时每刻都存在的。时间轴上的每一个时刻都存在一个全局一致性快照（类似拍照片）。flink 做 cp，sp，类似于每隔固定的时间从时间轴上的一个点拿出来这个时间点对应的一个全局一致性状态</p><ol start="6"><li>全局一致性快照的标准定义</li></ol><p>假如说有两个事件，a 和 b，在绝对时间下，如果 a 发生在 b 之前，且 b 被包含在快照当中，那么则 a 事件或者其对快照产生的影响也被包含在快照当中</p><ol start="7"><li>怎么实现全局一致性快照？</li></ol><p>同步去做，包括时钟同步、Stop-the-world，但是这两种方法都不可接受；<br>既然同步无法做，那如果异步能做出相同的全局一致性状态也可以</p><ol start="8"><li>分布式应用的全局一致性快照其 Process 状态和 Channel 状态到底需要记录什么？怎么记录 Channel 的状态？</li></ol><p>不是必须要在同一时刻嘛，为啥还能异步去做？只要异步做出来的状态和同步做出来的状态效果一致也可以。并且详细分析了 process 和 channel 中的状态包括什么，以及记录 channel 状态的方法。</p><ol start="9"><li>分布式应用全局一致性快照算法流程总结</li></ol><p>通过第 8 章节的分析，总结出一套分布式应用的通用异步全局一致性快照算法</p><ol start="10"><li>Chandy-Lamport 算法流程、例子</li></ol><p>介绍 Chandy-Lamport 算法流程并以一个例子介绍其执行过程，并且说明和第 9 章节分析得出的方法之间的关系，两者之间互相是不冲突的</p><ol start="11"><li>flink 实现的全局一致性快照介绍</li></ol><p>大致介绍 flink 的全局一致性快照，并且也说明了和第 9 章节得出的方法之间的关系，两者之间也是不冲突的</p><ol start="12"><li>分布式应用异步全局一致性快照方法、Chandy-Lamport 算法、flink 全局一致性快照之间的关系</li></ol><p>Chandy-Lamport 算法、flink 全局一致性快照也满足第 8 章节的结论；并且 Chandy-Lamport 算法、flink 全局一致性快照是分布式应用异步全局一致性快照方法一种特殊形式</p><ol start="13"><li>参考文章</li></ol><p>本文编写过程中参考的文章</p><h1 id="什么是状态？（了解状态）"><a href="#什么是状态？（了解状态）" class="headerlink" title="什么是状态？（了解状态）"></a>什么是状态？（了解状态）</h1><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>首先想让大家发散思维的去思考状态？我们所理解的状态不仅仅只限于 flink 的状态。让大家了解到状态是一个普遍存在的东西</p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>就是当前计算需要依赖到之前计算的结果，那么之前计算的结果就是状态</p><h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><ol><li><p>比如生活中的例子：为什么我知道我的面前放着一台电脑，因为眼睛接收到外界的图案，然后我的大脑接收到这个图案后，拿记忆中存储的图案进行对比，匹配得到这是电脑，那么记忆中存储的图案就是状态；还有比如日久生情，为什么感情会越来越深，因为今天的感情 = 今天积累的感情 + 以前积累的感情，以前积累的感情就是状态。这其中都存在状态</p></li><li><p>比如 web server 应用中的状态：打开 github 页面，列表展示了我的归属仓库。其中就是 web client 发了查询我的归属仓库请求，web server 接收到请求之后，然后去存储引擎中进行查询匹配返回。那么存储引擎中存储的内容就是状态<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/1.png" alt="1"></p></li><li><p>比如 flink 应用中的状态：要做当天 uid 去重，就要存储所有的 uid；要获取当前最大值，那么历史最大值就是状态<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/2.jpeg" alt="2"></p></li></ol><h1 id="什么是全局一致性快照？（了解全局一致性快照）"><a href="#什么是全局一致性快照？（了解全局一致性快照）" class="headerlink" title="什么是全局一致性快照？（了解全局一致性快照）"></a>什么是全局一致性快照？（了解全局一致性快照）</h1><ul><li><strong>全局：代表是一个分布式的</strong></li><li><strong>一致性快照：代表绝对时间的同一时刻的状态</strong></li><li><strong>相当于打开上帝视角，去观察同一时刻的应用所有的状态；这里的快照 = 状态，文章之后我可能会把这两个词混用，大家明白他们的意思一致即可</strong></li></ul><ol><li>比如生活中的例子：比如拍了一个照片，那么照片的内容就是当时的一个全局一致性快照；每一个首脑都是一个进程，所有的进程的状态在同一时刻的组合就是一个全局一致性快照</li></ol><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/3.png" alt="3"></p><ol start="2"><li>比如分布式应用的例子：首先是一个分布式应用，它有多个进程分布在多个服务器上；其次，它在应用内部有自己的处理逻辑和状态；第三，应用间是可以互相通信的；第四，在这种分布式的应用，有内部状态，硬件可以通信的情况下；某一时刻的全局状态，就叫做全局的快照。</li></ol><p><strong>分布式应用某一时刻的全局一致性快照 = 各个 process 的本地状态 + channel 中正在传递的消息</strong></p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/4.png" alt="4"></p><p>介绍完了几个例子之后，我们来看看我们为什么需要一致性快照？</p><h1 id="为什么需要一致性快照？全局一致性快照和-flink-的关系？"><a href="#为什么需要一致性快照？全局一致性快照和-flink-的关系？" class="headerlink" title="为什么需要一致性快照？全局一致性快照和 flink 的关系？"></a>为什么需要一致性快照？全局一致性快照和 flink 的关系？</h1><br><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/5.png" alt="5"></p><h2 id="实时案例"><a href="#实时案例" class="headerlink" title="实时案例"></a>实时案例</h2><ol><li>做检查点（全局一致性快照）用来故障恢复，<strong><font color=red>重点！！！重点！！！重点！！！就在于我们不必要从历史起点开始重跑所有的数据（其实这就是我们需要检查点的目的！！！）</font></strong>；因为<ul><li>流式应用的上游存储介质一般都不支持存储历史所有数据（比如上游为 kafka，kafka 不可能存储历史所有数据）</li><li>重跑时效性不能满足时效性要求（重跑历史数据的情况下，时效性是达不到要求的）</li></ul></li></ol><ol start="2"><li>可以做任务的死锁检测</li></ol><h2 id="全局一致性快照和-flink-的关系"><a href="#全局一致性快照和-flink-的关系" class="headerlink" title="全局一致性快照和 flink 的关系"></a>全局一致性快照和 flink 的关系</h2><p>flink 的 cp 和 sp 实际上就是全局一致性快照在分布式应用中 flink 的一个具体实现。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/6.png" alt="6"></p><h1 id="全局一致性快照的分布式应用案例？"><a href="#全局一致性快照的分布式应用案例？" class="headerlink" title="全局一致性快照的分布式应用案例？"></a>全局一致性快照的分布式应用案例？</h1><p>通过一个简单分布式应用介绍一下全局一致性状态是每时每刻都存在的。时间轴上的每一个时刻都存在一个全局一致性快照（可以用拍照片去类比）。</p><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>下面分布式应用的一个示例：</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/7.jpeg" alt="7"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/8.jpeg" alt="8"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/9.jpeg" alt="9"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/10.jpeg" alt="10"></p><h2 id="每时每刻都存在全局一致性快照"><a href="#每时每刻都存在全局一致性快照" class="headerlink" title="每时每刻都存在全局一致性快照"></a>每时每刻都存在全局一致性快照</h2><p>上面这个只是四个时刻的四个快照，其实应用的每一个时刻都存在一个全局一致性快照。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/11.png" alt="11"></p><h1 id="全局一致性快照的标准定义"><a href="#全局一致性快照的标准定义" class="headerlink" title="全局一致性快照的标准定义"></a>全局一致性快照的标准定义</h1><h2 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h2><p>假如说有两个事件，a 和 b，在绝对时间下，如果 a 发生在 b 之前，且 b 被包含在快照当中，那么则 a 也被包含在快照当中。满足这个条件的全局快照，就称为全局一致性快照。</p><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/12.png" alt="12"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/13.png" alt="13"></p><h2 id="楼主理解"><a href="#楼主理解" class="headerlink" title="楼主理解"></a>楼主理解</h2><p>就是如果将做了绝对时刻 T 的一个快照，那么这个绝对时刻 T 之前发生的所有事件以及其影响都会被包含在这个快照中</p><p>上文已经介绍了全局一致性快照的定义以及为什么我们需要全局一致性快照，那我们来放眼实际应用中，怎么才能做出一个满足生产实际要求的全局一致性快照呢？</p><h1 id="怎么实现全局一致性快照？"><a href="#怎么实现全局一致性快照？" class="headerlink" title="怎么实现全局一致性快照？"></a>怎么实现全局一致性快照？</h1><br>实现方式主要分为同步实现方式和异步实现方式两类。<h2 id="同步实现方式"><a href="#同步实现方式" class="headerlink" title="同步实现方式"></a>同步实现方式</h2><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/14.jpeg" alt="14"></p><ul><li><a href="https://baike.baidu.com/item/NTP%E6%9C%8D%E5%8A%A1%E5%99%A8/8633994?fr=aladdin。" title="什么是 NTP">NTP</a>: NTP服务器[Network Time Protocol（NTP）]是用来使计算机时间同步化的一种协议，它可以使计算机对其服务器或时钟源（如石英钟，GPS等等)做同步化，它可以提供高精准度的时间校正（LAN上与标准间差小于1毫秒，WAN上几十毫秒）<br>结论：无法实现</li><li><a href="https://www.jianshu.com/p/b210f9db19a3。" title="Stop-The-World 说明">Stop-The-World</a></li></ul><p><strong>结论：不满足需求，无法采用</strong></p><p><strong><font color=red>如果同步实现方式不满足需求，那么能使用异步方式做到同步相同的快照也是可以满足需求的。</font></strong></p><h2 id="异步实现方式"><a href="#异步实现方式" class="headerlink" title="异步实现方式"></a>异步实现方式</h2><ul><li><a href="https://www.microsoft.com/en-us/research/uploads/prod/2016/12/Determining-Global-States-of-a-Distributed-System.pdf?ranMID=24542&ranEAID=J84DHJLQkR4&ranSiteID=J84DHJLQkR4-mVoVymFnAblBx3zwyf98Pw&epi=J84DHJLQkR4-mVoVymFnAblBx3zwyf98Pw&irgwc=1&OCID=AID2000142_aff_7593_1243925&tduid=%28ir__1hs2uuow6wkfq3oxkk0sohzzwm2xpc33lxd0o6g200%29%287593%29%281243925%29%28J84DHJLQkR4-mVoVymFnAblBx3zwyf98Pw%29%28%29&irclickid=_1hs2uuow6wkfq3oxkk0sohzzwm2xpc33lxd0o6g200。" title="Chandy-Lamport 论文链接">Chandy-Lamport</a></li></ul><p><strong><font color=red>在介绍 Chandy-Lamport 算法之前，我们先介绍一些理论和数学上的概念铺垫铺垫，帮助我们理解 Chandy-Lamport 算法。</font></strong><br><strong><font color=red>这些概念主要就是介绍分布式应用的 Process 和 Channel 应该存储什么内容？以及怎样去存储这些内容？只有我们知道了要存储什么东西才好去设计和介绍具体算法嘛~</font></strong><br><strong>楼主也是看了很多博客才总结得到了这些条件！！！</strong></p><h1 id="分布式应用的全局一致性快照其-Process-状态和-Channel-状态记录了什么？怎么记录-Channel-的状态？"><a href="#分布式应用的全局一致性快照其-Process-状态和-Channel-状态记录了什么？怎么记录-Channel-的状态？" class="headerlink" title="分布式应用的全局一致性快照其 Process 状态和 Channel 状态记录了什么？怎么记录 Channel 的状态？"></a>分布式应用的全局一致性快照其 Process 状态和 Channel 状态记录了什么？怎么记录 Channel 的状态？</h1><h2 id="分布式应用要记录的状态"><a href="#分布式应用要记录的状态" class="headerlink" title="分布式应用要记录的状态"></a>分布式应用要记录的状态</h2><p>如下图案例 <strong>Single-Token conservation</strong>，是一个分布式应用，有 p 和 q 两个进程，p 可以通过 Channel pq（记为 Cpq） 向 q 发消息，q 可以通过 Channel qp（记为 Cqp） 向 p 发消息，其中有一个叫 token 的消息，在这个系统中一直不停的流转。<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/15.png" alt="15"></p><p>如之前所述，分布式应用的全局一致性快照包含 Process 状态和 Channel 状态<br>那么上图 <strong>Single-Token conservation</strong> 示例中的全局一致性快照 <strong><kbd>S = S(p) + S(Cpq) + S(q) + S(Cqp)</kbd></strong></p><p>其中：</p><ul><li><kbd>S</kbd>：全局一致性快照</li><li><kbd>S(p)</kbd>：p 进程的状态</li><li><kbd>S(Cpq)</kbd>：p 进程到 q 进程的 Channel 状态</li><li><kbd>S(q)</kbd>：q 进程的状态</li><li><kbd>S(Cqp)</kbd>：q 进程到 p 进程的 Channel 状态</li></ul><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/15.png" alt="15"></p><p><strong><font color=red>问题：</font></strong><br><strong><font color=red>这里就碰到了我们要分析的关键问题：做全局一致性快照时，其中 <kbd>S(p)</kbd>，<kbd>S(q)</kbd> 好理解，但是 <kbd>S(Cpq)</kbd>，<kbd>S(Cqp)</kbd> 到底记录了什么东西？应该记录什么东西？接下来详细讲讲我的理解</font></strong></p><h2 id="Process-状态应该记录什么内容？"><a href="#Process-状态应该记录什么内容？" class="headerlink" title="Process 状态应该记录什么内容？"></a>Process 状态应该记录什么内容？</h2><p>记录和用户业务需求相关的状态内容，用到了关于状态的地方，进行记录就好了，这部分是好理解的。<br>举例：uid 去重就存储历史所有的 uid 就可以了</p><h2 id="Channel-状态应该记录什么内容？"><a href="#Channel-状态应该记录什么内容？" class="headerlink" title="Channel 状态应该记录什么内容？"></a>Channel 状态应该记录什么内容？</h2><h3 id="Single-Token-conservation-的全局一致性快照"><a href="#Single-Token-conservation-的全局一致性快照" class="headerlink" title="Single-Token conservation 的全局一致性快照"></a>Single-Token conservation 的全局一致性快照</h3><p>token 在 p 时（对应第一张图），这时的全局一致性快照为：<br><kbd>S(token-in-p) = S(p-token-in-p) + S(Cpq-token-in-p) + S(q-token-in-p) + S(Cqp-token-in-p) </kbd></p><p>其中：</p><ul><li><kbd>S(token-in-p)</kbd>：token 在 p 时，做的全局一致性快照</li><li><kbd>S(p-token-in-p)</kbd>：token 在 p 时，p 进程的状态</li><li><kbd>S(Cpq-token-in-p)</kbd>：token 在 p 时，p 进程到 q 进程的 Channel 状态</li><li><kbd>S(q-token-in-p)</kbd>：token 在 p 时，q 进程的状态</li><li><kbd>S(Cqp-token-in-p)</kbd>：token 在 p 时，q 进程到 p 进程的 Channel 状态</li></ul><h3 id="提出问题"><a href="#提出问题" class="headerlink" title="提出问题"></a>提出问题</h3><p>注意，上述这个表达式其实是结论，这个结论是很好理解的，但是你有想过站在实际应用的角度去思考下面的问题吗？</p><ol><li><p>问题1：为什么第一张图的全局一致性状态是 Process 和 Channel 做的快照都有 token-in-p 呢？<br>根据之前的拍照片的类比，当前这个绝对时刻做快照时，token 在 p；那么所有的 process 和 channel 记录状态时，token 都应该在 p。</p></li><li><p>问题2：S(p-token-in-p) 好理解，在这个时刻 token 还没有从 p 发出去，p 做快照时肯定知道 token 还在 p；但是站在 Cpq 做状态的角度来说：</p></li></ol><ul><li>Cpq 做状态时，怎么保障 Cpq 知道 token in p？需要我们探索下有什么方法怎么让 Cpq 在做状态时知道 token in p？</li><li>站在实际在应用中实现的角度时，满足怎样的数学条件（我们要开始实现一个真实的全局一致性快照啦，肯定会涉及到一些数据知识，别急，往后看，用到的数学知识并不复杂）才能做出一个 S(Cpq) ？</li></ul><h3 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h3><h4 id="分析-S-Cpq-记录了什么内容？"><a href="#分析-S-Cpq-记录了什么内容？" class="headerlink" title="分析 S(Cpq) 记录了什么内容？"></a>分析 <kbd>S(Cpq)</kbd> 记录了什么内容？</h4><p>这里我们简单先理解下，<kbd>S(Cpq)</kbd> 其实就是在 <kbd>S(p)</kbd> 和 <kbd>S(q)</kbd> 自己的状态做成时，还在 Channel pq 之间发送的那些消息。</p><p>那么我们怎么用数学的方式理解 Cpq 记录的这些消息以及 Process 和 Channel 做状态时需要满足的条件呢？让我们往下看</p><h4 id="变量定义"><a href="#变量定义" class="headerlink" title="变量定义"></a>变量定义</h4><ul><li>n：在 p 的状态记录前，p 记录的 p 发往 Cpq 的 msg 数；</li><li>n′：在 Cpq 的状态记录前，Cpq 记录的 p 发往 Cpq 的 msg 数；</li><li>m：在 q 的状态记录前，q 记录的 q 从 Cpq 中接收到的 msg 数；</li><li>m′：在 Cpq 的状态记录前，Cpq 记录的 q 从 Cpq 中接收到的 msg 数；</li></ul><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p><strong><font color=red>Cpq 记录 <kbd>S(Cpq)</kbd> 时，必然会有 n = n’ ≥ m = m’；（注意这是充分必要条件喔~）</font></strong><br>即一个 Channel 要记录的状态是，它 sender 记录自己状态之前它所接收到的 msg 列表，再减去 receiver 记录自己状态之前它已经收到的 msg 列表，减去的之后的数据列表就是还在通道中的数据列表，这个列表是需要 Channel 作为状态记录下来的。<br>而如果 n′ = m′，那么 Channel c 中要记录的 msg 列表就是 empty 列表。如果 n′ &gt; m′，那么要记录的列表是 (m′+1),…n′ 号消息对应的 msg 列表。</p><h4 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h4><p>首先是 n = n’，利用反证法：如果 n != n’，则会有两种情况：</p><ol><li>n &gt; n’ 时：<ul><li>可能会出现 n = 10（p 记录状态前，p 记录 p 发往 Cpq msg 数为 10（msg 编号 1 - 10））；</li><li>n’ = 7（Cpq 记录状态前，Cpq 记录 p 发往 Cpq 的 msg 数为 7（msg 编号 1 - 7））；</li><li>那么假设 token 的编号为 9，就会出现 p 记录的状态为 <kbd>S(p-token-in-Cpq)</kbd>，Cpq 记录的状态为 <kbd>S(p-token-in-p)</kbd>，实际是不可能出现的；</li></ul></li><li>n &lt; n’ 时：<ul><li>可能会出现 n = 7（p 记录状态前，p 记录 p 发往 Cpq msg 数为 7（编号 1 - 7））；</li><li>n’ = 10（Cpq 记录状态前，Cpq 记录 p 发往 Cpq 的 msg 数为 10（编号 1 - 10））；</li><li>那么假设 token 的编号为 9，就会出现 p 记录的状态为 <kbd>S(p-token-in-p)</kbd>，Cpq 记录的状态为 <kbd>S(p-token-in-Cpq)</kbd>，实际是不可能出现的；</li></ul></li><li>n = n’ 时：保障了无论什么情况下，只要 p 做出 <kbd>S(p-token-in-p)</kbd> 的状态时，因为 n = n’，代表 p 没有把 token 发出去，Cpq 也没有接受到 token，就能让 Cpq 也做出 <kbd>S(Cpq-token-in-p)</kbd>；</li></ol><p>然后是 m = m’，同样利用反证法</p><ol><li>m &gt; m’ 时：<ul><li>可能会出现 n = n’ = m &gt; m’，q 记录状态前，Cpq 记录 q 从 Cpq 接收到的 msg 数为 10（编号 1 - 10，因为 n = n’ = m 也即 Cpq 记录的 p 发往 Cpq 的那些 msg）；</li><li>Cpq记录状态前，Cpq 记录的 q 从 Cpq 接收到的 msg 数为 7（编号 1 - 7）；</li><li>那么假设 token 的编号为 9，就会出现 Cpq 记录的状态为 <kbd>S(Cpq-token-in-Cpq)</kbd>，q 记录的状态为 <kbd>S(q-token-in-p)</kbd>，实际是不可能出现的；</li></ul></li></ol><p>最后是 n’ ≥ m’ and n ≥ m：在任何一种情况下，做全局一致性快照时，都会有 Cpq 下游接收到的 msg 数不可能超过 p 发送给 Channel 的 msg 条数，即：n’ ≥ m’ 以及 n ≥ m（也可使用反证法证明）</p><p><strong>分析到这里，上节提的两个问题也就被解决了。</strong></p><p><strong>为了帮大家更容易的理解全局一致性快照包含的内容，接下来我用伪代码描述一下，会比文字更好理解~</strong></p><h3 id="来段伪代码描述全局一致快照包含的内容"><a href="#来段伪代码描述全局一致快照包含的内容" class="headerlink" title="来段伪代码描述全局一致快照包含的内容"></a>来段伪代码描述全局一致快照包含的内容</h3><h4 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// S_all 即全局一致性快照</span></span><br><span class="line">S_all = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 假设总共有 x 个 process</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= x; i++) &#123;</span><br><span class="line"><span class="comment">// 第 i 个 process 的状态为 S_i，直接按照 += 写，勿喷</span></span><br><span class="line">  S_all += S_i;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 假设总共有 y 个 channel</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= y; i++) &#123;</span><br><span class="line">  <span class="comment">// 1.S_C_out_in_i：第 i 个 channel 的状态，in 代表第 i 个 channel 的输入，out 含义为第 i 个 channel 的输出</span></span><br><span class="line">  <span class="comment">// 2.m_out_in_i 和 n_out_in_i 其实就是上文中的 n 和 m</span></span><br><span class="line">  <span class="comment">// 2.m_out_in_i：第 i 个 channel 做快照前，发往 in process（下游）的消息个数</span></span><br><span class="line">  <span class="comment">// 3.n_out_in_i：第 i 个 channel 做快照前，接受到 out process（上游） 的消息个数</span></span><br><span class="line">  <span class="comment">// 4.需要注意，每一个 channel 的 m_out_in_i 和 n_out_in_i 都可能是不一样的，这里是伪代码所以直接按照下面的方式写了</span></span><br><span class="line">  S_C_out_in_i = Message[m_out_in_i + <span class="number">1</span>] + ... + Message[n_out_in_i];</span><br><span class="line">  </span><br><span class="line">  S_all += S_C_out_in_i;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 状态做完啦~</span></span><br></pre></td></tr></table></figure><h2 id="怎样去记录-S-Cpq-？"><a href="#怎样去记录-S-Cpq-？" class="headerlink" title="怎样去记录 S(Cpq)？"></a>怎样去记录 <kbd>S(Cpq)</kbd>？</h2><p>通过上面的分析，我们已经讨论得到了 <kbd>S(Cpq)</kbd> 都包含了什么内容，并且其之间要满足什么样的数学关系。但是在现实实际生活中，消息在 Channel 上乱飞时，我们是无法记录这些消息作为 Channel 的状态的。<br><strong><font color=red>但是这些消息终究会到达目的地，我们可以在消息的目的地去记录这些消息作为 Channel 的状态。即我们可以在 q 中记录 Channel pq 的 <kbd>S(Cpq)</kbd>，在 p 中记录 Channel pq 的 <kbd>S(Cqp)</kbd>。</font></strong></p><h3 id="伪代码-1"><a href="#伪代码-1" class="headerlink" title="伪代码"></a>伪代码</h3><p>顺便那么上面那段伪代码就可以简化为下面这样：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// S_all 即全局一致性快照</span></span><br><span class="line">S_all = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 假设总共有 x 个 process</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= x; i++) &#123;</span><br><span class="line">  <span class="comment">// S_i_all：第 i 个 process 要记录的状态</span></span><br><span class="line">  S_i_all = <span class="keyword">null</span>;</span><br><span class="line">  </span><br><span class="line"><span class="comment">// S_i：第 i 个 process 的状态</span></span><br><span class="line">  S_i_all += S_i; <span class="comment">// 【直接按照 += 写，勿喷】</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 第 i 个 process 总共有 y 个 input channel，即有 y 个上游 process，下文中 j 即指代第 j 个 channel，也代指 j channel 的上游 j process</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= y; j++) &#123;</span><br><span class="line">    <span class="comment">// 1.S_C_j_i：第 i 个 channel 的状态</span></span><br><span class="line">    <span class="comment">// 2.m_j_i 和 n_j_i 其实就是上文中的 n 和 m</span></span><br><span class="line">    <span class="comment">// 2.m_j_i：第 j 个 channel 做快照前，发往 i（下游）的消息个数</span></span><br><span class="line">    <span class="comment">// 3.n_j_i：第 j 个 channel 做快照前，接受到 j（上游） 的消息个数</span></span><br><span class="line">    <span class="comment">// 4.需要注意，每一个 channel 的 m_j_i 和 n_j_i 都可能是不一样的，这里是伪代码所以直接按照下面的方式写了</span></span><br><span class="line">    S_C_j_i = Message[m_j_i + <span class="number">1</span>] + ... + Message[n_j_i];</span><br><span class="line"></span><br><span class="line">    S_i_all += S_C_j_i;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  S_all += S_i_all;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 状态做完啦~</span></span><br></pre></td></tr></table></figure><h3 id="记录-S-Cpq-需要满足的条件"><a href="#记录-S-Cpq-需要满足的条件" class="headerlink" title="记录 S(Cpq) 需要满足的条件"></a>记录 <kbd>S(Cpq)</kbd> 需要满足的条件</h3><p><strong><font color=red>重点重点重点！！！</font></strong><br><strong><font color=red>分析上面的伪代码后，我们可以发现，要得到 S_all，其中只有一个变量在进程做快照时不知道的，那就是 n_j_i（即第 i 个 channel 做快照前，接受到 j（上游） 的消息个数），别忘了 n = n‘，即也可以定义为 j 做快照前，j 发往 channel 的消息个数。那么实际上这个值 j process 是知道的，就代表 i 进程需要知道 j 告诉他 n_j_i 的值是多少。重点来了，当 i process 做完快照之后，直接发一个 marker 下去，这个 marker 不会对计算有任何影响（即不会对状态产生任何影响），marker 只是一个标识，j process 做完自己的快照之后，直到接收到 marker 之间的消息就是Channel ij 的状态。i 就是通过 marker 来告诉 j process n_j_i 的值是多少的。（其他的变量为什么都知道就不详细分析了，很容易理解）</font></strong></p><h1 id="分布式应用全局一致性快照算法流程总结"><a href="#分布式应用全局一致性快照算法流程总结" class="headerlink" title="分布式应用全局一致性快照算法流程总结"></a>分布式应用全局一致性快照算法流程总结</h1><h2 id="算法流程总结"><a href="#算法流程总结" class="headerlink" title="算法流程总结"></a>算法流程总结</h2><ol><li><strong>发起快照</strong>：有一个 manager process（这个 manager 可以是所有 process 中的任意一个 process，也可以是一个单独的中央管理者）告诉所有的 process 说可以开始做状态了；</li><li><strong>执行快照</strong>：所有 process 就开始记录自己本地的状态（非所有 input channel）了，记录完本地状态，然后发 marker 给下游所有的 channel，然后开始记录上游所有 input channel 的消息（直到接收到上游所有的 marker）；</li><li><strong>执行快照</strong>：每个 process 对于每一个 input channel 来说，都将自己做完状态后直到收到 marker 之间的消息记录下来，作为这个 input channel 的状态；</li><li><strong>执行快照</strong>：当收到上游所有 marker 之后，这个 process 要记录的状态就全部得到了，然后告诉 manager process 说做完状态了；</li><li><strong>终止快照</strong>：manager process 接收到所有 process 做完的消息之后，就标记所有的状态以及完成了。</li></ol><h2 id="算法流程示例"><a href="#算法流程示例" class="headerlink" title="算法流程示例"></a>算法流程示例</h2><h3 id="发起快照"><a href="#发起快照" class="headerlink" title="发起快照"></a>发起快照</h3><ul><li>有一个 manager process（这个 manager 可以是所有 process 中的任意一个 process，也可以是一个单独的中央管理者）告诉所有的 process 说可以开始做状态了；<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/41.png" alt="41"></li></ul><h3 id="执行快照"><a href="#执行快照" class="headerlink" title="执行快照"></a>执行快照</h3><ul><li>所有 process 就开始记录自己本地的状态（非所有 input channel）了，记录完本地状态，然后发 marker 给下游所有的 channel，然后开始记录上游所有 input channel 的消息（直到接收到上游所有的 marker）；<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/42.png" alt="42"></li><li>每个 process 对于每一个 input channel 来说，都将自己做完状态后直到收到 marker 之间的消息记录下来，作为这个 input channel 的状态；<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/43.png" alt="43"></li><li>当收到上游所有 marker 之后，这个 process 要记录的状态就全部得到了，然后告诉 manager process 说做完状态了；</li></ul><h3 id="终止快照"><a href="#终止快照" class="headerlink" title="终止快照"></a>终止快照</h3><ul><li>manager process 接收到所有 process 做完的消息之后，就标记所有的状态以及完成了。<br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/44.png" alt="44"></li></ul><h1 id="Chandy-Lamport-算法流程、示例"><a href="#Chandy-Lamport-算法流程、示例" class="headerlink" title="Chandy-Lamport 算法流程、示例"></a>Chandy-Lamport 算法流程、示例</h1><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/16.jpeg" alt="16"></p><h3 id="发起快照-1"><a href="#发起快照-1" class="headerlink" title="发起快照"></a>发起快照</h3><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/17.png" alt="17"><br>解读：</p><ul><li>本次快照的起始点，先把起始点的快照给做了，然后发出 marker（<strong>这个 marker 消息是干啥用的呢？没错，就是我们之前分析的结论</strong>），开始记录 input channel</li></ul><h3 id="执行快照-1"><a href="#执行快照-1" class="headerlink" title="执行快照"></a>执行快照</h3><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/18.jpeg" alt="18"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/19.jpeg" alt="19"><br>解读：</p><ul><li>Pi 记录本地快照，标记 Cki 为空：因为从 Cki 接收到了 marker，这时的状态是 Pk 刚刚做完快照，Pk 做完快照发往 Cki 的消息个数 = Pi 做完快照从 Cki 接收到的消息个数。即 n = n’ = m’ = m；即 Cki = [Empty]；</li><li>Pi 开始向所有 output channel 发 marker，开始记录除 Cki 之外的 input channel 消息，因为本地快照已经做完了；然后上游还有部分进程没有做完快照，为了记录除 Cki 之外的 input Channel 消息，</li></ul><p>解读：</p><ul><li>结合前一张图说的开始记录 input channel 消息，Pi 停止记录 Cki 的消息，同时将此前记录所有 Cki 收到的消息作为本次快照中的最终状态；n’ &gt; m’，在 Pi 这里记录了 Cki 的状态，即 Cki = [m‘ + 1, m’ + 2…n]</li></ul><h3 id="终止快照-1"><a href="#终止快照-1" class="headerlink" title="终止快照"></a>终止快照</h3><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/20.jpeg" alt="20"></p><h2 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h2><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/21.png" alt="21"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/22.png" alt="22"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/23.png" alt="23"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/24.png" alt="24"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/25.png" alt="25"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/26.png" alt="26"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/27.png" alt="27"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/28.png" alt="28"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/29.png" alt="29"></p><h2 id="Chandy-Lamport-与上节分布式应用全局一致性快照算法的异同"><a href="#Chandy-Lamport-与上节分布式应用全局一致性快照算法的异同" class="headerlink" title="Chandy-Lamport 与上节分布式应用全局一致性快照算法的异同"></a>Chandy-Lamport 与上节分布式应用全局一致性快照算法的异同</h2><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/47.png" alt="47"></p><p>Chandy-Lamport 就是上节<a href="#分布式应用全局一致性快照算法流程总结">分布式应用全局一致性快照算法</a>的其中一种特殊形式；<a href="#分布式应用全局一致性快照算法流程总结">分布式应用全局一致性快照算法</a>中说的是每个 process 在接收到 manager 做快照的消息之后就直接可以开始记录状态了，而 Chandy-Lamport 其实就是把这个 manager 的消息用接收到的第一个 marker 消息给代替了，用数学表达式表示就是接收到第一个 marker 的 channel 的 n = n’ = m = m’，剩余的 channel 满足 n = n’ ≥ m = m’，并且 Chandy-Lamport 算法也都满足第 8 章节介绍的各种条件。</p><h1 id="flink-实现的全局一致性快照介绍（flink-容错机制）"><a href="#flink-实现的全局一致性快照介绍（flink-容错机制）" class="headerlink" title="flink 实现的全局一致性快照介绍（flink 容错机制）"></a>flink 实现的全局一致性快照介绍（flink 容错机制）</h1><h2 id="Chandy-Lamport-与-Flink之间的关系"><a href="#Chandy-Lamport-与-Flink之间的关系" class="headerlink" title="Chandy-Lamport 与 Flink之间的关系"></a>Chandy-Lamport 与 Flink之间的关系</h2><p><a href="https://arxiv.org/pdf/1506.08603.pdf。" title="flink 全局一致性快照论文链接">flink 全局一致性快照论文</a></p><p>Flink 是分布式系统，所以 Flink 会采用全局一致性快照的方式形成检查点，来支持故障恢复。Flink 的异步全局一致性快照算法跟 Chandy-Lamport 算法的区别主要有以下几点：</p><ul><li>第一，Chandy-Lamport 支持强连通图，而 Flink支持弱连通图；</li><li>第二，Flink采用的是裁剪的（Tailored）Chandy-Lamport 异步快照算法；</li><li>第三，Flink的异步快照算法在DAG场景下不需要存储 Channel state，从而极大减少快照的存储空间。</li></ul><h2 id="flink-的容错机制"><a href="#flink-的容错机制" class="headerlink" title="flink 的容错机制"></a>flink 的容错机制</h2><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/30.png" alt="30"></p><h2 id="端到端的Exactly-once"><a href="#端到端的Exactly-once" class="headerlink" title="端到端的Exactly once"></a>端到端的Exactly once</h2><p>Exactly once 的意思是，作业结果总是正确的，但是很可能产出多次；所以它的要求是需要有可重放的 source。<br>端到端的 Exactly once，是指作业结果正确且只会被产出一次，它的要求除了有可重放的 source 外，还要求有事务型的 sink 和可以接收幂等的产出结果。</p><h2 id="flink-的全局一致性快照"><a href="#flink-的全局一致性快照" class="headerlink" title="flink 的全局一致性快照"></a>flink 的全局一致性快照</h2><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/31.png" alt="31"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/32.png" alt="32"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/33.png" alt="33"></p><h2 id="Barrier-对齐"><a href="#Barrier-对齐" class="headerlink" title="Barrier 对齐"></a>Barrier 对齐</h2><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/34.png" alt="34"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/35.png" alt="35"><br><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/36.png" alt="36"></p><h2 id="flink-的全局一致性快照与上节分布式应用全局一致性快照算法的异同"><a href="#flink-的全局一致性快照与上节分布式应用全局一致性快照算法的异同" class="headerlink" title="flink 的全局一致性快照与上节分布式应用全局一致性快照算法的异同"></a>flink 的全局一致性快照与上节分布式应用全局一致性快照算法的异同</h2><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/46.png" alt="46"></p><p>flink 的全局一致性快照就是上节<a href="#分布式应用全局一致性快照算法流程总结">分布式应用全局一致性快照算法</a>的其中一种特殊形式；<a href="#分布式应用全局一致性快照算法流程总结">分布式应用全局一致性快照算法</a>中说的是每个 process 在接收到 manager 做快照的消息之后就直接可以开始记录状态了，而 flink 其实就是将各个 process 开始做状态的时间点设为了接收到上游 input channel 所有的 barrier，这样一个好处就是由于各个 process 是接收到了上游所有 barrier 之后开始的，用数学表达式表示其实就满足了 n = n’ = m = m’，就没有必要存储 channel 中的状态了；并且 flink 算法也都满足第 8 章节介绍的各种条件。</p><h2 id="状态后端"><a href="#状态后端" class="headerlink" title="状态后端"></a>状态后端</h2><h3 id="JVM-Heap"><a href="#JVM-Heap" class="headerlink" title="JVM Heap"></a>JVM Heap</h3><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/37.png" alt="37"><br>第一种，JVM Heap，它里面的数据是以Java对象形式存在的，读写也是以对象形式去完成的，所以速度很快。但是也存在两个弊端：第一个弊端，以对象方式存储所需的空间是磁盘上序列化压缩后的数据大小的很多倍，所以占用的内存空间很大；第二个弊端，虽然读写不用做序列化，但是在形成 snapshot 时需要做序列化，所以它的异步 snapshot 过程会比较慢。</p><h3 id="RocksDB"><a href="#RocksDB" class="headerlink" title="RocksDB"></a>RocksDB</h3><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/38.png" alt="38"><br>第二种，RocksDB，这个类型在读写时就需要做序列化，所以它读写的速度比较慢。但是它有一个好处，基于LSM的数据结构在快照之后会形成 sst 文件，它的异步 checkpoint 过程就是文件拷贝的过程，CPU 消耗会比较低。</p><h1 id="分布式应用全局一致性快照算法、Chandy-Lamport-算法、flink-全局一致性快照之间的关系"><a href="#分布式应用全局一致性快照算法、Chandy-Lamport-算法、flink-全局一致性快照之间的关系" class="headerlink" title="分布式应用全局一致性快照算法、Chandy-Lamport 算法、flink 全局一致性快照之间的关系"></a>分布式应用全局一致性快照算法、Chandy-Lamport 算法、flink 全局一致性快照之间的关系</h1><p><img src="/blog-img/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/03_state/01_state-1/45.png" alt="45"></p><h1 id="参考的文章"><a href="#参考的文章" class="headerlink" title="参考的文章"></a>参考的文章</h1><ul><li><a href="https://developer.aliyun.com/article/667562">https://developer.aliyun.com/article/667562</a></li><li><a href="https://matt33.com/2019/10/27/paper-chandy-lamport/">https://matt33.com/2019/10/27/paper-chandy-lamport/</a></li><li><a href="https://developer.aliyun.com/article/448900">https://developer.aliyun.com/article/448900</a></li></ul>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，帮助大家理解全局一致性快照。可能很多小伙伴都知道 flink 是使用 barrier 来做全局一致性快照，但是我提两个问题，为什么 flink 的 barrier 能够保证全局一致性快照的正确性？barrier 到底发挥了怎样的作用？小伙伴们能回答上来么，有想过背后的原因嘛，楼主通过本篇文章抛砖引玉，希望小伙伴们能够喜欢~
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink x TiDB meetup 小记</title>
    <link href="https://yangyichao-mango.github.io/2021/03/12/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/10_meetup/01_apache-flink-meetup-20210710-beijing/"/>
    <id>https://yangyichao-mango.github.io/2021/03/12/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/02_%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E5%BB%BA%E8%AE%BE/03_one-engine/01_%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/01_flink/10_meetup/01_apache-flink-meetup-20210710-beijing/</id>
    <published>2021-03-12T06:21:53.000Z</published>
    <updated>2021-07-11T06:50:20.929Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>现场回顾。PPT 公众号回复 20210710 获取。</p></blockquote><p>博主是带着一些问题去参加本次 meetup 的。</p><p>第一个就是实时数仓的建设经验，不论是引擎、工具链还是数仓层面。</p><p>第二个就是实时数仓的数据时效性、质量保障经验。</p><p>但是总结来说这次 meetup 博主更多的是得到了开发技术方面的输入，保障层面的输入还是较少。</p><h1 id="网易-JFlink-on-TiDB"><a href="#网易-JFlink-on-TiDB" class="headerlink" title="网易 - JFlink on TiDB"></a>网易 - JFlink on TiDB</h1><blockquote><p>博主在听完分享之后比较感兴趣的几个问题，和林佳老师交流了下。</p></blockquote><h2 id="JFlink-SDK-是干什么的？"><a href="#JFlink-SDK-是干什么的？" class="headerlink" title="JFlink-SDK 是干什么的？"></a>JFlink-SDK 是干什么的？</h2><ul><li>引擎侧支持：提供引擎侧运维能力，metric 采集等功能；</li><li>工具链侧支持：在 flink datastream api 能力的基础上提供 DSL api 能力，让用户通过简单的声明配置就可以上线任务。</li></ul><h2 id="JFlink-SDK-目前支持-flink-sql-吗？"><a href="#JFlink-SDK-目前支持-flink-sql-吗？" class="headerlink" title="JFlink-SDK 目前支持 flink sql 吗？"></a>JFlink-SDK 目前支持 flink sql 吗？</h2><p>目前暂时不支持 flink sql，不过目前 JFlink-SDK 的能力支持类似于 flink sql。举个例子：用户使用 JFlink-SDK 的方式就是类似于 flink sql create table 后的 with xxx properties。<br>通过声明 properties 进行开发。<br>由于两者的类似性，所以之后 JFlink-SDK 会考虑融合 flink sql。</p><p>博主理解其实就是在目前 JFlink-SDK 的能力上加上 + schema = flink sql。所以这个融合过程复杂度应该不是很高。</p><h2 id="目前网易是怎样做实时数据的时效性、质量监控以及保障的？"><a href="#目前网易是怎样做实时数据的时效性、质量监控以及保障的？" class="headerlink" title="目前网易是怎样做实时数据的时效性、质量监控以及保障的？"></a>目前网易是怎样做实时数据的时效性、质量监控以及保障的？</h2><ul><li>时效性：监控通过采集的 metrics 发现并且进行报警；并且采样算子级别的平均处理时延。</li><li>质量监控：目前质量监控这块首先是从开发阶段就要求所有的实时任务都一定具备可回溯性。举个例子：当任务出现数据计算错误时，可回溯性的任务从历史 ck 进行数据回溯即可。<br>但是针对某些大状态实时指标，使用了外部 state 引擎（比如 redis 等），也只能是重新手动清除 redis 进行数据回溯。</li></ul><h2 id="目前端到端-exactly-once-的适用情况？"><a href="#目前端到端-exactly-once-的适用情况？" class="headerlink" title="目前端到端 exactly-once 的适用情况？"></a>目前端到端 exactly-once 的适用情况？</h2><ul><li>sink 为 TIDB：通过幂等操作实现：at-least-once + 主键重入实现。</li><li>sink 为 kafka：通过两阶段提交实现，目前网易对 kafka 的两阶段提交使用是很广泛的。但是两阶段必然会引进数据产出延迟的问题，因此会将两阶段的参数暴露给用户，让用户根据自己的业务诉求进行选择。</li></ul><h2 id="对于大状态任务有没有什么实战优化经验？"><a href="#对于大状态任务有没有什么实战优化经验？" class="headerlink" title="对于大状态任务有没有什么实战优化经验？"></a>对于大状态任务有没有什么实战优化经验？</h2><p>目前也没有非常好的的优化方案。但是可以从以下两个角度去进行优化。</p><ul><li>大状态 flink state：rocksdb compaction 压缩</li><li>大状态外存：redis 等</li></ul><h1 id="PingCAP-实时数仓之美"><a href="#PingCAP-实时数仓之美" class="headerlink" title="PingCAP - 实时数仓之美"></a>PingCAP - 实时数仓之美</h1><ul><li>TiDB（HTAP） = TiKV（OLTP，行存）+ TiFlush（OLAP，列存）= 大号的 MySQL。</li></ul><p>TiDB 对标 MySQL，在此基础之上，如果用户有分析需求，可以进行配置打开 TiFlush 能力，进行 OLAP 分析。底层是两套存储。</p><ul><li>flink = 接换发 = 接（接收数据） + 换（进行转换处理） + 发（发送数据）</li></ul><blockquote><p>分享之后和王天宜老师的交流。</p></blockquote><h2 id="计算引擎侧-flink-实现了流批一体，存储引擎呢？"><a href="#计算引擎侧-flink-实现了流批一体，存储引擎呢？" class="headerlink" title="计算引擎侧 flink 实现了流批一体，存储引擎呢？"></a>计算引擎侧 flink 实现了流批一体，存储引擎呢？</h2><p>存储引擎其实目前已经有了开源的方案，可以了解下 <a href="https://mp.weixin.qq.com/s/4h7JDItBFhWkEex6eeaJTg。" title="流批存储引擎">pravega</a>。但是底层引擎能力切换的推动还是需要从上往下。<br>即业务遇到了瓶颈，需要这种解决方案，才能更快速的去推动引擎以及架构的改变。这种新架构的变革从引擎底层进行推动还是比较困难的。</p><p>说道这里，博主认为 TiDB 是一个非常好的存储引擎。<br>博主认为其好的地方就在于其 HTAP 能力。<br>站在用户角度，用户只关心我需要 OLAP + OLTP 的能力，接口简单明了就完事了。底层的引擎不应该是用户关心的。无论你底层是 MySQL 还是 Postgre（OLTP），ClickHouse 还是 Druid（OLAP），只要接口层能以一套 API 给我使用就 very good 了。<br>而目前 TiDB 能把 HTAP 的能力都能以 MySQL 这样的统一接口协议给用户。用户的理解、使用成本是很低的。</p><h2 id="两阶段提交一定会保障不重、不丢数据吗？"><a href="#两阶段提交一定会保障不重、不丢数据吗？" class="headerlink" title="两阶段提交一定会保障不重、不丢数据吗？"></a>两阶段提交一定会保障不重、不丢数据吗？</h2><p>是没法 100% 保障的。举个其他的例子：hdfs 三副本也没法 100% 保障 hdfs 数据是完全可用的。但是目前在我们的场景中两阶段提交没有出现过问题。其实目前有还有<a href="https://segmentfault.com/a/1190000012534071。" title="三阶段说明">三阶段</a>。可以根据业务需求进行选择。</p><h2 id="TiFlush-vs-ClickHouse-vs-Doris？"><a href="#TiFlush-vs-ClickHouse-vs-Doris？" class="headerlink" title="TiFlush vs ClickHouse vs Doris？"></a>TiFlush vs ClickHouse vs Doris？</h2><p>因为目前 TiDB 定位是 HTAP，TiFlush 相比于专门定位与 OLAP 场景 ClickHouse vs Doris 的能力还不是非常完善，性能也不如其好。但是 TiFlush 也是在一直持续不断的学习以及完善过程中。</p><h1 id="知乎-TiDB-x-Flink-的端到端实时计算"><a href="#知乎-TiDB-x-Flink-的端到端实时计算" class="headerlink" title="知乎 - TiDB x Flink 的端到端实时计算"></a>知乎 - TiDB x Flink 的端到端实时计算</h1><blockquote><p>没听，哈哈，和王天宜老师交流 high 了。果然和大佬们交流还是很爽的。</p></blockquote><h1 id="360-Flink-SQL-在奇虎-360-的实践"><a href="#360-Flink-SQL-在奇虎-360-的实践" class="headerlink" title="360 - Flink SQL 在奇虎 360 的实践"></a>360 - Flink SQL 在奇虎 360 的实践</h1><blockquote><p>主要介绍了一些 flink sql 在引擎 + 工具链层面的定制化优化。</p></blockquote><ul><li>flink sql udf 结果复用</li></ul><p>这个功能还是很有用的，尤其是做 ods -&gt; dwd 清洗时，由于 flink sql 的各种上推优化，会导致 udf 重复执行，当你遇到一个大 json 解析时，不好意思，你的任务以及废了一半了。顺带夸一嘴，博主所在公司已经实现了表达式复用。</p><ul><li>flink sql 算子自定义并行度</li></ul><p>就是将 dag 序列化成 json，用户在界面上重新配吧配吧算子并发。</p><p>博主的理解：这个功能在资源没有非常紧张的情况下的优化效果其实不是很明显的。</p><p>个人理解，个人理解，勿喷勿喷。下面有原因。</p><p>第一个方面，在任务运行侧，因为大多数 flink sql 任务当任务出现瓶颈时，往往都是其中一个算子出现了瓶颈，那么基本上这个算子就是所有算子中的并发度最大的，这个时候，flink sql 默认其他算子的并发也是这个并发，但是其他算子又没有出现性能瓶颈，只是并发给了大一点，都是空跑，资源的浪费其实不是非常明显的。<br>至少目前博主遇到过的场景中是这样的。</p><p>第二个方面，在 DE 开发侧，flink sql 毕竟面向的还是 DE，大多数 DE 干的工作就是关注业务场景写一个简单的 sql，很多场景下，没时间以及精力做那么多优化。最简单的一个道理，一个引擎能真正用起来，在接口层看的不是你这个引擎配置化能力多牛逼，更多的还是接口层简单，让用户先用起来，上手方便。</p><p>但是，站在资源治理的角度上，这个功能是很有效的。</p><p>上述都是博主站在自己目前的场景下进行的一些分析。</p><h1 id="阿里-flink-cdc"><a href="#阿里-flink-cdc" class="headerlink" title="阿里 - flink-cdc"></a>阿里 - flink-cdc</h1><blockquote><p>作为一个个人孵化的子项目，受到很多同学、公司的青睐。</p></blockquote><ul><li>flink cdc 与各开源 cdc 的优劣势对比</li><li>flink cdc 目前不支持 ck，失败之后要重跑</li><li>flink cdc 2.0 的规划</li></ul><p>博主目前所在的公司，binlog 采集都是由基础架构统一管理的。cdc 只能处于一个探索阶段。</p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从实际生产出发，帮助大家理解全局一致性快照。可能很多小伙伴都知道 flink 是使用 barrier 来做全局一致性快照，但是我提两个问题，为什么 flink 的 barrier 能够保证全局一致性快照的正确性？barrier 到底发挥了怎样的作用？小伙伴们能回答上来么，有想过背后的原因嘛，楼主通过本篇文章抛砖引玉，希望小伙伴们能够喜欢~
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>实时数据时效监控体系建设</title>
    <link href="https://yangyichao-mango.github.io/2020/12/01/wechat-blog/apache-flink:realtime-time-monitor/"/>
    <id>https://yangyichao-mango.github.io/2020/12/01/wechat-blog/apache-flink:realtime-time-monitor/</id>
    <published>2020-12-01T06:21:53.000Z</published>
    <updated>2021-04-04T11:10:16.919Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列介绍了实时数据时效监控体系的建设。</p></blockquote><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>数据延迟为是实时数据的一个最大问题。<br>其实不管是哪部分产生延迟，最终的结果是都会直接影响到上层指标（数据质量问题 + 数据时效问题）。<br>为了方便我们在开发阶段，运维阶段快速定位、解决延迟导致的问题；<br>以及为后续可能的报警能力提供基础能力，因此需要建设实时数据流时效监控体系。</p><p>以一张图描述整个传输链路与耗时相关的问题。</p><p><img src="/blog-img/apache-flink:realtime-time-monitor/time-cost.png" alt="架构"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>上述图总共分为以下三部分。</p><ul><li><strong>数据延迟监控</strong></li><li><strong>数据乱序监控</strong></li><li><strong>数据加工延迟监控</strong></li></ul>]]></content>
    
    <summary type="html">
    
      本系列介绍了实时数据时效监控体系的建设。
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>生产实践 | Flink + 直播（三）| 如何建设当前正在直播 xx 数？</title>
    <link href="https://yangyichao-mango.github.io/2020/11/11/wechat-blog/apache-flink:realtime-live-stream-3/"/>
    <id>https://yangyichao-mango.github.io/2020/11/11/wechat-blog/apache-flink:realtime-live-stream-3/</id>
    <published>2020-11-11T06:21:53.000Z</published>
    <updated>2021-04-04T11:10:38.910Z</updated>
    
    <content type="html"><![CDATA[<h1 id="生产实践-Flink-直播（三）-如何建设当前正在直播-xx-数？"><a href="#生产实践-Flink-直播（三）-如何建设当前正在直播-xx-数？" class="headerlink" title="生产实践 | Flink + 直播（三）| 如何建设当前正在直播 xx 数？"></a>生产实践 | Flink + 直播（三）| 如何建设当前正在直播 xx 数？</h1><blockquote><p>本系列每篇文章都是从一些实际的 case 出发，分析一些生产环境中经常会遇到的问题，抛砖引玉，以帮助小伙伴们解决一些实际问题。本篇文章主要介绍直播间生产侧指标的建设过程，如果对小伙伴有帮助的话，欢迎点赞 + 再看~</p></blockquote><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p>本文主要介绍<strong>生产侧指标的建设</strong>，比如当前正在直播直播间数，或者主播数等。在介绍生产侧指标的建设过程之前，我们先回顾下上一节的<strong>架构</strong>图。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-3/tec-arc.png" alt="架构"></p><p>而本篇要介绍的<strong>生产侧指标</strong>的数据链路主要对应以下几个模块。</p><ul><li>数据源：读取直播生产，比如开播，关播等 kafka 数据源日志；</li><li>数据处理：使用生产侧数据源 + 实时画像维表 + flink 建设生产侧实时指标；</li><li>数据汇：将处理完成的指标数据写入到 kafka 中。</li></ul><p>我用另一张图进行了标注，图中<strong>标红</strong>模块为生产侧指标的数据链路涉及到的模块。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-3/metric-prod-tec-arc.png" alt="生产侧架构"></p><p>其中直播间实时画像维表的介绍已经在上节进行了介绍，感兴趣的话可以点击以下链接，跳转到上节进行阅读~</p><p>本小节就不针对<strong>生产侧指标的建设</strong>中所有涉及指标的建设过程进行详细介绍了，我们主要以<strong>当前分钟正在开播直播间数</strong>作为<strong>生产侧指标建设</strong>的一个代表性案例，介绍这个指标的整个建设过程。<br>来为大家还原生产侧指标的业务过程以及技术方案。</p><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><p>仍然从几个问题入手，介绍<strong>当前分钟正在开播直播间数</strong>的建设过程。</p><ul><li><strong>当前分钟正在开播直播间数</strong>的定义什么？业务过程是怎么样的？举例？</li><li>怎样去建设这个指标？整体的指标计算流程？</li></ul><h2 id="1-聊聊定义？"><a href="#1-聊聊定义？" class="headerlink" title="1.聊聊定义？"></a>1.聊聊定义？</h2><p>当前分钟正在开播直播间数，其定义就是整个平台中，当前分钟正在开播的直播间数 + 单层维度下钻的当前分钟正在开播的直播间数。</p><p>举例：</p><p>现在的时间点是 2020-11-11 12:42，真实直播的直播间数为 3000 个（平台维度下钻：IOS 平台为 1500，安卓平台为 1500）</p><p>到了 12:43 时，有 200 个直播间进行了关播（其中 100 个为 IOS，100 个为安卓），有 100 个直播间开播（全部为 IOS），则当前正在直播的直播间数为 2900（平台维度下钻：IOS 平台为 1500，安卓平台为 1400）。</p><p>其中 2020-11-11 12:42 的 3000 以及 2020-11-11 12:43 的 2900 以及按照平台下钻的数值就为当前时间正在开播的直播间数。</p><p>因此根据上述定义和分析，我们可以直接将数据源和数据汇的 schema 定义下来，主体信息如下。</p><h3 id="数据源-schema"><a href="#数据源-schema" class="headerlink" title="数据源 schema"></a>数据源 schema</h3><table><thead><tr><th>字段</th><th>备注</th></tr></thead><tbody><tr><td>live_stream_id</td><td>直播间 id</td></tr><tr><td>author_id</td><td>主播 id</td></tr><tr><td>start_or_end</td><td>开播还是关播</td></tr><tr><td>timestamp</td><td>时间戳</td></tr><tr><td>…</td><td>…</td></tr></tbody></table><h3 id="数据汇-schema"><a href="#数据汇-schema" class="headerlink" title="数据汇 schema"></a>数据汇 schema</h3><table><thead><tr><th>字段</th><th>备注</th></tr></thead><tbody><tr><td>timestamp</td><td>时间戳，汇总到分钟粒度</td></tr><tr><td>metric_name</td><td>指标名，举例：开播直播间数</td></tr><tr><td>metric_value</td><td>指标值，举例：3000（开播直播间数）</td></tr><tr><td>dim_name</td><td>维度名，举例：平台，版本</td></tr><tr><td>dim_value</td><td>维度值，举例：IOS，8.1</td></tr><tr><td>…</td><td>…</td></tr></tbody></table><blockquote><p>Notes:</p><p><strong>metric_name 和 metric_value</strong>：</p><p>这两个字段是为了之后进行指标扩充时进行的设计。比如后续如果需要加入开播主播数，开播时长等指标，不用修改数据汇 schema，只需要加一种 metric_name，就可以使用原有 schema 进行数据产出。</p><p><strong>dim_name 和 dim_value</strong>：</p><p>目前我们建设的指标只提供了进行单维度下钻的能力，所以设计了 dim_name 和 dim_value 两个字段，可满足用户查看平台为 IOS 的当前开播直播间数或者使用开播软件版本为 8.1 的当前开播直播间数。<br>如果后续业务场景需要多维下钻能力，可以在字段上面进行扩充。或者也可以提供明细数据在 OLAP 中进行多维下钻。</p></blockquote><h2 id="2-怎样建设？"><a href="#2-怎样建设？" class="headerlink" title="2.怎样建设？"></a>2.怎样建设？</h2><p>对于当前分钟正在开播直播间数来说，其计算方式很简单，就是下面这个数学公式：</p><p><strong>当前分钟正在开播直播间数</strong> = <strong>上一分钟正在开播直播间数</strong> + <strong>当前分钟开播直播间数</strong> - <strong>当前分钟关播直播间数</strong></p><p>可以从上面的公式可以看出，对于当前分钟正在开播直播间数的计算来说，是依赖上下文信息的，即<strong>上一分钟正在开播直播间数</strong>，这也就是我们所说的<strong>状态</strong>。</p><h3 id="指标处理逻辑"><a href="#指标处理逻辑" class="headerlink" title="指标处理逻辑"></a>指标处理逻辑</h3><p>从获取到数据源，到产出指标的整体处理逻辑如下图所示。这里就不进行赘述了。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-3/metric-current-live-live-stream-number-life-cycle.png" alt="技术架构"></p><p>其中标为<strong>粉色</strong>的模块为任务中的<strong>状态</strong>，即任务中一直存储的当前分钟正在开播直播间数。</p><h3 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h3><p>上述指标涉及到了，状态，那么我这里讲一下我对<strong>状态</strong>的理解。如有错误，请在文末讨论中进行指出，我会和大家讨论。</p><p>状态其实就是一个记录上下文信息的东西，如果当前的计算过程依赖到上次计算的结果，那么上次计算的结果就是状态。举几个🌰；</p><ul><li><p><strong>流处理</strong>：如本节介绍的<strong>当前分钟正在开播直播间数</strong>的计算，就是依赖上一分钟的正在开播直播间数（状态）进行的计算。<br>可能有小伙伴会说，我不依赖上一分钟，我从头开始计算可以不？答案是可以的，但是从头开始计算，也需要将所有历史数据进行存储，这些历史数据其实也就是状态，只不过我们将其优化为了上一分钟开播直播间数。</p></li><li><p><strong>批处理</strong>：今天的全量表 = 昨天全量表（状态） + 今天的增量表。</p></li><li><p><strong>数据库存储</strong>：最常见的 mysql 主键自增，unique key 等。<br>为什么新插入一条数据主键会自增？因为 mysql 存储了主键的上一个值（状态）。<br>为什么插入相同数据时，由于 unique key 会导致报错，就是因为 mysql 存储了所有 unique key 的字段的数据（状态）。</p></li><li><p><strong>生活</strong>：当前的手机电量 = 上一分钟的手机电量（状态） + （充电/用电量）。<br>为什么你越来越喜欢你的另一半？因为你对她的感觉 = 前一秒你对她的感觉（状态） + 当前这一秒她亲了你一下。</p></li></ul><p>生活中随处可见状态，即使你不是程序员，我相信也都可以理解状态的概念。</p><h3 id="指标计算代码示例"><a href="#指标计算代码示例" class="headerlink" title="指标计算代码示例"></a>指标计算代码示例</h3><p>按照最简单的实现方式举例如下。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LiveStreamRealtimeMetricProdProcessorJob</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStream&lt;SourceModel&gt; source = SourceFactory.getSourceDataStream(...);</span><br><span class="line"></span><br><span class="line">        DataStream&lt;SinkModel&gt; result = source</span><br><span class="line">                .keyBy(<span class="keyword">new</span> KeySelector&lt;SourceModel, Long&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Long <span class="title">getKey</span><span class="params">(SourceModel commonModel)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> commonModel.getLiveStreamId() % <span class="number">1000</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .timeWindow(Time.seconds(<span class="number">60</span>))</span><br><span class="line">                .process(<span class="keyword">new</span> ProcessWindowFunction&lt;SourceModel, SinkModel, Long, TimeWindow&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">private</span> ValueState&lt;Long&gt; playingLiveStreamNumberValueState;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                        <span class="keyword">this</span>.playingLiveStreamNumberValueState = getRuntimeContext().getState(...);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Long bucket, Context context, Iterable&lt;SourceModel&gt; iterable,</span></span></span><br><span class="line"><span class="function"><span class="params">                            Collector&lt;SinkModel&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        Long playingLiveStreamNumber = <span class="keyword">this</span>.playingLiveStreamNumberValueState.value();</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">if</span> (<span class="keyword">null</span> == playingLiveStreamNumber) &#123;</span><br><span class="line">                            playingLiveStreamNumber = <span class="number">0L</span>;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        List&lt;SourceModel&gt; sourceModels = (List&lt;SourceModel&gt;) iterable;</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">for</span> (SourceModel sourceModel : sourceModels) &#123;</span><br><span class="line">                            <span class="keyword">if</span> (BizType.I == sourceModel.getBizType()) &#123;</span><br><span class="line">                                playingLiveStreamNumber++;</span><br><span class="line">                            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                                playingLiveStreamNumber--;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">this</span>.playingLiveStreamNumberValueState.update(playingLiveStreamNumber);</span><br><span class="line"></span><br><span class="line">                        collector.collect(</span><br><span class="line">                                SinkModel.builder().build()</span><br><span class="line">                        );</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        SinkFactory.setSinkDataStream(...);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Data</span></span><br><span class="line">    <span class="meta">@Builder</span></span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SourceModel</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 直播间id</span></span><br><span class="line">        <span class="keyword">private</span> Long liveStreamId;</span><br><span class="line">        <span class="comment">// 开播时间，关播时间</span></span><br><span class="line">        <span class="keyword">private</span> Long time;</span><br><span class="line">        <span class="comment">// 主播id</span></span><br><span class="line">        <span class="keyword">private</span> Long authorId;</span><br><span class="line">        <span class="comment">// binlog 时间戳</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">long</span> binlogTimestamp;</span><br><span class="line">        <span class="comment">// 开播，关播</span></span><br><span class="line">        <span class="keyword">private</span> BizType bizType;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">enum</span> <span class="title">BizType</span> </span>&#123;</span><br><span class="line">        I, <span class="comment">// 开播</span></span><br><span class="line">        D, <span class="comment">// 关播</span></span><br><span class="line">        ;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Data</span></span><br><span class="line">    <span class="meta">@Builder</span></span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SinkModel</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 时间戳，汇总到分钟粒度</span></span><br><span class="line">        <span class="keyword">private</span> Long timestamp;</span><br><span class="line">        <span class="comment">// 指标名</span></span><br><span class="line">        <span class="keyword">private</span> String metricName;</span><br><span class="line">        <span class="comment">// 指标值</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">double</span> metricValue;</span><br><span class="line">        <span class="comment">// 维度名</span></span><br><span class="line">        <span class="keyword">private</span> String dimName;</span><br><span class="line">        <span class="comment">// 维度值</span></span><br><span class="line">        <span class="keyword">private</span> String dimValue;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文衔接上文，主要介绍直播间<strong>生产侧指标的建设</strong>，以<strong>当前分钟正在开播直播间数</strong>为代表举例。提出定义以及建设过程相关的问题，以这两个个问题出发，引出了以下两小节。</p><p>第一节简单介绍了当前分钟正在开播直播间数的定义。</p><p>第二节主要介绍了当前分钟正在开播直播间数的建设逻辑以及过程，并对<strong>状态</strong>这个概念进行了一个拓展介绍。</p><p>最后一节对本文进行了总结。</p><p>如果你也有相同的指标建设需求，或者存在一些指标建设过程中的问题，欢迎关注博主公众号，或者添加博主微信，互相交流~</p><p>记得点赞 + 再看喔~</p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从一些实际的 case 出发，分析一些生产环境中经常会遇到的问题，抛砖引玉，以帮助小伙伴们解决一些实际问题。本篇文章主要介绍直播间生产侧指标的建设过程，如果对小伙伴有帮助的话，欢迎点赞 + 再看~
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>生产实践 | Flink + 直播（二）| 如何建设实时公共画像维表？</title>
    <link href="https://yangyichao-mango.github.io/2020/11/02/wechat-blog/apache-flink:realtime-live-stream-2/"/>
    <id>https://yangyichao-mango.github.io/2020/11/02/wechat-blog/apache-flink:realtime-live-stream-2/</id>
    <published>2020-11-02T06:21:53.000Z</published>
    <updated>2021-04-04T11:11:38.241Z</updated>
    
    <content type="html"><![CDATA[<h1 id="生产实践-Flink-直播（二）-如何建设实时公共画像维表？"><a href="#生产实践-Flink-直播（二）-如何建设实时公共画像维表？" class="headerlink" title="生产实践 | Flink + 直播（二）| 如何建设实时公共画像维表？"></a>生产实践 | Flink + 直播（二）| 如何建设实时公共画像维表？</h1><blockquote><p>本系列每篇文章都是从一些实际生产实践需求出发，解决一些生产实践中的问题，抛砖引玉，以帮助小伙伴们解决一些实际生产问题。本篇文章主要介绍直播间画像实时维表建设的整个过程，如果对小伙伴有帮助的话，欢迎点赞 + 再看~</p></blockquote><h2 id="技术架构"><a href="#技术架构" class="headerlink" title="技术架构"></a>技术架构</h2><p>回顾上一节的<strong>技术架构</strong>图。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-2/tec-arc.png" alt="技术架构"></p><p>整个架构相对来说是比较好理解的。从数据源到数据处理以及最后到数据汇部分。</p><p>但是大家的疑惑点可能就集中在三个维表的建设上，包含<strong>主播用户画像维表，观众用户画像维表，直播间画像维表</strong>。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-2/dim-tec-arc.png" alt="技术架构"></p><p>我们依然从以下几个角度的问题出发，通过分析场景，解答这几个问题来给大家介绍以上三个维表的建设过程。</p><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><ul><li><strong>WHAT：直播实时公共画像维表是指什么？离线公共画像维表又指什么？区别？</strong></li><li><strong>WHY：为什么架构图中的三类公共画像维表要按照实时和离线进行划分？为什么需要建设实时公共画像维表，离线公共画像维表不能满足需求？</strong></li><li><strong>HOW：怎样才能建设满足直播实时数据的实时公共画像维表？</strong></li><li><strong>WHO：需要使用什么样的组件建设直播实时公共画像维表？为什么选用这些组件进行建设？</strong></li></ul><h2 id="WHAT：实时-amp-离线公共画像维表？"><a href="#WHAT：实时-amp-离线公共画像维表？" class="headerlink" title="WHAT：实时 &amp; 离线公共画像维表？"></a>WHAT：实时 &amp; 离线公共画像维表？</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>首先简单介绍下，<strong>实时 &amp; 离线公共画像维表</strong>中存储的内容就是实体的固有属性（比如用户的年龄等），我理解这两个词本身是高层抽象的概念，本文中介绍的<strong>主播用户画像维表，观众用户画像维表，直播间画像维表</strong>是其具体实现。</p><p>其他大佬的文章解释中会对<strong>实时公共画像维表</strong> &amp; <strong>离线公共画像维表</strong>有更加深度的理解，这里我只说明我在直播实时数据建设过程中的理解~</p><h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><p>其实这两个词的区别从名字上就可以区分出来，实时公共画像维表和离线公共画像维表的最大区别就是数据建设和应用场景要求的时效性不同。</p><h3 id="离线公共画像维表"><a href="#离线公共画像维表" class="headerlink" title="离线公共画像维表"></a>离线公共画像维表</h3><p>特点：</p><ul><li><strong>场景</strong>：适合离线场景，<strong>时效性要求比较弱</strong>的场景，为指标提供画像维度填充或者打标服务</li><li><strong>建设</strong>：一般都是以离线 t + 1 的方式进行建设</li><li><strong>应用</strong>：使用的数据为离线 t + 1 的数据</li><li><strong>举例</strong>：数据仓库中的用户画像维表，为应用层数据提供画像服务；比如不但需要统计总 uv，还需要统计分年龄段的 uv。</li></ul><h3 id="实时公共画像维表"><a href="#实时公共画像维表" class="headerlink" title="实时公共画像维表"></a>实时公共画像维表</h3><p>特点：</p><ul><li><strong>场景</strong>：适合实时场景，<strong>时效性要求比较强</strong>的场景，为指标提供画像维度填充或者打标服务</li><li><strong>建设</strong>：实时的进行建设，延迟一般在秒级别</li><li><strong>应用</strong>：使用的数据都是实时建设好的，必须可以实时获取（秒级别延迟后获取到）并使用</li></ul><h2 id="WHY：为什么建设实时公共画像维表？"><a href="#WHY：为什么建设实时公共画像维表？" class="headerlink" title="WHY：为什么建设实时公共画像维表？"></a>WHY：为什么建设实时公共画像维表？</h2><p>为什么架构图中的三类公共画像维表要按照实时和离线进行划分？为什么需要建设实时公共画像维表，离线公共画像维表不能满足需求？</p><p>这几个问题其实围绕着我们的直播实时数据建设以及应用的场景就可以展开解答。</p><p>接上篇技术架构图，其中直播实时数据需要建设的公共维表分为以下三类：</p><ul><li><strong>直播间画像维表</strong>：包含直播对应的直播类别、开播客户端、标题、开播地址等信息</li><li><strong>主播画像维表</strong>：主播对应的主播名、主播类别、性别、年龄段等</li><li><strong>观众画像维表</strong>：观众对应的观众性别、年龄段等</li></ul><h3 id="直播间画像维表"><a href="#直播间画像维表" class="headerlink" title="直播间画像维表"></a>直播间画像维表</h3><p>首先抛出结论：<strong>直播间画像都是直播间的固有属性画像，直播间画像维表的建设过程是实时的</strong>。</p><p>由于大多数直播的时长都在几小时不等，随着直播的开始，主播域观众的互动也随即产生，从而直播生产和消费的指标也开始产出，随着直播的结束，主播和观众的互动也就结束了，对应的直播生产和消费指标也就不存在了，因此直播间画像的所能提供给其他指标作为维表的价值也就快速消失了，所以直播间画像（标题，开播地址）的应用场景特点就是<strong>时效性很强</strong>。<br>因此直播间画像维表对于直播生产消费指标的建设和应用来说，需要满足可实时建设、可实时查询获取的要求。</p><h3 id="主播-amp-观众用户画像维表"><a href="#主播-amp-观众用户画像维表" class="headerlink" title="主播 &amp; 观众用户画像维表"></a>主播 &amp; 观众用户画像维表</h3><p>结论：<strong>这类画像都是用户的固有属性画像，而非直播间固有属性，和直播间是非强相关的。主播 &amp; 观众用户画像维表的建设过程可以是离线的</strong>。</p><p>无论直播间的开播关播，直播过程中的生产消费，主播画像和观众画像基本上不会产生变动。<br>（举例：大多数情况下，当已经判定一个用户的年龄段画像为 18 - 23 时，即使这个用户开了 10 场直播，或者这个用户观看了 10 场直播，其年龄段判定也基本不会有变化）。<br>因此主播用户画像维表 &amp; 观众用户画像维表对于直播生产消费指标的建设和应用来说，可以满足离线 t + 1 建设，提供数据服务进行实时获取的要求。</p><blockquote><p>Notes：</p><p>主播 &amp; 观众用户画像需要根据用户生产消费行为以及其他信息，使用到机器学习进行性别和年龄段等的用户画像信息判定产出。<br>也有非常多的场景将这类画像进行实时建设，用于实时个性化推荐等。只不过本文的直播实时数据建设对于这两类画像的时效性要求较弱，所以采用了离线的方式进行建设。</p></blockquote><h2 id="HOW-WHO：怎样建设？用什么建设？"><a href="#HOW-WHO：怎样建设？用什么建设？" class="headerlink" title="HOW + WHO：怎样建设？用什么建设？"></a>HOW + WHO：怎样建设？用什么建设？</h2><h3 id="直播间生命周期-amp-数据流转"><a href="#直播间生命周期-amp-数据流转" class="headerlink" title="直播间生命周期 &amp; 数据流转"></a>直播间生命周期 &amp; 数据流转</h3><p>直播间整个生命周期如图所示。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-2/live-stream-life-cycle.png" alt="生命周期"></p><ul><li>1.主播创建直播间，直播间进入开播的状态；</li><li>2.观众进入直播间后，在直播间内与主播进行互动；</li><li>3.最后就是主播对直播间进行关播，标识着直播间生命周期的结束状态。</li></ul><h3 id="直播间画像维表-实时"><a href="#直播间画像维表-实时" class="headerlink" title="直播间画像维表-实时"></a>直播间画像维表-实时</h3><p>实时画像维表的建设。上图中<strong>红色</strong>的字体为实时画像维表的建设和应用过程。</p><h4 id="直播间画像实时数据流转"><a href="#直播间画像实时数据流转" class="headerlink" title="直播间画像实时数据流转"></a>直播间画像实时数据流转</h4><ul><li>1.当主播开播，直播间进行直播后，直播间产生了直播间画像信息，这时可以将画像信息实时的建设到直播间画像实时维表中。<br>并且可以同时建设生产侧的实时指标，利用建设好的<strong>直播间画像实时维表 + 主播 &amp; 观众画像离线维表</strong>进行生产侧指标的维度填充；</li><li>2.当观众进入直播间后，与主播进行互动，产生一系列的消费行为，随即可以建设消费侧的实时指标，利用建设的<strong>直播间画像实时维表 + 主播 &amp; 观众画像离线维表</strong>进行消费侧指标的维度填充；</li><li>3.当主播对直播间进行关播的时候，从直播间画像实时维表中就可以对该直播间的画像进行删除。</li></ul><h4 id="组件选型"><a href="#组件选型" class="headerlink" title="组件选型"></a>组件选型</h4><p>通过上文的分析，可以了解到直播间画像实时维表建设的要求如下：</p><ul><li>实时画像：首先需要支持实时建设，实时访问；</li><li>实时画像：建设的数据都为实时指标，即要求低延迟的请求响应时间；</li><li>公共画像：需要支撑多个大流量生产消费实时任务的访问请求，即提供高 QPS 画像数据服务；</li><li>公共画像：高稳定性。</li></ul><p>因此组件选型就自然落在了高速缓存的范畴中，我们最后经过方案对比之后，选择了 redis 作为我们的实时维表的存储引擎。</p><p>使用了 redis 中的 hash 作为维表存储结构，其中直播间画像维度存储设计如下图。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-2/live-stream-dim-redis-hash.png" alt="维度存储"></p><h4 id="flink-实时维表建设代码示例"><a href="#flink-实时维表建设代码示例" class="headerlink" title="flink 实时维表建设代码示例"></a>flink 实时维表建设代码示例</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LiveStreamRealtimeDimBuilderJob</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStream&lt;<span class="keyword">byte</span>[]&gt; source = SourceFactory.getSourceDataStream();</span><br><span class="line">        source.process(<span class="keyword">new</span> ProcessFunction&lt;<span class="keyword">byte</span>[], String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(<span class="keyword">byte</span>[] bytes, Context context, Collector&lt;String&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                CommonModel c = CommonModel.parseFrom(bytes);</span><br><span class="line">                <span class="comment">// 开播</span></span><br><span class="line">                <span class="keyword">if</span> (c.isStartLiveStream()) &#123;</span><br><span class="line">                    RedisConfig</span><br><span class="line">                            .get()</span><br><span class="line">                            .hmset(c.getLiveStreamId()</span><br><span class="line">                                    , ImmutableMap.&lt;String, String&gt;builder()</span><br><span class="line">                                            .put(<span class="string">&quot;type&quot;</span>, c.getType())</span><br><span class="line">                                            .put(<span class="string">&quot;client&quot;</span>, c.getClient())</span><br><span class="line">                                            .put(<span class="string">&quot;title&quot;</span>, c.getTitle())</span><br><span class="line">                                            .put(<span class="string">&quot;address&quot;</span>, c.getAddress())</span><br><span class="line">                                            .build()</span><br><span class="line">                            );</span><br><span class="line">                    RedisConfig</span><br><span class="line">                            .get()</span><br><span class="line">                            .expire(c.getLiveStreamId(), <span class="number">30</span> * <span class="number">24</span> * <span class="number">60</span> * <span class="number">60</span>);</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (c.isEndLiveStream()) &#123;</span><br><span class="line">                <span class="comment">// 关播</span></span><br><span class="line">                    RedisConfig</span><br><span class="line">                            .get()</span><br><span class="line">                            .expire(c.getLiveStreamId(), <span class="number">2</span> * <span class="number">24</span> * <span class="number">60</span> * <span class="number">60</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Data</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CommonModel</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> String liveStreamId; <span class="comment">// 直播间 id</span></span><br><span class="line">        <span class="keyword">private</span> String type; <span class="comment">// 直播间类型</span></span><br><span class="line">        <span class="keyword">private</span> String client; <span class="comment">// 开播客户端</span></span><br><span class="line">        <span class="keyword">private</span> String title; <span class="comment">// 直播间标题</span></span><br><span class="line">        <span class="keyword">private</span> String address; <span class="comment">// 直播间开播地址</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> CommonModel <span class="title">parseFrom</span><span class="params">(<span class="keyword">byte</span>[] bytes)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 逻辑根据业务逻辑判定</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isStartLiveStream</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 逻辑根据业务逻辑判定</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isEndLiveStream</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 逻辑根据业务逻辑判定</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="主播-amp-观众用户画像维表-离线"><a href="#主播-amp-观众用户画像维表-离线" class="headerlink" title="主播 &amp; 观众用户画像维表-离线"></a>主播 &amp; 观众用户画像维表-离线</h3><p>离线画像维表的建设。主要包含主播和观众的用户画像，性别，年龄等信息。如下图<strong>蓝色</strong>的字体为离线画像维表的应用过程。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-2/live-stream-life-cycle.png" alt="生命周期"></p><h4 id="主播-amp-观众画像数据流转"><a href="#主播-amp-观众画像数据流转" class="headerlink" title="主播 &amp; 观众画像数据流转"></a>主播 &amp; 观众画像数据流转</h4><p>在产出直播间生产侧、消费侧实时数据时，使用主播 &amp; 观众画像进行了画像维度填充。</p><h4 id="存储组件"><a href="#存储组件" class="headerlink" title="存储组件"></a>存储组件</h4><p>其中离线画像维表的存储组件选型与实时相同，同为 redis，画像信息存储方式也是使用 redis hash 结构进行存储。</p><p>以 t + 1 的方式进行画像数据建设并进行数据同步，将建设好的全量主播和观众用户画像同步到 redis 高速缓存当中。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文衔接上文，主要介绍直播间实时维表的建设过程。提出几个建设的问题，以这几个问题出发，引出了一下三小节。</p><p>第一节简单介绍了实时 &amp; 离线公共画像维表的概念。</p><p>第二节从数据应用场景的角度出发，介绍了为什么需要建设实时的公共画像维表。</p><p>第三节主要介绍了实时画像维表的建设过程以及详细的技术方案。</p><p>最后一节对本文进行了总结。</p><p>如果你也建设过实时画像维表，或者有相同的需求，欢迎留言或者留下你的文章链接，相互交流~</p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从一些实际生产实践需求出发，解决一些生产实践中的问题，抛砖引玉，以帮助小伙伴们解决一些实际生产问题。本篇文章主要介绍直播间画像实时维表建设的整个过程，如果对小伙伴有帮助的话，欢迎点赞 + 再看~
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>生产实践 | 基于 Flink 的直播实时数据建设 （一）| 需求和架构篇</title>
    <link href="https://yangyichao-mango.github.io/2020/10/12/wechat-blog/apache-flink:realtime-live-stream-1/"/>
    <id>https://yangyichao-mango.github.io/2020/10/12/wechat-blog/apache-flink:realtime-live-stream-1/</id>
    <published>2020-10-12T06:21:53.000Z</published>
    <updated>2021-04-04T11:09:09.227Z</updated>
    
    <content type="html"><![CDATA[<h1 id="生产实践-基于-Flink-的直播实时数据建设-（一）-需求和架构篇"><a href="#生产实践-基于-Flink-的直播实时数据建设-（一）-需求和架构篇" class="headerlink" title="生产实践 | 基于 Flink 的直播实时数据建设 （一）| 需求和架构篇"></a>生产实践 | 基于 Flink 的直播实时数据建设 （一）| 需求和架构篇</h1><blockquote><p>本系列每篇文章都是从一些实际生产实践需求出发，解决一些生产实践中的问题，抛砖引玉，以帮助小伙伴们解决一些实际生产问题。相信大家或多或少都观看过直播，那大家有没有想过，如果自己负责建设公司内整体直播实时数据，会怎样去建设呢？本系列文章主要介绍直播实时数据建设的整个过程，如果对小伙伴有帮助的话，欢迎点赞 + 再看~</p></blockquote><h2 id="首先思考几个问题"><a href="#首先思考几个问题" class="headerlink" title="首先思考几个问题"></a>首先思考几个问题</h2><ul><li><strong>WHAT：相信大家或多或少都观看过直播，甚至自己就是一名主播或负责的业务就是直播相关的，那大家有没有思考过，在直播业务场景中，你最关心什么指标以及需要关注、建设什么数据？</strong></li><li><strong>WHY：为什么需要建设直播实时数据？离线建设不能满足吗？</strong></li><li><strong>HOW：直播实时数据怎样赋能业务的？怎样根据公司直播场景的需求去划分直播实时数据？怎样去建设直播实时数据体？</strong></li><li><strong>WHO：在建设直播实时数据的过程中，需要使用什么样的组件进行建设？每个组件都负责哪一部分？</strong></li></ul><p>让我们带着以上几个问题出发~</p><h2 id="直播-短视频，内容运营的下一个战场"><a href="#直播-短视频，内容运营的下一个战场" class="headerlink" title="直播 + 短视频，内容运营的下一个战场"></a>直播 + 短视频，内容运营的下一个战场</h2><p>随着互联网络技术的发展，网络直播受到越来越多人的关注，直播在经过几年前的喷涌式大爆发之后，近段时间热度有所降低。内容的同质化和变现困难是直播现在面临的主要问题，随着移动终端普及和网络的提速，短视频以短平快的大流量传播方式快速获得各大平台、粉丝和资本的青睐，所以众多直播软件开始接入短视频的功能。<br>同时，一些以短视频为主发展起来的 app 也在软件中加入了直播功能，直播和短视频两者互相弥补不足，相辅相成，给用户带来了更好的使用体验，也给各大平台带来更多的流量，”直播 + 短视频”的模式已经也成为新的发展趋势。</p><p>本系列文章主要围绕着直播实时数据建设而展开。本文是本系列文章的的第一篇，需求和架构篇，主要分为三个部分，按顺序为<strong>WHY - WHAT - HOW</strong>，以这三个角度出发，解答开头提出的三个问题，其中 <strong>WHO</strong>部分在本系列文章的后续建设细节章节进行介绍！</p><h2 id="WHY：为什么建设直播实时数据？"><a href="#WHY：为什么建设直播实时数据？" class="headerlink" title="WHY：为什么建设直播实时数据？"></a>WHY：为什么建设直播实时数据？</h2><p>相比短视频的生产消费来说，直播的主播和观看直播的观众的纽带都是在直播间建立的，相互之间的互动行为也都只在直播间内产生，并且通常情况下，一场直播的时长也就在几个小时之内，因此直播的生产消费时效性相比短视频会更强，因而直播数据对于实时性的诉求也就更高。</p><h2 id="WHAT：需要关注、建设什么直播实时数据？"><a href="#WHAT：需要关注、建设什么直播实时数据？" class="headerlink" title="WHAT：需要关注、建设什么直播实时数据？"></a>WHAT：需要关注、建设什么直播实时数据？</h2><p>需要关注、建设什么直播实时数据？换一句话来说就是根据<strong>数据分析业务的需求</strong>出发，决定建设什么样的直播实时数据？</p><p>直播就是一个主播和观众联络互动的纽带，其中一切操作都是围绕着主播和观众而展开的，数据分析的同学都会以这个最基础的角度出发进行分析，因此首先我们就可以将整个直播的数据按照<strong>直播生产</strong>和<strong>直播消费</strong>进最基本的划分。</p><p>除此角度之外，数据分析的同学也还会从<strong>全局直播业务洞察</strong>和<strong>单个直播间洞察</strong>不同粒度上进行分析洞察，因此还可以按照<strong>大盘数据</strong>、<strong>单直播间数据</strong>进行划分。</p><p>从这两个角度出发，基本可以涵盖对于直播业务分析场景的诉求，因此直播实时数据也自然可以从这两个角度进行划分和建设。</p><p>综上则整体<strong>直播实时数据业务划分和赋能应用架构</strong>如下图所示。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-1/biz-arc.png" alt="业务划分和应用架构"></p><p>其中</p><p><strong>直播大盘实时数据</strong>在宏观上监控直播业务，提供预测大盘的能力；其中分钟粒度时间序列可快速定位直播各行为的高峰时刻，可以基于该时刻进行详细归因。除此之外，当直播在做运营活动时，也能快速基于实时数据来看运营活动的活动效果，赋能活动策略实时优化。</p><p><strong>单直播间直播实时数据</strong>可以以细粒度监控单直播间的直播业务，用来在直播过程中对外输出直播数据战报、以及可基于数据战报效果实时对单直播间进资源投放进行实时效果评估和合理调配。</p><p>详细的直播实时数据需求和样例如下文。</p><h3 id="大盘"><a href="#大盘" class="headerlink" title="大盘"></a>大盘</h3><p><strong>生产侧</strong></p><ul><li><strong>指标</strong>：总体开播直播间数…</li><li><strong>维度</strong>：直播间画像、主播用户画像</li><li><strong>举例</strong>：[开播直播间为游戏类直播]的[总开播主播数]</li></ul><p><strong>消费侧</strong></p><ul><li><strong>指标</strong>：总体观众观看、点赞、评论数…</li><li><strong>维度</strong>：观众用户画像、日志上报其他维度</li><li><strong>举例</strong>：[目前在河北观看直播]的[总观众数]</li></ul><h3 id="单直播间"><a href="#单直播间" class="headerlink" title="单直播间"></a>单直播间</h3><p><strong>生产侧</strong><br>单直播间一般都是一些画像信息，所以此类指标较少，暂时不做讨论。</p><p><strong>消费侧</strong></p><ul><li><strong>指标</strong>：单直播间观众观看、点赞、评论数…</li><li><strong>维度</strong>：观众用户画像、日志上报其他维度</li><li><strong>举例</strong>：某直播间[18-23岁年龄段]的[总观众数]</li></ul><p>目前已经了解了要建设直播实时数据都包含了什么内容，接下来就是大干一场的时候了。</p><h2 id="HOW：怎样去建设？"><a href="#HOW：怎样去建设？" class="headerlink" title="HOW：怎样去建设？"></a>HOW：怎样去建设？</h2><p>怎样去建设？换一句话来说就是从技术的角度出发，怎样将<strong>直播实时数据的业务需求</strong>转化为<strong>直播实时数据的技术方案</strong>进行落地？</p><p>从技术角度出发，上述直播实时数据需要建设的需求内容总结下来就是一个词：<strong>直播实时多维指标</strong>。</p><h3 id="多维"><a href="#多维" class="headerlink" title="多维"></a>多维</h3><p>即产出指标是多维度的，包含公共维度和非公共维度。</p><p>第一类是<strong>公共维度</strong>。包含三部分，直播间画像，主播用户画像，观众用户画像，公共两字代表这类维度是可以被多个指标进行共享使用的。举例：某直播间开播之后，该直播间画像只需要一次建设，就可以被多个指标多次重复使用，不但可以作为大盘侧生产、消费指标的维度，也可以作为单直播间生产、消费指标的维度。</p><p>第二类是<strong>非公共维度</strong>。非公共维度是和特定消费行为绑定的，也就是和某个指标绑定的，随着日志上报一同上报的维度。举例：某观众观看直播时的客户端类型（安卓？IOS？），观看直播时的省份等维度，这类维度只和当前的消费行为相关，不能被其他指标所共享。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-1/dim.png" alt="多维"></p><h3 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h3><p>其实都是 pv，uv 类指标。简单理解就是各个维度下对应的 xx 量。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-1/metric.png" alt="指标"></p><h3 id="实时数据建设技术架构"><a href="#实时数据建设技术架构" class="headerlink" title="实时数据建设技术架构"></a>实时数据建设技术架构</h3><p>对应到直播实时数据建设的过程主要包含两部分：公共部分和非公共部分。</p><p>公共部分就是实时公共维表的建设。</p><p>非公共部分就是指标非公共维度以及对应生产、消费指标建设。</p><p>直接给出总体<strong>技术架构</strong>图，本系列后续的文章进行介绍这样进行整体架构设计的详细原因。</p><p><img src="/blog-img/apache-flink:realtime-live-stream-1/tec-arc.png" alt="技术架构"></p><p>简单说明下。</p><p>其中数据源包含生产侧，消费侧数据源；</p><p>数据处理部分包含公共实时维表建设，和指标建设，其中一部分公共维表的建设也使用了离线的方式提供了支持；</p><p>最后就是数据汇部分，产出了生产侧，消费侧的多维指标供数据分析师使用。</p><h2 id="下节预告"><a href="#下节预告" class="headerlink" title="下节预告"></a>下节预告</h2><p>下节主要介绍<strong>直播实时公共画像的建设</strong>，其中是技术架构图中的<strong>主播用户、关注用户画像、以及直播间画像</strong>的建设方案。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文首先提出了几个关于直播实时数据建设的问题。以这几个问题触发，引出了一下三小节。</p><p>第一节简单介绍了直播时效性强的原因，因此直播对于实时数据的需求更加强烈。</p><p>第二节从数据分析的角度出发，引出了我们需要建设的直播实时数据都包含哪些内容，并且从大盘/单直播间，生产/消费角度进行了模块划分。</p><p>第三节对数据需求进行了技术方案的整体架构设计。</p><p>最后一节对本文进行了总结。</p><p>如果你也有相同的建设需求或者你以及建设了直播实时数据，欢迎留言或者留下你的文章链接，相互交流~</p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都比较短小，不定期更新，从一些实际的 case 出发抛砖引玉，提高小伙伴的姿♂势水平。本文介绍 Flink sink schema 字段设计小技巧，阅读时长大概 2 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>前沿 | 流批一体的一些理解</title>
    <link href="https://yangyichao-mango.github.io/2020/10/12/wechat-blog/apache-flink:stream-batch-integration/"/>
    <id>https://yangyichao-mango.github.io/2020/10/12/wechat-blog/apache-flink:stream-batch-integration/</id>
    <published>2020-10-12T06:21:53.000Z</published>
    <updated>2021-06-28T00:13:31.975Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前沿-流批一体的一些理解"><a href="#前沿-流批一体的一些理解" class="headerlink" title="前沿 | 流批一体的一些理解"></a>前沿 | 流批一体的一些理解</h1><blockquote><p>每家数字化企业在目前遇到流批一体概念的时候，都会对这个概念抱有一些疑问，到底什么是流批一体？这个概念的来源？这个概念能为用户、开发人员以及企业带来什么样的好处？跟随着博主的理解和脑洞出发吧。</p></blockquote>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都比较短小，不定期更新，从一些博主的脑洞想法出发抛砖引玉，提高小伙伴的姿♂势水平。本文介绍博主对流批一体来源以及未来发展方向的一些理解，阅读时长大概 2 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>前沿 | 流批一体的一些理解</title>
    <link href="https://yangyichao-mango.github.io/2020/10/12/wechat-blog/read/"/>
    <id>https://yangyichao-mango.github.io/2020/10/12/wechat-blog/read/</id>
    <published>2020-10-12T06:21:53.000Z</published>
    <updated>2021-06-28T02:54:41.390Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前沿-流批一体的一些理解"><a href="#前沿-流批一体的一些理解" class="headerlink" title="前沿 | 流批一体的一些理解"></a>前沿 | 流批一体的一些理解</h1><blockquote><p>每家数字化企业在目前遇到流批一体概念的时候，都会对这个概念抱有一些疑问，到底什么是流批一体？这个概念的来源？这个概念能为用户、开发人员以及企业带来什么样的好处？跟随着博主的理解和脑洞出发吧。</p></blockquote><p>自流程化：此前没有人正式提过这一说法，在与政府、金融客户沟通时经常会提到：当业务能够实现对象数字化、规则数字化、结果数据化时，业务自身的流程也就可以按照规则自由、自行组建和优化了。</p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都比较短小，不定期更新，从一些博主的脑洞想法出发抛砖引玉，提高小伙伴的姿♂势水平。本文介绍博主对流批一体来源以及未来发展方向的一些理解，阅读时长大概 2 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>踩坑记 | Flink 天级别窗口中存在的时区问题</title>
    <link href="https://yangyichao-mango.github.io/2020/10/03/wechat-blog/apache-flink:realtime-tips-3-utc/"/>
    <id>https://yangyichao-mango.github.io/2020/10/03/wechat-blog/apache-flink:realtime-tips-3-utc/</id>
    <published>2020-10-03T06:21:53.000Z</published>
    <updated>2021-04-04T11:12:43.580Z</updated>
    
    <content type="html"><![CDATA[<h1 id="踩坑记-Flink-天级别窗口中存在的时区问题"><a href="#踩坑记-Flink-天级别窗口中存在的时区问题" class="headerlink" title="踩坑记 | Flink 天级别窗口中存在的时区问题"></a>踩坑记 | Flink 天级别窗口中存在的时区问题</h1><blockquote><p>本系列每篇文章都是从一些实际的 case 出发，分析一些生产环境中经常会遇到的问题，抛砖引玉，以帮助小伙伴们解决一些实际问题。本文介绍 Flink 时间以及时区问题，分析了在天级别的窗口时会遇到的时区问题，如果对小伙伴有帮助的话，欢迎点赞 + 再看~</p></blockquote><p>本文主要分为两部分：</p><p>第一部分（第 1 - 3 节）的分析主要针对 flink，分析了 flink 天级别窗口的中存在的时区问题以及解决方案。</p><p>第二部分（第 4 节）的分析可以作为所有时区问题的分析思路，主要以解决方案中的时区偏移量为什么是加 8 小时为案例做了通用的深度解析。</p><p>为了让读者能对本文探讨的问题有一个大致了解，本文先给出问题 sql，以及解决方案。后文给出详细的分析~</p><h2 id="1-问题以及解决方案"><a href="#1-问题以及解决方案" class="headerlink" title="1.问题以及解决方案"></a>1.问题以及解决方案</h2><h3 id="问题-sql"><a href="#问题-sql" class="headerlink" title="问题 sql"></a>问题 sql</h3><p>sql 很简单，用来统计当天累计 uv。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--------------- 伪代码 ---------------</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span></span><br><span class="line">  kafka_sink_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="comment">-- 窗口开始时间</span></span><br><span class="line">  <span class="built_in">CAST</span>(</span><br><span class="line">    TUMBLE_START(proctime, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>) <span class="keyword">AS</span> <span class="type">bigint</span></span><br><span class="line">  ) <span class="keyword">AS</span> window_start,</span><br><span class="line">  <span class="comment">-- 当前记录处理的时间</span></span><br><span class="line">  <span class="built_in">cast</span>(<span class="built_in">max</span>(proctime) <span class="keyword">AS</span> <span class="type">BIGINT</span>) <span class="keyword">AS</span> current_ts,</span><br><span class="line">  <span class="comment">-- 每个桶内的 uv</span></span><br><span class="line">  <span class="built_in">count</span>(<span class="keyword">DISTINCT</span> id) <span class="keyword">AS</span> part_daily_full_uv</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  kafka_source_table</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  <span class="built_in">mod</span>(id, bucket_number),</span><br><span class="line">  <span class="comment">-- bucket_number 为常数，根据具体场景指定具体数值</span></span><br><span class="line">  TUMBLE(proctime, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>)</span><br><span class="line"><span class="comment">--------------- 伪代码 ---------------</span></span><br></pre></td></tr></table></figure><p>你是否能一眼看出这个 sql 所存在的问题？（PS：数据源以及数据汇时区都为东八区）</p><p><strong>没错，天级别窗口所存在的时区问题，即这段代码统计的不是楼主所在东八区一整天数据的 uv，这段代码统计的一整天的范围在东八区是第一天早 8 点至第二天早 8 点。</strong></p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>楼主目前所处时区为东八区，解决方案如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--------------- 伪代码 ---------------</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> view_table <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">   id,</span><br><span class="line">   <span class="comment">-- 通过注入时间解决</span></span><br><span class="line">   <span class="comment">-- 加上东八区的时间偏移量，设置注入时间为时间戳列</span></span><br><span class="line">   <span class="built_in">CAST</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">AS</span> <span class="type">BIGINT</span>) <span class="operator">*</span> <span class="number">1000</span> <span class="operator">+</span> <span class="number">8</span> <span class="operator">*</span> <span class="number">60</span> <span class="operator">*</span> <span class="number">60</span> <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">as</span> ingest_time</span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">   source_table;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span></span><br><span class="line">  target_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="built_in">CAST</span>(</span><br><span class="line">    TUMBLE_START(ingest_time, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>) <span class="keyword">AS</span> <span class="type">bigint</span></span><br><span class="line">  ) <span class="keyword">AS</span> window_start,</span><br><span class="line">  <span class="built_in">cast</span>(<span class="built_in">max</span>(ingest_time) <span class="keyword">AS</span> <span class="type">BIGINT</span>) <span class="operator">-</span> <span class="number">8</span> <span class="operator">*</span> <span class="number">3600</span> <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">AS</span> current_ts,</span><br><span class="line">  <span class="built_in">count</span>(<span class="keyword">DISTINCT</span> id) <span class="keyword">AS</span> part_daily_full_uv</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  view_table</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  <span class="built_in">mod</span>(id, <span class="number">1024</span>),</span><br><span class="line">   <span class="comment">-- 根据注入时间划分天级别窗口</span></span><br><span class="line">  TUMBLE(ingest_time, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>)</span><br><span class="line"><span class="comment">--------------- 伪代码 ---------------</span></span><br></pre></td></tr></table></figure><p>通过上述方案，就可以将统计的数据时间范围调整为东八区的今日 0 点至明日 0 点。下文详细说明整个需求场景以及解决方案的实现和分析过程。</p><h2 id="2-需求场景以及实现方案"><a href="#2-需求场景以及实现方案" class="headerlink" title="2.需求场景以及实现方案"></a>2.需求场景以及实现方案</h2><h3 id="需求场景"><a href="#需求场景" class="headerlink" title="需求场景"></a>需求场景</h3><p>coming，需求场景比较简单，就是消费上游的一个埋点日志数据源，根据埋点中的 id 统计当天 0 点至当前时刻的累计 uv，按照分钟级别产出到下游 OLAP 引擎中进行简单的聚合，最后在 BI 看板进行展示，没有任何维度字段（感动到哭😭）。</p><h3 id="数据链路以及组件选型"><a href="#数据链路以及组件选型" class="headerlink" title="数据链路以及组件选型"></a>数据链路以及组件选型</h3><p>客户端用户行为埋点日志 -&gt; logServer -&gt; kafka -&gt; flink（sql） -&gt; kafka -&gt; druid -&gt; BI 看板。</p><p>实现方案以及具体的实现方式很多，这次使用的是 sql API。</p><h3 id="flink-sql-schema"><a href="#flink-sql-schema" class="headerlink" title="flink sql schema"></a>flink sql schema</h3><p>source 和 sink 表 schema 如下（只保留关键字段）：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--------------- 伪代码 ---------------</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> kafka_sink_table (</span><br><span class="line">  <span class="comment">-- 天级别窗口开始时间</span></span><br><span class="line">  window_start <span class="type">BIGINT</span>,</span><br><span class="line">  <span class="comment">-- 当前记录处理的时间</span></span><br><span class="line">  current_ts <span class="type">BIGINT</span>,</span><br><span class="line">  <span class="comment">-- 每个桶内的 uv（处理过程对 id 进行了分桶）</span></span><br><span class="line">  part_daily_full_uv <span class="type">BIGINT</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"> <span class="comment">-- ... </span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> kafka_source_table (</span><br><span class="line">  <span class="comment">-- ... </span></span><br><span class="line">  <span class="comment">-- 需要进行 uv 计算的 id</span></span><br><span class="line">  id <span class="type">BIGINT</span>,</span><br><span class="line">  <span class="comment">-- 处理时间</span></span><br><span class="line">  proctime <span class="keyword">AS</span> PROCTIME()</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="comment">-- ... </span></span><br><span class="line">);</span><br><span class="line"><span class="comment">--------------- 伪代码 ---------------</span></span><br></pre></td></tr></table></figure><h3 id="flink-sql-transform"><a href="#flink-sql-transform" class="headerlink" title="flink sql transform"></a>flink sql transform</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--------------- 伪代码 ---------------</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span></span><br><span class="line">  kafka_sink_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="comment">-- 窗口开始时间</span></span><br><span class="line">  <span class="built_in">CAST</span>(</span><br><span class="line">    TUMBLE_START(proctime, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>) <span class="keyword">AS</span> <span class="type">bigint</span></span><br><span class="line">  ) <span class="keyword">AS</span> window_start,</span><br><span class="line">  <span class="comment">-- 当前记录处理的时间</span></span><br><span class="line">  <span class="built_in">cast</span>(<span class="built_in">max</span>(proctime) <span class="keyword">AS</span> <span class="type">BIGINT</span>) <span class="keyword">AS</span> current_ts,</span><br><span class="line">  <span class="comment">-- 每个桶内的 uv</span></span><br><span class="line">  <span class="built_in">count</span>(<span class="keyword">DISTINCT</span> id) <span class="keyword">AS</span> part_daily_full_uv</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  kafka_source_table</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  <span class="built_in">mod</span>(id, bucket_number),</span><br><span class="line">  <span class="comment">-- bucket_number 为常数，根据具体场景指定具体数值</span></span><br><span class="line">  TUMBLE(proctime, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>)</span><br><span class="line"><span class="comment">--------------- 伪代码 ---------------</span></span><br></pre></td></tr></table></figure><p>使用 early-fire 机制（同 DataStream API 中的 ContinuousProcessingTimeTrigger），并设定触发间隔为 60 s。</p><p>在上述实现 sql 中，我们对 id 进行了分桶，那么每分钟输出的数据条数即为 bucket_number 条，最终在 druid 中按照分钟粒度将所有桶的数据进行 sum 聚合，即可得到从当天 0 点累计到当前分钟的全量 uv。</p><h3 id="时区问题"><a href="#时区问题" class="headerlink" title="时区问题"></a>时区问题</h3><blockquote><p>激情场景还原：</p><p><strong>头文字 ∩ 技术小哥哥</strong>：使用 sql，easy game，闲坐摸鱼…</p><p><strong>头文字 ∩ 技术小哥哥</strong>：等到 <strong>00:00</strong> 时，发现指标还在不停地往上涨，难道是 sql 逻辑错了，不应该啊，试过分钟，小时级别窗口都木有这个问题</p><p><strong>头文字 ∩ 技术小哥哥</strong>：抠头ing，算了，稍后再分析这个问题吧，现在还有正事要干😏</p><p><strong>头文字 ∩ 技术小哥哥</strong>：到了早上，瞅了一眼配置的时间序列报表，发现在 <strong>08:00</strong> 点的时候指标归零，重新开始累计。想法一闪而过，东八区？（当时为啥没 format 下 sink 数据中的 window_start…）</p></blockquote><h2 id="3-问题定位"><a href="#3-问题定位" class="headerlink" title="3.问题定位"></a>3.问题定位</h2><h3 id="问题说明"><a href="#问题说明" class="headerlink" title="问题说明"></a>问题说明</h3><p>flink 在使用时间的这个概念的时候是基于 java 时间纪元（即格林威治 1970/01/01 00:00:00，也即 Unix 时间戳为 0）概念的，窗口对齐以及触发也是基于 <a href="https://cloud.tencent.com/developer/article/1447368" title="java 时间纪元">java 时间纪元</a>。</p><h3 id="问题场景复现"><a href="#问题场景复现" class="headerlink" title="问题场景复现"></a>问题场景复现</h3><p>可以通过直接查看 sink 数据的 window_start 得出上述结论。</p><p>但为了还原整个过程，我们按照如下 source 和 sink 数据进行整个问题的复现：</p><p>source 数据如下：<br>| id       | proctime | proctime UTC + 0（格林威治） 格式化时间 | proctime UTC + 8（北京） 格式化时间<br>| ——— | – | – | – |<br>| 1     | 1599091140000     | 2020/09/02 23:59:00     |  2020/09/03 07:59:00  |<br>| 2     | 1599091140000     | 2020/09/02 23:59:00     |  2020/09/03 07:59:00  |<br>| 3     | 1599091140000     | 2020/09/02 23:59:00     |  2020/09/03 07:59:00  |<br>| 1     | 1599091200000     | 2020/09/03 00:00:00     |  2020/09/03 08:00:00  |<br>| 2     | 1599091200000     | 2020/09/03 00:00:00     |  2020/09/03 08:00:00  |<br>| 3     | 1599091260000     | 2020/09/03 00:01:00     |  2020/09/03 08:01:00  |</p><p>sink 数据（<strong>为了方便理解，直接按照 druid 聚合之后的数据展示</strong>）：<br>| window_start       | current_ts | part_daily_full_uv | window_start  UTC + 8（北京） 格式化时间 | current_ts  UTC + 8（北京） 格式化时间<br>| ——— | – | ———– | ———– | ———– |<br>| 1599004800000     |  1599091140000  | 3 | 2020/09/02 08:00:00 | 2020/09/03 07:59:00 |<br>| 1599091200000     |  1599091200000  | 2 | 2020/09/03 08:00:00 | 2020/09/03 08:00:00 |<br>| 1599091200000     |  1599091260000  | 3 | 2020/09/03 08:00:00 | 2020/09/03 08:01:00 |</p><p>从上述数据可以发现，天级别窗口<strong>开始时间</strong>在 UTC + 8（北京）的时区是每天早上 8 点，即 UTC + 0（格林威治）的凌晨 0 点。</p><p><strong>下文先给出解决方案，然后详细解析各个时间以及时区概念~</strong></p><h3 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h3><ul><li><strong>框架层面解决</strong>：<a href="https://www.alibabacloud.com/help/zh/doc-detail/96910.htm?spm=a2c63.p38356.b99.48.28613830DXb6FQ" title="Blink Planner 时区设置">Blink Planner 支持时区设置</a></li><li><strong>sql层面解决</strong>：从 sql 实现层面给出解决方案</li></ul><h3 id="sql-层面解决方案"><a href="#sql-层面解决方案" class="headerlink" title="sql 层面解决方案"></a>sql 层面解决方案</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--------------- 伪代码 ---------------</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> view_table <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">   id,</span><br><span class="line">   <span class="comment">-- 通过注入时间解决</span></span><br><span class="line">   <span class="comment">-- 加上东八区的时间偏移量，设置注入时间为时间戳列</span></span><br><span class="line">   <span class="built_in">CAST</span>(<span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">AS</span> <span class="type">BIGINT</span>) <span class="operator">*</span> <span class="number">1000</span> <span class="operator">+</span> <span class="number">8</span> <span class="operator">*</span> <span class="number">60</span> <span class="operator">*</span> <span class="number">60</span> <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">as</span> ingest_time</span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">   source_table;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span></span><br><span class="line">  target_table</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="built_in">CAST</span>(</span><br><span class="line">    TUMBLE_START(ingest_time, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>) <span class="keyword">AS</span> <span class="type">bigint</span></span><br><span class="line">  ) <span class="keyword">AS</span> window_start,</span><br><span class="line">  <span class="built_in">cast</span>(<span class="built_in">max</span>(ingest_time) <span class="keyword">AS</span> <span class="type">BIGINT</span>) <span class="operator">-</span> <span class="number">8</span> <span class="operator">*</span> <span class="number">3600</span> <span class="operator">*</span> <span class="number">1000</span> <span class="keyword">AS</span> current_ts,</span><br><span class="line">  <span class="built_in">count</span>(<span class="keyword">DISTINCT</span> id) <span class="keyword">AS</span> part_daily_full_uv</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  view_table</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  <span class="built_in">mod</span>(id, <span class="number">1024</span>),</span><br><span class="line">   <span class="comment">-- 根据注入时间划分天级别窗口</span></span><br><span class="line">  TUMBLE(ingest_time, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">DAY</span>)</span><br><span class="line"><span class="comment">--------------- 伪代码 ---------------</span></span><br></pre></td></tr></table></figure><p>我目前所属的时区是东八区（北京时间），通过上述 sql，设置注入时间，并对注入时间加上 8 小时的偏移量进行天级别窗口的划分，就可以对此问题进行解决（也可以在 create table 时，在 schema 中根据计算列添加对应的注入时间戳进行解决）。如果你在 sql 层面有更好的解决方案，欢迎讨论~</p><blockquote><p>Notes：</p><ul><li><strong>东 n 区的解决方案就是时间戳 +n * 3600 秒的偏移量，西 n 区的解决方案就是时间戳 -n * 3600 秒的偏移量</strong></li><li><strong>DataStream API 存在相同的天级别窗口时区问题</strong></li></ul></blockquote><p>这里提出一个问题，为什么东八区是需要在时间戳上加 8 小时偏移量进行天级别窗口计算，而不是减 8 小时或是加上 32（24 + 8） 小时，小伙伴们有详细分析过嘛~</p><p>根据上述问题，引出本文的第二大部分，即深度解析时区偏移量问题，这部分可以作为所有时区问题的分析思路。</p><h2 id="4-为什么东八区是加-8-小时？"><a href="#4-为什么东八区是加-8-小时？" class="headerlink" title="4.为什么东八区是加 8 小时？"></a>4.为什么东八区是加 8 小时？</h2><h3 id="时间和时区基本概念"><a href="#时间和时区基本概念" class="headerlink" title="时间和时区基本概念"></a>时间和时区基本概念</h3><p><strong><a href="https://baike.baidu.com/item/%E6%97%B6%E5%8C%BA/491122?fr=aladdin" title="时区">时区</a></strong>：由于世界各国家与地区经度不同，地方时也有所不同，因此会划分为不同的时区。</p><p><strong><a href="https://baike.baidu.com/item/unix%E6%97%B6%E9%97%B4%E6%88%B3/2078227?fr=aladdin" title="Unix 时间戳(Unix timestamp)">Unix 时间戳(Unix timestamp)</a></strong>： Unix 时间戳(Unix timestamp)，或称 Unix 时间(Unix time)、POSIX 时间(POSIX time)，是一种时间表示方式，定义为从格林威治时间 1970 年 01 月 01 日 00 时 00 分 00 秒（UTC/GMT的午夜）起至现在的总秒数。<br>Unix 时间戳不仅被使用在 Unix 系统、类 Unix 系统中，也在许多其他操作系统中被广泛采用。</p><p><strong>GMT</strong>：Greenwich Mean Time 格林威治标准时间。这是以英国格林威治天文台观测结果得出的时间，这是英国格林威治当地时间，这个地方的当地时间过去被当成世界标准的时间。</p><p><strong>UT</strong>：Universal Time 世界时。根据原子钟计算出来的时间。</p><p><strong>UTC</strong>：Coordinated Universal Time 协调世界时。因为地球自转越来越慢，每年都会比前一年多出零点几秒，每隔几年协调世界时组织都会给世界时 +1 秒，让基于原子钟的世界时和基于天文学（人类感知）的格林威治标准时间相差不至于太大。并将得到的时间称为 UTC，这是现在使用的世界标准时间。<br>协调世界时不与任何地区位置相关，也不代表此刻某地的时间，所以在说明某地时间时要加上时区也就是说 GMT 并不等于 UTC，而是等于 UTC + 0，只是格林威治刚好在 0 时区上。</p><h3 id="白话时间和时区"><a href="#白话时间和时区" class="headerlink" title="白话时间和时区"></a>白话时间和时区</h3><p>当时看完这一系列的时间以及时区说明之后我大脑其实是一片空白。…ojbk…，我用自己现在的一些理解，尝试将上述所有涉及到时间的概念解释一下。</p><ul><li><strong>GMT</strong>：格林威治标准时间。</li><li><strong>UTC</strong>：基于原子钟协调之后的世界标准时间。可以认为 UTC 时间和格林威治标准时间一致。即 GMT = UTC + 0，其中 0 代表格林威治为 0 时区。</li><li><strong>时区</strong>：逆向思维来解释下（只从技术层面解释，不从其他复杂层面解释），没有时区划分代表着全世界都是同一时区，那么同一时刻看到的外显时间是一样的。举个🌰：假如全世界都按照格林威治时间作为统一时间，在格林威治时间 0 点时，对于北京和加拿大的两个同学来说，这两个同学感知到的是北京是太阳刚刚升起（清晨），加拿大是太阳刚刚落下（傍晚）。<br>但是由于没有时区划分，这两个同学看到的时间都是 0 点，因此这是不符合人类对<strong>感知到的时间</strong>和自己<strong>看到的时间</strong>的理解的。所以划分时区之后，可以满足北京（东八区 UTC + 8）同学看到的时间是上午 8 点，加拿大（西四区 UTC - 4）同学看到的时间是下午 8 点。注意时区的划分是和 UTC 绑定的。东八区即 UTC + 8。</li><li><strong>flink 时间</strong>：flink 使用的时间基于 java 时间纪元（GMT 1970/01/01 00:00:00，UTC + 0 1970/01/01 00:00:00）。</li><li><strong>Unix 时间戳</strong>：世界上任何一个地方，同时接收到的数据的对应的 Unix 时间戳都是相同的，类似时区中我们举的不分时区的🌰，全世界同一时刻的 Unix 时间戳一致。</li><li><strong>Unix 时间戳为 0</strong>：对应的格林威治时间：1970-01-01 00:00:00，对应的北京时间（东八区）：1970-01-01 08:00:00**</li></ul><p>概念关系如图所示：</p><h3 id="为什么东八区是加-8-小时？"><a href="#为什么东八区是加-8-小时？" class="headerlink" title="为什么东八区是加 8 小时？"></a>为什么东八区是加 8 小时？</h3><p>下述表格只对一些重要的时间进行了标注：</p><table><thead><tr><th>Unix 时间戳</th><th>格林威治时间（外显）</th><th>北京时间（外显）</th></tr></thead><tbody><tr><td>-8 * 3600</td><td>-</td><td>1970/01/01 00:00:00</td></tr><tr><td>0</td><td>1970/01/01 00:00:00</td><td>1970/01/01 08:00:00</td></tr><tr><td>16 * 3600</td><td>-</td><td>1970/01/02 00:00:00</td></tr><tr><td>24 * 3600</td><td>1970/01/02 00:00:00</td><td>-</td></tr></tbody></table><p>拿第一条数据解释下，其代表在北京时间 1970/01/01 00:00:00 时，生成的一条数据所携带的 Unix 时间戳为 -8 * 3600。</p><p>根据需求和上图和上述表格内容，我们可以得到如下推导过程：</p><ul><li><p>需求场景是统计一个整天的 uv，即天级别窗口，比如统计北京时间 1970/01/01 00:00:00 - 1970/01/02 00:00:00 范围的数据时，这个日期范围内的数据所携带的 Unix 时间戳范围为 -8 * 3600 到 16 * 3600</p></li><li><p>对于 flink 来说，默认情况下它所能统计的一个整天的 Unix 时间戳的范围是 0 到 24 * 3600</p></li><li><p>所以当我们想通过 flink 实现正确统计北京时间（1970/01/01 00:00:00 - 1970/01/02 00:00:00）范围内的数据时，即统计 Unix 时间戳为 -8 * 3600 到 16 * 3600 的数据时，就需要对时间戳做个映射。</p></li><li><p>映射方法如下，就是将整体范围内的时间戳做在时间轴上做平移映射，就是把 -8 * 3600 映射到 0，16 * 3600 映射到 24 * 3600。相当于是对北京时间的 Unix 时间戳整体加 8 * 3600。</p></li><li><p>最后在产出的时间戳上把加上的 8 小时再减掉（因为外显时间会自动按照时区对 Unix 时间戳进行格式化）。</p></li></ul><blockquote><p>Notes：</p><ul><li><strong>可以加 32 小时吗？答案是可以。在东八区，对于天级别窗口的划分，加 8 小时和加 8 + n * 24（其中 n 为整数）小时后进行的天级别窗口划分和计算的效果是一样的，flink 都会将东八区的整一天内的数据划分到一个天级别窗口内。所以加 32（8 + 24），56（8 + 48），-16（8 - 24）小时效果都相同，上述例子只是选择了时间轴平移最小的距离，即 8 小时。注意某些系统的 Unix 时间戳为负值时会出现异常。</strong></li><li><strong>此推理过程适用于所有遇到时区问题的场景，如果你也有其他应用场景有这个问题，也可以按照上述方式解决</strong></li></ul></blockquote><h3 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h3><p>求输入 Unix 时间戳对应的东八区每天 0 点的 Unix 时间戳。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> ONE_DAY_MILLS = <span class="number">24</span> * <span class="number">60</span> * <span class="number">60</span> * <span class="number">1000L</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">long</span> <span class="title">transform</span><span class="params">(<span class="keyword">long</span> timestamp)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> timestamp - (timestamp + <span class="number">8</span> * <span class="number">60</span> * <span class="number">60</span> * <span class="number">1000</span>) % ONE_DAY_MILLS;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h2><p>本文首先介绍了直接给出了我们的问题 sql 和解决方案。</p><p>第二节从需求场景以及整个数据链路的实现方案出发，解释了我们怎样使用 flink sql 进行了需求实现，并进而引出了 sql 中天级别窗口存在的时区问题。</p><p>第三节确认了天级别窗口时区问题原因，引出了 flink 使用了 java 时间纪元，并针对此问题给出了引擎层面和 sql 层面的解决方案。也进而提出了一个问题：为什么我们的解决方案是加 8 小时偏移量？</p><p>第四节针对加 8 小时偏移量的原因进行了分析，并详细阐述了时区，UTC，GMT，Unix 时间戳之间的关系。</p><p>最后一节对本文进行了总结。</p><p>如果你有更方便的时区偏移量理解方式，欢迎留言~</p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都是从一些实际的 case 出发，分析一些生产环境中经常会遇到的问题，抛砖引玉，以帮助小伙伴们解决一些实际问题。本文介绍 Flink 时间以及时区问题，分析了在天级别的窗口时会遇到的时区问题，如果对小伙伴有帮助的话，欢迎点赞 + 再看~
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Tips | Flink 使用 union 代替 join、cogroup</title>
    <link href="https://yangyichao-mango.github.io/2020/10/03/wechat-blog/apache-flink:realtime-tips-2-union-join/"/>
    <id>https://yangyichao-mango.github.io/2020/10/03/wechat-blog/apache-flink:realtime-tips-2-union-join/</id>
    <published>2020-10-03T06:21:53.000Z</published>
    <updated>2021-04-04T11:12:18.034Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Tips-Flink-使用-union-代替-join、cogroup"><a href="#Tips-Flink-使用-union-代替-join、cogroup" class="headerlink" title="Tips | Flink 使用 union 代替 join、cogroup"></a>Tips | Flink 使用 union 代替 join、cogroup</h1><blockquote><p>本系列每篇文章都比较短小，不定期更新，从一些实际的 case 出发抛砖引玉，提高小伙伴的姿♂势水平。本文介绍在满足原有需求、实现原有逻辑的场景下，在 Flink 中使用 union 代替 cogroup(或者join) ，简化任务逻辑，提升任务性能的方法，阅读时长大概一分钟，话不多说，直接进入正文！</p></blockquote><h2 id="需求场景分析"><a href="#需求场景分析" class="headerlink" title="需求场景分析"></a>需求场景分析</h2><h3 id="需求场景"><a href="#需求场景" class="headerlink" title="需求场景"></a>需求场景</h3><p>需求诱诱诱来了。。。数据产品妹妹想要统计单个短视频粒度的<strong>点赞，播放，评论，分享，举报</strong>五类实时指标，并且汇总成 photo_id、1 分钟时间粒度的实时视频消费宽表（即宽表字段至少为：<strong>photo_id + play_cnt + like_cnt + comment_cnt + share_cnt + negative_cnt + minute_timestamp</strong>）产出至实时大屏。</p><p>问题在于对同一个视频，五类视频消费行为的触发机制以及上报时间是不同，也就决定了对实时处理来说五类行为日志对应着五个不同的数据源。sql boy 们自然就想到了 join 操作将五类消费行为日志合并，可是实时 join(cogroup) 真的那么完美咩~，下文细谈。</p><h3 id="source-输入以及特点"><a href="#source-输入以及特点" class="headerlink" title="source 输入以及特点"></a>source 输入以及特点</h3><p>首先我们分析下需求中的 source 特点：</p><ul><li>photo_id 粒度 play（播放）、like（点赞）、comment（评论）、share（分享）、negative（举报）明细数据，<strong>用户播放（点赞、评论…）n 次，客户端\服务端就会上传 n 条播放（点赞、评论…）日志至数据源</strong></li><li>五类视频消费行为日志的 source schema 都为：<strong>photo_id + timestamp + 其他维度</strong></li></ul><h3 id="sink-输出以及特点"><a href="#sink-输出以及特点" class="headerlink" title="sink 输出以及特点"></a>sink 输出以及特点</h3><p>sink 特点如下：</p><ul><li>photo_id 粒度 play（播放）、like（点赞）、comment（评论）、share（分享）、negative（举报）<strong>1 分钟级别窗口聚合数据</strong></li><li>实时视频消费宽表 sink schema 为：<strong>photo_id + play_cnt + like_cnt + comment_cnt + share_cnt + negative_cnt +  minute_timestamp</strong></li></ul><h3 id="source、sink-样例数据"><a href="#source、sink-样例数据" class="headerlink" title="source、sink 样例数据"></a>source、sink 样例数据</h3><p>source 数据：<br>| photo_id       | timestamp |         user_id | 说明 |<br>| ——— | – | ———– | ———– |<br>| 1     |  2020/10/3 11:30:33  |     3 | 播放 |<br>| 1   |  2020/10/3 11:30:33  |   4 | 播放 |<br>| 1   |  2020/10/3 11:30:33  |   5 | 播放 |<br>| 1   |  2020/10/3 11:30:33  |   4 | 点赞 |<br>| 2 |  2020/10/3 11:30:33  | 5 | 点赞 |<br>| 1 |  2020/10/3 11:30:33  | 5 | 评论 |</p><p>sink 数据：<br>| photo_id       | timestamp | play_cnt | like_cnt | comment_cnt<br>| ——— | – | ———– | ———– | ———– |<br>| 1     |  2020/10/3 11:30:00  |     3 | 1 | 1 |<br>| 2   |  2020/10/3 11:30:00  |   0 | 1 | 0 |</p><p>我们已经对数据源输入和输出有了完整的分析，那就瞧瞧有什么方案可以实现上述需求吧。</p><h2 id="实现方案"><a href="#实现方案" class="headerlink" title="实现方案"></a>实现方案</h2><ul><li>方案1：<strong>本小节 cogroup 方案</strong>直接消费原始日志数据，对五类不同的视频消费行为日志使用 cogroup 或者 join 进行窗口聚合计算</li><li>方案2：对五类不同的视频消费行为日志分别单独聚合计算出分钟粒度指标数据，下游再对聚合好的指标数据按照 photo_id 进行合并</li><li>方案3：<strong>本小节 union 方案</strong>既然数据源 schema 相同，直接对五类不同的视频消费行为日志做 union 操作，在后续的窗口函数中对五类指标进行聚合计算。后文介绍 union 方案的设计过程</li></ul><p>我们先上 cogroup 方案的示例代码。</p><h3 id="cogroup"><a href="#cogroup" class="headerlink" title="cogroup"></a>cogroup</h3><p>cogroup 实现示例如下，示例代码直接使用了处理时间（也可替换为事件时间~），因此对数据源的时间戳做了简化（直接干掉）：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Cogroup</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Long -&gt; photo_id 播放一次</span></span><br><span class="line">        DataStream&lt;Long&gt; play = SourceFactory.getDataStream(xxx);</span><br><span class="line">        <span class="comment">// Long -&gt; photo_id 点赞一次</span></span><br><span class="line">        DataStream&lt;Long&gt; like = SourceFactory.getDataStream(xxx);</span><br><span class="line">        <span class="comment">// Long -&gt; photo_id 评论一次</span></span><br><span class="line">        DataStream&lt;Long&gt; comment = SourceFactory.getDataStream(xxx);</span><br><span class="line">        <span class="comment">// Long -&gt; photo_id 分享一次</span></span><br><span class="line">        DataStream&lt;Long&gt; share = SourceFactory.getDataStream(xxx);</span><br><span class="line">        <span class="comment">// Long -&gt; photo_id 举报一次</span></span><br><span class="line">        DataStream&lt;Long&gt; negative = SourceFactory.getDataStream(xxx);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Tuple3&lt;Long, Long, Long&gt; -&gt; photo_id + play_cnt + like_cnt 播放和点赞的数据合并</span></span><br><span class="line">        DataStream&lt;Tuple3&lt;Long, Long, Long&gt;&gt; playAndLikeCnt = play</span><br><span class="line">            .coGroup(like)</span><br><span class="line">            .where(KeySelectorFactory.get(Function.identity()))</span><br><span class="line">            .equalTo(KeySelectorFactory.get(Function.identity()))</span><br><span class="line">            .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">60</span>)))</span><br><span class="line">            .apply(xxx1);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Tuple4&lt;Long, Long, Long, Long&gt; -&gt; photo_id + play_cnt + like_cnt + comment_cnt 播放、点赞、评论的数据合并</span></span><br><span class="line">        DataStream&lt;Tuple4&lt;Long, Long, Long, Long, Long&gt;&gt; playAndLikeAndComment = playAndLikeCnt</span><br><span class="line">            .coGroup(comment)</span><br><span class="line">            .where(KeySelectorFactory.get(playAndLikeModel -&gt; playAndLikeModel.f0))</span><br><span class="line">            .equalTo(KeySelectorFactory.get(Function.identity()))</span><br><span class="line">            .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">60</span>)))</span><br><span class="line">            .apply(xxx2);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Tuple5&lt;Long, Long, Long, Long, Long&gt; -&gt; photo_id + play_cnt + like_cnt + comment_cnt + share_cnt 播放、点赞、评论、分享的数据合并</span></span><br><span class="line">        DataStream&lt;Tuple5&lt;Long, Long, Long, Long, Long, Long&gt;&gt; playAndLikeAndCommentAndShare = playAndLikeAndComment</span><br><span class="line">            .coGroup(share)</span><br><span class="line">            .where(KeySelectorFactory.get(playAndLikeAndCommentModel -&gt; playAndLikeAndCommentModel.f0))</span><br><span class="line">            .equalTo(KeySelectorFactory.get(Function.identity()))</span><br><span class="line">            .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">60</span>)))</span><br><span class="line">            .apply(xxx2);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Tuple7&lt;Long, Long, Long, Long, Long, Long, Long&gt; -&gt; photo_id + play_cnt + like_cnt + comment_cnt + share_cnt + negative_cnt + minute_timestamp 播放、点赞、评论、分享、举报的数据合并</span></span><br><span class="line">        <span class="comment">// 同上~</span></span><br><span class="line">        DataStream&lt;Tuple7&lt;Long, Long, Long, Long, Long, Long, Long&gt;&gt; playAndLikeAndCommentAndShare = ***;</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>粗暴一想，上面这样一搞不就结束了么，事情没那么简单，我们来做一个详细点的分析。</p><h3 id="上述实现可能会存在的问题点"><a href="#上述实现可能会存在的问题点" class="headerlink" title="上述实现可能会存在的问题点"></a>上述实现可能会存在的问题点</h3><ul><li><strong>从 flink 消费到 play 数据源的一条数据到最终产出这条数据被聚合后的数据，整个过程的数据延迟 &gt; 3 分钟…</strong></li><li><strong>如果数据源持续增加（比如添加其他视频消费操作数据源），则整个任务算子变多，数据链路更长，任务稳定性会变差，产出数据延迟也会随着窗口计算变多，延迟更久</strong></li></ul><blockquote><p><strong>数据产品妹妹</strong>：🤩，小哥哥好棒，既然问题点都分析出来了，技术小哥哥就帮人家解决一下嘛~</p><p><strong>头文字 ∩ 技术小哥哥</strong>：搞。</p><p><strong>头文字 ∩ 技术小哥哥</strong>：既然可能由于过多的窗口导致数据产出延迟，job 不稳定，那有没有什么方法减少窗口数量呢，思路转换一下。我们直接以整个 job 中只包含一个窗口算子操作为基点，逆推一下，则有以下数据链路。</p></blockquote><h3 id="逆推链路"><a href="#逆推链路" class="headerlink" title="逆推链路"></a>逆推链路</h3><p>1 - 5 为逆推的整条链路。</p><ul><li><strong>1.五类指标的数据都在单个窗口中计算</strong></li><li><strong>2.五类指标的窗口 model 相同</strong></li><li><strong>3.keyby 中的 key 一致（photo_id）</strong></li><li><strong>4.五类指标的数据源都为 photo_id 粒度，并且五类数据源的 model 都必须相同，并且可以做合并</strong></li><li><strong>5.union 算子可以对五类数据源做合并！！！</strong></li></ul><p>话不多说直接上 union 方案代码。</p><h2 id="union"><a href="#union" class="headerlink" title="union"></a>union</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Union</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Tuple2&lt;Long, String&gt; -&gt; photo_id + &quot;PLAY&quot;标签</span></span><br><span class="line">        DataStream&lt;Tuple2&lt;Long, String&gt;&gt; play = SourceFactory.getDataStream(xxx);</span><br><span class="line">        <span class="comment">// Tuple2&lt;Long, String&gt; -&gt; photo_id + &quot;LIKE&quot;标签</span></span><br><span class="line">        DataStream&lt;Tuple2&lt;Long, String&gt;&gt; like = SourceFactory.getDataStream(xxx);</span><br><span class="line">        <span class="comment">// Tuple2&lt;Long, String&gt; -&gt; photo_id + &quot;COMMENT&quot;标签</span></span><br><span class="line">        DataStream&lt;Tuple2&lt;Long, String&gt;&gt; comment = SourceFactory.getDataStream(xxx);</span><br><span class="line">        <span class="comment">// Tuple2&lt;Long, String&gt; -&gt; photo_id + &quot;SHARE&quot;标签</span></span><br><span class="line">        DataStream&lt;Tuple2&lt;Long, String&gt;&gt; share = SourceFactory.getDataStream(xxx);</span><br><span class="line">        <span class="comment">// Tuple2&lt;Long, String&gt; -&gt; photo_id + &quot;NEGATIVE&quot;标签</span></span><br><span class="line">        DataStream&lt;Tuple2&lt;Long, String&gt;&gt; negative = SourceFactory.getDataStream(xxx);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Tuple5&lt;Long, Long, Long, Long&gt; -&gt; photo_id + play_cnt + like_cnt + comment_cnt + window_start_timestamp</span></span><br><span class="line">        DataStream&lt;Tuple3&lt;Long, Long, Long&gt;&gt; playAndLikeCnt = play</span><br><span class="line">            .union(like)</span><br><span class="line">            .union(comment)</span><br><span class="line">            .union(share)</span><br><span class="line">            .union(negative)</span><br><span class="line">            .keyBy(KeySelectorFactory.get(i -&gt; i.f0))</span><br><span class="line">            .timeWindow(Time.seconds(<span class="number">60</span>))</span><br><span class="line">            .process(xxx);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以发现，无论上游数据源怎样进行变化，上述 union 方案中始终可以保持只有一个窗口算子处理和计算数据，则可以解决之前列举的数据延迟以及 flink 任务算子过多的问题。</p><p>在数据源的 schema 相同（或者不同但经过处理之后可以 format 成相同格式）的情况下，或者处理逻辑相同的话，可以使用 union 进行逻辑简化。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文首先介绍了我们的需求场景，第二部分分析了使用 cogroup（案例代码）是如何解决此需求场景，再分析了此实现方案可能会存在一些问题，并引出了 union 解决方案的逆推和设计思路。<br>在第三部分针对此场景使用 union 代替 cogroup 进行了一定程度上的优化。如果针对此场景，大佬们有更好的优化方案的话，期待留言喔。</p><p><img src="/blog-img/gzh/wechat.png" alt="公众号"></p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都比较短小，不定期更新，从一些实际的 case 出发抛砖引玉，提高小伙伴的姿♂势水平。本文介绍在满足原有需求、实现原有逻辑的场景下，在 Flink 中使用 union 代替 cogroup(或者join) ，简化任务逻辑，提升任务性能的方法，阅读时长大概一分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Tips | Flink sink schema 字段设计小技巧</title>
    <link href="https://yangyichao-mango.github.io/2020/09/12/wechat-blog/apache-flink:realtime-tips-1/"/>
    <id>https://yangyichao-mango.github.io/2020/09/12/wechat-blog/apache-flink:realtime-tips-1/</id>
    <published>2020-09-12T06:21:53.000Z</published>
    <updated>2021-04-04T11:12:18.040Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Tips-Flink-sink-schema-字段设计小技巧"><a href="#Tips-Flink-sink-schema-字段设计小技巧" class="headerlink" title="Tips | Flink sink schema 字段设计小技巧"></a>Tips | Flink sink schema 字段设计小技巧</h1><blockquote><p>本系列每篇文章都比较短小，不定期更新，从一些实际的 case 出发抛砖引玉，提高小伙伴的姿♂势水平。本文介绍 Flink sink schema 字段设计小技巧，阅读时长大概 2 分钟，话不多说，直接进入正文！</p></blockquote><h2 id="sink-schema-中添加-version-版本字段"><a href="#sink-schema-中添加-version-版本字段" class="headerlink" title="sink schema 中添加 version 版本字段"></a>sink schema 中添加 version 版本字段</h2><p>如 title，直接上实践案例和使用方式。</p><h3 id="实践案例及使用方式"><a href="#实践案例及使用方式" class="headerlink" title="实践案例及使用方式"></a>实践案例及使用方式</h3><ul><li><strong>非故障场景下产出的每条记录的 version 字段值为 1</strong></li><li><strong>故障场景下，可以在同一 sink 中产出 version &gt; 1（非 1）的数据，代表故障修复数据提供给下游消费</strong></li></ul><h3 id="可应对的故障场景"><a href="#可应对的故障场景" class="headerlink" title="可应对的故障场景"></a>可应对的故障场景</h3><p>上游 flink 任务 A 发生故障导致产出脏数据至 kafka X，并且下游消费方可以按照下面两类进行划分：</p><ul><li><strong>下游为 flink 任务</strong>：flink 任务 B 消费 kafka X 中的脏数据，结果计算并产出错误数据</li><li><strong>下游为 OLAP 引擎以及 BI 看板</strong>：结果导致看板展示数据异常</li></ul><p>首先介绍下避免以及处理上述问题的整体思路：</p><ul><li><strong>1.优化逻辑，保障上游任务稳定性</strong>：首先通过一些优化手段，尽可能保证上游 flink 任务 A 不出现故障</li><li><strong>2.配置作业监控报警</strong>：针对整条链路配置对应的监控报警等，以及时发现和定位问题</li><li><strong>3.制定故障处理、修复预案</strong>：需要制定对应的故障处理、修复预案，一旦出现故障，需要有可处理故障的能力</li><li><strong>4.下游针对数据源特性改进消费和处理方式</strong>：保障即使消费了脏数据也不会对业务逻辑产生影响</li></ul><p>下文主要介绍<strong>第 2 点</strong>，出现上述故障时修复的方案，针对以上场景，目前有如下 3 种可选方案修复数据：</p><ul><li><strong>方案 1 - 离线方式修复</strong>：通过离线方式产出修复数据，对脏数据进行覆盖操作。缺点是故障修复延迟较高，需要切换离线、实时数据源，人工操作成本较高</li><li><strong>方案 2 - 实时方式修复</strong>：重跑修数逻辑，产出修复数据至 kafka X-fix，下游 flink 任务 B 重新从 kafka X-fix 中的指定 offset 开始消费，计算并产出正确的数据。此方案对下游 flink 任务 B 来说，需要改动代码逻辑，存在修数 topic 和原 topic 切换逻辑，修复逻辑较为复杂</li><li><strong>方案 3 - 实时方式修复（本小节 version 字段方案）</strong>：为避免下游产生数据源切换操作带来的高成本操作，可在原有 kafka topic 中产出修复数据，通过 version 字段区分正常产出数据以及修复数据，相对方案 1 和 2 的优点在于，不存在数据源切换逻辑，下游通过控制 version 字段值就可消费到对应的修复数据，明显降低人工操作成本，且修复逻辑相对简单</li></ul><blockquote><p>Note: 方案 3 需要对 Kafka X 预留一定的 buffer，否则在产出修复数据时，由于写入或读出 Kafka X 的 QPS 过高，会影响正常产出数据的任务。</p></blockquote><h2 id="sink-schema-中添加时间戳字段"><a href="#sink-schema-中添加时间戳字段" class="headerlink" title="sink schema 中添加时间戳字段"></a>sink schema 中添加时间戳字段</h2><h3 id="实践案例及使用方式-1"><a href="#实践案例及使用方式-1" class="headerlink" title="实践案例及使用方式"></a>实践案例及使用方式</h3><p>有窗口场景中，sink schema 中可添加以下字段：</p><ul><li><strong>flink_process_start_time(long)：代表 flink 窗口开始逻辑处理的时间戳</strong></li><li><strong>flink_process_end_time(long)：代表 flink 窗口结束逻辑处理的时间戳</strong></li><li><strong>window_start(long)：代表 flink 窗口开始时间戳</strong></li><li><strong>window_end(long)：代表 flink 窗口结束时间戳</strong></li></ul><h3 id="生产实践案例"><a href="#生产实践案例" class="headerlink" title="生产实践案例"></a>生产实践案例</h3><ul><li><strong>flink_process_start_time，flink_process_end_time 在开发、测试、验数阶段可帮助用户定位数据偏差原因</strong></li><li><strong>window_start，window_end 可以帮助用户定位每个窗口处理是否有丢数，及每个窗口处理的具体数据</strong></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要介绍了在 sink schema 中添加 version（版本），时间戳扩展字段的小技巧，以帮助用户在生产环境中提升实时数据故障修复效率以及可用性。</p><p><img src="/blog-img/gzh/wechat.png" alt="公众号"></p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都比较短小，不定期更新，从一些实际的 case 出发抛砖引玉，提高小伙伴的姿♂势水平。本文介绍 Flink sink schema 字段设计小技巧，阅读时长大概 2 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Tips | flink watermark 一定只能用时间戳衡量？？？</title>
    <link href="https://yangyichao-mango.github.io/2020/09/12/wechat-blog/apache-flink:realtime-tips-4-watermark/"/>
    <id>https://yangyichao-mango.github.io/2020/09/12/wechat-blog/apache-flink:realtime-tips-4-watermark/</id>
    <published>2020-09-12T06:21:53.000Z</published>
    <updated>2021-04-04T11:12:52.610Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列每篇文章都比较短小，不定期更新，从一些实际的 case 出发抛砖引玉，提高小伙伴的姿♂势水平。本文从另一种角度介绍 flink 的 watermark，阅读时长大概 2 分钟，话不多说，直接进入正文！</p></blockquote><h1 id="关于-watermark"><a href="#关于-watermark" class="headerlink" title="关于 watermark"></a>关于 watermark</h1><p>Nicki<br>是某一线互联网大厂的数据开发，</p><p>最近<br>由于公司业务的发展，<br>以及业务对数据实时性要求变高，<br>Nicki 开始使用 flink 进行实时数据开发，</p><p>今天<br>Nicki 在使用 flink datastream api 进行开发，<br>当她写万 watermark 分配器之后，<br>突然有了一个疑问，<br>watermark 的分配只能使用时间戳吗？</p><p>带着这个疑问<br>Nicki 找到了 lsp 数据羊。</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>时间戳作为 watermark 的原因<br>时间戳是我们最常用的标识以及分析方式</p><p>但是可以作为 watermark 的<br>不止时间戳</p>]]></content>
    
    <summary type="html">
    
      本系列每篇文章都比较短小，不定期更新，从一些实际的 case 出发抛砖引玉，提高小伙伴的姿♂势水平。本文从另一种角度介绍 flink 的 watermark，阅读时长大概 2 分钟，话不多说，直接进入正文！
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>踩坑记 | Flink 事件时间语义下数据乱序丢数踩坑</title>
    <link href="https://yangyichao-mango.github.io/2020/09/11/wechat-blog/apache-flink:realtime-out-of-order/"/>
    <id>https://yangyichao-mango.github.io/2020/09/11/wechat-blog/apache-flink:realtime-out-of-order/</id>
    <published>2020-09-11T06:21:53.000Z</published>
    <updated>2021-04-04T11:12:34.013Z</updated>
    
    <content type="html"><![CDATA[<h1 id="踩坑记-Flink-事件时间语义下数据乱序丢数踩坑"><a href="#踩坑记-Flink-事件时间语义下数据乱序丢数踩坑" class="headerlink" title="踩坑记 | Flink 事件时间语义下数据乱序丢数踩坑"></a>踩坑记 | Flink 事件时间语义下数据乱序丢数踩坑</h1><blockquote><p>本文详细介绍了在上游使用处理时间语义的 flink 任务出现故障后，重启消费大量积压在上游的数据并产出至下游数据乱序特别严重时，下游 flink 任务使用事件时间语义时遇到的大量丢数问题以及相关的解决方案。</p></blockquote><p>本文分为以下几个部分：</p><ul><li><strong>1.本次踩坑的应用场景</strong></li><li><strong>2.应用场景中发生的丢数故障分析</strong></li><li><strong>3.待修复的故障点</strong></li><li><strong>4.丢数故障解决方案及原理</strong></li><li><strong>5.总结</strong></li></ul><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>应用场景如下：</p><ul><li><strong>flink 任务 A</strong> 以<strong>处理时间</strong>语义做过滤产出新增 xx 明细数据至 <strong>Kafka Y</strong></li><li><strong>flink 任务 B</strong> 以<strong>事件时间</strong>语义消费 <strong>Kafka Y</strong> 做窗口聚合操作产出分钟级别聚合指标至 <strong>Kafka Z</strong></li><li><strong>Kafka Z</strong> 实时导入至 <strong>Druid</strong> 以做即时 OLAP 分析，并且展示在 BI 应用看板</li></ul><p><img src="/blog-img/apache-flink:realtime-out-of-order/stream.png" alt="场景"></p><h2 id="丢数故障分析"><a href="#丢数故障分析" class="headerlink" title="丢数故障分析"></a>丢数故障分析</h2><p>简要介绍下这次生产中故障场景。整条故障追踪链路如下：</p><p>故障一：</p><ul><li>收到报警反馈 <strong>flink 任务 A</strong> 入口流量为 0</li><li>定位 <strong>flink 任务 A</strong> 中某个算子的故障导致整个 job 卡住</li><li>导致此 <strong>flink 任务 A</strong> 上游 <strong>kafka X</strong> 积压了大量数据</li><li>重启 <strong>flink 任务 A</strong>后，消费大量积压在上游 <strong>kafka X</strong> 数据完成，任务恢复正常</li></ul><p>故障一从而引发下游的故障二：</p><ul><li>由于 <strong>flink 任务 A</strong> 使用了<strong>处理时间</strong>语义处理数据，并且有过滤和 keyBy 分桶窗口逻辑，在重启后消费大量积压在上游的数据时，导致 sink rebalance 后产出到下游 <strong>kafka Y</strong> 各个分区数据中的 server_timestamp 是乱序的</li><li>下游 <strong>flink 任务 B</strong> 在消费 <strong>Kafka Y</strong> 时使用了<strong>事件时间</strong>语义处理数据，并且使用了数据中的 server_timestamp 作为<strong>事件时间</strong>时间戳</li><li><strong>flink 任务 B</strong> 消费了乱序很严重的数据之后，导致在窗口聚合计算时丢失了大量数据</li><li>最终展示在 BI 应用中的报表有丢失数据的情况</li></ul><p><img src="/blog-img/apache-flink:realtime-out-of-order/bug_stream.png" alt="故障场景"></p><h2 id="待修复的故障点"><a href="#待修复的故障点" class="headerlink" title="待修复的故障点"></a>待修复的故障点</h2><ul><li>1.<strong>flink 任务 A</strong> 的稳定性故障，这部分解决方案暂不在本文中介绍</li><li>2.<strong>flink 任务 B</strong> 消费上游乱序丢数故障，解决方案在下文介绍</li></ul><h2 id="解决方案以及原理"><a href="#解决方案以及原理" class="headerlink" title="解决方案以及原理"></a>解决方案以及原理</h2><h3 id="丢数故障解决方案"><a href="#丢数故障解决方案" class="headerlink" title="丢数故障解决方案"></a>丢数故障解决方案</h3><p>解决方案是以下游 <strong>flink 任务 B</strong> 作为切入点，直接给出 <strong>flink 任务 B</strong> 的 sql 代码解决方案，java code 也可以按照这个方案实现，其本质原理相同。下文进行原理解释。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  to_unix_timestamp(server_timestamp <span class="operator">/</span> bucket) <span class="keyword">AS</span> <span class="type">timestamp</span>, <span class="comment">-- format 成原有的事件时间戳</span></span><br><span class="line">  <span class="built_in">count</span>(id) <span class="keyword">as</span> id_cnt,</span><br><span class="line">  <span class="built_in">sum</span>(duration) <span class="keyword">as</span> duration_sum</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  source_table</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">  TUMBLE(proctime, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">MINUTE</span>),</span><br><span class="line">  server_timestamp <span class="operator">/</span> bucket <span class="comment">-- 根据事件时间分桶计算，将相同范围（比如 1 分钟）事件时间的数据分到一个桶内</span></span><br></pre></td></tr></table></figure><h3 id="解决方案原理"><a href="#解决方案原理" class="headerlink" title="解决方案原理"></a>解决方案原理</h3><p>首先明确一个无法避免的问题，在不考虑 watermark 允许延迟设置特别大的情况下，只要上游使用到了处理时间语义，下游使用事件时间语义，一旦上游发生故障重启并在短时间内消费大量数据，就不可避免的会出现上述错误以及故障。</p><p>在下游消费方仍然需要将对应事件时间戳的数据展示在 BI 平台报表中、并且全链路时间语义都为处理时间保障不丢数的前提下。解决方案就是在聚合并最终产出对应事件时间戳的数据。</p><p>最后的方案如下：<br>整条链路全部为处理时间语义，窗口计算也使用处理时间，但是产出数据中的时间戳全部为事件时间戳。<br>在出现故障的场景下，一分钟的窗口内的数据的事件时间戳可能相差几个小时，但在最终窗口聚合时可以根据事件时间戳划分到对应的事件时间窗口内，下游 BI 应用展示时使用此事件时间戳即可。</p><blockquote><p>注意：sql 中的 bucket 需要根据具体使用场景进行设置，如果设置过于小，比如非故障场景下按照处理时间开 1 分钟的窗口，bucket<br>设为 60000（1 分钟），那么极有可能，这个时间窗口中所有数据的 server_timestamp 都集中在某两分钟内，那么这些数据就会被分到两个桶（bucket）内，则会导致严重的数据倾斜。</p></blockquote><h3 id="输入数据样例"><a href="#输入数据样例" class="headerlink" title="输入数据样例"></a>输入数据样例</h3><p>模拟上述故障，<strong>flink B</strong> 的任务某一个窗口内的数据输入如下。</p><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">server_timestamp</th>            <th style="width: 20%; text-align: center;">id</th>            <th style="width: 20%; text-align: center;">duration</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">2020/9/01 21:14:38</td>            <td>1</td>            <td>300</td>        </tr>        <tr>            <td style="text-align: center;">2020/9/01 21:14:50</td>            <td>1</td>            <td>500</td>        </tr>        <tr>            <td style="text-align: center;">2020/9/01 21:25:38</td>            <td>2</td>            <td>600</td>        </tr>        <tr>            <td style="text-align: center;">2020/9/01 21:25:38</td>            <td>3</td>            <td>900</td>        </tr>        <tr>            <td style="text-align: center;">2020/9/01 21:25:38</td>            <td>2</td>            <td>800</td>        </tr>    </tbody></table><h3 id="输出数据样例"><a href="#输出数据样例" class="headerlink" title="输出数据样例"></a>输出数据样例</h3><p>按照上述解决方案中的 sql 处理过后，输出数据如下，则可以解决此类型丢数故障。</p><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">timestamp</th>            <th style="width: 20%; text-align: center;">id_cnt</th>            <th style="width: 20%; text-align: center;">duration_sum</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">2020/9/01 21:14:00</td>            <td>2</td>            <td>900</td>        </tr>        <tr>            <td style="text-align: center;">2020/9/01 21:25:00</td>            <td>3</td>            <td>2300</td>        </tr>    </tbody></table><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文分析了在 flink 应用中：</p><ul><li><strong>上游使用处理时间语义的 flink 任务出现故障、重启消费大量积压数据并产出至下游数据乱序特别严重时，下游使用事件时间语义时遇到的大量丢数问题</strong></li><li><strong>以整条链路为处理时间语义的前提下，产出的数据时间戳为事件时间戳解决上述问题</strong></li><li><strong>以 sql 代码给出了丢数故障解决方案样例</strong></li></ul><h2 id="学习资料"><a href="#学习资料" class="headerlink" title="学习资料"></a>学习资料</h2><h3 id="flink"><a href="#flink" class="headerlink" title="flink"></a>flink</h3><ul><li><a href="https://github.com/flink-china/flink-training-course/blob/master/README.md">https://github.com/flink-china/flink-training-course/blob/master/README.md</a></li><li><a href="https://ververica.cn/developers-resources/">https://ververica.cn/developers-resources/</a></li><li><a href="https://space.bilibili.com/33807709">https://space.bilibili.com/33807709</a></li></ul><p><img src="/blog-img/gzh/wechat.png" alt="公众号"></p>]]></content>
    
    <summary type="html">
    
      本文详细介绍了在上游使用处理时间语义的 flink 任务出现故障后，重启消费大量积压在上游的数据并产出至下游数据乱序特别严重时，下游 flink 任务使用事件时间语义时遇到的大量丢数问题以及相关的解决方案。
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>生产实践 | 基于 Flink 的短视频生产消费监控</title>
    <link href="https://yangyichao-mango.github.io/2020/09/01/wechat-blog/apache-flink:realtime-monitor-video/"/>
    <id>https://yangyichao-mango.github.io/2020/09/01/wechat-blog/apache-flink:realtime-monitor-video/</id>
    <published>2020-09-01T06:21:53.000Z</published>
    <updated>2021-04-04T11:09:41.195Z</updated>
    
    <content type="html"><![CDATA[<h1 id="生产实践-基于-Flink-的短视频生产消费监控"><a href="#生产实践-基于-Flink-的短视频生产消费监控" class="headerlink" title="生产实践 | 基于 Flink 的短视频生产消费监控"></a>生产实践 | 基于 Flink 的短视频生产消费监控</h1><blockquote><p>本文详细介绍了实时监控类指标的数据流转链路以及技术方案，大多数的实时监控类指标都可按照本文中的几种方案实现。</p></blockquote><h2 id="短视频生产消费监控"><a href="#短视频生产消费监控" class="headerlink" title="短视频生产消费监控"></a>短视频生产消费监控</h2><p>短视频带来了全新的传播场域和节目形态，小屏幕、快节奏成为行业潮流的同时，也催生了新的用户消费习惯，为创作者和商户带来收益。而多元化的短视频也可以为品牌方提供营销机遇。</p><p>其中对于垂类生态短视频的生产消费热点的监控分析目前成为了实时数据处理很常见的一个应用场景，比如对某个圈定的垂类生态下的视频生产或者视频消费进行监控，对热点视频生成对应的优化推荐策略，促进热点视频的生产或者消费，构建整个生产消费数据链路的闭环，从而提高创作者收益以及消费者留存。</p><p>本文将完整分析垂类生态短视频生产消费数据的整条链路流转方式，并基于 Flink 提供几种对于垂类视频生产消费监控的方案设计。通过本文，你可以了解到：</p><ul><li><p><strong>垂类生态短视频生产消费数据链路闭环</strong></p></li><li><p><strong>实时监控短视频生产消费的方案设计</strong></p></li><li><p><strong>不同监控量级场景下的代码实现</strong></p></li><li><p><strong>flink 学习资料</strong></p></li></ul><h2 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h2><p>垂类生态短视频生产消费数据链路流转架构图如下，此数据流转图也适用于其他场景：</p><p><img src="/blog-img/monitor/data_transform.png" alt="链路"></p><p>在上述场景中，用户生产和消费短视频，从而客户端、服务端以及数据库会产生相应的行为操作日志，这些日志会通过日志抽取中间件抽取到消息队列中，我们目前的场景中是使用 Kafka 作为消息队列；然后使用 flink 对垂类生态中的视频进行生产或消费监控（内容生产通常是圈定垂类作者 id 池，内容消费通常是圈定垂类视频 id 池），最后将实时聚合数据产出到下游；下游可以以数据服务，实时看板的方式展现，运营同学或者自动化工具最终会帮助我们分析当前垂类下的生产或者消费热点，从而生成推荐策略。</p><h2 id="方案设计"><a href="#方案设计" class="headerlink" title="方案设计"></a>方案设计</h2><p><img src="/blog-img/monitor/monitor_flink.png" alt="架构"></p><p>其中数据源如下：</p><ul><li><strong>Kafka</strong> 为全量内容生产和内容消费的日志。</li><li><strong>Rpc/Http/Mysql/配置中心/Redis/HBase</strong> 为需要监控的垂类生态内容 id 池（内容生产则为作者 id 池，内容消费则为视频 id 池），其主要是提供给运营同学动态配置需要监控的 id 范围，其可以在 flink 中进行实时查询，解析运营同学想要的监控指标范围，以及监控的指标和计算方式，然后加工数据产出，可以支持随时配置，实时数据随时计算产出。</li></ul><p>其中数据汇为聚类好的内容生产或者消费热点话题或者事件指标：</p><ul><li><strong>Redis/HBase</strong> 主要是以低延迟（Redis 5ms p99，HBase 100ms p99，不同公司的服务能力不同）并且高 QPS 提供数据服务，给 Server 端或者线上用户提供低延迟的数据查询。</li><li><strong>Druid/Mysql</strong> 可以做为 OLAP 引擎为 BI 分析提供灵活的上卷下钻聚合分析能力，供运营同学配置可视化图表使用。</li><li><strong>Kafka</strong> 可以以流式数据产出，从而提供给下游继续消费或者进行特征提取。</li></ul><p>废话不多说，我们直接上方案和代码，下述几种方案按照监控 id 范围量级区分，不同的量级对应着不同的方案，其中的代码示例为 ProcessWindowFunction，也可以使用 AggregateFunction 代替，其中主要监控逻辑都相同。</p><h3 id="方案-1"><a href="#方案-1" class="headerlink" title="方案 1"></a>方案 1</h3><p>适合监控 id 数据量小的场景（<strong>几千 id</strong>），其实现方式是在 flink 任务初始化时将需要监控的 id 池或动态配置中心的 id 池加载到内存当中，之后只需要在内存中判断内容生产或者消费数据是否在这个监控池当中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">ProcessWindowFunction p = <span class="keyword">new</span> ProcessWindowFunction&lt;CommonModel, CommonModel, Long, TimeWindow&gt;() &#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 配置中心动态 id 池</span></span><br><span class="line">    <span class="keyword">private</span> Config&lt;Set&lt;Long&gt;&gt; needMonitoredIdsConfig;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.needMonitoredIdsConfig = ConfigBuilder</span><br><span class="line">                .buildSet(<span class="string">&quot;needMonitoredIds&quot;</span>, Long.class);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Long bucket, Context context, Iterable&lt;CommonModel&gt; iterable, Collector&lt;CommonModel&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Set&lt;Long&gt; needMonitoredIds = needMonitoredIdsConfig.get();</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 判断 commonModel 中的 id 是否在 needMonitoredIds 池中</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>监控的 id 池可以按照固定或者可配置从而分出两种获取方式：第一种是在 flink 任务开始时就全部加载进内存中，这种方式适合监控 id 池不变的情况；第二种是使用动态配置中心，每次都从配置中心访问到最新的监控 id 池，其可以满足动态配置或者更改 id 池的需求，并且这种实现方式通常可以实时感知到配置更改，几乎无延迟。</p></blockquote><h3 id="方案-2"><a href="#方案-2" class="headerlink" title="方案 2"></a>方案 2</h3><p>适合监控 id 数据量适中（<strong>几十万 id</strong>），监控数据范围会不定时发生变动的场景。其实现方式是在 flink 算子中定时访问接口获取最新的监控 id 池，以获取最新监控数据范围。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">ProcessWindowFunction p = <span class="keyword">new</span> ProcessWindowFunction&lt;CommonModel, CommonModel, Long, TimeWindow&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> lastRefreshTimestamp;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Set&lt;Long&gt; needMonitoredIds;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        <span class="keyword">this</span>.refreshNeedMonitoredIds(System.currentTimeMillis());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Long bucket, Context context, Iterable&lt;CommonModel&gt; iterable, Collector&lt;CommonModel&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> windowStart = context.window().getStart();</span><br><span class="line">        <span class="keyword">this</span>.refreshNeedMonitoredIds(windowStart);</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 判断 commonModel 中的 id 是否在 needMonitoredIds 池中</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">refreshNeedMonitoredIds</span><span class="params">(<span class="keyword">long</span> windowStart)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 每隔 10 秒访问一次</span></span><br><span class="line">        <span class="keyword">if</span> (windowStart - <span class="keyword">this</span>.lastRefreshTimestamp &gt;= <span class="number">10000L</span>) &#123;</span><br><span class="line">            <span class="keyword">this</span>.lastRefreshTimestamp = windowStart;</span><br><span class="line">            <span class="keyword">this</span>.needMonitoredIds = Rpc.get(...)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>根据上述代码实现方式，按照时间间隔的方式刷新 id 池，其缺点在于不能实时感知监控 id 池的变化，所以刷新时间可能会和需求场景强耦合（如果 id 池会频繁更新，那么就需要缩小刷新时间间隔）。也可根据需求场景在每个窗口开始前刷新 id 池，这样可保证每个窗口中的 id 池中的数据一直保持更新。</p></blockquote><h3 id="方案-3"><a href="#方案-3" class="headerlink" title="方案 3"></a>方案 3</h3><p>方案 3 对方案 2 的一个优化（<strong>几十万 id，我们生产环境中最常用的</strong>）。其实现方式是在 flink 中使用 broadcast 算子定时访问监控 id 池，并将 id 池以广播的形式下发给下游参与计算的各个算子。其优化点在于：比如任务的并行度为 500，每 1s 访问一次，采用方案 2 则访问监控 id 池接口的 QPS 为 500，在使用 broadcast 算子之后，其访问 QPS 可以减少到 1，可以大大减少对接口的访问量，减轻接口压力。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Example</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Slf4j</span></span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">NeedMonitorIdsSource</span> <span class="keyword">implements</span> <span class="title">SourceFunction</span>&lt;<span class="title">Map</span>&lt;<span class="title">Long</span>, <span class="title">Set</span>&lt;<span class="title">Long</span>&gt;&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isCancel;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Map&lt;Long, Set&lt;Long&gt;&gt;&gt; sourceContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">while</span> (!<span class="keyword">this</span>.isCancel) &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    TimeUnit.SECONDS.sleep(<span class="number">1</span>);</span><br><span class="line">                    Set&lt;Long&gt; needMonitorIds = Rpc.get(...);</span><br><span class="line">                    <span class="comment">// 可以和上一次访问的数据做比较查看是否有变化，如果有变化，才发送出去</span></span><br><span class="line">                    <span class="keyword">if</span> (CollectionUtils.isNotEmpty(needMonitorIds)) &#123;</span><br><span class="line">                        sourceContext.collect(<span class="keyword">new</span> HashMap&lt;Long, Set&lt;Long&gt;&gt;() &#123;&#123;</span><br><span class="line">                            put(<span class="number">0L</span>, needMonitorIds);</span><br><span class="line">                        &#125;&#125;);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">                    <span class="comment">// 防止接口访问失败导致的错误导致 flink job 挂掉</span></span><br><span class="line">                    log.error(<span class="string">&quot;need monitor ids error&quot;</span>, e);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.isCancel = <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        ParameterTool parameterTool = ParameterTool.fromArgs(args);</span><br><span class="line">        InputParams inputParams = <span class="keyword">new</span> InputParams(parameterTool);</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> MapStateDescriptor&lt;Long, Set&lt;Long&gt;&gt; broadcastMapStateDescriptor = <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line">                <span class="string">&quot;config-keywords&quot;</span>,</span><br><span class="line">                BasicTypeInfo.LONG_TYPE_INFO,</span><br><span class="line">                TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Set&lt;Long&gt;&gt;() &#123;</span><br><span class="line">                &#125;));</span><br><span class="line"></span><br><span class="line">        <span class="comment">/********************* kafka source *********************/</span></span><br><span class="line">        BroadcastStream&lt;Map&lt;Long, Set&lt;Long&gt;&gt;&gt; broadcastStream = env</span><br><span class="line">                .addSource(<span class="keyword">new</span> NeedMonitorIdsSource()) <span class="comment">// redis photoId 数据广播</span></span><br><span class="line">                .setParallelism(<span class="number">1</span>)</span><br><span class="line">                .broadcast(broadcastMapStateDescriptor);</span><br><span class="line"></span><br><span class="line">        DataStream&lt;CommonModel&gt; logSourceDataStream = SourceFactory.getSourceDataStream(...);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/********************* dag *********************/</span></span><br><span class="line">        DataStream&lt;CommonModel&gt; resultDataStream = logSourceDataStream</span><br><span class="line">                .keyBy(KeySelectorFactory.getStringKeySelector(CommonModel::getKeyField))</span><br><span class="line">                .connect(broadcastStream)</span><br><span class="line">                .process(<span class="keyword">new</span> KeyedBroadcastProcessFunction&lt;String, CommonModel, Map&lt;Long, Set&lt;Long&gt;&gt;, CommonModel&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">private</span> Set&lt;Long&gt; needMonitoredIds;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                        <span class="keyword">this</span>.needMonitoredIds = Rpc.get(...)</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(CommonModel commonModel, ReadOnlyContext readOnlyContext, Collector&lt;CommonModel&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="comment">// 判断 commonModel 中的 id 是否在 needMonitoredIds 池中</span></span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processBroadcastElement</span><span class="params">(Map&lt;Long, Set&lt;Long&gt;&gt; longSetMap, Context context, Collector&lt;CommonModel&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="comment">// 需要监控的字段</span></span><br><span class="line">                        Set&lt;Long&gt; needMonitorIds = longSetMap.get(<span class="number">0L</span>);</span><br><span class="line">                        <span class="keyword">if</span> (CollectionUtils.isNotEmpty(needMonitorIds)) &#123;</span><br><span class="line">                            <span class="keyword">this</span>.needMonitoredIds = needMonitorIds;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/********************* kafka sink *********************/</span></span><br><span class="line">        SinkFactory.setSinkDataStream(...);</span><br><span class="line">        </span><br><span class="line">        env.execute(inputParams.jobName);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="方案-4"><a href="#方案-4" class="headerlink" title="方案 4"></a>方案 4</h3><p>适合于超大监控范围的数据（<strong>几百万，我们自己的生产实践中使用扩量到 500 万</strong>）。其原理是将监控范围接口按照 id 按照一定规则分桶。flink 消费到日志数据后将 id 按照 监控范围接口 id 相同的分桶方法进行分桶 keyBy，这样在下游算子中每个算子中就可以按照桶名称，从接口中拿到对应桶的监控 id 数据，这样 flink 中并行的每个算子只需要获取到自己对应的桶的数据，可以大大减少请求的压力。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Example</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        ParameterTool parameterTool = ParameterTool.fromArgs(args);</span><br><span class="line">        InputParams inputParams = <span class="keyword">new</span> InputParams(parameterTool);</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> MapStateDescriptor&lt;Long, Set&lt;Long&gt;&gt; broadcastMapStateDescriptor = <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line">                <span class="string">&quot;config-keywords&quot;</span>,</span><br><span class="line">                BasicTypeInfo.LONG_TYPE_INFO,</span><br><span class="line">                TypeInformation.of(<span class="keyword">new</span> TypeHint&lt;Set&lt;Long&gt;&gt;() &#123;</span><br><span class="line">                &#125;));</span><br><span class="line"></span><br><span class="line">        <span class="comment">/********************* kafka source *********************/</span></span><br><span class="line"></span><br><span class="line">        DataStream&lt;CommonModel&gt; logSourceDataStream = SourceFactory.getSourceDataStream(...);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/********************* dag *********************/</span></span><br><span class="line">        DataStream&lt;CommonModel&gt; resultDataStream = logSourceDataStream</span><br><span class="line">                .keyBy(KeySelectorFactory.getLongKeySelector(CommonModel::getKeyField))</span><br><span class="line">                .timeWindow(Time.seconds(inputParams.accTimeWindowSeconds))</span><br><span class="line">                .process(<span class="keyword">new</span> ProcessWindowFunction&lt;CommonModel, CommonModel, Long, TimeWindow&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">private</span> <span class="keyword">long</span> lastRefreshTimestamp;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">private</span> Set&lt;Long&gt; oneBucketNeedMonitoredIds;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Long bucket, Context context, Iterable&lt;CommonModel&gt; iterable, Collector&lt;CommonModel&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">long</span> windowStart = context.window().getStart();</span><br><span class="line">                        <span class="keyword">this</span>.refreshNeedMonitoredIds(windowStart, bucket);</span><br><span class="line">                        <span class="comment">/**</span></span><br><span class="line"><span class="comment">                         * 判断 commonModel 中的 id 是否在 needMonitoredIds 池中</span></span><br><span class="line"><span class="comment">                         */</span></span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">refreshNeedMonitoredIds</span><span class="params">(<span class="keyword">long</span> windowStart, <span class="keyword">long</span> bucket)</span> </span>&#123;</span><br><span class="line">                        <span class="comment">// 每隔 10 秒访问一次</span></span><br><span class="line">                        <span class="keyword">if</span> (windowStart - <span class="keyword">this</span>.lastRefreshTimestamp &gt;= <span class="number">10000L</span>) &#123;</span><br><span class="line">                            <span class="keyword">this</span>.lastRefreshTimestamp = windowStart;</span><br><span class="line">                            <span class="keyword">this</span>.oneBucketNeedMonitoredIds = Rpc.get(bucket, ...)</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/********************* kafka sink *********************/</span></span><br><span class="line">        SinkFactory.setSinkDataStream(...);</span><br><span class="line"></span><br><span class="line">        env.execute(inputParams.jobName);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文首先介绍了，在短视频领域中，短视频生产消费数据链路的整个闭环，并且其数据链路闭环一般情况下也适用于其他场景；以及对应的实时监控方案的设计和不同场景下的代码实现，包括：</p><ul><li><p><strong>垂类生态短视频生产消费数据链路闭环：用户操作行为日志的流转，日志上传，实时计算，以及流转到 BI，数据服务，最后数据赋能的整个流程</strong></p></li><li><p><strong>实时监控方案设计：监控类实时计算流程中各类数据源，数据汇的选型</strong></p></li><li><p><strong>监控 id 池在不同量级场景下具体代码实现</strong></p></li></ul><h2 id="学习资料"><a href="#学习资料" class="headerlink" title="学习资料"></a>学习资料</h2><h3 id="flink"><a href="#flink" class="headerlink" title="flink"></a>flink</h3><ul><li><a href="https://github.com/flink-china/flink-training-course/blob/master/README.md">https://github.com/flink-china/flink-training-course/blob/master/README.md</a></li><li><a href="https://ververica.cn/developers-resources/">https://ververica.cn/developers-resources/</a></li><li><a href="https://space.bilibili.com/33807709">https://space.bilibili.com/33807709</a></li></ul>]]></content>
    
    <summary type="html">
    
      本文详细介绍了实时监控类指标的数据流转链路以及技术方案，大多数的实时监控类指标都可按照本文中的几种方案实现。
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>实时新增类指标标准化处理方案</title>
    <link href="https://yangyichao-mango.github.io/2020/09/01/wechat-blog/apache-flink:realtime-new-id/"/>
    <id>https://yangyichao-mango.github.io/2020/09/01/wechat-blog/apache-flink:realtime-new-id/</id>
    <published>2020-09-01T06:21:53.000Z</published>
    <updated>2021-04-04T11:12:35.311Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实时新增类指标标准化处理方案"><a href="#实时新增类指标标准化处理方案" class="headerlink" title="实时新增类指标标准化处理方案"></a>实时新增类指标标准化处理方案</h1><blockquote><p>实时指标整个链路开发过程中的一些经验。</p></blockquote><h2 id="实时新增类指标"><a href="#实时新增类指标" class="headerlink" title="实时新增类指标"></a>实时新增类指标</h2><p>大体上可以将实时新增类指标以以下两种维度进行分类。</p><h3 id="identity-id-类型维度"><a href="#identity-id-类型维度" class="headerlink" title="identity id 类型维度"></a>identity id 类型维度</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">identity id 类型</th>            <th style="width: 20%; text-align: center;">备注</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">number(long) 类型 identity id</td>            <td>数值类型 identity id 的好处在于可以使用 Bitmap 类组件做到精确去重。</td>        </tr>        <tr>            <td style="text-align: center;">字符类型 identity id</td>            <td>字符类型 identity id 去重相对复杂，有两种方式，在误差允许范围之内使用 BloomFilter 进行去重，或者使用 key-value 组件进行精确去重。</td>        </tr>    </tbody></table><h3 id="产出数据类型维度"><a href="#产出数据类型维度" class="headerlink" title="产出数据类型维度"></a>产出数据类型维度</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">产出数据类型</th>            <th style="width: 20%; text-align: center;">备注</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">明细类数据</td>            <td>此类数据一般是要求将新增的数据明细产出，uv 的含义是做过滤，产出的明细数据中的 identity id 不会有重复。输出明细数据的好处在于，我们可以在下游使用 OLAP 引擎对明细数据进行各种维度的聚合计算，从而很方便的产出不同维度下的 uv 数据。</td>        </tr>        <tr>            <td style="text-align: center;">聚合类数据</td>            <td>将一个时间窗口内的 uv 进行聚合，并且可以计算出分维度的 uv，其产出数据一般都是[维度 + uv_count]，但是这里的维度一般情况下是都是固定维度。如果需要拓展则需要改动源码。</td>        </tr>    </tbody></table><h2 id="计算链路"><a href="#计算链路" class="headerlink" title="计算链路"></a>计算链路</h2><p>因此新增产出的链路多数就是以上两种维度因子的相互组合。</p><h3 id="number-long-类型-identity-id"><a href="#number-long-类型-identity-id" class="headerlink" title="number(long) 类型 identity id"></a>number(long) 类型 identity id</h3><p><img src="/blog-img/new/new_uid_roaringbitmap.png" alt="使用 RoaringBitmap 的 uv 计算链路"></p><p>代码示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">RoaringBitmapDuplicateable</span>&lt;<span class="title">Model</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> DEFAULT_DUPLICATE_MILLS = <span class="number">24</span> * <span class="number">3600</span> * <span class="number">1000L</span>;</span><br><span class="line"></span><br><span class="line">    BiPredicate&lt;Long, Long&gt; ROARING_BIT_MAP_CLEAR_BI_PREDICATE =</span><br><span class="line">            (start, end) -&gt; end - start &gt;= DEFAULT_DUPLICATE_MILLS;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化</span></span><br><span class="line">    <span class="keyword">default</span> ValueState&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt; getBitMapValueState(String name) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.getRuntimeContext().getState(</span><br><span class="line">                <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(name, TypeInformation.of(</span><br><span class="line">                        <span class="keyword">new</span> TypeHint&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt;() &#123; &#125;))</span><br><span class="line">        );</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">RuntimeContext <span class="title">getRuntimeContext</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">getLongId</span><span class="params">(Model model)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">Optional&lt;Logger&gt; <span class="title">getLogger</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> BiPredicate&lt;Long, Long&gt; <span class="title">roaringBitMapClearBiPredicate</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> ROARING_BIT_MAP_CLEAR_BI_PREDICATE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> List&lt;Model&gt; <span class="title">duplicateAndGet</span><span class="params">(List&lt;Model&gt; models, <span class="keyword">long</span> windowStartTimestamp</span></span></span><br><span class="line"><span class="function"><span class="params">            , ValueState&lt;Tuple2&lt;Date, Roaring64NavigableMap&gt;&gt; bitMapValueState)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Tuple2&lt;Date, Roaring64NavigableMap&gt; bitMap = checkAndGetState(windowStartTimestamp, bitMapValueState);</span><br><span class="line"></span><br><span class="line">        Map&lt;Long, Model&gt; idModelsMap = models</span><br><span class="line">                .stream()</span><br><span class="line">                .collect(Collectors.toMap(<span class="keyword">this</span>::getLongId, Function.identity(), (oldOne, newOne) -&gt; oldOne));</span><br><span class="line"></span><br><span class="line">        Set&lt;Long&gt; ids = idModelsMap.keySet();</span><br><span class="line"></span><br><span class="line">        List&lt;Model&gt; newModels = Lists.newArrayList();</span><br><span class="line">        <span class="keyword">for</span> (Long id : ids) &#123;</span><br><span class="line">            <span class="keyword">if</span> (!bitMap.f1.contains(id)) &#123;</span><br><span class="line">                <span class="keyword">if</span> (idModelsMap.containsKey(id)) &#123;</span><br><span class="line">                    newModels.add(idModelsMap.get(id));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        newModels.stream()</span><br><span class="line">                .map(<span class="keyword">this</span>::getLongId)</span><br><span class="line">                .forEach(bitMap.f1::add);</span><br><span class="line">        bitMapValueState.update(bitMap);</span><br><span class="line">        <span class="keyword">return</span> newModels;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> <span class="keyword">long</span> <span class="title">duplicateAndCount</span><span class="params">(List&lt;Model&gt; models, <span class="keyword">long</span> windowStartTimestamp</span></span></span><br><span class="line"><span class="function"><span class="params">            , ValueState&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt; bitMapValueState)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Tuple2&lt;Long, Roaring64NavigableMap&gt; bitMap = checkAndGetState(windowStartTimestamp, bitMapValueState);</span><br><span class="line"></span><br><span class="line">        Set&lt;Long&gt; ids = models</span><br><span class="line">                .stream()</span><br><span class="line">                .map(<span class="keyword">this</span>::getLongId)</span><br><span class="line">                .collect(Collectors.toSet());</span><br><span class="line"></span><br><span class="line">        List&lt;Long&gt; newIds = Lists.newArrayList();</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (Long id : ids) &#123;</span><br><span class="line">            <span class="keyword">if</span> (!bitMap.f1.contains(id)) &#123;</span><br><span class="line">                newIds.add(id);</span><br><span class="line">                count++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        newIds.forEach(bitMap.f1::add);</span><br><span class="line">        bitMapValueState.update(bitMap);</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> Tuple2&lt;Long, Roaring64NavigableMap&gt; <span class="title">checkAndGetState</span><span class="params">(<span class="keyword">long</span> windowStartTimestamp</span></span></span><br><span class="line"><span class="function"><span class="params">            , ValueState&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt; bitMapValueState)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Tuple2&lt;Long, Roaring64NavigableMap&gt; bitmap = bitMapValueState.value();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> == bitmap) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">this</span>.getLogger().ifPresent(logger -&gt;</span><br><span class="line">                    logger.info(<span class="string">&quot;New RoaringBitMapValueState Timestamp=&#123;&#125;&quot;</span>, windowStartTimestamp));</span><br><span class="line">            Tuple2&lt;Long, Roaring64NavigableMap&gt; newBitMap = Tuple2.of(windowStartTimestamp, <span class="keyword">new</span> Roaring64NavigableMap());</span><br><span class="line">            bitMapValueState.update(newBitMap);</span><br><span class="line">            <span class="keyword">return</span> newBitMap;</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="keyword">this</span>.roaringBitMapClearBiPredicate().test(bitmap.f0, windowStartTimestamp)) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">this</span>.getLogger().ifPresent(logger -&gt;</span><br><span class="line">                    logger.info(<span class="string">&quot;Clear RoaringBitMapValueState, from start=&#123;&#125; to end=&#123;&#125;&quot;</span>, bitmap.f0, windowStartTimestamp));</span><br><span class="line"></span><br><span class="line">            bitMapValueState.clear();</span><br><span class="line">            bitmap.f1.clear();</span><br><span class="line">            Tuple2&lt;Long, Roaring64NavigableMap&gt; newBitMap = Tuple2.of(windowStartTimestamp, <span class="keyword">new</span> Roaring64NavigableMap());</span><br><span class="line">            bitMapValueState.update(newBitMap);</span><br><span class="line">            <span class="keyword">return</span> newBitMap;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> bitmap;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="字符类型-identity-id"><a href="#字符类型-identity-id" class="headerlink" title="字符类型 identity id"></a>字符类型 identity id</h3><h4 id="使用-Flink-state"><a href="#使用-Flink-state" class="headerlink" title="使用 Flink state"></a>使用 Flink state</h4><p><img src="/blog-img/new/new_did_flink_state.png" alt="使用 flink state 的 uv 计算链路"></p><h4 id="使用-key-value-外存"><a href="#使用-key-value-外存" class="headerlink" title="使用 key-value 外存"></a>使用 key-value 外存</h4><p><img src="/blog-img/new/new_did_key_value.png" alt="使用 key-value 的 uv 计算链路"></p><p>如果选用的是 Redis 作为 key-value 过滤，那么这里会有一个巧用 Redis bit 特性的优化。举一个一般场景下的方案与使用 Redis bit 特性的方案做对比：</p><p>场景：假如需要同一天有几十场活动，并且都希望计算出这几十场活动的 uv，那么我们就可以按照下图设计 Redis bit 结构。</p><p>通常方案：</p><p><img src="/blog-img/new/new_did_redis.png" alt="使用 Redis 的 多 uv 指标计算链路"></p><p>这种场景下，如果有 1 亿用户，需要同时计算 50 个活动或者 50 个不同维度下的 uv。那么理论上最大 key 数量为 1 亿 * 50 = 50 亿个 key。</p><p>Redis bit 方案：</p><p><img src="/blog-img/new/new_did_redis_bit.png" alt="使用 Redis bit 特性的 多 uv 指标计算链路"></p><p>这样做的一个优点，就是这几十场活动的 uv 计算都使用了相同的 Redis key 来计算，可以大幅度减少 Redis 的容量占用。使用此方案的话，以上述相同的用户和活动场数，理论上最大<br>key 数量仅仅为 1 亿，只是 value 数量会多占几十个 bit。</p>]]></content>
    
    <summary type="html">
    
      实时指标整个链路开发过程中的一些经验。
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>实时开发标准化处理方案</title>
    <link href="https://yangyichao-mango.github.io/2020/09/01/wechat-blog/realtime/"/>
    <id>https://yangyichao-mango.github.io/2020/09/01/wechat-blog/realtime/</id>
    <published>2020-09-01T06:21:53.000Z</published>
    <updated>2020-09-06T02:00:13.380Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实时开发标准化处理方案"><a href="#实时开发标准化处理方案" class="headerlink" title="实时开发标准化处理方案"></a>实时开发标准化处理方案</h1><blockquote><p>实时指标整个链路开发过程中的一些经验。</p></blockquote><h2 id="指标类型"><a href="#指标类型" class="headerlink" title="指标类型"></a>指标类型</h2><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">指标类型</th>            <th style="width: 20%; text-align: center;">备注</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">pv</td>            <td>简单 pv 类型指标，来一条日志信息加一，计算 count</td>        </tr>        <tr>            <td style="text-align: center;">uv</td>            <td>uv 类型指标，需要在一段时间范围内（一小时、一天、一场活动）正对 user_id 等 identity_id 去重计数。</td>        </tr>        <tr>            <td style="text-align: center;">监控圈定集合内的 identity_id 数据表现</td>            <td>有一组鉴定的 identity_id 集合，实时的监控或者计算这组 identity_id 相关的数据</td>        </tr>        <tr>            <td style="text-align: center;">排名</td>            <td>实时监控某些数据并根据某些指标进行排序</td>        </tr>    </tbody></table>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>项目经验 | 一定要多思考，过脑子</title>
    <link href="https://yangyichao-mango.github.io/2020/09/01/wechat-blog/work:life-cycle/"/>
    <id>https://yangyichao-mango.github.io/2020/09/01/wechat-blog/work:life-cycle/</id>
    <published>2020-09-01T06:21:53.000Z</published>
    <updated>2020-11-29T13:06:55.630Z</updated>
    
    <content type="html"><![CDATA[<p>项目经验 | 一定要多思考，过脑子</p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>老妹是一个数据开发同学，最近在参与一个中台项目的实时数据建设，这也是她第一次完全的投入到一个项目体系当中（之前都是在某一个项目中负责一小块）。<br>就在做这个项目的过程中，遇到了一些问题。</p><p>比如可行性调研和技术方案的细节也七七八八的写了不少，但是实际上需求整体的推进进度很慢，迟迟没有进入到开发阶段。<br>依赖的上游接口不确定，整个项目的推进就暂停了。<br>和上游沟通了半天，但是聊完之后发现好像没有什么可行的结论。</p><p>总的来说就是，需求推进慢，推进方向不正确，导致浪费了很多时间。</p><p>于是老妹抱着这些问题找到了羊老狗。</p><h1 id="需求提出"><a href="#需求提出" class="headerlink" title="需求提出"></a>需求提出</h1><p>需求评审阶段之前好像还有一个阶段，提出需求阶段？？？<br>开会时候说的【这个可以提成需求】？？？不太理解，这里是否有一个阶段？？？</p><h1 id="需求评审阶段"><a href="#需求评审阶段" class="headerlink" title="需求评审阶段"></a>需求评审阶段</h1><p>需求评审阶段非常重要，需要根据目前的能力合理评估需求</p><ul><li>A：哪部分需求，以目前的能力经验能评估可行性或者收益很低的，可以直接同步出去；</li><li>B：哪部分需求能做，但是从长远来看，目前做完的结果只是一种过渡方案，可能 1-2 个月之后就会有代替方案的项目，这部分可以支持<br>，但是没必要为了做成而去支持复杂模块，不必要投入全部人力做这类临时的方案；</li><li>C：哪部分可以做，并且从长远来看，收益很高的，这部分我们可以深度调研方案。</li></ul><p>分别举例如下</p><h2 id="A"><a href="#A" class="headerlink" title="A"></a>A</h2><ul><li>目前从技术上是不可行的，或者从资源层面是不可行的，比如我们目前的能力是能抗住 100w qps 的压力，但是需求可能是 1000w qps；</li><li>分析场景上不合理，我是数据人，有的数据需求可能从分析场景上不合理，比如这个场景其实是算 pv 就可以得出结论的，但是需求是 uv，这类就没有必要，而且 uv 相对 pv 肯定更加耗时耗力。</li></ul><h2 id="B"><a href="#B" class="headerlink" title="B"></a>B</h2><ul><li>我目前是做实时数据的，用数据举例。目前的项目有两种方案：第一种方案是实时数据产出 A 维度数据到 OLAP 中，后端在查询 OLAP 时填充 B、C 维度，最后在看板展示数据；但是目前由于人力问题，后端没法支持完成。<br>因此就有了第二中方案，第二种方案是实时数据产出 A、B、C 维度数据到 OLAP 中，但是数据源没有 B、C 维度，B、C 维度的数据只有后端才存储，所以实时这边需要去存储了 B、C 维度的维表当中做关联操作，并且关联存在一定的难度。<br>这类情况下，实时关联 B、C 只是一个过渡方案，从长期来看，其实这部分没必要因为一个临时方案，而过度投入人力在收益很小的这个方案里面，可以做一些舍弃。</li></ul><h2 id="C"><a href="#C" class="headerlink" title="C"></a>C</h2><ul><li>从长远来看可以做，之后会有越来越多的业务需要使用我们产出的数据，那么我们就可以针对这些业务做通用化模型建设，收益高。即使多加一些人力在这类需求上也可以。</li></ul><p>以上这些情况在需求评审阶段，都需要对产品有一定的观点输出，可以表达我们的观点，技术可以做什么和技术应该做什么。</p><p>需求评审完成之后，就可以进入需求技术方案调研阶段。</p><h1 id="需求技术方案调研阶段"><a href="#需求技术方案调研阶段" class="headerlink" title="需求技术方案调研阶段"></a>需求技术方案调研阶段</h1><h2 id="技术方案调研-设计（5W-1H）"><a href="#技术方案调研-设计（5W-1H）" class="headerlink" title="技术方案调研 + 设计（5W 1H）"></a>技术方案调研 + 设计（5W 1H）</h2><p><strong>我们和上游能够沟通的内容是哪些？比如数据就是加字段，能不能同步一些表？？还能做什么沟通？？<br>排期能不能沟通，应不应该沟通，应不应该自己去沟通，或者最好什么时候去沟通？比如等整体的技术方案确定下来，确定了工作量，然后去定排期</strong></p><p><strong>和产品能够沟通的东西是哪些？</strong><br>需求合理性？<br>需求目前遇到的问题？<br>需求的技术方案可行性？<br>得等技术方案完全确认才能够评估工作量？</p><p><strong>和上下游能够沟通的东西是哪些？</strong><br>能否做支持？</p><p>整体就是 5W 1H 的方式去调研整个技术方案。</p><p>首先第一点，最最最重要的就是<strong>一定要有目的的盘东西</strong>。我最终要产出什么（WHAT），我的上游依赖是什么（WHAT）。<br>举例：需要产出的东西 - 目前手上有的东西 = 上游依赖。只要明确了目的，其实这部分东西很快就能够盘清楚。</p><p>第二点，我们盘清楚上游依赖之后，就是需要去想一下这些上游依赖可能存在的提供方式。<br>举例：这些上游依赖是必须都要其他上游提供吗？能不能通过一些其他的方式自己进行实现？</p><p>第三点，如果目前的技术方案满足不了需求，那么还有没有其他的方式进行实现，<strong>一定要多想可行性方案</strong>，可以多提供方案，但是方案的优劣可以让产品进行取舍抉择。<br>举例：比如数据上面需要一些维度数据，我之前可能就评估各类方法去关联维表去填充这些维度信息。其实还可以推动上游数据去添加这些维度数据。<br>让可行性方案变多，我们只需要去评估可行性方案的优劣。<br>推动上游去做改动时，一定要通过业务角度解释这些数据的通用性，不可能每来一次需求添加一次，那样会被喷，上下游压力都很大。</p><p>第四点，数据人遇到问题时能使用数据说话就使用数据说话，包括方案的可行性等。<br>举例：调研后发现，这个实现成本很高，反馈的时候一定要用数据说话，比如成本达到 xxx，使用 xxx 台机器，这个成本是不是可控的。尽量避免技术方案评审时只能反馈实现成本很高。</p><p>第五点，遇到问题时一定需要尽早的抛出问题，推动各方解决问题，而不能 block 在自己这里。<br>举例：比如在技术方案上，自己可以先和 tech leader 详细讨论，一定注意详细讨论的前提是自己对整个项目目前的需求理解一定要到位，理解有问题的地方及时和产品沟通，明确理解需求。<br>我就犯过比较明显的错误，经常会被 tech leader 问你调研的这个东西合不合理，有什么价值，这个东西能不能用其他的东西进行替代，或者你的方案为什么是这样的，这只是一个过渡的方案，我们有没有必要去完全按照需求要求的产出。</p><p>第六点，技术调研过程中，如果遇到上下游依赖的话，可以先和上下游方进行沟通，<strong>与上下游沟通一定要把握一个度</strong>，这部分可以慢慢学习理解。<br>需要站在自己做的事情的全局上和未来的通用性上去思考问题，不但要思考自己的东西，还要站在上下游的角度上思考问题。</p><p>举例：沟通时可以和上游确认这部分上游能不能做支持，哪部分能帮忙支持，哪部分不能支持。站在对方的角度上想问题，比如自己需要的某些字段或者数据，其实站在上下游的角度上是不需要的，这个就需要多考量。<br>上下游支持如果还存在难度的话，那就需要和产品同步目前的整体问题，让产品去推动各方或者是做一些取舍。</p><p>第七点，方案的每个细节都需要确认。做数据，那么数据每个字段都得清清楚楚的列出计算逻辑才算需求调研完成！！！不然很容易出现口径不一致和可行性问题。</p><p>通过上述几个步骤之后，都要最终和产品确认我们产出最终交付物。</p><p>这里结束之后，其实我们自己大致的工作量也就大致可以评估出来了，工作量是一个很迷的东西，如果工作经验不长的话，很难估计一个准确的工作量。</p><p><strong>不一定就是利用现在手上有的东西去做需求，还可以去推动其他上下游去支持需求。</strong></p><p>做项目时，没必要一口吃一个胖子，也没有必要钻一些牛角尖，有些东西比较难做，我们可以先实现简单的，后续再优化方案，进行迭代，没有什么项目是一次性能做完美的。</p><h1 id="需求排期"><a href="#需求排期" class="headerlink" title="需求排期"></a>需求排期</h1><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><p>1.什么时候可以进入排期阶段？</p><p>技术方案敲定后排人力，按照项目上下游情况排期？</p><p>2.排期的时候如果依赖上游，上游的排期应该怎么沟通？或者我们和上游能够沟通的内容是哪些？</p><p>上游的排期不应该自己去沟通。但是产品又会问，你拍的这个期有和上游沟通吗？</p><p>3.有哪些方式去做排期？</p><p>倒排：比如产品希望这个需求或者功能在某个时间点上线，那么我们就需要按照这个时间点，往前排。如果有上游依赖，还需要定下来上游依赖最晚给到的时间点。<br>举例：比如产品希望 11.30 号整体上线，如果开发、自测需要 5 人天，联调预估需要 2 人天的话，我们就就可以排 11.22 开始开发，并且还需要根据上游依赖的强弱指定上游依赖给到的时间，比如 11.23 号给到。<br>最后还需要留一定的 buffer 给自己，避免中途出现问题。</p><p>正排：这种情况下，一般都没有给定的截止日期。</p><p>3.为什么要这样划分排期？</p><p>排期一般划分为一下几个：</p><ul><li>开发</li><li>自测</li><li>联调</li><li>回测</li><li>上线</li></ul><p>很多情况下，尤其是多个项目组合作的项目，一般都只能排到联调</p><p>4.排期过程中容易忽略的关键点？</p><ul><li><strong>上下游依赖</strong>：上下游依赖很重要，上下游如果一旦发生 delay，咱的排期可能就会受到很大的影响。资源、上下游接口等等等等。</li><li><strong>风险控制</strong>：一定要说明风险点。比如资源，上游依赖，上线前的前置依赖等。最好可以有一个 checklist，根据经验列举自己在开发过程中可能会遇到的各类问题。</li></ul><p>5.所有的东西不能都以 mock 的形式进行，比如为了赶排期，复杂项目的联调全部使用 mock 数据就会存在问题</p><h1 id="开发"><a href="#开发" class="headerlink" title="开发"></a>开发</h1><p>1.开发过程中最需要注意的就是单测的编写，一定要记得编写单测，保证每个单元的代码都是正确的</p><p>2.数据开发完成后需要验数，验数过程是很重要的</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>项目经验 | 一定要多思考，过脑子</title>
    <link href="https://yangyichao-mango.github.io/2020/09/01/wechat-blog/work:thinker/"/>
    <id>https://yangyichao-mango.github.io/2020/09/01/wechat-blog/work:thinker/</id>
    <published>2020-09-01T06:21:53.000Z</published>
    <updated>2020-12-06T04:25:19.341Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列每篇文章都比较短小，不定期更新，从一些实际的经历出发抛砖引玉，希望给小伙伴一些启发。<br>本文介绍了博主从做数据到玩数据的整个思考过程的转变，阅读时长大概 2 分钟，话不多说，直接进入正文！</p></blockquote><p>项目经验 | 怎么从做数据转变到玩数据？</p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>是不是还一直是在<br>执行工作<br>而不是<br>真正的去思考工作？</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>为什么hashmap的数组初始化大小都是2的次方大小时，hashmap的效率最高</title>
    <link href="https://yangyichao-mango.github.io/2020/01/06/java:study-hashmap/"/>
    <id>https://yangyichao-mango.github.io/2020/01/06/java:study-hashmap/</id>
    <published>2020-01-06T12:39:35.000Z</published>
    <updated>2020-01-07T06:32:24.844Z</updated>
    
    <content type="html"><![CDATA[<p>为什么hashmap的数组初始化大小都是2的次方大小时，hashmap的效率最高</p><span id="more"></span><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">indexFor</span><span class="params">(<span class="keyword">int</span> hashcode, <span class="keyword">int</span> length)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> hashcode &amp; (length-<span class="number">1</span>);  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="怎样提高get-key-效率？"><a href="#怎样提高get-key-效率？" class="headerlink" title="怎样提高get(key)效率？"></a>怎样提高get(key)效率？</h1><p>怎样提高get(key)效率 = 怎样提高确定key的所在hashmap中数组index的效率</p><p>hashmap的数据结构是数组和链表的结合，所以我们当然希望这个hashmap里面的元素位置尽量的分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，而不用再去遍历链表。</p><p>看下图，左边两组是数组长度为16（2的4次方），右边两组是数组长度为15。两组的hashcode均为8和9，但是很明显，当它们和1110“与”的时候，产生了相同的结果，也就是说它们会定位到数组中的同一个位置上去，这就产生了碰撞，8和9会被放到同一个链表上，那么查询的时候就需要遍历这个链表，得到8或者9，这样就降低了查询的效率。同时，我们也可以发现，当数组长度为15的时候，hashcode的值会与14（1110）进行“与”，那么最后一位永远是0，而0001，0011，0101，1001，1011，0111，1101这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组长度小了很多，这意味着进一步增加了碰撞的几率，减慢了查询的效率！<br><img src="/blog-img/java:study-hashmap/hashmap-index-for.jpg" alt="hashmap-index"></p><p>1.假设key的hashcode为h，数组长度为length，为了将数据打散，使hashmap中的数组下标对应的Entry链表都有数据，首先想到的就是对length取模，计算方法如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">indexFor</span><span class="params">(<span class="keyword">int</span> h, <span class="keyword">int</span> length)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> h % length;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>因此确定了hashmap的indexFor函数的计算方式。</p><h1 id="怎样确定hashmap数组的length"><a href="#怎样确定hashmap数组的length" class="headerlink" title="怎样确定hashmap数组的length"></a>怎样确定hashmap数组的length</h1><p>“模”运算的消耗还是比较大的，能不能找一种更快速，消耗更小的方式？我们发现做位运算的消耗是很小的，所以尝试将取模运算转换成位预算，由此发现length为2的n次方时会有</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">indexFor</span><span class="params">(<span class="keyword">int</span> h, <span class="keyword">int</span> length)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> h % length; <span class="comment">// 等价于 h &amp; (length - 1);</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于 &amp; 运算符计算效率大大高于 % 运算符，所以上述计算转换为：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">indexFor</span><span class="params">(<span class="keyword">int</span> h, <span class="keyword">int</span> length)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> h &amp; (length - <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>也规定了 length 必须是2的n次方（n&gt;0）<br>除此之外，我们也可以发现，当数组长度为15的时候，hashcode的值会与14（1110）进行“与”，那么最后一位永远是0，而0001，0011，0101，1001，1011，0111，1101这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组长度小了很多，这意味着进一步增加了碰撞的几率，减慢了查询的效率！ </p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="JAVA" scheme="https://yangyichao-mango.github.io/categories/JAVA/"/>
    
    
      <category term="JAVA" scheme="https://yangyichao-mango.github.io/tags/JAVA/"/>
    
  </entry>
  
  <entry>
    <title>apache-flink:study-flink</title>
    <link href="https://yangyichao-mango.github.io/2019/11/22/apache-flink:study-flink/"/>
    <id>https://yangyichao-mango.github.io/2019/11/22/apache-flink:study-flink/</id>
    <published>2019-11-22T03:30:41.000Z</published>
    <updated>2020-05-10T10:38:05.892Z</updated>
    
    <content type="html"><![CDATA[<p>所有operator中的初始化如果写在构造函数当中就会出错，问题是序列化时的问题<br>flink从jobmanager序列化到各个 taskmanager时可能会出问题</p><p>split组件功能可以减少多个flatmap的性能损失<br>多个flatmap数据每个都是使用全部的流进行filter<br>split一个就可以满足</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;所有operator中的初始化如果写在构造函数当中就会出错，问题是序列化时的问题&lt;br&gt;flink从jobmanager序列化到各个 taskmanager时可能会出问题&lt;/p&gt;
&lt;p&gt;split组件功能可以减少多个flatmap的性能损失&lt;br&gt;多个flatmap数据每个
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：Jobs 和 Scheduling</title>
    <link href="https://yangyichao-mango.github.io/2019/11/20/apache-flink:study-flink-jobs-and-scheduling/"/>
    <id>https://yangyichao-mango.github.io/2019/11/20/apache-flink:study-flink-jobs-and-scheduling/</id>
    <published>2019-11-20T03:27:03.000Z</published>
    <updated>2019-11-20T03:35:25.990Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：Jobs 和 Scheduling</p><span id="more"></span><h1 id="Scheduling"><a href="#Scheduling" class="headerlink" title="Scheduling"></a><strong>Scheduling</strong></h1><p>Flink中的执行资源是通过 Task Slots 定义的。每个 TaskManager 都有一个或多个 Task Slots，每个 Slot 可以运行一个并行任务流。<br>并行任务流由多个连续的任务组成，例如 MapFunction 的第n个并行实例和 ReduceFunction 的第n个并行实例。请注意，Flink 经常并发地执行连续的任务：对于流式程序，基本上都会使用并行任务，对于批处理程序，也会经常使用并行任务。</p><p>下图说明了这一点。一个具有数据源、MapFunction 和 ReduceFunction 的程序。源函数和 MapFunction 的并行度为4，而 ReduceFunction 的并行度为3。流由 Source - Map - Reduce 组成。<br>在这个集群中，有两个 TaskManager，每个 TaskManager 有三个 slot，则程序将按如下所述执行。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：DataStream Api 中 State 和容错——State 的使用</title>
    <link href="https://yangyichao-mango.github.io/2019/11/19/apache-flink:study-flink-datastream-state-and-fault-tolerance-working-with-state/"/>
    <id>https://yangyichao-mango.github.io/2019/11/19/apache-flink:study-flink-datastream-state-and-fault-tolerance-working-with-state/</id>
    <published>2019-11-19T07:48:16.000Z</published>
    <updated>2019-11-20T03:29:23.261Z</updated>
    
    <content type="html"><![CDATA[<p>flink有两种基本的state，分别是Keyed State以及Operator State(non-keyed state)；其中Keyed State只能在KeyedStream上的functions及operators上使用；每个operator state会跟parallel operator中的一个实例绑定；Operator State支持parallelism变更时进行redistributing<br>Keyed State及Operator State都分别有managed及raw两种形式，managed由flink runtime来管理，由runtime负责encode及写入checkpoint；raw形式的state由operators自己管理，flink runtime无法了解该state的数据结构，将其视为raw bytes；所有的datastream function都可以使用managed state，而raw state一般仅限于自己实现operators来使用<br>stateful function可以通过CheckpointedFunction接口或者ListCheckpointed接口来使用managed operator state；CheckpointedFunction定义了snapshotState、initializeState两个方法；每当checkpoint执行的时候，snapshotState会被调用；而initializeState方法在每次用户定义的function初始化的时候(第一次初始化或者从前一次checkpoint recover的时候)被调用，该方法不仅可以用来初始化state，还可以用于处理state recovery的逻辑<br>对于manageed operator state，目前仅仅支持list-style的形式，即要求state是serializable objects的List结构，方便在rescale的时候进行redistributed；关于redistribution schemes的模式目前有两种，分别是Even-split redistribution(在restore/redistribution的时候每个operator仅仅得到整个state的sublist)及Union redistribution(在restore/redistribution的时候每个operator得到整个state的完整list)<br>FunctionSnapshotContext继承了ManagedSnapshotContext接口，它定义了getCheckpointId、getCheckpointTimestamp方法；FunctionInitializationContext继承了ManagedInitializationContext接口，它定义了isRestored、getOperatorStateStore、getKeyedStateStore方法，可以用来判断是否是在前一次execution的snapshot中restored，以及获取OperatorStateStore、KeyedStateStore对象</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Hadoop 学习：hdfs架构</title>
    <link href="https://yangyichao-mango.github.io/2019/11/13/apache-hadoop:study-hadoop-hdfs-design/"/>
    <id>https://yangyichao-mango.github.io/2019/11/13/apache-hadoop:study-hadoop-hdfs-design/</id>
    <published>2019-11-13T06:04:52.000Z</published>
    <updated>2019-11-13T07:24:26.271Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Hadoop 学习：hdfs架构</p><span id="more"></span><h1 id="文件系统-Namespace"><a href="#文件系统-Namespace" class="headerlink" title="文件系统 Namespace"></a><strong>文件系统 Namespace</strong></h1><p><img src="/blog-img/apache-hadoop:study-hadoop-hdfs-design/hdfs%E6%9E%B6%E6%9E%84.png" alt="hdfs架构"></p><p>HDFS支持传统的分层文件组织。用户或应用程序可以在这些目录中创建目录并存储文件。文件系统命名空间层次结构与大多数其他现有文件系统相似；可以创建和删除文件，将文件从一个目录移动到另一个目录，或者重命名文件。HDFS支持用户配额和访问权限。HDFS不支持硬链接或软链接。然而，HDFS体系结构并不排除实现这些特性。</p><p>虽然HDFS遵循文件系统的命名约定，但某些路径和名称（例如/.reserved和.snapshot）是保留的。透明加密和快照等功能使用保留路径。</p><p>NameNode维护文件系统名称空间。对文件系统命名空间或其属性的任何更改都由NameNode记录。应用程序可以指定HDFS应该维护的文件副本的数量。文件的副本数称为该文件的复制因子。此信息由NameNode存储。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/categories/Apache-Hadoop/"/>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/tags/Apache-Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：Table Api &amp; SQL</title>
    <link href="https://yangyichao-mango.github.io/2019/11/12/apache-flink:study-flink-table-api-and-sql/"/>
    <id>https://yangyichao-mango.github.io/2019/11/12/apache-flink:study-flink-table-api-and-sql/</id>
    <published>2019-11-12T01:51:01.000Z</published>
    <updated>2019-11-13T01:53:02.813Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：Table Api &amp; SQL</p><span id="more"></span>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：DataStream Api 中 Operators（算子）——Joining</title>
    <link href="https://yangyichao-mango.github.io/2019/11/11/apache-flink:study-flink-datastream-operators-joining/"/>
    <id>https://yangyichao-mango.github.io/2019/11/11/apache-flink:study-flink-datastream-operators-joining/</id>
    <published>2019-11-11T10:30:13.000Z</published>
    <updated>2019-11-13T02:14:11.303Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：DataStream Api 中 Operators（算子）——Joining</p><span id="more"></span><h1 id="Window-Join"><a href="#Window-Join" class="headerlink" title="Window Join"></a><strong>Window Join</strong></h1><p>Window Join 可以将两个流中相同key并且在同一个窗口中的元素进行链接。窗口可以使用 Window Assigner 进行定义，并且对来自不同的流的元素进行计算。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">stream.join(otherStream)</span><br><span class="line">    .where(&lt;KeySelector&gt;)</span><br><span class="line">    .equalTo(&lt;KeySelector&gt;)</span><br><span class="line">    .window(&lt;WindowAssigner&gt;)</span><br><span class="line">    .apply(&lt;JoinFunction&gt;)</span><br></pre></td></tr></table></figure><p>关于一些语义的解释：<br>1.两个流的成对组合的过程类似于 Inner Join，意味着如果一个流中的元素没有另一个流的元素要与之连接，则不会发出这些元素。</p><p>2.那些被连接的元素的时间戳是位于相应窗口中的最大时间戳。例如，以[5，10)为边界的窗口，则进行连接的元素的时间戳为9。</p><h2 id="Tumbling-Window-Join"><a href="#Tumbling-Window-Join" class="headerlink" title="Tumbling Window Join"></a>Tumbling Window Join</h2><p>执行 Tumbling Window Join，在相同 key，相同时间窗口内的元素会进行笛卡尔积组合，这种组合类似于 inner join，如果一个流的对应流的同一窗口中没有元素，则这个流的当前窗口数据不会发出去。</p><p><img src="/blog-img/apache-flink:study-flink-datastream-operators-joining/tumbling-window-join.svg" alt="tumbling-window-join"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"> </span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">DataStream&lt;Integer&gt; orangeStream = ...</span><br><span class="line">DataStream&lt;Integer&gt; greenStream = ...</span><br><span class="line"></span><br><span class="line">orangeStream.join(greenStream)</span><br><span class="line">    .where(&lt;KeySelector&gt;)</span><br><span class="line">    .equalTo(&lt;KeySelector&gt;)</span><br><span class="line">    .window(TumblingEventTimeWindows.of(Time.milliseconds(<span class="number">2</span>)))</span><br><span class="line">    .apply (<span class="keyword">new</span> JoinFunction&lt;Integer, Integer, String&gt; ()&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">join</span><span class="params">(Integer first, Integer second)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> first + <span class="string">&quot;,&quot;</span> + second;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><h2 id="Sliding-Window-Join"><a href="#Sliding-Window-Join" class="headerlink" title="Sliding Window Join"></a>Sliding Window Join</h2><p>执行 Sliding Window Join，具有公共 key 和公共滑动窗口的所有元素都作为成对组合进行连接，并传递给 JoinFunction 或 FlatJoinFunction。也是 inner join，当前流窗口匹配不到对应流窗口的元素则不会发送数据到下游！请注意，某些元素可能在一个滑动窗口中连接，在另一个滑动窗口中不会进行连接！</p><p><img src="/blog-img/apache-flink:study-flink-datastream-operators-joining/sliding-window-join.svg" alt="sliding-window-join"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">DataStream&lt;Integer&gt; orangeStream = ...</span><br><span class="line">DataStream&lt;Integer&gt; greenStream = ...</span><br><span class="line"></span><br><span class="line">orangeStream.join(greenStream)</span><br><span class="line">    .where(&lt;KeySelector&gt;)</span><br><span class="line">    .equalTo(&lt;KeySelector&gt;)</span><br><span class="line">    .window(SlidingEventTimeWindows.of(Time.milliseconds(<span class="number">2</span>) <span class="comment">/* size */</span>, Time.milliseconds(<span class="number">1</span>) <span class="comment">/* slide */</span>))</span><br><span class="line">    .apply (<span class="keyword">new</span> JoinFunction&lt;Integer, Integer, String&gt; ()&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">join</span><span class="params">(Integer first, Integer second)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> first + <span class="string">&quot;,&quot;</span> + second;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><h2 id="Session-Window-Join"><a href="#Session-Window-Join" class="headerlink" title="Session Window Join"></a>Session Window Join</h2><p>执行 Session Window Join，具有相同 key 的所有元素（当“组合”满足会话条件时）将以成对组合联接，并传递给JoinFunction或FlatJoinFunction。同样，也是 inner join，当前流窗口匹配不到对应流窗口的元素则不会发送数据到下游！</p><p><img src="/blog-img/apache-flink:study-flink-datastream-operators-joining/session-window-join.svg" alt="session-window-join"></p><h1 id="Interval-Join"><a href="#Interval-Join" class="headerlink" title="Interval Join"></a><strong>Interval Join</strong></h1><p>interval join 用一个公共 key 连接两个流的元素（流A和流B），其中流B的元素具有与流A中元素的时间戳相对时间间隔内的时间戳，那么这个时间间隔内两个流的元素就会 join。</p><p>即：<strong>b.timestamp ∈ [a.timestamp + lowerBound; a.timestamp + upperBound] or a.timestamp + lowerBound &lt;= b.timestamp &lt;= a.timestamp + upperBound</strong></p><p>其中 lowerBound 和 upperBound 可正可负，只要 lowerBound &lt;= upperBound。</p><p><img src="/blog-img/apache-flink:study-flink-datastream-operators-joining/interval-join.svg" alt="interval-join"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.co.ProcessJoinFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">DataStream&lt;Integer&gt; orangeStream = ...</span><br><span class="line">DataStream&lt;Integer&gt; greenStream = ...</span><br><span class="line"></span><br><span class="line">orangeStream</span><br><span class="line">    .keyBy(&lt;KeySelector&gt;)</span><br><span class="line">    .intervalJoin(greenStream.keyBy(&lt;KeySelector&gt;))</span><br><span class="line">    .between(Time.milliseconds(-<span class="number">2</span>), Time.milliseconds(<span class="number">1</span>))</span><br><span class="line">    .process (<span class="keyword">new</span> ProcessJoinFunction&lt;Integer, Integer, String()&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Integer left, Integer right, Context ctx, Collector&lt;String&gt; out)</span> </span>&#123;</span><br><span class="line">            out.collect(first + <span class="string">&quot;,&quot;</span> + second);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>apache-flink:study-allowedLateness-and-maxOutOfOrderness</title>
    <link href="https://yangyichao-mango.github.io/2019/11/11/apache-flink:study-allowedLateness-and-maxOutOfOrderness/"/>
    <id>https://yangyichao-mango.github.io/2019/11/11/apache-flink:study-allowedLateness-and-maxOutOfOrderness/</id>
    <published>2019-11-11T02:32:17.000Z</published>
    <updated>2019-11-11T02:51:52.244Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Apache Kafka 学习：kafka在数据处理中的应用</title>
    <link href="https://yangyichao-mango.github.io/2019/11/10/apache-kafka:study-kafka-in-data-process/"/>
    <id>https://yangyichao-mango.github.io/2019/11/10/apache-kafka:study-kafka-in-data-process/</id>
    <published>2019-11-10T15:12:14.000Z</published>
    <updated>2019-11-11T01:42:14.697Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Kafka 学习：kafka在数据处理中的应用的一些个人理解。</p><span id="more"></span><h1 id="离线数据处理"><a href="#离线数据处理" class="headerlink" title="离线数据处理"></a><strong>离线数据处理</strong></h1><h2 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h2><p>在离线数据处理的过程中。<br>如果不使用消息队列，在并发量很小的情况下，所有的客户端数据日志直接向hdfs写数据暂时不会产生什么问题。<br>如果不使用消息队列，在并发量很大的情况下，向 hdfs 写数据就会出现问题，首先，向hdfs写数据会有锁竞争的情况，可能会导致大部分写请求很长时间得不到锁，导致大量请求延迟或者超时，这是不能接受的；并且 hdfs 作为文件系统不能承受太大的并发量，在并发很高的情况下，集群可能会崩溃。</p><h2 id="问题原因总结"><a href="#问题原因总结" class="headerlink" title="问题原因总结"></a>问题原因总结</h2><p>总的来说，在这种情况下，问题的根本在于高并发情况下，hdfs 作为文件系统不能承受高并发请求的问题。</p><h2 id="实施方案"><a href="#实施方案" class="headerlink" title="实施方案"></a>实施方案</h2><p>所以需要一种工具可以将客户端日志这种高并发请求转换为低并发的请求，这时候就可以使用kafka这样的消息队列，客户端的高并发请求直接写入 kafka，然后 hdfs 以低并发消费 kafka 中的数据。这样就解决了文件系统不能支持高并发的情况。并且由于 kafka 的HA特性，可以保证数据的正确性。</p><h2 id="kafka作用"><a href="#kafka作用" class="headerlink" title="kafka作用"></a>kafka作用</h2><p>将高并发请求以低并发方式处理。这种方式中解耦效果不明显，下面的实时数据处理使用到的解耦效果比较明显。</p><h1 id="实时数据处理"><a href="#实时数据处理" class="headerlink" title="实时数据处理"></a><strong>实时数据处理</strong></h1><h2 id="场景-1"><a href="#场景-1" class="headerlink" title="场景"></a>场景</h2><p>在实时数据处理的过程中。<br>如果不使用消息队列，上游数据写入到类似 flink 这样的实时处理引擎当中，flink处理完成后向下游 olap 引擎（druid，clickhouse等）或者hdfs，hive，es等的文件系统写数据时，就需要为每一种 olap 引擎开发一种 connector，这样的情况下，每出现一种 olap 引擎或者每当下游的 olap 引擎升级版本引入新特性时，就需要 flink 开发工程师开发一种新的 connector 或者跟随 olap 引擎的升级而升级自己的 connector，这样 flink 开发工程师的维护成本之后就会特别高。<br>这里有同学可能会说可以在数据处理的过程中使用下游 olap 等的引擎提供的 sdk，这种方法是可以的，但是实时处理打不风情况下并发量很高，olap 引擎提供的 sdk 应对这种高并发的场景可能会有很多问题。</p><h2 id="问题原因总结-1"><a href="#问题原因总结-1" class="headerlink" title="问题原因总结"></a>问题原因总结</h2><p>问题的根本在于实时处理引擎和下游之间的耦合问题，这就需要一种HA的中间件来将各个模块进行解耦，kafka这样的消息队列可以很好的解决这中模块之间高度耦合的情况。</p><h2 id="实施方案-1"><a href="#实施方案-1" class="headerlink" title="实施方案"></a>实施方案</h2><p>在 flink 和 druid中间使用 kafka 进行解耦，让 flink 向 kafka 生成数据，druid 消费 kafka 的数据。<br>这样就使得 flink 可以只开发和维护一套针对于 kafka 的 connector，druid也只用开发和维护一套针对 kafka 的 connector，这样无论是实时处理引擎的升级或替换，或者是实时处理引擎下游的模块的升级或替换，都不会互相影响，并且这些模块的工程师只需要对消息队列的 connector 进行维护即可，并且可以根据其特性进行更好的优化。</p><h2 id="kafka作用-1"><a href="#kafka作用-1" class="headerlink" title="kafka作用"></a>kafka作用</h2><p>模块之间的解耦。让各个模块各司其职。<br>拓展：<br>1.Java 虚拟机在 Java 语言和各个系统之间的作用。<br>2.Sql 进行三范式优化，不需要将所有数据都放在一张表当中，将n对n的表拆分出一张维表进行解耦。将维度拆分为维表可以减少数据量，更好划分和使用维度数据。<br>3.经典网络五层模型，划分为五层，则在每一层中更新或者新建协议栈只需要对上下层进行兼容即可，不需要对整个网络架构模型做调整。<br>4.Maven multiModule 划分。<br>5.springMvc。<br>等等。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Kafka" scheme="https://yangyichao-mango.github.io/categories/Apache-Kafka/"/>
    
    
      <category term="Apache Kafka" scheme="https://yangyichao-mango.github.io/tags/Apache-Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：DataStream Api 中 Operators（算子）——窗口</title>
    <link href="https://yangyichao-mango.github.io/2019/11/09/apache-flink:study-flink-datastream-operators-windows/"/>
    <id>https://yangyichao-mango.github.io/2019/11/09/apache-flink:study-flink-datastream-operators-windows/</id>
    <published>2019-11-09T14:48:53.000Z</published>
    <updated>2019-11-11T10:45:12.300Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：DataStream Api 中 Operators（算子）——窗口</p><span id="more"></span><p>Keyed Windows</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">       .keyBy(...)               &lt;-  keyed versus non-keyed windows</span><br><span class="line">       .window(...)              &lt;-  required: <span class="string">&quot;assigner&quot;</span></span><br><span class="line">      [.trigger(...)]            &lt;-  optional: <span class="string">&quot;trigger&quot;</span> (<span class="keyword">else</span> <span class="keyword">default</span> trigger)</span><br><span class="line">      [.evictor(...)]            &lt;-  optional: <span class="string">&quot;evictor&quot;</span> (<span class="keyword">else</span> no evictor)</span><br><span class="line">      [.allowedLateness(...)]    &lt;-  optional: <span class="string">&quot;lateness&quot;</span> (<span class="keyword">else</span> zero)</span><br><span class="line">      [.sideOutputLateData(...)] &lt;-  optional: <span class="string">&quot;output tag&quot;</span> (<span class="keyword">else</span> no side output <span class="keyword">for</span> late data)</span><br><span class="line">       .reduce/aggregate/fold/apply()      &lt;-  required: <span class="string">&quot;function&quot;</span></span><br><span class="line">      [.getSideOutput(...)]      &lt;-  optional: <span class="string">&quot;output tag&quot;</span></span><br></pre></td></tr></table></figure><p>Non-Keyed Windows</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">       .windowAll(...)           &lt;-  required: <span class="string">&quot;assigner&quot;</span></span><br><span class="line">      [.trigger(...)]            &lt;-  optional: <span class="string">&quot;trigger&quot;</span> (<span class="keyword">else</span> <span class="keyword">default</span> trigger)</span><br><span class="line">      [.evictor(...)]            &lt;-  optional: <span class="string">&quot;evictor&quot;</span> (<span class="keyword">else</span> no evictor)</span><br><span class="line">      [.allowedLateness(...)]    &lt;-  optional: <span class="string">&quot;lateness&quot;</span> (<span class="keyword">else</span> zero)</span><br><span class="line">      [.sideOutputLateData(...)] &lt;-  optional: <span class="string">&quot;output tag&quot;</span> (<span class="keyword">else</span> no side output <span class="keyword">for</span> late data)</span><br><span class="line">       .reduce/aggregate/fold/apply()      &lt;-  required: <span class="string">&quot;function&quot;</span></span><br><span class="line">      [.getSideOutput(...)]      &lt;-  optional: <span class="string">&quot;output tag&quot;</span></span><br></pre></td></tr></table></figure><h1 id="Window-生命周期"><a href="#Window-生命周期" class="headerlink" title="Window 生命周期"></a><strong>Window 生命周期</strong></h1><p>简而言之，当属于该窗口的第一个元素到达时，将会创建一个窗口，并且当时间（Event Time 或者 Processing Time）超过其结束时间戳加上用户指定的允许延迟时间时，将完全删除该窗口，<strong>注意窗口都是左开右闭，比如：[0, 5)</strong>。<br>Flink 保证只删除基于时间的窗口，而不删除其他类型的窗口，例如 Global Window。例如，使用基于事件时间的窗口，并且创建一个窗口大小为5分钟的滚动（Tumbing）窗口，并且允许延迟1分钟。当时间戳属于12:00到12:05之间的第一个元素到达时，Flink 将创建一个新窗口，当 Watermark 通过12:06时间戳时，就会把这个窗口删除。</p><p>此外，每个窗口都包含一个 Trigger 和一个函数（ProcessWindowFunction, ReduceFunction, AggregateFunction or FoldFunction）。函数包含了要应用于窗口内容的计算，而 Trigger 制定了什么情况下才触发执行这些函数。比如，触发策略可能类似于“当窗口中的元素数超过4时”或“当 WaterMark 通过窗口结束时”进行触发。Trigger 还可以决定什么时候删除窗口中的元素。</p><p>除上述内容外，您还可以指定一个 Evictor，该 Evictor 将能够在 Trigger 触发后、应用函数之前和/或之后从窗口中移除元素。</p><p>下面例子中的窗口都是按照 Event Time 或者 Processing Time进行指定。</p><h1 id="Keyed-vs-Non-Keyed-Windows"><a href="#Keyed-vs-Non-Keyed-Windows" class="headerlink" title="Keyed vs Non-Keyed Windows"></a><strong>Keyed vs Non-Keyed Windows</strong></h1><p>首先要指定的是是否应该为流设置 key。使用keyBy（…）可以将无限流拆分为逻辑 keyed 流。</p><p>在 Keyed Stream 的情况下，传入 event 的任何属性都可以用作键。拥有一个 Keyed Stream 将允许您的窗口计算由多个任务并行执行，因为每个逻辑 Keyed Stream 都可以独立于其他任务进行处理。所有引用同一个键的 event 都将被发送到同一个并行任务（通过 partitioner 完成）。</p><p>如果不是 Keyed Stream，则不会将原始流拆分为多个逻辑流，所有窗口逻辑将由单个任务执行，即并行度为1。</p><h1 id="Tumbling-Windows"><a href="#Tumbling-Windows" class="headerlink" title="Tumbling Windows"></a><strong>Tumbling Windows</strong></h1><p><img src="/blog-img/apache-flink:study-flink-datastream-operators-windows/%E6%BB%9A%E5%8A%A8%E7%AA%97%E5%8F%A3.svg" alt="滚动窗口"></p><p>滚动窗口，如果你指定的滚动窗口大小为一天计算一次，并且你需要更具你本地的时间的 00:00:00 开始，则必须按照时区来指定窗口。<br>可以看到上图中，无论是多少个 key，每个 key 的窗口的起始和截止时间都相同。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Creates a new &#123;<span class="doctag">@code</span> TumblingEventTimeWindows&#125; &#123;<span class="doctag">@link</span> WindowAssigner&#125; that assigns</span></span><br><span class="line"><span class="comment"> * elements to time windows based on the element timestamp and offset.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 可以根据 时间戳 以及 偏移量 来指定 窗口的范围</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * &lt;p&gt;For example, if you want window a stream by hour,but window begins at the 15th minutes</span></span><br><span class="line"><span class="comment"> * of each hour, you can use &#123;<span class="doctag">@code</span> of(Time.hours(1),Time.minutes(15))&#125;,then you will get</span></span><br><span class="line"><span class="comment"> * time windows start at 0:15:00,1:15:00,2:15:00,etc.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 比如，如果需要一个窗口大小为一个小时，从每个小时的第15分钟开始计数的窗口，则可以使用下面的代码实现</span></span><br><span class="line"><span class="comment"> * of(Time.hours(1),Time.minutes(15))</span></span><br><span class="line"><span class="comment"> * 这样获得的窗口就是 0:15:00，1:15:00，2:15:00 ...</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;Rather than that,if you are living in somewhere which is not using UTC±00:00 time,</span></span><br><span class="line"><span class="comment"> * such as China which is using UTC+08:00,and you want a time window with size of one day,</span></span><br><span class="line"><span class="comment"> * and window begins at every 00:00:00 of local time,you may use &#123;<span class="doctag">@code</span> of(Time.days(1),Time.hours(-8))&#125;.</span></span><br><span class="line"><span class="comment"> * The parameter of offset is &#123;<span class="doctag">@code</span> Time.hours(-8))&#125; since UTC+08:00 is 8 hours earlier than UTC time.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 除此之外，如果您的时区不是 UTC±00:00 时间，比如在 中国（时区是 UTC+08:00），并且你需要一个一天大小的窗口，</span></span><br><span class="line"><span class="comment"> * 并且窗口时间是本地 00:00:00开始，则可以使用下面的代码实现</span></span><br><span class="line"><span class="comment"> * of(Time.days(1), Time.hours(-8))</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> size The size of the generated windows.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> offset The offset which window start would be shifted by.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> The time policy.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> TumblingEventTimeWindows <span class="title">of</span><span class="params">(Time size, Time offset)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> TumblingEventTimeWindows(size.toMilliseconds(), offset.toMilliseconds());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// daily tumbling event-time windows offset by -8 hours.</span></span><br><span class="line">    input</span><br><span class="line">        .keyBy(&lt;key selector&gt;)</span><br><span class="line">        .window(TumblingEventTimeWindows.of(Time.days(<span class="number">1</span>), Time.hours(-<span class="number">8</span>)))</span><br><span class="line">        .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Sliding-Windows"><a href="#Sliding-Windows" class="headerlink" title="Sliding Windows"></a><strong>Sliding Windows</strong></h1><p><img src="/blog-img/apache-flink:study-flink-datastream-operators-windows/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3.svg" alt="滑动窗口"></p><p>滑动窗口，如果你指定窗口大小和滑动步长一样，那么和滚动窗口的作用一样，滑动窗口的一个明显的特征就是：<strong>窗口可能会重叠，即同一个元素可能会属于不同的窗口</strong>。</p><p>和滚动窗口相同，如果你指定的滚动窗口大小为一天计算一次，你需要更具你本地的时间的 00:00:00 开始，则必须按照时区来指定窗口。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// sliding processing-time windows offset by -8 hours</span></span><br><span class="line">    input</span><br><span class="line">        .keyBy(&lt;key selector&gt;)</span><br><span class="line">        .window(SlidingProcessingTimeWindows.of(Time.hours(<span class="number">12</span>), Time.hours(<span class="number">1</span>), Time.hours(-<span class="number">8</span>)))</span><br><span class="line">        .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Session-Windows"><a href="#Session-Windows" class="headerlink" title="Session Windows"></a><strong>Session Windows</strong></h1><p><img src="/blog-img/apache-flink:study-flink-datastream-operators-windows/%E4%BC%9A%E8%AF%9D%E7%AA%97%E5%8F%A3.svg" alt="会话窗口"></p><p>会话窗口，根据会话来指定窗口，与滚动和滑动窗口相比，会话窗口不重叠，并且没有固定的开始和结束时间。会话窗口可以指定静态指定会话间隔，或者可以让用户动态指定会话间隔。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;T&gt; input = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">// event-time session windows with static gap</span></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(EventTimeSessionWindows.withGap(Time.minutes(<span class="number">10</span>)))</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br><span class="line">    </span><br><span class="line"><span class="comment">// event-time session windows with dynamic gap</span></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(EventTimeSessionWindows.withDynamicGap((element) -&gt; &#123;</span><br><span class="line">        <span class="comment">// determine and return session gap</span></span><br><span class="line">    &#125;))</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br></pre></td></tr></table></figure><h1 id="Global-Windows"><a href="#Global-Windows" class="headerlink" title="Global Windows"></a><strong>Global Windows</strong></h1><p><img src="/blog-img/apache-flink:study-flink-datastream-operators-windows/%E5%85%A8%E5%B1%80%E7%AA%97%E5%8F%A3.svg" alt="全局窗口"></p><p>全局窗口（即无窗口），代表所有元素斗数以一个全局窗口，如果你不指定 Trigger，那么永远也不会生产出数据，因为全局窗口没有窗口开始和窗口结束的概念。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;T&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">    .keyBy(<span class="xml"><span class="tag">&lt;<span class="name">key</span> <span class="attr">selector</span>&gt;</span>)</span></span><br><span class="line"><span class="xml">    .window(GlobalWindows.create())</span></span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br></pre></td></tr></table></figure><h1 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a><strong>窗口函数</strong></h1><p>在定义 window assigner 之后，我们需要指定要在每个窗口上执行的计算。当系统确定窗口准备好处理时（Trigger决定），这些窗口函数就可以用于处理每个（Keyed / Non-Keyed）窗口的元素。</p><p>窗口函数可以是ReduceFunction、AggregateFunction、FoldFunction或ProcessWindowFunction之一。前两个函数执行起来会更高效，因为 Flink 可以在每个窗口中的元素到达时递增地聚合元素。ProcessWindowFunction获取包含在窗口中的所有元素的Iterable，以及有关元素所属窗口的其他元信息。</p><p>使用ProcessWindowFunction的窗口函数不能像其他函数那样高效地执行，因为Flink在调用函数之前必须在内部缓冲窗口的所有元素。这可以通过将ProcessWindowFunction与ReduceFunction、AggregateFunction或FoldFunction组合使用来提高效率，从而使得其他窗口元数据或者窗口元素的进行增量聚合。</p><h2 id="ReduceFunction"><a href="#ReduceFunction" class="headerlink" title="ReduceFunction"></a>ReduceFunction</h2><p>变量：两个输入生成一个输出，三个变量的类型必须相同。<br>触发时间：在每个窗口的元素到来的时候进行增量聚合。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(&lt;window assigner&gt;)</span><br><span class="line">    .reduce(<span class="keyword">new</span> ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt; &#123;</span><br><span class="line">      <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Long&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Long&gt; v1, Tuple2&lt;String, Long&gt; v2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(v1.f0, v1.f1 + v2.f1);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><h2 id="AggregateFunction"><a href="#AggregateFunction" class="headerlink" title="AggregateFunction"></a>AggregateFunction</h2><p>AggregateFunction 是 ReduceFunction 的一个扩展版本。</p><p>变量：三个变量，一个是输入，一个是 accumulator 累加器，还有一个是输出，三个变量的类型可以不同。<br>触发时间：在每个窗口的元素到来的时候进行增量聚合。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The accumulator is used to keep a running sum and a count. The &#123;<span class="doctag">@code</span> getResult&#125; method</span></span><br><span class="line"><span class="comment"> * computes the average.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">AverageAggregate</span></span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;, <span class="title">Tuple2</span>&lt;<span class="title">Long</span>, <span class="title">Long</span>&gt;, <span class="title">Double</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">0L</span>, <span class="number">0L</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">add</span><span class="params">(Tuple2&lt;String, Long&gt; value, Tuple2&lt;Long, Long&gt; accumulator)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(accumulator.f0 + value.f1, accumulator.f1 + <span class="number">1L</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Double <span class="title">getResult</span><span class="params">(Tuple2&lt;Long, Long&gt; accumulator)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> ((<span class="keyword">double</span>) accumulator.f0) / accumulator.f1;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">merge</span><span class="params">(Tuple2&lt;Long, Long&gt; a, Tuple2&lt;Long, Long&gt; b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(a.f0 + b.f0, a.f1 + b.f1);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(&lt;window assigner&gt;)</span><br><span class="line">    .aggregate(<span class="keyword">new</span> AverageAggregate());</span><br></pre></td></tr></table></figure><h2 id="FoldFunction"><a href="#FoldFunction" class="headerlink" title="FoldFunction"></a>FoldFunction</h2><p>FoldFunction 是 AggregateFunction 的一个简易版本。</p><p>变量：三个个变量，一个是输出值的初始化值，一个是输入，还有一个是输出，三个变量中输入和输出的类型可以不同，但是输出和输出初始化值必须相同。<br>触发时间：在每个窗口的元素到来的时候进行增量聚合。</p><p>FoldFunction 指定如何将窗口的输入元素与输出类型的元素组合。对添加到窗口的每个元素和当前输出值增量调用 FoldFunction。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(&lt;window assigner&gt;)</span><br><span class="line">    .fold(<span class="string">&quot;&quot;</span>, <span class="keyword">new</span> FoldFunction&lt;Tuple2&lt;String, Long&gt;, String&gt;&gt; &#123;</span><br><span class="line">       <span class="function"><span class="keyword">public</span> String <span class="title">fold</span><span class="params">(String acc, Tuple2&lt;String, Long&gt; value)</span> </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> acc + value.f1;</span><br><span class="line">       &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><h2 id="ProcessWindowFunction"><a href="#ProcessWindowFunction" class="headerlink" title="ProcessWindowFunction"></a>ProcessWindowFunction</h2><p>ProcessWindowFunction 可以得到一个包含窗口的所有元素的迭代器，以及一个访问时间和状态信息的上下文对象，使得它能够提供比其他窗口函数更好的灵活性。<br>但是这是以性能和资源消耗为代价的，因为元素不能增量聚合，而是需要在内部缓冲，直到窗口可以处理为止。</p><p>变量：四个变量，一个是窗口的key，一个是包含了窗口信息的上下文，一个是窗口内所有元素的迭代器，一个是输出数据的收集器。<br>触发时间：窗口内有数据并且 Watermark 到达了窗口结束时间时触发。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">IN</span>, <span class="title">OUT</span>, <span class="title">KEY</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">implements</span> <span class="title">Function</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Evaluates the window and outputs none or several elements.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key The key for which this window is evaluated.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context The context in which the window is being evaluated.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> elements The elements in the window being evaluated.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> out A collector for emitting elements.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception The function may throw exceptions to fail the program and trigger recovery.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            KEY key,</span></span></span><br><span class="line"><span class="function"><span class="params">            Context context,</span></span></span><br><span class="line"><span class="function"><span class="params">            Iterable&lt;IN&gt; elements,</span></span></span><br><span class="line"><span class="function"><span class="params">            Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * The context holding window metadata.</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Context</span> <span class="keyword">implements</span> <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span> </span>&#123;</span><br><span class="line">       <span class="comment">/**</span></span><br><span class="line"><span class="comment">        * Returns the window that is being evaluated.</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">       <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> W <span class="title">window</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">       <span class="comment">/** Returns the current processing time. */</span></span><br><span class="line">       <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">long</span> <span class="title">currentProcessingTime</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">       <span class="comment">/** Returns the current event-time watermark. */</span></span><br><span class="line">       <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">long</span> <span class="title">currentWatermark</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">       <span class="comment">/**</span></span><br><span class="line"><span class="comment">        * State accessor for per-key and per-window state.</span></span><br><span class="line"><span class="comment">        *</span></span><br><span class="line"><span class="comment">        * &lt;p&gt;&lt;b&gt;<span class="doctag">NOTE:</span>&lt;/b&gt;If you use per-window state you have to ensure that you clean it up</span></span><br><span class="line"><span class="comment">        * by implementing &#123;<span class="doctag">@link</span> ProcessWindowFunction#clear(Context)&#125;.</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">       <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> KeyedStateStore <span class="title">windowState</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">       <span class="comment">/**</span></span><br><span class="line"><span class="comment">        * State accessor for per-key global state.</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">       <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> KeyedStateStore <span class="title">globalState</span><span class="params">()</span></span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">  .keyBy(t -&gt; t.f0)</span><br><span class="line">  .timeWindow(Time.minutes(<span class="number">5</span>))</span><br><span class="line">  .process(<span class="keyword">new</span> MyProcessWindowFunction());</span><br><span class="line"></span><br><span class="line"><span class="comment">/* ... */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProcessWindowFunction</span> </span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;, <span class="title">String</span>, <span class="title">String</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String key, Context context, Iterable&lt;Tuple2&lt;String, Long&gt;&gt; input, Collector&lt;String&gt; out)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (Tuple2&lt;String, Long&gt; in: input) &#123;</span><br><span class="line">      count++;</span><br><span class="line">    &#125;</span><br><span class="line">    out.collect(<span class="string">&quot;Window: &quot;</span> + context.window() + <span class="string">&quot;count: &quot;</span> + count);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用 ProcessWindowFunction 进行元素 count 是非常低效的，下面会讲到怎样将 ReduceFunction 或者 AggregateFunction 与 ProcessWindowFunction 结合使用将增量的数据的与 ProcessWindowFunction 配合进行使用。</p><p>为什么需要用到 ProcessWindowFunction：如果必要的话，一般的业务逻辑是没必要使用到 ProcessWindowFunction 的，但是有的需求需要获取到当前元素时间戳，窗口开始结束等等的信息，这时就需要使用 ProcessWindowFunction 来获取这些信息了。</p><h3 id="ProcessWindowFunction-与-ReduceFunction"><a href="#ProcessWindowFunction-与-ReduceFunction" class="headerlink" title="ProcessWindowFunction 与 ReduceFunction"></a>ProcessWindowFunction 与 ReduceFunction</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;SensorReading&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">  .keyBy(&lt;key selector&gt;)</span><br><span class="line">  .timeWindow(&lt;duration&gt;)</span><br><span class="line">  .reduce(<span class="keyword">new</span> MyReduceFunction(), <span class="keyword">new</span> MyProcessWindowFunction());</span><br><span class="line"></span><br><span class="line"><span class="comment">// Function definitions</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReduceFunction</span> <span class="keyword">implements</span> <span class="title">ReduceFunction</span>&lt;<span class="title">SensorReading</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> SensorReading <span class="title">reduce</span><span class="params">(SensorReading r1, SensorReading r2)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> r1.value() &gt; r2.value() ? r2 : r1;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProcessWindowFunction</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">SensorReading</span>, <span class="title">Tuple2</span>&lt;<span class="title">Long</span>, <span class="title">SensorReading</span>&gt;, <span class="title">String</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String key,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Context context,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Iterable&lt;SensorReading&gt; minReadings,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Collector&lt;Tuple2&lt;Long, SensorReading&gt;&gt; out)</span> </span>&#123;</span><br><span class="line">      SensorReading min = minReadings.iterator().next();</span><br><span class="line">      out.collect(<span class="keyword">new</span> Tuple2&lt;Long, SensorReading&gt;(context.window().getStart(), min));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="ProcessWindowFunction-与-AggregateFunction"><a href="#ProcessWindowFunction-与-AggregateFunction" class="headerlink" title="ProcessWindowFunction 与 AggregateFunction"></a>ProcessWindowFunction 与 AggregateFunction</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">  .keyBy(&lt;key selector&gt;)</span><br><span class="line">  .timeWindow(&lt;duration&gt;)</span><br><span class="line">  .aggregate(<span class="keyword">new</span> AverageAggregate(), <span class="keyword">new</span> MyProcessWindowFunction());</span><br><span class="line"></span><br><span class="line"><span class="comment">// Function definitions</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The accumulator is used to keep a running sum and a count. The &#123;<span class="doctag">@code</span> getResult&#125; method</span></span><br><span class="line"><span class="comment"> * computes the average.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">AverageAggregate</span></span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;, <span class="title">Tuple2</span>&lt;<span class="title">Long</span>, <span class="title">Long</span>&gt;, <span class="title">Double</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="number">0L</span>, <span class="number">0L</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">add</span><span class="params">(Tuple2&lt;String, Long&gt; value, Tuple2&lt;Long, Long&gt; accumulator)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(accumulator.f0 + value.f1, accumulator.f1 + <span class="number">1L</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Double <span class="title">getResult</span><span class="params">(Tuple2&lt;Long, Long&gt; accumulator)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> ((<span class="keyword">double</span>) accumulator.f0) / accumulator.f1;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">merge</span><span class="params">(Tuple2&lt;Long, Long&gt; a, Tuple2&lt;Long, Long&gt; b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(a.f0 + b.f0, a.f1 + b.f1);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProcessWindowFunction</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">Double</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Double</span>&gt;, <span class="title">String</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String key,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Context context,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Iterable&lt;Double&gt; averages,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Collector&lt;Tuple2&lt;String, Double&gt;&gt; out)</span> </span>&#123;</span><br><span class="line">      Double average = averages.iterator().next();</span><br><span class="line">      out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(key, average));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="ProcessWindowFunction-与-FoldFunction"><a href="#ProcessWindowFunction-与-FoldFunction" class="headerlink" title="ProcessWindowFunction 与 FoldFunction"></a>ProcessWindowFunction 与 FoldFunction</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;SensorReading&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">  .keyBy(&lt;key selector&gt;)</span><br><span class="line">  .timeWindow(&lt;duration&gt;)</span><br><span class="line">  .fold(<span class="keyword">new</span> Tuple3&lt;String, Long, Integer&gt;(<span class="string">&quot;&quot;</span>,<span class="number">0L</span>, <span class="number">0</span>), <span class="keyword">new</span> MyFoldFunction(), <span class="keyword">new</span> MyProcessWindowFunction())</span><br><span class="line"></span><br><span class="line"><span class="comment">// Function definitions</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyFoldFunction</span></span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">FoldFunction</span>&lt;<span class="title">SensorReading</span>, <span class="title">Tuple3</span>&lt;<span class="title">String</span>, <span class="title">Long</span>, <span class="title">Integer</span>&gt; &gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Tuple3&lt;String, Long, Integer&gt; <span class="title">fold</span><span class="params">(Tuple3&lt;String, Long, Integer&gt; acc, SensorReading s)</span> </span>&#123;</span><br><span class="line">      Integer cur = acc.getField(<span class="number">2</span>);</span><br><span class="line">      acc.setField(cur + <span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line">      <span class="keyword">return</span> acc;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProcessWindowFunction</span></span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">Tuple3</span>&lt;<span class="title">String</span>, <span class="title">Long</span>, <span class="title">Integer</span>&gt;, <span class="title">Tuple3</span>&lt;<span class="title">String</span>, <span class="title">Long</span>, <span class="title">Integer</span>&gt;, <span class="title">String</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String key,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Context context,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Iterable&lt;Tuple3&lt;String, Long, Integer&gt;&gt; counts,</span></span></span><br><span class="line"><span class="function"><span class="params">                    Collector&lt;Tuple3&lt;String, Long, Integer&gt;&gt; out)</span> </span>&#123;</span><br><span class="line">    Integer count = counts.iterator().next().getField(<span class="number">2</span>);</span><br><span class="line">    out.collect(<span class="keyword">new</span> Tuple3&lt;String, Long, Integer&gt;(key, context.window().getEnd(),count));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="ProcessWindowFunction-中使用-state"><a href="#ProcessWindowFunction-中使用-state" class="headerlink" title="ProcessWindowFunction 中使用 state"></a>ProcessWindowFunction 中使用 state</h3><h2 id="WindowFunction（遗留）"><a href="#WindowFunction（遗留）" class="headerlink" title="WindowFunction（遗留）"></a>WindowFunction（遗留）</h2><p>在一些可以使用 ProcessWindowFunction 的地方，你也可以使用 WindowFunction，这是较旧版本的 ProcessWindowFunction，它提供的上下文信息较少，并且没有一些高级功能，例如 per-window keyed state。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">WindowFunction</span>&lt;<span class="title">IN</span>, <span class="title">OUT</span>, <span class="title">KEY</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">extends</span> <span class="title">Function</span>, <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Evaluates the window and outputs none or several elements.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> key The key for which this window is evaluated.</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> window The window that is being evaluated.</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> input The elements in the window being evaluated.</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> out A collector for emitting elements.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@throws</span> Exception The function may throw exceptions to fail the program and trigger recovery.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">apply</span><span class="params">(KEY key, W window, Iterable&lt;IN&gt; input, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(&lt;window assigner&gt;)</span><br><span class="line">    .apply(<span class="keyword">new</span> MyWindowFunction());</span><br></pre></td></tr></table></figure><h1 id="Triggers"><a href="#Triggers" class="headerlink" title="Triggers"></a><strong>Triggers</strong></h1><p>触发器决定了窗口函数什么时候处理窗口中的数据。每一个 WindowAssigner 都会带有一个默认的 Trigger，如果默认的 Trigger 不符合需求，你可以使用 trigger(…) 指定你需要的触发器。</p><p>一个 Trigger 接口有五个方法，可以通过编写函数指定如何对不同的 event 做出相应。</p><table>    <thead>        <tr>            <th style="width: 10%">Function</th>            <th style="width: 20%">作用</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;"><strong>TriggerResult onElement(T element, long timestamp, W window, TriggerContext ctx)</strong>            </td>            <td>每向窗口添加一个元素时触发一次。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>TriggerResult onEventTime(long time, W window, TriggerContext ctx)</strong>            </td>            <td>Event Time timer 触发时调用。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>TriggerResult onProcessingTime(long time, W window, TriggerContext ctx)</strong>            </td>            <td>Processing Time timer 调用时触发。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>void onMerge(W window, OnMergeContext ctx)</strong>            </td>            <td>方法与有状态 Trigger 相关，并在两个触发器的相应窗口合并时合并它们的状态，例如在使用会话窗口时（会话窗口每添加一个元素就会产生一个窗口）。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>void clear(W window, TriggerContext ctx)</strong>            </td>            <td>将当前窗口的 state 清除。            </td>        </tr>    </tbody></table><p>前三个函数通过 TriggerResult 决定如何处理它们的调用事件。</p><table>    <thead>        <tr>            <th style="width: 10%">TriggerResult</th>            <th style="width: 20%">作用</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;"><strong>TriggerResult.CONTINUE</strong>            </td>            <td>什么都不做。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>TriggerResult.FIRE</strong>            </td>            <td>触发计算。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>TriggerResult.PURGE</strong>            </td>            <td>清除窗口内的元素。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>TriggerResult.FIRE_AND_PURGE</strong>            </td>            <td>触发计算，并且在此之后清除窗口内的元素。            </td>        </tr>    </tbody></table><h2 id="触发运算并且清除元素"><a href="#触发运算并且清除元素" class="headerlink" title="触发运算并且清除元素"></a>触发运算并且清除元素</h2><h2 id="WindowAssigners-的默认-Triggers"><a href="#WindowAssigners-的默认-Triggers" class="headerlink" title="WindowAssigners 的默认 Triggers"></a>WindowAssigners 的默认 Triggers</h2><p>很多 WindowAssigners 的默认 Triggers是适用于很多场景的。例如，所有的 event-time window assigners 都将 EventTimeTrigger 作为默认的 Trigger。这个 Trigger 的作用就是当 Watermark 到达了窗口结束时间时就触发。<br><strong>提示1：GlobalWindow 的默认 Trigger 是永远不会触发的 NeverTrigger。</strong><br><strong>提示2：通过使用 trigger() 指定触发器，您将覆盖 WindowAssigner 的默认触发器。例如，如果为 TumblingEventTimeWindows 指定 CountTrigger，则不会再根据时间进度而仅按 count 触发窗口。</strong></p><h2 id="通用的-Triggers"><a href="#通用的-Triggers" class="headerlink" title="通用的 Triggers"></a>通用的 Triggers</h2><table>    <thead>        <tr>            <th style="width: 10%">Trigger</th>            <th style="width: 20%">作用</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;"><strong>EventTimeTrigger</strong>            </td>            <td>根据由 Watermark 计算的 Event Time 进度触发。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>ProcessingTimeTrigger</strong>            </td>            <td>根据 Processing Time 触发。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>CountTrigger</strong>            </td>            <td>在窗口中的元素数超过给定限额时触发。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>PurgingTrigger</strong>            </td>            <td>将另一个 Trigger 作为参数，并将其转换为清除触发器。            </td>        </tr>    </tbody></table><h1 id="Evictors"><a href="#Evictors" class="headerlink" title="Evictors"></a><strong>Evictors</strong></h1><p>Flink 的窗口模型中允许指定除 WindowAssigner 和 Trigger 之外的可选逐出器（Evictor）。可以使用exictor(…)方法指定。Exictor 能够在触发器触发之后，并在使用窗口函数之前或者之后移除元素。为此，逐出器接口有两个方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Optionally evicts elements. Called before windowing function.</span></span><br><span class="line"><span class="comment"> * 在执行窗口函数之前执行。</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> elements The elements currently in the pane.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> size The current number of elements in the pane.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> window The &#123;<span class="doctag">@link</span> Window&#125;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> evictorContext The context for the Evictor</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">evictBefore</span><span class="params">(Iterable&lt;TimestampedValue&lt;T&gt;&gt; elements, <span class="keyword">int</span> size, W window, EvictorContext evictorContext)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Optionally evicts elements. Called after windowing function.</span></span><br><span class="line"><span class="comment"> * 在执行窗口函数之后执行。</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> elements The elements currently in the pane.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> size The current number of elements in the pane.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> window The &#123;<span class="doctag">@link</span> Window&#125;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> evictorContext The context for the Evictor</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">evictAfter</span><span class="params">(Iterable&lt;TimestampedValue&lt;T&gt;&gt; elements, <span class="keyword">int</span> size, W window, EvictorContext evictorContext)</span></span>;</span><br></pre></td></tr></table></figure><table>    <thead>        <tr>            <th style="width: 10%">Evictor</th>            <th style="width: 20%">作用</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;"><strong>CountEvictor</strong>            </td>            <td>保持窗口内元素数量符合用户指定数量，如果多于用户指定的数量，从窗口缓冲区的开头丢弃剩余的元素。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>DeltaEvictor</strong>            </td>            <td>使用 DeltaFunction 和一个阈值，计算窗口缓冲区中的最后一个元素与其余每个元素之间的 delta 值，并删除 delta 值大于或等于阈值的元素。            </td>        </tr>        <tr>            <td style="text-align: center;"><strong>TimeEvictor</strong>            </td>            <td>以毫秒为单位的时间间隔作为参数，对于给定的窗口，找到元素中的最大的时间戳max_ts，并删除时间戳小于max_ts - interval的所有元素。            </td>        </tr>    </tbody></table><p>默认情况下，都只会在执行 WindowFunction 之前执行 Evictor。</p><p><strong>提示：Flink 不能保证窗口中元素的顺序。这意味着尽管逐出器可能会从窗口的开头移除元素，但这些元素不一定是最先到达或最后到达的元素。</strong></p><h1 id="Allowed-Lateness"><a href="#Allowed-Lateness" class="headerlink" title="Allowed Lateness"></a><strong>Allowed Lateness</strong></h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>使用 Event Time 窗口时，可能会发生元素到达晚的情况，即 Flink 用于跟踪 Event Time 进度的 Watermark 已超过元素所属窗口的结束时间戳。</p><p>默认情况下，当发现 Watermark 已经超过到达的元素所属的窗口结束时间时，将删除这个延迟元素。但是 Flink 可以给窗口算子指定一个最大允许延迟时间。Allowed lateness 指定元素在被删除之前可以延迟多少时间，其默认值为0。</p><p>在迟到元素到达时，如果 Watermark 大于其所属窗口的结束时间时，并且 Watermark 小于窗口结束时间加上 allowed lateness，这个迟到的元素仍然可以被加到这个窗口内进行运算。有的触发器会出现在延迟但是没有丢弃的元素到达时，使得窗口再次计算，比如 EventTimeTrigger。</p><p><strong>提示1：在 assignTimestampsAndWatermarks 时有一个 maxOutOfOrderness 的概念，maxOutOfOrderness 是生成 Watermark 所需要的，是指元素最大无序时间。而 Allowed Lateness 是指在 Watermark 到达窗口结束时间之后允许延迟多长时间，两个概念不一样。</strong><br><strong>提示2：<br>a.通过 watermark 机制来处理 out-of-order 的问题，属于第一层防护，属于全局性的防护，通常说的乱序问题的解决办法，就是指这类；<br>b.通过窗口上的 allowedLateness 机制来处理 out-of-order 的问题，属于第二层防护，属于特定 window operator 的防护，late element 的问题就是指这类</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;T&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(&lt;window assigner&gt;)</span><br><span class="line">    .allowedLateness(&lt;time&gt;)</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br></pre></td></tr></table></figure><p><strong>提示：当使用 GlobalWindows 时，没有元素会被认为是迟到的，因为这个窗口哦的结束时间时 Long.MAX_VALUE。</strong></p><h2 id="迟到的数据做旁路输出（side-output）"><a href="#迟到的数据做旁路输出（side-output）" class="headerlink" title="迟到的数据做旁路输出（side output）"></a>迟到的数据做旁路输出（side output）</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> OutputTag&lt;T&gt; lateOutputTag = <span class="keyword">new</span> OutputTag&lt;T&gt;(<span class="string">&quot;late-data&quot;</span>)&#123;&#125;;</span><br><span class="line"></span><br><span class="line">DataStream&lt;T&gt; input = ...;</span><br><span class="line"></span><br><span class="line">SingleOutputStreamOperator&lt;T&gt; result = input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(&lt;window assigner&gt;)</span><br><span class="line">    .allowedLateness(&lt;time&gt;)</span><br><span class="line">    .sideOutputLateData(lateOutputTag)</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br><span class="line"></span><br><span class="line">DataStream&lt;T&gt; lateStream = result.getSideOutput(lateOutputTag);</span><br></pre></td></tr></table></figure><h2 id="拓展思考"><a href="#拓展思考" class="headerlink" title="拓展思考"></a>拓展思考</h2><p>当指定 Allowed Lateness &gt; 0 时，在 Watermark 通过窗口结束时间后，<strong>将保留窗口及其内容</strong>。在这种情况下，当一个延迟但未被丢弃的元素到达时，它可能会再次触发窗口运算。这些被触发运算的被称为 late firing。在使用会话窗口时，它们可能会将两个预先存在的未合并窗口进行合并，下面是一个例子。</p><p>比如：有一个会话窗口且 Gap 为3分钟，现在有两个窗口，第一个窗口起始和结束时间为（01:00:00，01:00:05），第二个窗口起始和结束时间为（01:00:09，01:00:15），如果我们在此时不设置 Allowed Lateness 时，那么如果不保存第一个窗口的数据，运算第二个窗口的数据时，不会有什么问题，但是如果我们设置了 Allowed Lateness = 5 min，那么这时就会有问题了，比如有迟到元素01:00:15才到达，元素自己的时间戳为01:00:07，这样这个元素就可以将两个窗口的数据结合为一个窗口。</p><p><strong>提示：延迟数据触发的运算应该将之前的计算结果更新，所以如果下游 sink 使用了 kafka，则这种情况不是很适用（除非消费 kafka 的是一些 updateable dfs），否则，你将会得到很多的对相同组数据计算的结果。</strong></p><h1 id="窗口结果的使用"><a href="#窗口结果的使用" class="headerlink" title="窗口结果的使用"></a><strong>窗口结果的使用</strong></h1><p>窗口计算的结果也会转化为一个数据流，这份结果中不会包含窗口操作的任何信息，所以如果后续计算中需要这些信息，你必须使用 ProcessWindowFunction 将这些信息通过编码传输进去。</p><h2 id="窗口和-Watermark-的联系"><a href="#窗口和-Watermark-的联系" class="headerlink" title="窗口和 Watermark 的联系"></a>窗口和 Watermark 的联系</h2><p>当 Watermark 到达窗口算子处时，会触发两个事件：<br>1.Watermark 会触发所有的窗口中的最大时间戳（窗口结束时间戳 - 1）&lt; 到达的最新 Watermark的窗口运算。<br>2.将 Watermark 发送到下游算子。<br>Intuitively, a watermark “flushes” out any windows that would be considered late in downstream operations once they receive that watermark.</p><h2 id="连续的窗口算子"><a href="#连续的窗口算子" class="headerlink" title="连续的窗口算子"></a>连续的窗口算子</h2><h2 id="设置窗口时产生的状态大小的注意事项"><a href="#设置窗口时产生的状态大小的注意事项" class="headerlink" title="设置窗口时产生的状态大小的注意事项"></a>设置窗口时产生的状态大小的注意事项</h2><p>窗口大小可以定义的很大（如天、周或月），因此可能会累积非常大的状态（state）。所以在估计窗口计算的存储需求时，需要记住以下几个规则：</p><p>1.Flink 会为每个元素所属的窗口创建一个副本。因此，滚动的窗口保留每个元素的一个副本（一个元素只属于一个窗口）。相反，滑动窗口可能会创建每个元素中的几个副本。因此，1天大小的窗口，1秒滑动步长的滑动窗口可能不是一个好主意。</p><p>2.ReduceFunction、AggregateFunction 和 FoldFunction 可以显著减少存储需求，因为它们在元素到达时就聚合元素，并且每个窗口只存储一个值。相反，仅仅使用 ProcessWindowFunction 就需要累积所有元素。</p><p>3.使用 Evictor 可防止任何预聚合，因为在应用计算之前，窗口的所有元素都必须通过 Evictor 传递。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：DataStream Api 中 Operators（算子）——概览</title>
    <link href="https://yangyichao-mango.github.io/2019/11/09/apache-flink:study-flink-datastream-operators-overview/"/>
    <id>https://yangyichao-mango.github.io/2019/11/09/apache-flink:study-flink-datastream-operators-overview/</id>
    <published>2019-11-09T08:22:30.000Z</published>
    <updated>2019-11-09T14:47:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>描述了基本 Operators 的 Transformations，应用这些转换后如何进行 physical partitioning（物理分区），以及对Flink算子链的深入了解。</p><span id="more"></span><h1 id="DataStream-Transformations"><a href="#DataStream-Transformations" class="headerlink" title="DataStream Transformations"></a><strong>DataStream Transformations</strong></h1><h1 id="Physical-partitioning"><a href="#Physical-partitioning" class="headerlink" title="Physical partitioning"></a><strong>Physical partitioning</strong></h1><table>    <thead>        <tr>            <th style="width: 10%">Transformation</th>            <th style="width: 20%">Description</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">Custom partitioning<br>                DataStream → DataStream<br>                <strong>CustomPartitionerWrapper<<T>k, T></strong>            </td>            <td>使用自定义的 partitioner 为每一条 record 选择下一个 task<br>                <strong>dataStream.partitionCustom(partitioner, "someKey");</strong><br>                <strong>dataStream.partitionCustom(partitioner, 0);</strong>            </td>        </tr>        <tr>            <td style="text-align: center;">Random partitioning<br>                DataStream → DataStream<br>                <strong>ShufflePartitioner<<T>T></strong>            </td>            <td>按随机均匀划分元素<br>                <strong>dataStream.shuffle();</strong>            </td>        </tr>        <tr>            <td style="text-align: center;">Rebalancing (Round-robin partitioning)<br>                DataStream → DataStream<br>                <strong>RebalancePartitioner<<T>T></strong>            </td>            <td>分区循环划分元素，为每个下游创建相等的负载。对于存在数据倾斜的性能优化非常有用。<br>                <strong>dataStream.rebalance();</strong>            </td>        </tr>        <tr>            <td style="text-align: center;">Rescaling<br>                DataStream → DataStream<br>                <strong>RescalePartitioner<<T>T></strong>            </td>            <td>将元素循环（round robin）分配到下游 operator 的子集。如果你的 pipeline 是一下的情况，那么这种方式会非常有用。<br>                例如，将并行数据源的每个实例的数据传输到的下游多个算子（operators）一个子集以分配负载。但不希望 full rebalance，则这非常有用。<br>                如果合理配置 TaskManager 的 slot数量，则数据传输只需要本地传输，而不需要通过网络传输数据。<br><br>                上游 operators 向的下游 operators 发送 record 取决于上游 operators 和下游 operators 的并行度。<br>                例如，如果上游 operator 的并行度为2，而下游 operator 的并行度为6，则一个上游 operator 将 record 分配给三个下游 operator，而另一个上游 operator 将 record 分配给其他三个下游 operator。相反，如果下游 operator 的并行度为2，而上游 operator 的并行度为6，则三个上游 operator 将分配给一个下游 operator，而其他三个上游 operator 将分配给另一个下游 operator。<br><br>                如果上下游算子的并行度不是彼此的倍数，则一个或多个下游 operator 将具有来自上游 operator 的不同数量的输入。                如下图：                <figure>                    <div class="img-lightbox">                        <div class="overlay"></div>                        <img src="/blog-img/apache-flink:study-flink-datastream-operators-overview/rescale_partition.svg" alt="rescale_partition" title="">                    </div>                </figure>                <strong>dataStream.rescale();</strong>            </td>        </tr>        <tr>            <td style="text-align: center;">Broadcasting<br>                DataStream → DataStream<br>                <strong>BroadcastPartitioner<<T>T></strong>            </td>            <td>广播数据到下游的每个partition。<br>                <strong>dataStream.broadcast();</strong>            </td>        </tr>        <tr>            <td style="text-align: center;">Local Forward<br>                DataStream → DataStream<br>                <strong>ForwardPartitioner<<T>T></strong>            </td>            <td>数据传输到本地的下游算子。<br>                <strong>dataStream.forward();</strong>            </td>        </tr>        <tr>            <td style="text-align: center;">GlobalPartitioner<br>                DataStream → DataStream<br>                <strong>GlobalPartitioner<<T>T></strong>            </td>            <td>数据传输到下游子任务id为0的task中。<br>                <strong>dataStream.forward();</strong>            </td>        </tr>        <tr>            <td style="text-align: center;">Key Groups<br>                DataStream → DataStream<br>                <strong>KeyGroupStreamPartitioner<<T>T, K></strong>            </td>            <td>相同key的值会传输到同一个下游。类似于Rescaling，但是不用再api中指定，再使用keyBy时会自动指定此方法。<br>                <strong>dataStream.keyBy();</strong>            </td>        </tr>    </tbody></table><h1 id="Task-chaining-和-资源组"><a href="#Task-chaining-和-资源组" class="headerlink" title="Task chaining 和 资源组"></a><strong>Task chaining 和 资源组</strong></h1><p>链接两个 Transformations 意味着可以将它们共同放在在同一个线程中执行以获得更好的性能。<br>默认情况下，Flink会尽可能链接两个算子（例如，两个 Map Transformations）。如果需要，可以使用API对 Task chaining 进行细粒度控制：</p><p>在 Flink 中，一个 slot 就是一个资源组。如果需要的话，你可以通过使用api把上下游算子隔离在不同的 slot 中运行。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：DataStream Api 中 EventTime</title>
    <link href="https://yangyichao-mango.github.io/2019/11/09/apache-flink:study-flink-datastream-eventtime/"/>
    <id>https://yangyichao-mango.github.io/2019/11/09/apache-flink:study-flink-datastream-eventtime/</id>
    <published>2019-11-09T04:09:56.000Z</published>
    <updated>2019-11-09T07:41:48.694Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：DataStream Api 中 EventTime</p><span id="more"></span><p><img src="/blog-img/apache-flink:study-flink-datastream-eventtime/%E6%97%B6%E9%97%B4.svg" alt="时间"></p><h1 id="Processing-time"><a href="#Processing-time" class="headerlink" title="Processing time"></a><strong>Processing time</strong></h1><p>处理时间是指执行相应操作的机器的系统时间</p><h1 id="Event-time"><a href="#Event-time" class="headerlink" title="Event time"></a><strong>Event time</strong></h1><p>事件时间是每个事件在其生产设备（生产event的设备，手机等的源头设备）上发生的时间</p><h2 id="Event-Time-和-Watermark"><a href="#Event-Time-和-Watermark" class="headerlink" title="Event Time 和 Watermark"></a>Event Time 和 Watermark</h2><p>Flink中测量事件时间进度的机制是Watermark。Watermark作为数据流的一部分流动，并带有时间戳t。Watermark（t）声明该流中的 Event Time 已达到时间t，这意味着流中不应再有时间戳t’&lt;=t的元素（即 Event Time 早于或等于 Watermark 的事件）</p><p>下图显示了具有时间戳的事件流，以及内联流动的水印。在这个例子中，事件是有序的，这意味着 Watermark 只是流中的周期性标记。</p><p><img src="/blog-img/apache-flink:study-flink-datastream-eventtime/%E9%A1%BA%E5%BA%8F%E6%B5%81%E7%9A%84Watermark.svg" alt="顺序流的Watermark"></p><p><strong>Watermark对于无序流是至关重要的</strong>，如下所示，其中事件到达顺序不是按时间戳顺序。Watermark代表通过流中的该点，这个时间戳之前的事件都应该到达了。一旦Watermark到达算子，算子就可以将其内部事件时钟更新到Watermark的值。</p><p><img src="/blog-img/apache-flink:study-flink-datastream-eventtime/%E6%97%A0%E5%BA%8F%E6%B5%81%E7%9A%84Watermark.svg" alt="无序流的Watermark"></p><h2 id="并行流的-Watermarks"><a href="#并行流的-Watermarks" class="headerlink" title="并行流的 Watermarks"></a>并行流的 Watermarks</h2><p>Watermark 在源数据 Function 处生成，或直接在源数据 Function 之后生成。源数据 Function 的每个并行子任务通常独立生成其 Watermark。这些 Watermark 定义并行源数据的 Event Time。</p><p>当 Watermark 流过程序时，会更新到达算子的 Event Time，当一个 Watermark 更新了算子的 Event Time 时，它会为其下游的后续算子生成一个新的 Watermark。</p><p>某些算子消费多个输入流，例如：union 算子或者跟在 keyBy，partition 算子的之后的算子。这样的算子的 Event Time 是所有输入 stream 中最小的 Watermark，即：<br><strong>Operator Event Time = min(Input Stream 1 Watermark, Input Stream 2 Watermark…)</strong><br>算子会跟着输入流 Watermark 的更新来更新算子自己的 Event Time。</p><p>下图显示了流经并行流的事件和 Watermark 以及算子获取 Event Time 的示例。</p><p><img src="/blog-img/apache-flink:study-flink-datastream-eventtime/%E5%B9%B6%E8%A1%8C%E6%B5%81%E7%9A%84Watermarks.svg" alt="并行流的Watermarks"></p><p>注意，Kafka支持分区 Watermark</p><h1 id="Ingestion-time"><a href="#Ingestion-time" class="headerlink" title="Ingestion time"></a><strong>Ingestion time</strong></h1><p>注入时间是事件进入Flink Job的时间。在源Operator处，每条Operator获取源数据的时间作为Ingestion time时间戳</p><h1 id="生成-Timestamps-和-Watermarks"><a href="#生成-Timestamps-和-Watermarks" class="headerlink" title="生成 Timestamps 和 Watermarks"></a><strong>生成 Timestamps 和 Watermarks</strong></h1><h2 id="分配-Timestamps"><a href="#分配-Timestamps" class="headerlink" title="分配 Timestamps"></a>分配 Timestamps</h2><h3 id="数据流源生成-Timestamps-和-Watermarks"><a href="#数据流源生成-Timestamps-和-Watermarks" class="headerlink" title="数据流源生成 Timestamps 和 Watermarks"></a>数据流源生成 Timestamps 和 Watermarks</h3><p>数据源可以直接为它们产生的数据分配 Timestamp，并且他们也能发送 Watermark。这样做的话，在后面的处理中就没必要再去定义 Timestamp 分配器了，需要注意的是：如果在后面的处理中使用了一个 timestamp 分配器，由数据源提供的任何 timestamp 和 watermark 都会被重写。</p><h3 id="Timestamp-分配器-Watermark生成器"><a href="#Timestamp-分配器-Watermark生成器" class="headerlink" title="Timestamp 分配器 / Watermark生成器"></a>Timestamp 分配器 / Watermark生成器</h3><p>Timestamp 分配器获取一个流并生成一个新的带有 Timestamp 元素和 Watermark 的流。如果上游的原始数据流已经有 Timestamp 或 Watermark，则 Timestamp 分配器将覆盖上游的 Timestamp 或 Watermark</p><p>Timestamp 分配器通常在数据源之后立即指定，但这并不是严格要求的。通常是在 Timestamp 分配器之前先解析（MapFunction）和过滤（FilterFunction）数据源。在任何情况下，都需要在基于 Event Time 算子（例如 window 操作）运行之前指定 Timestamp 分配程序。<br>有一个特殊情况，当使用 Kafka 作为流作业的数据源时，Flink 允许在数据源内部指定 Timestamp 分配器和 Watermark 生成器。更多关于如何进行的信息请参考Kafka Connector的文档。</p><p>直接在FlinkKafkaConsumer010上面使用assignTimestampsAndWatermarks可以根据kafka source的partitions的特性进行设置Timestamps和Watermarks，让用户做一些特殊的处理</p><p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka<br>partition, allows users to let them exploit the per-partition characteristics.</p><h1 id="Kafka-分区的-Timestamp"><a href="#Kafka-分区的-Timestamp" class="headerlink" title="Kafka 分区的 Timestamp"></a><strong>Kafka 分区的 Timestamp</strong></h1><p>当使用 Apache Kafka 作为数据源时，每个 Kafka 分区可能有一个简单的 Event Time 模式（递增的时间戳或有界无序）。然而，当 Flink Job 使用来自Kafka的流时，多个分区常常并行消费，每一个 operator 算子并行消费时就会破坏各个分区的时间模式（这是 Kafka 客户端消费 Kafka 数据必然发生的）。</p><p>在这种情况下，可以使用 Flink’s Kafka-partition-aware watermark generation，使用该功能，每个 Kafka 分区在 Kafka consumer 内部生成 Watermark，每个分区合并 Watermark 的方式与流 shuffles 时合并 Watermark 的方式相同。</p><p>例如，如果事件时间戳严格按照 Kafka 分区递增，则使用递增时间戳 Watermark 生成器生成每个分区的 Watermark 将是完美的全局 Watermark。</p><p>下图显示了如何使用 Flink’s Kafka-partition-aware watermark generation，以及在这种情况下 Watermark 如何通过流数据流传播。</p><p><img src="/blog-img/apache-flink:study-flink-datastream-eventtime/KafkaSource%E5%A4%9A%E5%88%86%E5%8C%BAWatermark.svg" alt="KafkaSource多分区Watermark"></p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：slot和parallelism设置的关系</title>
    <link href="https://yangyichao-mango.github.io/2019/11/08/apache-flink:study-flink-slot-parallelism/"/>
    <id>https://yangyichao-mango.github.io/2019/11/08/apache-flink:study-flink-slot-parallelism/</id>
    <published>2019-11-08T07:31:08.000Z</published>
    <updated>2019-11-08T15:55:53.544Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：slot和parallelism设置的关系</p><span id="more"></span><h1 id="如何设置-parallelism"><a href="#如何设置-parallelism" class="headerlink" title="如何设置 parallelism"></a><strong>如何设置 parallelism</strong></h1><h2 id="flink-conf-yaml"><a href="#flink-conf-yaml" class="headerlink" title="flink-conf.yaml"></a>flink-conf.yaml</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cat flink-conf.yaml | grep parallelism</span><br><span class="line"></span><br><span class="line"><span class="comment"># The parallelism used for programs that did not specify and other parallelism.</span></span><br><span class="line">parallelism.default: 1</span><br></pre></td></tr></table></figure><h2 id="命令行启动"><a href="#命令行启动" class="headerlink" title="命令行启动"></a>命令行启动</h2><p>如果你是用命令行启动你的 Flink job，那么你也可以这样设置并行度(使用 -p 并行度)：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/flink run -p 10 ../word-count.jar</span><br></pre></td></tr></table></figure><h2 id="代码设置整个程序的并行度"><a href="#代码设置整个程序的并行度" class="headerlink" title="代码设置整个程序的并行度"></a>代码设置整个程序的并行度</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setParallelism(<span class="number">10</span>);</span><br></pre></td></tr></table></figure><p>注意：这样设置的并行度是你整个程序的并行度，那么后面如果你的每个算子不单独设置并行度覆盖的话，那么后面每个算子的并行度就都是这里设置的并行度的值了。</p><h2 id="每个算子单独设置并行度"><a href="#每个算子单独设置并行度" class="headerlink" title="每个算子单独设置并行度"></a>每个算子单独设置并行度</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data.keyBy(<span class="keyword">new</span> xxxKey())</span><br><span class="line">    .flatMap(<span class="keyword">new</span> XxxFlatMapFunction()).setParallelism(<span class="number">5</span>)</span><br><span class="line">    .map(<span class="keyword">new</span> XxxMapFunction).setParallelism(<span class="number">5</span>)</span><br><span class="line">    .addSink(<span class="keyword">new</span> XxxSink()).setParallelism(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>如上，就是在每个算子后面单独的设置并行度，这样的话，就算你前面设置了 env.setParallelism(10) 也是会被覆盖的。</p><p>这也说明优先级是：算子设置并行度 &gt; env 设置并行度 &gt; 配置文件默认并行度</p><h1 id="slot"><a href="#slot" class="headerlink" title="slot"></a><strong>slot</strong></h1><p><img src="/blog-img/apache-flink:study-flink-slot-parallelism/FlinkJob%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84.svg" alt="FlinkJob运行架构"></p><p>图中 Task Manager 是从 Job Manager 处接收需要部署的 Task，任务的并行性由每个 Task Manager 上可用的 slot 决定。每个任务代表分配给任务槽的一组资源，slot 在 Flink 里面可以认为是资源组，Flink 将每个任务分成子任务并且将这些子任务分配到 slot 来并行执行程序。</p><p>例如，如果 Task Manager 有四个 slot，那么它将为每个 slot 分配 25％ 的内存。 可以在一个 slot 中运行一个或多个线程。 同一 slot 中的线程共享相同的 JVM。 同一 JVM 中的任务共享 TCP 连接和心跳消息。Task Manager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离。默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 task 的 subtask，只要它们来自相同的 job。这种共享可以有更好的资源利用率。</p><p><img src="/blog-img/apache-flink:study-flink-slot-parallelism/TaskSlots%E6%89%A7%E8%A1%8C.svg" alt="TaskSlots执行"></p><p>默认情况下，Flink 允许 subtasks 共享 slots，即使它们是不同 tasks 的 subtasks，只要它们来自同一个 job。因此，一个 slot 可能会负责这个 job 的整个管道（pipeline）。允许 slot sharing 有两个好处：</p><p>1.Flink 集群需要与 job 中使用的最高并行度一样多的 slots。这样不需要计算作业总共包含多少个 tasks（具有不同并行度）。</p><p>2.更好的资源利用率。在没有 slot sharing 的情况下，简单的 subtasks（source/map()）将会占用和复杂的 subtasks （window）一样多的资源。通过 slot sharing，将示例中的并行度从 2 增加到 6 可以充分利用 slot 的资源，同时确保繁重的 subtask 在 TaskManagers 之间公平地获取资源。</p><p>下图即为Flink subtasks 共享 slots的模式：</p><p><img src="/blog-img/apache-flink:study-flink-slot-parallelism/TaskSlotsSharing%E6%89%A7%E8%A1%8C.svg" alt="TaskSlotsSharing执行"></p><p>上面图片中有两个 Task Manager，每个 Task Manager 有三个 slot，这样我们的算子最大并行度那么就可以达到 6 个，在同一个 slot 里面可以执行 1 至多个子任务。</p><p>那么再看上面的图片，source/map/keyby/window/apply 最大可以有 6 个并行度，sink 只用了 1 个并行。</p><p>每个 Flink TaskManager 在集群中提供 slot。 slot 的数量通常与每个 TaskManager 的可用 CPU 内核数成比例。一般情况下你的 slot 数是你每个 TaskManager 的 cpu 的核数</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：Flink Job ExecutionGraph生成过程</title>
    <link href="https://yangyichao-mango.github.io/2019/11/08/apache-flink:study-flink-job-ExecutionGraph/"/>
    <id>https://yangyichao-mango.github.io/2019/11/08/apache-flink:study-flink-job-ExecutionGraph/</id>
    <published>2019-11-08T01:49:18.000Z</published>
    <updated>2019-11-08T02:37:23.427Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：Flink Job 执行计划生成过程</p><span id="more"></span><h1 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a><strong>Transformations</strong></h1><p><img src="/blog-img/apache-flink:study-flink-job-ExecutionGraph/TransformationClasses.png" alt="TransformationClasses"></p><p>并不是每一个 Transformation 都会转换成runtime层中的物理操作。有一些只是逻辑概念，比如union、split/select、partition等。如下图所示的转换树，在运行时会优化成下方的操作图。</p><p><img src="/blog-img/apache-flink:study-flink-job-ExecutionGraph/Transformations.png" alt="Transformations"></p><h1 id="执行计划转换过程"><a href="#执行计划转换过程" class="headerlink" title="执行计划转换过程"></a><strong>执行计划转换过程</strong></h1><p><img src="/blog-img/apache-flink:study-flink-job-ExecutionGraph/4%E5%B1%82%E8%BD%AC%E5%8C%96.png" alt="4层转换"></p><p>1.转换过程 StreamExecutionEnvironment 存放的 transformation -&gt; StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; 物理执行图<br>2.StreamExecutionEnvironment 存放的 transformation -&gt; StreamGraph -&gt; JobGraph 在<strong>客户端</strong>完成，然后提交 JobGraph 到 JobManager<br>3.JobManager 的主节点 JobMaster，将 JobGraph 转化为 ExecutionGraph，然后发送到不同的 taskManager，得到实际的物理执行图</p><h2 id="LocalStreamEnvironment-中-parallelism"><a href="#LocalStreamEnvironment-中-parallelism" class="headerlink" title="LocalStreamEnvironment 中 parallelism"></a><strong>LocalStreamEnvironment 中 parallelism</strong></h2><p>其中 LocalStreamEnvironment Task 中的 parallelism 数量是根据以下代码生成的</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Public</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamExecutionEnvironment</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> defaultLocalParallelism = Runtime.getRuntime().availableProcessors();</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Creates a &#123;<span class="doctag">@link</span> LocalStreamEnvironment&#125;. The local execution environment</span></span><br><span class="line"><span class="comment">     * will run the program in a multi-threaded fashion in the same JVM as the</span></span><br><span class="line"><span class="comment">     * environment was created in. The default parallelism of the local</span></span><br><span class="line"><span class="comment">     * environment is the number of hardware contexts (CPU cores / threads),</span></span><br><span class="line"><span class="comment">     * unless it was specified differently by &#123;<span class="doctag">@link</span> #setParallelism(int)&#125;.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> A local execution environment.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> LocalStreamEnvironment <span class="title">createLocalEnvironment</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> createLocalEnvironment(defaultLocalParallelism);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Creates a &#123;<span class="doctag">@link</span> LocalStreamEnvironment&#125;. The local execution environment</span></span><br><span class="line"><span class="comment">     * will run the program in a multi-threaded fashion in the same JVM as the</span></span><br><span class="line"><span class="comment">     * environment was created in. It will use the parallelism specified in the</span></span><br><span class="line"><span class="comment">     * parameter.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parallelism</span></span><br><span class="line"><span class="comment">     * The parallelism for the local environment.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> A local execution environment with the specified parallelism.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> LocalStreamEnvironment <span class="title">createLocalEnvironment</span><span class="params">(<span class="keyword">int</span> parallelism)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> createLocalEnvironment(parallelism, <span class="keyword">new</span> Configuration());</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：异步IO之RichAsyncFunction</title>
    <link href="https://yangyichao-mango.github.io/2019/11/06/apache-flink:study-async-io-RichAsyncFunction/"/>
    <id>https://yangyichao-mango.github.io/2019/11/06/apache-flink:study-async-io-RichAsyncFunction/</id>
    <published>2019-11-06T10:45:59.000Z</published>
    <updated>2019-11-08T02:42:18.399Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：异步IO之RichAsyncFunction</p><span id="more"></span><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a><strong>问题</strong></h2><h3 id="设置kafka-consumer并行度的语义"><a href="#设置kafka-consumer并行度的语义" class="headerlink" title="设置kafka consumer并行度的语义"></a>设置kafka consumer并行度的语义</h3><p>1.如果设置kafka consumer的并发度为100，并且申请到集群中资源的Task Manager的slot个数也为100个，则每个slot中运行的任务都生成这么多数量的kafka consumer，还是每个slot一个kafka consumer?</p><p>2.场景：一个keyBy过后设置了一分钟的窗口dataStream中，如果保证每次触发这个窗口时，窗口的数据永远只有一条的话，并且在保证窗口为1分钟大小的情况下，接口返回速度保证在10秒，使用Async IO是否就没有意义了，因为当前请求队列里面只有一条数据</p><p>3.flink 默认执行一个Job的slot中线程数为什么是8，在哪里设置的</p><h3 id="使用AsyncIO需要考虑的指标"><a href="#使用AsyncIO需要考虑的指标" class="headerlink" title="使用AsyncIO需要考虑的指标"></a>使用AsyncIO需要考虑的指标</h3><p>1.每个slot中Flink Job的线程数<br>2.如果需要使用时间窗口：时间窗口的大小，几分钟的窗口<br>2.如果需要keyBy：每个slot中Flink Job的大概key的个数（什么情况使用，什么情况不使用）</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a><strong>简介</strong></h2><p>我们知道flink对于外部数据源的操作可以通过自带的连接器，或者自定义sink和source实现数据的交互，那么为啥还需要异步IO呢？<br>那时因为对于实时处理，当我们需要使用外部存储数据参与计算时，与外部系统之间的交互延迟对流处理的整个工作进度起决定性的影响。<br>如果我们是使用传统方式mapfunction等算子里访问外部存储，实际上该交互过程是同步的，比如下图中：请求a发送到数据库，那么function会一直等待响应。在很多案例中，这个等待过程是非常浪费函数时间的。</p><p><img src="/blog-img/apache-flink:study-async-io-RichAsyncFunction/%E5%BC%82%E6%AD%A5IO.jpeg" alt="异步IO"></p><p>图中棕色的长条表示等待时间，可以发现网络等待时间极大地阻碍了吞吐和延迟。为了解决同步访问的问题，异步模式可以并发地处理多个请求和回复。<br>也就是说，你可以连续地向数据库发送用户a、b、c等的请求，与此同时，哪个请求的回复先返回了就处理哪个回复，从而连续的请求之间不需要阻塞等待，如上图右边所示。<br>这也正是 Async I/O 的实现原理。</p><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a><strong>目的</strong></h2><p>将MapFunction或者FlatMapFunction中的同步访问外部存储设备的方法通过AsyncFunction替换以实现异步访问<br>在执行过程中，如果使用了keyBy，则相同的key整个执行周期都使用同一个线程，但是不同的key也可以使用同一个线程</p><h2 id="如何使用Async-I-O"><a href="#如何使用Async-I-O" class="headerlink" title="如何使用Async I/O"></a><strong>如何使用Async I/O</strong></h2><p>我们需要自定义一个类实现RichAsyncFunction这个抽象类，实现其中的抽象方法，这点和自定义source很像。<br>主要是的抽象方法如下，然后在asyncInvoke()使用CompletableFuture执行异步操作（CompletableFuture会提供一个ForkJoinPool作为请求线程池）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">asyncInvoke</span><span class="params">(IN var1, ResultFuture&lt;OUT&gt; var2)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">default</span> <span class="keyword">void</span> <span class="title">timeout</span><span class="params">(IN input, ResultFuture&lt;OUT&gt; resultFuture)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    resultFuture.completeExceptionally(<span class="keyword">new</span> TimeoutException(<span class="string">&quot;Async function call has timed out.&quot;</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后在AsyncDataStream中使用我们定义好的类，去实现主流异步的访问外部数据源</p><h2 id="原理实现"><a href="#原理实现" class="headerlink" title="原理实现"></a><strong>原理实现</strong></h2><p>AsyncDataStream.(un)orderedWait 的主要工作就是创建了一个 AsyncWaitOperator。<br>AsyncWaitOperator 是支持异步 IO 访问的算子实现，该算子会运行 AsyncFunction 并处理异步返回的结果，其内部原理如下图所示</p><p><img src="/blog-img/apache-flink:study-async-io-RichAsyncFunction/%E5%BC%82%E6%AD%A5%E5%8E%9F%E7%90%86.jpeg" alt="异步原理"></p><p>如图所示，AsyncWaitOperator 主要由两部分组成：StreamElementQueue 和 Emitter。<br>StreamElementQueue 是一个 Promise 队列，所谓 Promise 是一种异步抽象表示将来会有一个值（参考 Scala Promise 了解更多），这个队列是未完成的 Promise 队列，也就是进行中的请求队列。<br>Emitter 是一个单独的线程，负责发送消息（收到的异步回复）给下游。</p><p>图中E5表示进入该算子的第五个元素（”Element-5”），在执行过程中首先会将其包装成一个 “Promise” P5，然后将P5放入队列。<br>最后调用 AsyncFunction 的 ayncInvoke 方法，该方法会向外部服务发起一个异步的请求，并注册回调。<br>该回调会在异步请求成功返回时调用 AsyncCollector.collect 方法将返回的结果交给框架处理。<br>实际上 AsyncCollector 也一个 Promise，也就是 P5，在调用 collect 的时候会标记 Promise 为完成状态，并通知 Emitter 线程有完成的消息可以发送了。<br>Emitter 就会从队列中拉取完成的 Promise ，并从 Promise 中取出消息发送给下游。</p><h2 id="消息的顺序性"><a href="#消息的顺序性" class="headerlink" title="消息的顺序性"></a><strong>消息的顺序性</strong></h2><p>上文提到 Async I/O 提供了两种输出模式。<br>其实细分有三种模式: 有序，ProcessingTime 无序，EventTime 无序。<br>Flink 使用队列来实现不同的输出模式，并抽象出一个队列的接口（StreamElementQueue），这种分层设计使得AsyncWaitOperator和Emitter不用关心消息的顺序问题。<br>StreamElementQueue有两种具体实现，分别是 OrderedStreamElementQueue 和 UnorderedStreamElementQueue。<br>UnorderedStreamElementQueue 比较有意思，它使用了一套逻辑巧妙地实现完全无序和 EventTime 无序</p><h3 id="有序"><a href="#有序" class="headerlink" title="有序"></a>有序</h3><p>有序比较简单，使用一个队列就能实现。<br>所有新进入该算子的元素（包括 watermark），都会包装成 Promise 并按到达顺序放入该队列。<br>如下图所示，尽管P4的结果先返回，但并不会发送，只有 P1 （队首）的结果返回了才会触发 Emitter 拉取队首元素进行发送</p><p><img src="/blog-img/apache-flink:study-async-io-RichAsyncFunction/%E6%9C%89%E5%BA%8F.jpeg" alt="有序"></p><h3 id="ProcessingTime-无序"><a href="#ProcessingTime-无序" class="headerlink" title="ProcessingTime 无序"></a>ProcessingTime 无序</h3><p>ProcessingTime 无序也比较简单，因为没有 watermark，不需要协调 watermark 与消息的顺序性，所以使用两个队列就能实现，一个 uncompletedQueue 一个 completedQueue。<br>所有新进入该算子的元素，同样的包装成 Promise 并放入 uncompletedQueue 队列，当uncompletedQueue队列中任意的Promise返回了数据，则将该 Promise 移到 completedQueue 队列中，并通知 Emitter 消费。<br>如下图所示：</p><p><img src="/blog-img/apache-flink:study-async-io-RichAsyncFunction/ProcessingTime%E6%97%A0%E5%BA%8F.jpeg" alt="ProcessingTime无序"></p><h3 id="EventTime-无序"><a href="#EventTime-无序" class="headerlink" title="EventTime 无序"></a>EventTime 无序</h3><p>EventTime 无序类似于有序与 ProcessingTime 无序的结合体。<br>因为有 watermark，需要协调 watermark 与消息之间的顺序性，所以uncompletedQueue中存放的元素从原先的 Promise 变成了 Promise 集合。<br>如果进入算子的是消息元素，则会包装成 Promise 放入队尾的集合中。<br>如果进入算子的是 watermark，也会包装成 Promise 并放到一个独立的集合中，再将该集合加入到 uncompletedQueue 队尾，最后再创建一个空集合加到 uncompletedQueue 队尾。<br>这样，watermark 就成了消息顺序的边界。<br>只有处在队首的集合中的 Promise 返回了数据，才能将该 Promise 移到 completedQueue 队列中，由 Emitter 消费发往下游。<br>只有队首集合空了，才能处理第二个集合。这样就保证了当且仅当某个 watermark 之前所有的消息都已经被发送了，该 watermark 才能被发送。过程如下图所示：</p><p><img src="/blog-img/apache-flink:study-async-io-RichAsyncFunction/EventTime%E6%97%A0%E5%BA%8F.jpeg" alt="EventTime无序"></p><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a><strong>说明</strong></h2><p>1、AsyncDataStream有2个方法，unorderedWait表示数据不需要关注顺序，处理完立即发送，orderedWait表示数据需要关注顺序，为了实现该目标，操作算子会在该结果记录之前的记录为发送之前缓存该记录。这往往会引入额外的延迟和一些Checkpoint负载，因为相比于无序模式结果记录会保存在Checkpoint状态内部较长的时间。<br>2、Timeout配置，主要是为了处理死掉或者失败的任务，防止资源被长期阻塞占用。<br>3、最后一个参数Capacity表示同时最多有多少个异步请求在处理，异步IO的方式会导致更高的吞吐量，但是对于实时应用来说该操作也是一个瓶颈。限制并发请求数，算子不会积压过多的未处理请求，但是一旦超过容量的显示会触发背压。<br>该参数可以不配置，但是默认是100</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Common 包学习：常用集合类Collections4学习</title>
    <link href="https://yangyichao-mango.github.io/2019/11/06/apache-common:study-apache-common-collections4/"/>
    <id>https://yangyichao-mango.github.io/2019/11/06/apache-common:study-apache-common-collections4/</id>
    <published>2019-11-06T08:04:06.000Z</published>
    <updated>2019-11-06T08:57:35.325Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Common 包学习：常用集合类Collections4学习</p><span id="more"></span><h2 id="Maven依赖"><a href="#Maven依赖" class="headerlink" title="Maven依赖"></a><strong>Maven依赖</strong></h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.commons<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-collections4<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="CollectionUtils"><a href="#CollectionUtils" class="headerlink" title="CollectionUtils"></a><strong>CollectionUtils</strong></h2><h3 id="lt-O-gt-Collection-lt-O-gt-subtract-final-Iterable-lt-extends-O-gt-a-final-Iterable-lt-extends-O-gt-b"><a href="#lt-O-gt-Collection-lt-O-gt-subtract-final-Iterable-lt-extends-O-gt-a-final-Iterable-lt-extends-O-gt-b" class="headerlink" title="&lt;O&gt; Collection&lt;O&gt; subtract(final Iterable&lt;? extends O&gt; a, final Iterable&lt;? extends O&gt; b)"></a>&lt;O<O>&gt; Collection&lt;O<O>&gt; subtract(final Iterable&lt;? extends O&gt; a, final Iterable&lt;? extends O&gt; b)</h3><p>a是做差集运算的左集，b是做差集运算的右集，下面是一个例子</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = LoggerFactory.getLogger(Demo.class);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Set&lt;Pair&lt;String, String&gt;&gt; allProductDevices = Sets.newHashSet(Pair.of(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>), Pair.of(<span class="string">&quot;c&quot;</span>, <span class="string">&quot;d&quot;</span>));</span><br><span class="line"></span><br><span class="line">        Set&lt;Pair&lt;String, String&gt;&gt; oldProductDevices = Sets.newHashSet(Pair.of(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>), Pair.of(<span class="string">&quot;e&quot;</span>, <span class="string">&quot;f&quot;</span>));</span><br><span class="line"></span><br><span class="line">        List&lt;Pair&lt;String, String&gt;&gt; newProductDevices =</span><br><span class="line">                (ArrayList&lt;Pair&lt;String, String&gt;&gt;) CollectionUtils.subtract(allProductDevices, oldProductDevices);</span><br><span class="line"></span><br><span class="line">        List&lt;String&gt; list1 = Lists.newArrayList(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>);</span><br><span class="line"></span><br><span class="line">        List&lt;String&gt; list2 = Lists.newArrayList(<span class="string">&quot;c&quot;</span>, <span class="string">&quot;b&quot;</span>);</span><br><span class="line"></span><br><span class="line">        List&lt;String&gt; list3 = (ArrayList&lt;String&gt;) CollectionUtils.subtract(list1, list2);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Common包" scheme="https://yangyichao-mango.github.io/categories/Apache-Common%E5%8C%85/"/>
    
    
      <category term="Apache Common包" scheme="https://yangyichao-mango.github.io/tags/Apache-Common%E5%8C%85/"/>
    
  </entry>
  
  <entry>
    <title>Google Guava 学习：guava cache缓存学习</title>
    <link href="https://yangyichao-mango.github.io/2019/11/06/google-guava:study-guava-cache/"/>
    <id>https://yangyichao-mango.github.io/2019/11/06/google-guava:study-guava-cache/</id>
    <published>2019-11-06T08:03:38.000Z</published>
    <updated>2019-11-06T10:38:23.662Z</updated>
    
    <content type="html"><![CDATA[<p>Google Guava 学习：guava cache缓存学习</p><span id="more"></span><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a><strong>背景</strong></h2><p>缓存的主要作用是暂时在内存中保存业务系统的数据处理结果，并且等待下次访问使用。在日长开发有很多场合，有一些数据量不是很大，不会经常改动，并且访问非常频繁。但是由于受限于硬盘IO的性能或者远程网络等原因获取可能非常的费时。会导致我们的程序非常缓慢，这在某些业务上是不能忍的！而缓存正是解决这类问题的神器！</p><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a><strong>正文</strong></h2><p>Guava Cache与ConcurrentMap很相似，但也不完全一样。最基本的区别是ConcurrentMap会一直保存所有添加的元素，直到显式地移除。相对地，Guava Cache为了限制内存占用，通常都设定为自动回收元素。在某些场景下，尽管LoadingCache 不回收元素，它也是很有用的，因为它会自动加载缓存</p><p>Guava Cache是在内存中缓存数据，相比较于数据库或redis存储，访问内存中的数据会更加高效。Guava官网介绍，下面的这几种情况可以考虑使用Guava Cache：</p><p>1.愿意消耗一些内存空间来提升速度。</p><p>2.预料到某些键会被多次查询。</p><p>3.缓存中存放的数据总量不会超出内存容量。</p><p>所以，可以将程序频繁用到的少量数据存储到Guava Cache中，以改善程序性能。下面对Guava Cache的用法进行详细的介绍。</p><h2 id="Maven依赖"><a href="#Maven依赖" class="headerlink" title="Maven依赖"></a><strong>Maven依赖</strong></h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>guava<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>23.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="构建缓存对象"><a href="#构建缓存对象" class="headerlink" title="构建缓存对象"></a><strong>构建缓存对象</strong></h2><p>接口Cache代表缓存，它有如下方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Cache</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function">V <span class="title">get</span><span class="params">(K key, Callable&lt;? extends V&gt; valueLoader)</span> <span class="keyword">throws</span> ExecutionException</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">ImmutableMap&lt;K, V&gt; <span class="title">getAllPresent</span><span class="params">(Iterable&lt;?&gt; keys)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">put</span><span class="params">(K key, V value)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">putAll</span><span class="params">(Map&lt;? extends K, ? extends V&gt; m)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">invalidate</span><span class="params">(Object key)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">invalidateAll</span><span class="params">(Iterable&lt;?&gt; keys)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">invalidateAll</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">size</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">CacheStats <span class="title">stats</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">ConcurrentMap&lt;K, V&gt; <span class="title">asMap</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">cleanUp</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以通过CacheBuilder类构建一个缓存对象，构建一个缓存对象代码如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Cache&lt;String,String&gt; cache = CacheBuilder.newBuilder().build();</span><br><span class="line">        cache.put(<span class="string">&quot;word&quot;</span>,<span class="string">&quot;Hello Guava Cache&quot;</span>);</span><br><span class="line">        System.out.println(cache.getIfPresent(<span class="string">&quot;word&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到Cache非常类似于JDK中的Map，但是相比于Map，Guava Cache提供了很多更强大的功能</p><h2 id="设置最大存储"><a href="#设置最大存储" class="headerlink" title="设置最大存储"></a><strong>设置最大存储</strong></h2><p>Guava Cache可以在构建缓存对象时指定缓存所能够存储的最大记录数量。当Cache中的记录数量达到最大值后再调用put方法向其中添加对象，Guava会先从当前缓存的对象记录中选择一条删除掉，腾出空间后再将新的对象存储到Cache中</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Cache&lt;String,String&gt; cache = CacheBuilder.newBuilder()</span><br><span class="line">                .maximumSize(<span class="number">2</span>)</span><br><span class="line">                .build();</span><br><span class="line">        cache.put(<span class="string">&quot;key1&quot;</span>, <span class="string">&quot;value1&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key2&quot;</span>, <span class="string">&quot;value2&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key3&quot;</span>, <span class="string">&quot;value3&quot;</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;第一个值：&quot;</span> + cache.getIfPresent(<span class="string">&quot;key1&quot;</span>));</span><br><span class="line">        System.out.println(<span class="string">&quot;第二个值：&quot;</span> + cache.getIfPresent(<span class="string">&quot;key2&quot;</span>));</span><br><span class="line">        System.out.println(<span class="string">&quot;第三个值：&quot;</span> + cache.getIfPresent(<span class="string">&quot;key3&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面代码在构造缓存对象时，通过CacheBuilder类的maximumSize方法指定Cache最多可以存储两个对象，然后调用Cache的put方法向其中添加了三个对象。程序执行结果如下图所示，可以看到第三条对象记录的插入，导致了第一条对象记录被删除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第一个值：null</span><br><span class="line">第二个值：value2</span><br><span class="line">第三个值：value3</span><br></pre></td></tr></table></figure><h2 id="设置过期时间"><a href="#设置过期时间" class="headerlink" title="设置过期时间"></a><strong>设置过期时间</strong></h2><p>在构建Cache对象时，可以通过CacheBuilder类的expireAfterAccess和expireAfterWrite两个方法为缓存中的对象指定过期时间，过期的对象将会被缓存自动删除。其中，expireAfterWrite方法指定对象被写入到缓存后多久过期，expireAfterAccess指定对象多久没有被访问后过期</p><h3 id="expireAfterWrite"><a href="#expireAfterWrite" class="headerlink" title="expireAfterWrite"></a>expireAfterWrite</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder()</span><br><span class="line">                .maximumSize(<span class="number">2</span>)</span><br><span class="line">                .expireAfterWrite(<span class="number">3</span>, TimeUnit.SECONDS)</span><br><span class="line">                .build();</span><br><span class="line">        cache.put(<span class="string">&quot;key1&quot;</span>, <span class="string">&quot;value1&quot;</span>);</span><br><span class="line">        <span class="keyword">int</span> time = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;第&quot;</span> + time++ + <span class="string">&quot;次取到key1的值为：&quot;</span> + cache.getIfPresent(<span class="string">&quot;key1&quot;</span>));</span><br><span class="line">            Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的代码在构造Cache对象时，通过CacheBuilder的expireAfterWrite方法指定put到Cache中的对象在3秒后会过期。在Cache对象中存储一条对象记录后，每隔1秒读取一次这条记录。程序运行结果如下图所示，可以看到，前三秒可以从Cache中获取到对象，超过三秒后，对象从Cache中被自动删除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">第1次取到key1的值为：value1</span><br><span class="line">第2次取到key1的值为：value1</span><br><span class="line">第3次取到key1的值为：value1</span><br><span class="line">第4次取到key1的值为：null</span><br><span class="line">第5次取到key1的值为：null</span><br><span class="line">第6次取到key1的值为：null</span><br><span class="line">第7次取到key1的值为：null</span><br><span class="line">第8次取到key1的值为：null</span><br></pre></td></tr></table></figure><h3 id="expireAfterAccess"><a href="#expireAfterAccess" class="headerlink" title="expireAfterAccess"></a>expireAfterAccess</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder()</span><br><span class="line">                .maximumSize(<span class="number">2</span>)</span><br><span class="line">                .expireAfterAccess(<span class="number">3</span>, TimeUnit.SECONDS)</span><br><span class="line">                .build();</span><br><span class="line">        cache.put(<span class="string">&quot;key1&quot;</span>, <span class="string">&quot;value1&quot;</span>);</span><br><span class="line">        <span class="keyword">double</span> time = <span class="number">1.5</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            Thread.sleep((<span class="keyword">long</span>) time * <span class="number">1000L</span>);</span><br><span class="line">            System.out.println(<span class="string">&quot;睡眠&quot;</span> + time++ + <span class="string">&quot;秒后取到key1的值为：&quot;</span> + cache.getIfPresent(<span class="string">&quot;key1&quot;</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过CacheBuilder的expireAfterAccess方法指定Cache中存储的对象如果超过3秒没有被访问就会过期。while中的代码每sleep一段时间就会访问一次Cache中存储的对象key1，每次访问key1之后下次sleep的时间会加长一秒。程序运行结果如下图所示，从结果中可以看出，当超过3秒没有读取key1对象之后，该对象会自动被Cache删除。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">睡眠1.5秒后取到key1的值为：value1</span><br><span class="line">睡眠2.5秒后取到key1的值为：value1</span><br><span class="line">睡眠3.5秒后取到key1的值为：null</span><br></pre></td></tr></table></figure><p>也可以同时用expireAfterAccess和expireAfterWrite方法指定过期时间，这时只要对象满足两者中的一个条件就会被自动过期删除。</p><h2 id="弱引用"><a href="#弱引用" class="headerlink" title="弱引用"></a><strong>弱引用</strong></h2><p>可以通过weakKeys和weakValues方法指定Cache只保存对缓存记录key和value的弱引用。这样当没有其他强引用指向key和value时，key和value对象就会被垃圾回收器回收</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Cache&lt;String, Object&gt; cache = CacheBuilder.newBuilder()</span><br><span class="line">                .maximumSize(<span class="number">2</span>)</span><br><span class="line">                .weakValues()</span><br><span class="line">                .build();</span><br><span class="line">        Object value = <span class="keyword">new</span> Object();</span><br><span class="line">        cache.put(<span class="string">&quot;key1&quot;</span>, value);</span><br><span class="line">        </span><br><span class="line">        value = <span class="keyword">new</span> Object(); <span class="comment">// 原对象不再有强引用</span></span><br><span class="line">        System.gc();</span><br><span class="line">        System.out.println(cache.getIfPresent(<span class="string">&quot;key1&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面代码的打印结果是null。构建Cache时通过weakValues方法指定Cache只保存记录值的一个弱引用。当给value引用赋值一个新的对象之后，就不再有任何一个强引用指向原对象。System.gc()触发垃圾回收后，原对象就被清除了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">null</span><br></pre></td></tr></table></figure><h2 id="显示清除"><a href="#显示清除" class="headerlink" title="显示清除"></a><strong>显示清除</strong></h2><p>可以调用Cache的invalidateAll或invalidate方法显示删除Cache中的记录。invalidate方法一次只能删除Cache中一个记录，接收的参数是要删除记录的key。invalidateAll方法可以批量删除Cache中的记录，当没有传任何参数时，invalidateAll方法将清除Cache中的全部记录。invalidateAll也可以接收一个Iterable类型的参数，参数中包含要删除记录的所有key值。下面代码对此做了示例</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder().build();</span><br><span class="line">        Object value = <span class="keyword">new</span> Object();</span><br><span class="line">        cache.put(<span class="string">&quot;key1&quot;</span>, <span class="string">&quot;value1&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key2&quot;</span>, <span class="string">&quot;value2&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key3&quot;</span>, <span class="string">&quot;value3&quot;</span>);</span><br><span class="line"></span><br><span class="line">        List&lt;String&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        list.add(<span class="string">&quot;key1&quot;</span>);</span><br><span class="line">        list.add(<span class="string">&quot;key2&quot;</span>);</span><br><span class="line"></span><br><span class="line">        cache.invalidateAll(list); <span class="comment">// 批量清除list中全部key对应的记录</span></span><br><span class="line">        System.out.println(cache.getIfPresent(<span class="string">&quot;key1&quot;</span>));</span><br><span class="line">        System.out.println(cache.getIfPresent(<span class="string">&quot;key2&quot;</span>));</span><br><span class="line">        System.out.println(cache.getIfPresent(<span class="string">&quot;key3&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>代码中构造了一个集合list用于保存要删除记录的key值，然后调用invalidateAll方法批量删除key1和key2对应的记录，只剩下key3对应的记录没有被删除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">null</span><br><span class="line">null</span><br><span class="line">value3</span><br></pre></td></tr></table></figure><h2 id="移除监听器"><a href="#移除监听器" class="headerlink" title="移除监听器"></a><strong>移除监听器</strong></h2><p>可以为Cache对象添加一个移除监听器，这样当有记录被删除时可以感知到这个事件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public class StudyGuavaCache &#123;</span><br><span class="line">    public static void main(String[] args) throws InterruptedException &#123;</span><br><span class="line">        RemovalListener&lt;String, String&gt; listener = notification -&gt;</span><br><span class="line">                System.out.println(<span class="string">&quot;[&quot;</span> + notification.getKey() + <span class="string">&quot;:&quot;</span> + notification.getValue() + <span class="string">&quot;] is removed!&quot;</span>);</span><br><span class="line">        Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder()</span><br><span class="line">                .maximumSize(3)</span><br><span class="line">                .removalListener(listener)</span><br><span class="line">                .build();</span><br><span class="line">        cache.put(<span class="string">&quot;key1&quot;</span>, <span class="string">&quot;value1&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key2&quot;</span>, <span class="string">&quot;value2&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key3&quot;</span>, <span class="string">&quot;value3&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key4&quot;</span>, <span class="string">&quot;value3&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key5&quot;</span>, <span class="string">&quot;value3&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key6&quot;</span>, <span class="string">&quot;value3&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key7&quot;</span>, <span class="string">&quot;value3&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key8&quot;</span>, <span class="string">&quot;value3&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>removalListener方法为Cache指定了一个移除监听器，这样当有记录从Cache中被删除时，监听器listener就会感知到这个事件。程序运行结果如下图所示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[key1:value1] is removed!</span><br><span class="line">[key2:value2] is removed!</span><br><span class="line">[key3:value3] is removed!</span><br><span class="line">[key4:value3] is removed!</span><br><span class="line">[key5:value3] is removed!</span><br></pre></td></tr></table></figure><h2 id="自动加载"><a href="#自动加载" class="headerlink" title="自动加载"></a><strong>自动加载</strong></h2><p>Cache的get方法有两个参数，第一个参数是要从Cache中获取记录的key，第二个记录是一个Callable对象。<br>当缓存中已经存在key对应的记录时，get方法直接返回key对应的记录。如果缓存中不包含key对应的记录，Guava会使用当前线程执行Callable对象中的call方法，call方法的返回值会作为key对应的值被存储到缓存中，并且被get方法返回。下面是一个多线程的例子：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = LoggerFactory.getLogger(StudyGuavaCache.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder()</span><br><span class="line">            .maximumSize(<span class="number">1</span>)</span><br><span class="line">            .build();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                LOGGER.info(<span class="string">&quot;1&quot;</span> + Thread.currentThread().getName());</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    String value = cache.get(<span class="string">&quot;key&quot;</span>, <span class="keyword">new</span> Callable&lt;String&gt;() &#123;</span><br><span class="line">                        <span class="function"><span class="keyword">public</span> String <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                            LOGGER.info(<span class="string">&quot;load1&quot;</span> + Thread.currentThread().getName()); <span class="comment">// 加载数据线程执行标志</span></span><br><span class="line">                            Thread.sleep(<span class="number">1000</span>); <span class="comment">// 模拟加载时间</span></span><br><span class="line">                            <span class="keyword">return</span> <span class="string">&quot;auto load by Callable1&quot;</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;);</span><br><span class="line">                    LOGGER.info(<span class="string">&quot;thread1 &quot;</span> + value);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                LOGGER.info(<span class="string">&quot;thread2&quot;</span>);</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    LOGGER.info(<span class="string">&quot;2&quot;</span> + Thread.currentThread().getName());</span><br><span class="line">                    String value = cache.get(<span class="string">&quot;key1&quot;</span>, <span class="keyword">new</span> Callable&lt;String&gt;() &#123;</span><br><span class="line">                        <span class="function"><span class="keyword">public</span> String <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                            LOGGER.info(<span class="string">&quot;load2&quot;</span> + Thread.currentThread().getName()); <span class="comment">// 加载数据线程执行标志</span></span><br><span class="line">                            Thread.sleep(<span class="number">1000</span>); <span class="comment">// 模拟加载时间</span></span><br><span class="line">                            <span class="keyword">return</span> <span class="string">&quot;auto load by Callable2&quot;</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;);</span><br><span class="line">                    LOGGER.info(<span class="string">&quot;thread2 &quot;</span> + value);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这段代码中有两个线程共享同一个Cache对象，两个线程同时调用get方法获取同一个key对应的记录。由于key对应的记录不存在，所以两个线程都在get方法处阻塞。此处在call方法中调用Thread.sleep(1000)模拟程序从外存加载数据的时间消耗</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">17:49:23.965 [Thread-2] INFO com.github.xxx.other.demo.guava.StudyGuavaCache - thread2</span><br><span class="line">17:49:23.965 [Thread-1] INFO com.github.xxx.other.demo.guava.StudyGuavaCache - 1Thread-1</span><br><span class="line">17:49:23.969 [Thread-2] INFO com.github.xxx.other.demo.guava.StudyGuavaCache - 2Thread-2</span><br><span class="line">17:49:23.983 [Thread-1] INFO com.github.xxx.other.demo.guava.StudyGuavaCache - load1Thread-1</span><br><span class="line">17:49:23.983 [Thread-2] INFO com.github.xxx.other.demo.guava.StudyGuavaCache - load2Thread-2</span><br></pre></td></tr></table></figure><p>从结果中可以看出，虽然是两个线程同时调用get方法，但只有一个get方法中的Callable会被执行(没有打印出load2)。<br>Guava可以保证当有多个线程同时访问Cache中的一个key时，如果key对应的记录不存在，Guava只会启动一个线程执行get方法中Callable参数对应的任务加载数据存到缓存。<br>当加载完数据后，任何线程中的get方法都会获取到key对应的值</p><h2 id="统计信息"><a href="#统计信息" class="headerlink" title="统计信息"></a><strong>统计信息</strong></h2><p>可以对Cache的命中率、加载数据时间等信息进行统计。在构建Cache对象时，可以通过CacheBuilder的recordStats方法开启统计信息的开关。<br>开关开启后Cache会自动对缓存的各种操作进行统计，调用Cache的stats方法可以查看统计后的信息</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder()</span><br><span class="line">                .maximumSize(<span class="number">3</span>)</span><br><span class="line">                .recordStats() <span class="comment">// 开启统计信息开关</span></span><br><span class="line">                .build();</span><br><span class="line">        cache.put(<span class="string">&quot;key1&quot;</span>, <span class="string">&quot;value1&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key2&quot;</span>, <span class="string">&quot;value2&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key3&quot;</span>, <span class="string">&quot;value3&quot;</span>);</span><br><span class="line">        cache.put(<span class="string">&quot;key4&quot;</span>, <span class="string">&quot;value4&quot;</span>);</span><br><span class="line"></span><br><span class="line">        cache.getIfPresent(<span class="string">&quot;key1&quot;</span>);</span><br><span class="line">        cache.getIfPresent(<span class="string">&quot;key2&quot;</span>);</span><br><span class="line">        cache.getIfPresent(<span class="string">&quot;key3&quot;</span>);</span><br><span class="line">        cache.getIfPresent(<span class="string">&quot;key4&quot;</span>);</span><br><span class="line">        cache.getIfPresent(<span class="string">&quot;key5&quot;</span>);</span><br><span class="line">        cache.getIfPresent(<span class="string">&quot;key6&quot;</span>);</span><br><span class="line">        </span><br><span class="line">        System.out.println(cache.stats()); <span class="comment">// 获取统计信息</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>程序执行结果如下所示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CacheStats&#123;hitCount=3, missCount=3, loadSuccessCount=0, loadExceptionCount=0, totalLoadTime=0, evictionCount=1&#125;</span><br></pre></td></tr></table></figure><p>这些统计信息对于调整缓存设置是至关重要的，在性能要求高的应用中应该密切关注这些数据</p><h2 id="LoadingCache"><a href="#LoadingCache" class="headerlink" title="LoadingCache"></a><strong>LoadingCache</strong></h2><p>LoadingCache是Cache的子接口，相比较于Cache，当从LoadingCache中读取一个指定key的记录时，如果该记录不存在，则LoadingCache可以自动执行加载数据到缓存的操作。<br>LoadingCache接口的定义如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">LoadingCache</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Cache</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt;, <span class="title">Function</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function">V <span class="title">get</span><span class="params">(K key)</span> <span class="keyword">throws</span> ExecutionException</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">V <span class="title">getUnchecked</span><span class="params">(K key)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">ImmutableMap&lt;K, V&gt; <span class="title">getAll</span><span class="params">(Iterable&lt;? extends K&gt; keys)</span> <span class="keyword">throws</span> ExecutionException</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">V <span class="title">apply</span><span class="params">(K key)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">refresh</span><span class="params">(K key)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function">ConcurrentMap&lt;K, V&gt; <span class="title">asMap</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>与构建Cache类型的对象类似，LoadingCache类型的对象也是通过CacheBuilder进行构建，不同的是，在调用CacheBuilder的build方法时，必须传递一个CacheLoader类型的参数，CacheLoader的load方法需要我们提供实现。<br>当调用LoadingCache的get方法时，如果缓存不存在对应key的记录，则CacheLoader中的load方法会被自动调用从外存加载数据，load方法的返回值会作为key对应的value存储到LoadingCache中，并从get方法返回</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudyGuavaCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException </span>&#123;</span><br><span class="line">        CacheLoader&lt;String, String&gt; loader = <span class="keyword">new</span> CacheLoader&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">load</span><span class="params">(String key)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                Thread.sleep(<span class="number">1000</span>); <span class="comment">// 休眠1s，模拟加载数据</span></span><br><span class="line">                System.out.println(key + <span class="string">&quot; is loaded from a cacheLoader!&quot;</span>);</span><br><span class="line">                <span class="keyword">return</span> key + <span class="string">&quot;&#x27;s value&quot;</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">        LoadingCache&lt;String, String&gt; loadingCache = CacheBuilder.newBuilder()</span><br><span class="line">                .maximumSize(<span class="number">3</span>)</span><br><span class="line">                .build(loader); <span class="comment">// 在构建时指定自动加载器</span></span><br><span class="line"></span><br><span class="line">        loadingCache.get(<span class="string">&quot;key1&quot;</span>);</span><br><span class="line">        loadingCache.get(<span class="string">&quot;key2&quot;</span>);</span><br><span class="line">        loadingCache.get(<span class="string">&quot;key3&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>程序执行结果如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">key1 is loaded from a cacheLoader!</span><br><span class="line">key2 is loaded from a cacheLoader!</span><br><span class="line">key3 is loaded from a cacheLoader!</span><br></pre></td></tr></table></figure><p>从LoadingCache查询的正规方式是使用get(K)方法。这个方法要么返回已经缓存的值，要么使用CacheLoader向缓存原子地加载新值（通过load(String key) 方法加载）。由于CacheLoader可能抛出异常，LoadingCache.get(K)也声明抛出ExecutionException异常。如果你定义的CacheLoader没有声明任何检查型异常，则可以通过getUnchecked(K)查找缓存；但必须注意，一旦CacheLoader声明了检查型异常，就不可以调用getUnchecked(K)。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="guava" scheme="https://yangyichao-mango.github.io/categories/guava/"/>
    
    
      <category term="guava" scheme="https://yangyichao-mango.github.io/tags/guava/"/>
    
  </entry>
  
  <entry>
    <title>apache-kafka:study-features</title>
    <link href="https://yangyichao-mango.github.io/2019/11/06/apache-kafka:study-features/"/>
    <id>https://yangyichao-mango.github.io/2019/11/06/apache-kafka:study-features/</id>
    <published>2019-11-06T01:57:17.000Z</published>
    <updated>2019-11-06T02:08:57.687Z</updated>
    
    <content type="html"><![CDATA[<h2 id="如何为Kafka集群选择合适的Partitions数量"><a href="#如何为Kafka集群选择合适的Partitions数量" class="headerlink" title="如何为Kafka集群选择合适的Partitions数量"></a><strong>如何为Kafka集群选择合适的Partitions数量</strong></h2><h3 id="越多的分区可以提供更高的吞吐量"><a href="#越多的分区可以提供更高的吞吐量" class="headerlink" title="越多的分区可以提供更高的吞吐量"></a>越多的分区可以提供更高的吞吐量</h3><p>首先我们需要明白以下事实：在kafka中，单个patition是kafka并行操作的最小单元。在producer和broker端，向每一个分区写入数据是可以完全并行化的，此时，可以通过加大硬件资源的利用率来提升系统的吞吐量，例如对数据进行压缩。在consumer段，kafka只允许单个partition的数据被一个consumer线程消费。因此，在consumer端，每一个Consumer Group内部的consumer并行度完全依赖于被消费的分区数量。综上所述，通常情况下，在一个Kafka集群中，partition的数量越多，意味着可以到达的吞吐量越大。</p><p>我们可以粗略地通过吞吐量来计算kafka集群的分区数量。假设对于单个partition，producer端的可达吞吐量为p，Consumer端的可达吞吐量为c，期望的目标吞吐量为t，那么集群所需要的partition数量至少为max(t/p,t/c)。在producer端，单个分区的吞吐量大小会受到批量大小、数据压缩方法、 确认类型（同步/异步）、复制因子等配置参数的影响。经过测试，在producer端，单个partition的吞吐量通常是在10MB/s左右。在consumer端，单个partition的吞吐量依赖于consumer端每个消息的应用逻辑处理速度。因此，我们需要对consumer端的吞吐量进行测量。</p><p>虽然随着时间的推移，我们能够对分区的数量进行添加，但是对于基于Key来生成的这一类消息需要我们重点关注。当producer向kafka写入基于key的消息时，kafka通过key的hash值来确定消息需要写入哪个具体的分区。通过这样的方案，kafka能够确保相同key值的数据可以写入同一个partition。kafka的这一能力对于一部分应用是极为重要的，例如对于同一个key的所有消息，consumer需要按消息的顺序进行有序消费。如果partition的数量发生改变，那么上面的有序性保证将不复存在。为了避免上述情况发生，通常的解决办法是多分配一些分区，以满足未来的需求。通常情况下，我们需要根据未来1到2年的目标吞吐量来设计kafka的分区数量。</p><p>一开始，我们可以基于当前的业务吞吐量为kafka集群分配较小的broker数量，随着时间的推移，我们可以向集群中增加更多的broker，然后在线方式将适当比例的partition转移到新增加的broker中去。通过这样的方法，我们可以在满足各种应用场景（包括基于key消息的场景）的情况下，保持业务吞吐量的扩展性。</p><p>在设计分区数时，除了吞吐量，还有一些其他因素值得考虑。正如我们后面即将看到的，对于一些应用场景，集群拥有过的分区将会带来负面的影响。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Kafka" scheme="https://yangyichao-mango.github.io/categories/Apache-Kafka/"/>
    
    
      <category term="Apache Kafka" scheme="https://yangyichao-mango.github.io/tags/Apache-Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：系统特性学习</title>
    <link href="https://yangyichao-mango.github.io/2019/11/03/apache-flink:study-features/"/>
    <id>https://yangyichao-mango.github.io/2019/11/03/apache-flink:study-features/</id>
    <published>2019-11-03T08:20:21.000Z</published>
    <updated>2019-11-11T07:18:07.272Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：系统特性学习</p><span id="more"></span><p><a href="https://github.com/flink-china/flink-training-course">教程</a></p><h2 id="window窗口"><a href="#window窗口" class="headerlink" title="window窗口"></a><strong>window窗口</strong></h2><h3 id="窗口大小"><a href="#窗口大小" class="headerlink" title="窗口大小"></a>窗口大小</h3><p>窗口大小是用户自己设定的，但是窗口的起始和结束时间点是系统根据窗口大小和自然数进行设定的，不会出现设置了一分钟的窗口，统计的数据是2:30到3:30的数据</p><p>[window_start_time, window_end_time)根据窗口大小和自然数进行设定</p><p>如果window大小是3秒，那么1分钟内会把window划分为如下的形式:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[00:00:00,00:00:03)</span><br><span class="line">[00:00:03,00:00:06)</span><br><span class="line">...</span><br><span class="line">[00:00:57,00:01:00)</span><br></pre></td></tr></table></figure><p>如果window大小是10秒，则window会被分为如下的形式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[00:00:00,00:00:10)</span><br><span class="line">[00:00:10,00:00:20)</span><br><span class="line">...</span><br><span class="line">[00:00:50,00:01:00)</span><br></pre></td></tr></table></figure><h3 id="watermark"><a href="#watermark" class="headerlink" title="watermark"></a>watermark</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> Watermark <span class="title">getCurrentWatermark</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// this guarantees that the watermark never goes backwards.</span></span><br><span class="line">    <span class="keyword">long</span> potentialWM = currentMaxTimestamp - maxOutOfOrderness;</span><br><span class="line">    <span class="keyword">if</span> (potentialWM &gt;= lastEmittedWatermark) &#123;</span><br><span class="line">        lastEmittedWatermark = potentialWM;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Watermark(lastEmittedWatermark);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>watermark = max( [当前已到达的时间戳最新的数据(currentMaxTimestamp)] - [最大乱序等待时间(maxOutOfOrderness)], watermark )</strong></p><h3 id="触发窗口运算条件"><a href="#触发窗口运算条件" class="headerlink" title="触发窗口运算条件"></a>触发窗口运算条件</h3><p>1.当前最新数据到达进行判断：当前到达event的time(timestamp) ＜ watermark则触发，表示数据是超过了最大等待时间，已经延迟到达的，则会触发</p><p>2.当前最新数据到达进行判断：最新的watermark &gt;= window_end_time（对于out-of-order以及正常的数据而言），在[window_start_time, window_end_time)中有数据存在</p><p>3.而且，这里要强调一点，watermark和currentMaxTimestamp是一个全局的值，不是某一个key下的值，所以即使不是同一个key的数据，其warmark也会增加</p><p>语义是：currentMaxTimestamp是当前到达的最大时间戳数据，代表时间戳为currentMaxTimestamp的数据已经到达了，所以所能等待的数据的时间戳（max_out_of_orderness）只能为watermark = currentMaxTimestamp - maxOutOfOrderness</p><h3 id="窗口计算"><a href="#窗口计算" class="headerlink" title="窗口计算"></a>窗口计算</h3><p>Window reduce，Window aggregate 和 Window Fold 是增量聚合，每来一条数据就计算一次，高效</p><p>Window apply（Window process 的老版本） 和 Window process 是全量聚合，触发窗口计算时全量计算</p><h3 id="被Keys化与非被Keys化Windows"><a href="#被Keys化与非被Keys化Windows" class="headerlink" title="被Keys化与非被Keys化Windows"></a>被Keys化与非被Keys化Windows</h3><p>要指定的第一件事是您的流是否应该使用keyedWindow，一般都与业务逻辑有关，比如说使用一分钟的窗口进行去重。使用keyBy(…)将您的无限流分成逻辑Key化的数据流。如果keyBy(…)未调用，则表示您的流不是被Keys化的。</p><p>对于被Key化的数据流，可以将传入数据（Object）的的任何属性用作键）。拥有被Key化的数据流将允许您的窗口计算由多个任务并行执行，因为每个Key化的数据流可以独立于其余任务进行处理。引用相同Keys的所有数据将被发送到同一个并行任务进行计算。</p><p>在非被Key化的数据流的情况下，您的原始流将不会被拆分为多个逻辑流，并且所有窗口逻辑将由单个任务执行，即并行度为1。</p><h2 id="sql"><a href="#sql" class="headerlink" title="sql"></a><strong>sql</strong></h2><p>1.在flinkSql中，如果使用groupBy，尽量使用窗口，否则会认为被groupBy的数据会默认人为整个窗口内的数据还没有到达，所以会一直等待，不会产出数据</p><p>update-mode: append / update</p><p>分为 update stream 模式和 append stream 模式</p><p>window聚合为append mode stream，groupby聚合为update mode stream</p><h2 id="Flink生成-Timestamps-和-Watermarks"><a href="#Flink生成-Timestamps-和-Watermarks" class="headerlink" title="Flink生成 Timestamps 和 Watermarks"></a><strong>Flink生成 Timestamps 和 Watermarks</strong></h2><p>为了让event time工作，Flink需要知道事件的时间戳，这意味着流中的每个元素都需要分配其事件时间戳。这个通常是通过抽取或者访问事件中某些字段的时间戳来获取的。</p><p>时间戳的分配伴随着水印的生成，告诉系统事件时间中的进度。</p><p>这里有两种方式来分配时间戳和生成水印:</p><ol><li>直接在数据流源中进行。</li><li>通过timestamp assigner和watermark generator生成:在Flink中，timestamp分配器也定义了用来发射的水印。</li></ol><h3 id="数据流源生成Timestamps和Watermarks"><a href="#数据流源生成Timestamps和Watermarks" class="headerlink" title="数据流源生成Timestamps和Watermarks"></a>数据流源生成Timestamps和Watermarks</h3><p>数据流源可以直接为它们产生的数据元素分配timestamp，并且他们也能发送水印。这样做的话，就没必要再去定义timestamp分配器了，需要注意的是:如果一个timestamp分配器被使用的话，由源提供的任何timestamp和watermark都会被重写。</p><h3 id="时间戳分配器-水印生成器（Timestamp-Assigners-Watermark-Generators）"><a href="#时间戳分配器-水印生成器（Timestamp-Assigners-Watermark-Generators）" class="headerlink" title="时间戳分配器/水印生成器（Timestamp Assigners / Watermark Generators）"></a>时间戳分配器/水印生成器（Timestamp Assigners / Watermark Generators）</h3><p>Timestamp分配器获取一个流并生成一个新的带有Timestamp元素和水印的流。如果原始流已经有时间戳和/或水印，则Timestamp分配程序将覆盖它们</p><p>Timestamp分配器通常在数据源之后立即指定，但这并不是严格要求的。通常是在timestamp分配器之前先解析(MapFunction)和过滤(FilterFunction)。在任何情况下，都需要在事件时间上的第一个操作(例如第一个窗口操作)之前指定timestamp分配程序。有一个特殊情况，当使用Kafka作为流作业的数据源时，Flink允许在源内部指定timestamp分配器和watermark生成器。更多关于如何进行的信息请参考Kafka Connector的文档。</p><p>直接在FlinkKafkaConsumer010上面使用assignTimestampsAndWatermarks可以根据kafka source的partitions的特性进行设置Timestamps和Watermarks，让用户做一些特殊的处理</p><p>Running timestamp extractors / watermark generators directly inside the Kafka source, per Kafka<br>partition, allows users to let them exploit the per-partition characteristics.</p><h2 id="log"><a href="#log" class="headerlink" title="log"></a><strong>log</strong></h2><h3 id="Web-UI查找log"><a href="#Web-UI查找log" class="headerlink" title="Web UI查找log"></a>Web UI查找log</h3><p>JobManger log： 展示整个作业的状态变化（例如，从create 到deploy到running再到failed），通过jobManger log可以查看作业历史失败的记录和直接原因。</p><p>TaskManager log： 调度到该TaskManager上的task 的打印的相关log。</p><h2 id="shuffle"><a href="#shuffle" class="headerlink" title="shuffle"></a><strong>shuffle</strong></h2><p>被keyBy的数据流中，相同的key的数据会被发送到同一个slot中运行（partitiner决定），也就是TaskManager中slot进行shuffle的过程<br>如果有多个producer并且producer的数量和partition数量相同，则每个producer写一个partition</p><h2 id="Savepoints和Checkpoints"><a href="#Savepoints和Checkpoints" class="headerlink" title="Savepoints和Checkpoints"></a><strong>Savepoints和Checkpoints</strong></h2><p>用 Data Stream API 编写的程序可以从 savepoint 继续执行。Savepoints 允许在不丢失任何状态的情况下升级程序和 Flink 集群。</p><p>Savepoints 是手动触发的 Checkpoints，它依靠常规的 Checkpoint 机制获取程序的快照并将其写入 state backend。在执行期间，程序会定期在 worker 节点上创建快照并生成 Checkpoints。对于恢复，Flink 仅需要最后完成的 Checkpoint，而一旦完成了新的 Checkpoint，旧的就可以被丢弃。</p><p>Savepoints 类似于这些定期的 Checkpoints，除了它们是由用户触发并且在新的 Checkpoints 完成时不会自动过期。你可以通过命令行 或在取消一个 job 时通过 REST API 来创建 Savepoints。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：实时场景下的应用</title>
    <link href="https://yangyichao-mango.github.io/2019/11/03/apache-flink:study-realtime-scenario/"/>
    <id>https://yangyichao-mango.github.io/2019/11/03/apache-flink:study-realtime-scenario/</id>
    <published>2019-11-03T05:30:05.000Z</published>
    <updated>2019-11-03T08:32:58.449Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：实时场景下的应用</p><span id="more"></span><h2 id="用户需求场景"><a href="#用户需求场景" class="headerlink" title="用户需求场景"></a><strong>用户需求场景</strong></h2><p>用户想要查看 版本，机型，国家，城市 等等维度下按照分钟的时间粒度的设备活跃，新增设备数，首次活跃设备数</p><h2 id="用户需求-gt-架构方案设计"><a href="#用户需求-gt-架构方案设计" class="headerlink" title="用户需求-&gt;架构方案设计"></a><strong>用户需求-&gt;架构方案设计</strong></h2><table><thead><tr><th align="center">数据所处阶段</th><th align="center">功能描述</th></tr></thead><tbody><tr><td align="center">数据source</td><td align="center">各种各样的打到kafka的用户行为数据的日志</td></tr><tr><td align="center">数据process</td><td align="center">实时引擎消费kafka，根据数据服务化提供的接口判断当前用户是否是新增，活跃，首次活跃，将用户的相关数据打到下游kafka</td></tr><tr><td align="center">数据sink</td><td align="center">结果kafka</td></tr><tr><td align="center">olap引擎</td><td align="center">消费sink kafka</td></tr><tr><td align="center">数据产品</td><td align="center">通过BI等的产品呈现给用户</td></tr></tbody></table><h2 id="架构设计-gt-选择实时计算引擎"><a href="#架构设计-gt-选择实时计算引擎" class="headerlink" title="架构设计-&gt;选择实时计算引擎"></a><strong>架构设计-&gt;选择实时计算引擎</strong></h2><p>为什么使用flink：</p><p>A.保证消费一次：checkpoint和savepoint 容错</p><p>B.时间属性：事件，注入，处理时间</p><p>优点：事件时间的属性可以被广泛应用，比如一般的分析场景都是分析用户某个时间段的用户相关指标，而不是事件处理某个时间段的用户相关指标</p><h2 id="flink实现方案"><a href="#flink实现方案" class="headerlink" title="flink实现方案"></a><strong>flink实现方案</strong></h2><h3 id="第一种方案"><a href="#第一种方案" class="headerlink" title="第一种方案"></a>第一种方案</h3><h4 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h4><p>新增场景下，每消费一条source kafka用户数据就判断一次是否为新增，判断方式可以选择自己维护历史全量数据，或者使用数据服务化提供的接口，最后将结果写入sink kafka</p><h4 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h4><table style="text-align: center;">    <thead>        <tr>            <th style="width: 20%">数据处理阶段</th>            <th style="width: 30%">问题</th>            <th style="width: 20%">结果（仅仅指当前问题会产生的结果）</th>            <th style="width: 10%">是否可解决</th>        </tr>    </thead>    <tbody>        <tr>            <td>数据source</td>            <td>同一个用户的行为数据到达时间间隔很小，几秒内就可能会产生几十条行为日志，判断是否为新增，活跃用户时可能会被重复判断</td>            <td>最终数据结果＞真实结果</td>            <td>可部分解决</td>        </tr>        <tr>            <td>数据process</td>            <td>自己维护全量数据<br/>1.每判断一条就更新历史全量数据就不存在问题<br/>2.如果历史全量数据更新有问题就会产生和数据服务化一样的下面两种问题</td>            <td></td>            <td>可部分解决</td>        </tr>        <tr>            <td>数据process</td>            <td>数据服务化维护全量数据且更新不及时<br/>在新增的场景下，一个新增用户使用app可能会在短时间内上报成百上千条行为日志，如果第一条数据判断出来这个用户是新增，下一条数据判断时，数据服务化提供的全量用户里还没有及时将这条新增用户数据添加进去，则这条数据也会被判断为新增，就会导致最终结果重复</td>            <td>最终数据结果＞真实结果</td>            <td>可部分解决</td>        </tr>        <tr>            <td>数据process</td>            <td>数据服务化维护全量数据且更新过快<br/>数据服务化更新速度快于flink消费source kafka的速度：就会导致本来是新增的设备被判断不是新增，导致最终结果漏判</td>            <td>最终数据结果＜真实结果</td>            <td>暂时无法解决</td>        </tr>    </tbody></table><h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><p>后续解决方法只讨论上述可部分解决的问题</p><table style="text-align: center;">    <thead>        <tr>            <th style="width: 10%">数据处理阶段</th>            <th style="width: 20%">问题</th>            <th style="width: 30%">解决方案</th>        </tr>    </thead>    <tbody>        <tr>            <td>数据source</td>            <td>同一个用户的行为数据到达时间间隔很小，可能会被重复判断</td>            <td rowspan="2">使用flink窗口解决部分问题<br/>使用滚动窗口解决，将一段时间内的用户行为收集，然后到达窗口结束时间处理后再进行上报。假设设置一小时的窗口，则将这一小时的用户行为数据只取一条进行判断是否为新增，则可以极大的保证当前用户判断为新增时，下一小时窗口中这个用户不太可能被判断为新增了，因为数据服务化没有那么慢</td>        </tr>        <tr>            <td>数据process</td>            <td>数据服务化维护全量数据且更新不及时，最终结果重复</td>        </tr>    </tbody></table><h3 id="第一种方案-gt-第二种方案"><a href="#第一种方案-gt-第二种方案" class="headerlink" title="第一种方案-&gt;第二种方案"></a>第一种方案-&gt;第二种方案</h3><h4 id="方案-1"><a href="#方案-1" class="headerlink" title="方案"></a>方案</h4><p>使用窗口可以部分解决在新增活跃等场景下用户行为数据重复的问题</p><p>但是使用了窗口也会引入问题，就是虽然大窗口可以保证尽可能去重，但是数据的实时性大大降低，所以窗口设置不能大也不能小，窗口大保证不了数据产出及时性，窗口小去重效果差，所以最大窗口就为一分钟，和用户期望看板中结果一致</p><h4 id="存在的问题-1"><a href="#存在的问题-1" class="headerlink" title="存在的问题"></a>存在的问题</h4><table style="text-align: center;">    <thead>        <tr>            <th style="width: 20%">数据处理阶段</th>            <th style="width: 30%">问题</th>            <th style="width: 20%">结果（仅仅指当前问题会产生的结果）</th>            <th style="width: 10%">是否可解决</th>        </tr>    </thead>    <tbody>        <tr>            <td>数据source</td>            <td rowspan="2">由于用户行为数据到达source，或者从source到达process阶段，由于网络延迟等的问题，会导致处理用户数据时有乱序情况<br/>比如计算实时活跃设备，上一分钟的数据如果下一分钟才到达，则该条数据就会被上一分钟漏算</td>            <td rowspan="2">最终数据结果＜真实结果</td>            <td rowspan="2">可部分解决</td>        </tr>        <tr>            <td>数据process</td>        </tr>    </tbody></table><h4 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h4><table style="text-align: center;">    <thead>        <tr>            <th style="width: 10%">数据处理阶段</th>            <th style="width: 20%">问题</th>            <th style="width: 30%">解决方案</th>        </tr>    </thead>    <tbody>        <tr>            <td>数据source</td>            <td rowspan="2">数据乱序延迟漏算</td>            <td rowspan="2">在flink窗口计算中，通过timestamp和watermark特性来尽可能解决</td>        </tr>        <tr>            <td>数据process</td>        </tr>    </tbody></table><h3 id="第二种方案-gt-第三种方案"><a href="#第二种方案-gt-第三种方案" class="headerlink" title="第二种方案-&gt;第三种方案"></a>第二种方案-&gt;第三种方案</h3><h4 id="方案-2"><a href="#方案-2" class="headerlink" title="方案"></a>方案</h4><p>设置一分钟的窗口，然后设置一分钟的最大延迟等待时间，其语义是保证数据最多延迟一分钟到达，只要可以保证这个语义可以保证最后数据的正确性</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
      <category term="实时计算" scheme="https://yangyichao-mango.github.io/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：BoundedOutOfOrdernessTimestampExtractor</title>
    <link href="https://yangyichao-mango.github.io/2019/11/02/apache-flink:study-BoundedOutOfOrdernessTimestampExtractor/"/>
    <id>https://yangyichao-mango.github.io/2019/11/02/apache-flink:study-BoundedOutOfOrdernessTimestampExtractor/</id>
    <published>2019-11-02T08:48:43.000Z</published>
    <updated>2019-11-02T11:03:54.828Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：BoundedOutOfOrdernessTimestampExtractor</p><span id="more"></span><h2 id="源码"><a href="#源码" class="headerlink" title="源码"></a><strong>源码</strong></h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Licensed to the Apache Software Foundation (ASF) under one</span></span><br><span class="line"><span class="comment"> * or more contributor license agreements.  See the NOTICE file</span></span><br><span class="line"><span class="comment"> * distributed with this work for additional information</span></span><br><span class="line"><span class="comment"> * regarding copyright ownership.  The ASF licenses this file</span></span><br><span class="line"><span class="comment"> * to you under the Apache License, Version 2.0 (the</span></span><br><span class="line"><span class="comment"> * &quot;License&quot;); you may not use this file except in compliance</span></span><br><span class="line"><span class="comment"> * with the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"> * See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"> * limitations under the License.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> org.apache.flink.streaming.api.functions.timestamps;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.watermark.Watermark;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * This is a &#123;<span class="doctag">@link</span> AssignerWithPeriodicWatermarks&#125; used to emit Watermarks that lag behind the element with</span></span><br><span class="line"><span class="comment"> * the maximum timestamp (in event time) seen so far by a fixed amount of time, &lt;code&gt;t_late&lt;/code&gt;. This can</span></span><br><span class="line"><span class="comment"> * help reduce the number of elements that are ignored due to lateness when computing the final result for a</span></span><br><span class="line"><span class="comment"> * given window, in the case where we know that elements arrive no later than &lt;code&gt;t_late&lt;/code&gt; units of time</span></span><br><span class="line"><span class="comment"> * after the watermark that signals that the system event-time has advanced past their (event-time) timestamp.</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">BoundedOutOfOrdernessTimestampExtractor</span>&lt;<span class="title">T</span>&gt; <span class="keyword">implements</span> <span class="title">AssignerWithPeriodicWatermarks</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** The current maximum timestamp seen so far. */</span></span><br><span class="line"><span class="comment">/** 数据流的最大时间戳 */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">long</span> currentMaxTimestamp;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** The timestamp of the last emitted watermark. */</span></span><br><span class="line"><span class="comment">/** 最后一次已提交的最新 [水印]（当前批次水印） */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">long</span> lastEmittedWatermark = Long.MIN_VALUE;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The (fixed) interval between the maximum seen timestamp seen in the records</span></span><br><span class="line"><span class="comment"> * and that of the watermark to be emitted.</span></span><br><span class="line"><span class="comment"> * 最大乱序时间间隔</span></span><br><span class="line"><span class="comment"> * 将要被提交的 [水印] 和 [数据流的最大时间戳] 的固定时间间隔</span></span><br><span class="line"><span class="comment"> * 如果 [数据流的最大时间戳] - [当前批次水印] &gt; [最大乱序时间间隔]</span></span><br><span class="line"><span class="comment"> * 则就会打上一个新的 [水印]</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> maxOutOfOrderness;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">BoundedOutOfOrdernessTimestampExtractor</span><span class="params">(Time maxOutOfOrderness)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (maxOutOfOrderness.toMilliseconds() &lt; <span class="number">0</span>) &#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;Tried to set the maximum allowed &quot;</span> +</span><br><span class="line"><span class="string">&quot;lateness to &quot;</span> + maxOutOfOrderness + <span class="string">&quot;. This parameter cannot be negative.&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">this</span>.maxOutOfOrderness = maxOutOfOrderness.toMilliseconds();</span><br><span class="line"><span class="keyword">this</span>.currentMaxTimestamp = Long.MIN_VALUE + <span class="keyword">this</span>.maxOutOfOrderness;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getMaxOutOfOrdernessInMillis</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> maxOutOfOrderness;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Extracts the timestamp from the given element.</span></span><br><span class="line"><span class="comment"> * 从当前数据流元素中获取 [时间戳] 字段，需要用户根据业务自定义</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> element The element that the timestamp is extracted from.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> The new timestamp.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(T element)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果 [当前数据流最大时间戳] - [最大乱序时间间隔] &gt;= [最后一次已提交的时间戳]</span></span><br><span class="line"><span class="comment"> * 则更新 [最后一次已提交的时间戳]</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> Watermark <span class="title">getCurrentWatermark</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">// this guarantees that the watermark never goes backwards.</span></span><br><span class="line"><span class="keyword">long</span> potentialWM = currentMaxTimestamp - maxOutOfOrderness;</span><br><span class="line"><span class="keyword">if</span> (potentialWM &gt;= lastEmittedWatermark) &#123;</span><br><span class="line">lastEmittedWatermark = potentialWM;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> Watermark(lastEmittedWatermark);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取数据流中当前最大时间戳</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(T element, <span class="keyword">long</span> previousElementTimestamp)</span> </span>&#123;</span><br><span class="line"><span class="keyword">long</span> timestamp = extractTimestamp(element);</span><br><span class="line"><span class="keyword">if</span> (timestamp &gt; currentMaxTimestamp) &#123;</span><br><span class="line">currentMaxTimestamp = timestamp;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> timestamp;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id=""><a href="#" class="headerlink" title="****"></a>****</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Sets the time characteristic for all streams create from this environment, e.g., processing</span></span><br><span class="line"><span class="comment"> * time, event time, or ingestion time.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;If you set the characteristic to IngestionTime of EventTime this will set a default</span></span><br><span class="line"><span class="comment"> * watermark update interval of 200 ms. If this is not applicable for your application</span></span><br><span class="line"><span class="comment"> * you should change it using &#123;<span class="doctag">@link</span> ExecutionConfig#setAutoWatermarkInterval(long)&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> characteristic The time characteristic.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setStreamTimeCharacteristic</span><span class="params">(TimeCharacteristic characteristic)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.timeCharacteristic = Preconditions.checkNotNull(characteristic);</span><br><span class="line">    <span class="keyword">if</span> (characteristic == TimeCharacteristic.ProcessingTime) &#123;</span><br><span class="line">        getConfig().setAutoWatermarkInterval(<span class="number">0</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        getConfig().setAutoWatermarkInterval(<span class="number">200</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="window触发机制"><a href="#window触发机制" class="headerlink" title="window触发机制"></a><strong>window触发机制</strong></h2><p>window的触发机制，是先按照自然时间将window划分，如果window大小是3秒，那么1分钟内会把window划分为如下的形式:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[00:00:00,00:00:03)</span><br><span class="line">[00:00:03,00:00:06)</span><br><span class="line">...</span><br><span class="line">[00:00:57,00:01:00)</span><br></pre></td></tr></table></figure><p>如果window大小是10秒，则window会被分为如下的形式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[00:00:00,00:00:10)</span><br><span class="line">[00:00:10,00:00:20)</span><br><span class="line">...</span><br><span class="line">[00:00:50,00:01:00)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Hadoop 学习：hdfs shell 命令</title>
    <link href="https://yangyichao-mango.github.io/2019/10/30/apache-hadoop:study-hdfs-shell/"/>
    <id>https://yangyichao-mango.github.io/2019/10/30/apache-hadoop:study-hdfs-shell/</id>
    <published>2019-10-30T08:27:45.000Z</published>
    <updated>2019-10-30T08:32:07.350Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Hadoop 学习：hdfs shell 命令</p><span id="more"></span><h3 id="ls"><a href="#ls" class="headerlink" title="ls"></a><strong>ls</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -ls /</span><br></pre></td></tr></table></figure><h3 id="put"><a href="#put" class="headerlink" title="put"></a><strong>put</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put localfile /user/hadoop/hadoopfile</span><br></pre></td></tr></table></figure><h3 id="get"><a href="#get" class="headerlink" title="get"></a><strong>get</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -get /user/hadoop/hadoopfile localfile</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/categories/Apache-Hadoop/"/>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/tags/Apache-Hadoop/"/>
    
      <category term="Hdfs" scheme="https://yangyichao-mango.github.io/tags/Hdfs/"/>
    
  </entry>
  
  <entry>
    <title>Mac安装Nginx-1.17.3以及相关配置文件</title>
    <link href="https://yangyichao-mango.github.io/2019/10/28/nginx:1-17-3-mac-install-and-confs-set/"/>
    <id>https://yangyichao-mango.github.io/2019/10/28/nginx:1-17-3-mac-install-and-confs-set/</id>
    <published>2019-10-28T13:19:10.000Z</published>
    <updated>2019-10-29T02:56:24.767Z</updated>
    
    <content type="html"><![CDATA[<p>Mac安装Nginx-1.17.3以及相关配置文件</p><span id="more"></span><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a><strong>安装</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install nginx</span><br></pre></td></tr></table></figure><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a><strong>配置</strong></h2><p>nginx是一个功能非常强大的web服务器加反向代理服务器，同时又是邮件服务器等等</p><p>在项目使用中，使用最多的三个核心功能是反向代理、负载均衡和静态服务器</p><p>这三个不同的功能的使用，都跟nginx的配置密切相关，nginx服务器的配置信息主要集中在nginx.conf这个配置文件中，并且所有的可配置选项大致分为以下几个部分</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">main                                <span class="comment"># 全局配置</span></span><br><span class="line"></span><br><span class="line">events &#123;                            <span class="comment"># nginx工作模式配置</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;                                <span class="comment"># http设置</span></span><br><span class="line">    ....</span><br><span class="line"></span><br><span class="line">    server &#123;                        <span class="comment"># 服务器主机配置</span></span><br><span class="line">        ....</span><br><span class="line">        location &#123;                    <span class="comment"># 路由配置</span></span><br><span class="line">            ....</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        location path &#123;</span><br><span class="line">            ....</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        location otherpath &#123;</span><br><span class="line">            ....</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    server &#123;</span><br><span class="line">        ....</span><br><span class="line"></span><br><span class="line">        location &#123;</span><br><span class="line">            ....</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    upstream name &#123;                    <span class="comment"># 负载均衡配置</span></span><br><span class="line">        ....</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如上述配置文件所示，主要由6个部分组成：</p><p>1.main：用于进行nginx全局信息的配置<br>2.events：用于nginx工作模式的配置<br>3.http：用于进行http协议信息的一些配置<br>4.server：用于进行服务器访问信息的配置<br>5.location：用于进行访问路由的配置<br>6.upstream：用于进行负载均衡的配置</p><h2 id="main模块"><a href="#main模块" class="headerlink" title="main模块"></a><strong>main模块</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># user nobody nobody;</span></span><br><span class="line">worker_processes 2;</span><br><span class="line"><span class="comment"># error_log logs/error.log</span></span><br><span class="line"><span class="comment"># error_log logs/error.log notice</span></span><br><span class="line"><span class="comment"># error_log logs/error.log info</span></span><br><span class="line"><span class="comment"># pid logs/nginx.pid</span></span><br><span class="line">worker_rlimit_nofile 1024;</span><br></pre></td></tr></table></figure><p>上述配置都是存放在main全局配置模块中的配置项</p><p>1.user：用来指定nginx worker进程运行用户以及用户组，默认nobody账号运行<br>2.worker_processes：指定nginx要开启的子进程数量，运行过程中监控每个进程消耗内存(一般几M~几十M不等)根据实际情况进行调整，通常数量是CPU内核数量的整数倍<br>3.error_log：定义错误日志文件的位置及输出级别【debug / info / notice / warn / error / crit】<br>4.pid：用来指定进程id的存储文件的位置<br>5.worker_rlimit_nofile：用于指定一个进程可以打开最多文件数量的描述</p><h2 id="events模块"><a href="#events模块" class="headerlink" title="events模块"></a><strong>events模块</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">events &#123;</span><br><span class="line">    worker_connections 1024;</span><br><span class="line">    multi_accept on;</span><br><span class="line">    use epoll;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述配置是针对nginx服务器的工作模式的一些操作配置</p><p>1.worker_connections：指定最大可以同时接收的连接数量，这里一定要注意，最大连接数量是和worker processes共同决定的<br>2.multi_accept：配置指定nginx在收到一个新连接通知后尽可能多的接受更多的连接<br>3.use epoll：配置指定了线程轮询的方法，如果是linux2.6+，使用epoll，如果是BSD如Mac请使用Kqueue</p><h2 id="http模块"><a href="#http模块" class="headerlink" title="http模块"></a><strong>http模块</strong></h2><p>作为web服务器，http模块是nginx最核心的一个模块，配置项也是比较多的，项目中会设置到很多的实际业务场景，需要根据硬件信息进行适当的配置，常规情况下，使用默认配置即可！</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">    <span class="comment">##</span></span><br><span class="line">    <span class="comment"># 基础配置</span></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line"></span><br><span class="line">    sendfile on;</span><br><span class="line">    tcp_nopush on;</span><br><span class="line">    tcp_nodelay on;</span><br><span class="line">    keepalive_timeout 65;</span><br><span class="line">    types_hash_max_size 2048;</span><br><span class="line">    <span class="comment"># server_tokens off;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># server_names_hash_bucket_size 64;</span></span><br><span class="line">    <span class="comment"># server_name_in_redirect off;</span></span><br><span class="line"></span><br><span class="line">    include /etc/nginx/mime.types;</span><br><span class="line">    default_type application/octet-stream;</span><br><span class="line"></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line">    <span class="comment"># SSL证书配置</span></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line"></span><br><span class="line">    ssl_protocols TLSv1 TLSv1.1 TLSv1.2; <span class="comment"># Dropping SSLv3, ref: POODLE</span></span><br><span class="line">    ssl_prefer_server_ciphers on;</span><br><span class="line"></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line">    <span class="comment"># 日志配置</span></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line"></span><br><span class="line">    access_log /var/<span class="built_in">log</span>/nginx/access.log;</span><br><span class="line">    error_log /var/<span class="built_in">log</span>/nginx/error.log;</span><br><span class="line"></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line">    <span class="comment"># Gzip 压缩配置</span></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line"></span><br><span class="line">    gzip on;</span><br><span class="line">    gzip_disable <span class="string">&quot;msie6&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># gzip_vary on;</span></span><br><span class="line">    <span class="comment"># gzip_proxied any;</span></span><br><span class="line">    <span class="comment"># gzip_comp_level 6;</span></span><br><span class="line">    <span class="comment"># gzip_buffers 16 8k;</span></span><br><span class="line">    <span class="comment"># gzip_http_version 1.1;</span></span><br><span class="line">    <span class="comment"># gzip_types text/plain text/css application/json application/javascript</span></span><br><span class="line"> text/xml application/xml application/xml+rss text/javascript;</span><br><span class="line"></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line">    <span class="comment"># 虚拟主机配置</span></span><br><span class="line">    <span class="comment">##</span></span><br><span class="line"></span><br><span class="line">    include /etc/nginx/conf.d/*.conf;</span><br><span class="line">    include /etc/nginx/sites-enabled/*;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="基础配置"><a href="#基础配置" class="headerlink" title="基础配置"></a>基础配置</h3><p>1.sendfile on：配置on让sendfile发挥作用，将文件的回写过程交给数据缓冲去去完成，而不是放在应用中完成，这样的话在性能提升有有好处<br>2.tc_nopush on：让nginx在一个数据包中发送所有的头文件，而不是一个一个单独发<br>3.tcp_nodelay on：让nginx不要缓存数据，而是一段一段发送，如果数据的传输有实时性的要求的话可以配置它，发送完一小段数据就立刻能得到返回值，但是不要滥用哦</p><p>4.keepalive_timeout 10：给客户端分配连接超时时间，服务器会在这个时间过后关闭连接。一般设置时间较短，可以让nginx工作持续性更好<br>5.client_header_timeout 10：设置请求头的超时时间<br>6.client_body_timeout 10：设置请求体的超时时间<br>7.send_timeout 10：指定客户端响应超时时间，如果客户端两次操作间隔超过这个时间，服务器就会关闭这个链接</p><p>8.limit_conn_zone $binary_remote_addr zone=addr:5m ：设置用于保存各种key的共享内存的参数，<br>9.limit_conn addr 100: 给定的key设置最大连接数</p><p>10.server_tokens：虽然不会让nginx执行速度更快，但是可以在错误页面关闭nginx版本提示，对于网站安全性的提升有好处哦<br>11.include /etc/nginx/mime.types：指定在当前文件中包含另一个文件的指令<br>12.default_type application/octet-stream：指定默认处理的文件类型可以是二进制<br>13.type_hash_max_size 2048：混淆数据，影响三列冲突率，值越大消耗内存越多，散列key冲突率会降低，检索速度更快；值越小key，占用内存较少，冲突率越高，检索速度变慢</p><h3 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h3><p>1.access_log logs/access.log：设置存储访问记录的日志<br>2.error_log logs/error.log：设置存储记录错误发生的日志</p><h3 id="压缩配置"><a href="#压缩配置" class="headerlink" title="压缩配置"></a>压缩配置</h3><p>1.gzip：是告诉nginx采用gzip压缩的形式发送数据。这将会减少我们发送的数据量。<br>2.gzip_disable：为指定的客户端禁用gzip功能。我们设置成IE6或者更低版本以使我们的方案能够广泛兼容。<br>3.gzip_static：告诉nginx在压缩资源之前，先查找是否有预先gzip处理过的资源。这要求你预先压缩你的文件（在这个例子中被注释掉了），从而允许你使用最高压缩比，这样nginx就不用再压缩这些文件了（想要更详尽的gzip_static的信息，请点击这里）。<br>4.gzip_proxied：允许或者禁止压缩基于请求和响应的响应流。我们设置为any，意味着将会压缩所有的请求。<br>5.gzip_min_length：设置对数据启用压缩的最少字节数。如果一个请求小于1000字节，我们最好不要压缩它，因为压缩这些小的数据会降低处理此请求的所有进程的速度。<br>6.gzip_comp_level：设置数据的压缩等级。这个等级可以是1-9之间的任意数值，9是最慢但是压缩比最大的。我们设置为4，这是一个比较折中的设置。<br>7.gzip_type：设置需要压缩的数据格式。上面例子中已经有一些了，你也可以再添加更多的格式。</p><h3 id="文件缓存配置"><a href="#文件缓存配置" class="headerlink" title="文件缓存配置"></a>文件缓存配置</h3><p>1.open_file_cache：打开缓存的同时也指定了缓存最大数目，以及缓存的时间。我们可以设置一个相对高的最大时间，这样我们可以在它们不活动超过20秒后清除掉。<br>2.open_file_cache_valid：在open_file_cache中指定检测正确信息的间隔时间。<br>3.open_file_cache_min_uses：定义了open_file_cache中指令参数不活动时间期间里最小的文件数。<br>4.open_file_cache_errors：指定了当搜索一个文件时是否缓存错误信息，也包括再次给配置中添加文件。我们也包括了服务器模块，这些是在不同文件中定义的。如果你的服务器模块不在这些位置，你就得修改这一行来指定正确的位置。</p><h3 id="server模块"><a href="#server模块" class="headerlink" title="server模块"></a>server模块</h3><p>server模块配置是http模块中的一个子模块，用来定义一个虚拟访问主机，也就是一个虚拟服务器的配置信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen        80;</span><br><span class="line">    server_name localhost    192.168.1.100;</span><br><span class="line">    root        /nginx/www;</span><br><span class="line">    index        index.php index.html index.html;</span><br><span class="line">    charset        utf-8;</span><br><span class="line">    access_log    logs/access.log;</span><br><span class="line">    error_log    logs/error.log;</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>核心配置信息如下：</p><p>1.server：一个虚拟主机的配置，一个http中可以配置多个server</p><p>2.server_name：用力啊指定ip地址或者域名，多个配置之间用空格分隔</p><p>3.root：表示整个server虚拟主机内的根目录，所有当前主机中web项目的根目录</p><p>4.index：用户访问web网站时的全局首页</p><p>5.charset：用于设置www/路径中配置的网页的默认编码格式</p><p>6.access_log：用于指定该虚拟主机服务器中的访问记录日志存放路径</p><p>7.error_log：用于指定该虚拟主机服务器中访问错误日志的存放路径</p><h3 id="location模块"><a href="#location模块" class="headerlink" title="location模块"></a>location模块</h3><p>location模块是nginx配置中出现最多的一个配置，主要用于配置路由访问信息</p><p>在路由访问信息配置中关联到反向代理、负载均衡等等各项功能，所以location模块也是一个非常重要的配置模块</p><h4 id="基本配置"><a href="#基本配置" class="headerlink" title="基本配置"></a>基本配置</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">location / &#123;</span><br><span class="line">    root    /nginx/www;</span><br><span class="line">    index    index.php index.html index.htm;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>location /：表示匹配访问根目录</p><p>root：用于指定访问根目录时，访问虚拟主机的web目录</p><p>index：在不指定访问具体资源时，默认展示的资源文件列表</p><h4 id="反向代理配置方式"><a href="#反向代理配置方式" class="headerlink" title="反向代理配置方式"></a>反向代理配置方式</h4><p>通过反向代理代理服务器访问模式，通过proxy_set配置让客户端访问透明化</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">location / &#123;</span><br><span class="line">    proxy_pass http://localhost:8888;</span><br><span class="line">    proxy_set_header X-real-ip <span class="variable">$remote_addr</span>;</span><br><span class="line">    proxy_set_header Host <span class="variable">$http_host</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="uwsgi配置"><a href="#uwsgi配置" class="headerlink" title="uwsgi配置"></a>uwsgi配置</h4><p>wsgi模式下的服务器配置访问方式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">location / &#123;</span><br><span class="line">    include uwsgi_params;</span><br><span class="line">    uwsgi_pass localhost:8888</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="upstream模块"><a href="#upstream模块" class="headerlink" title="upstream模块"></a>upstream模块</h3><p>upstream模块主要负责负载均衡的配置，通过默认的轮询调度方式来分发请求到后端服务器</p><p>简单的配置方式如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">upstream name &#123;</span><br><span class="line">    ip_hash;</span><br><span class="line">    server 192.168.1.100:8000;</span><br><span class="line">    server 192.168.1.100:8001 down;</span><br><span class="line">    server 192.168.1.100:8002 max_fails=3;</span><br><span class="line">    server 192.168.1.100:8003 fail_timeout=20s;</span><br><span class="line">    server 192.168.1.100:8004 max_fails=3 fail_timeout=20s;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>核心配置信息如下</p><p>1.ip_hash：指定请求调度算法，默认是weight权重轮询调度，可以指定</p><p>2.server host:port：分发服务器的列表配置</p><p>– down：表示该主机暂停服务</p><p>– max_fails：表示失败最大次数，超过失败最大次数暂停服务</p><p>– fail_timeout：表示如果请求受理失败，暂停指定的时间之后重新发起请求</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Nginx" scheme="https://yangyichao-mango.github.io/categories/Nginx/"/>
    
    
      <category term="Nginx" scheme="https://yangyichao-mango.github.io/tags/Nginx/"/>
    
  </entry>
  
  <entry>
    <title>Elastic-Job学习：分布式任务调度框架</title>
    <link href="https://yangyichao-mango.github.io/2019/10/26/elastic-job:study-distributed-scheduled-job-framework/"/>
    <id>https://yangyichao-mango.github.io/2019/10/26/elastic-job:study-distributed-scheduled-job-framework/</id>
    <published>2019-10-26T14:08:15.000Z</published>
    <updated>2019-10-28T03:23:31.499Z</updated>
    
    <content type="html"><![CDATA[<p>分布式任务调度框架学习</p><span id="more"></span><p><a href="http://elasticjob.io/docs/elastic-job-lite/00-overview/intro/">官方文档</a></p><p><a href="https://github.com/elasticjob">elastic-job github</a></p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="分布式调度框架" scheme="https://yangyichao-mango.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E8%B0%83%E5%BA%A6%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Apache Zookeeper" scheme="https://yangyichao-mango.github.io/tags/Apache-Zookeeper/"/>
    
      <category term="分布式调度框架" scheme="https://yangyichao-mango.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%B0%83%E5%BA%A6%E6%A1%86%E6%9E%B6/"/>
    
      <category term="SpringBoot" scheme="https://yangyichao-mango.github.io/tags/SpringBoot/"/>
    
  </entry>
  
  <entry>
    <title>Vue学习：Vue前端项目部署到SpringBoot工程下</title>
    <link href="https://yangyichao-mango.github.io/2019/10/26/vue:study-vue-project-deploy-in-springboot-project/"/>
    <id>https://yangyichao-mango.github.io/2019/10/26/vue:study-vue-project-deploy-in-springboot-project/</id>
    <published>2019-10-26T07:29:37.000Z</published>
    <updated>2019-10-26T07:52:06.519Z</updated>
    
    <content type="html"><![CDATA[<p>Vue项目部署到SpringBoot工程下</p><span id="more"></span><h2 id="Vue前端项目"><a href="#Vue前端项目" class="headerlink" title="Vue前端项目"></a><strong>Vue前端项目</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm run build</span><br></pre></td></tr></table></figure><p>运行上述命令，将前端Vue项目打包，命令运行完成之后会在项目根目录下生成一个生成一个dist文件夹, 编译好的静态文件就在这里面</p><h2 id="部署在SpringBoot项目下"><a href="#部署在SpringBoot项目下" class="headerlink" title="部署在SpringBoot项目下"></a><strong>部署在SpringBoot项目下</strong></h2><p>将前端项目中dist文件夹下的所有文件拷贝到SpringBoot工程的<strong>src/main/resources/static</strong>文件夹下</p><p>运行后端项目，可以看到控制台会有这样的输出，证明将静态页面加入了容器中，现在就可以访问到前端页面了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2019-10-26 15:47:09.607  INFO 1967 --- [           main] o.s.b.a.w.s.WelcomePageHandlerMapping    : Adding welcome page: class path resource [static/index.html]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Vue" scheme="https://yangyichao-mango.github.io/categories/Vue/"/>
    
    
      <category term="SpringBoot" scheme="https://yangyichao-mango.github.io/tags/SpringBoot/"/>
    
      <category term="Vue" scheme="https://yangyichao-mango.github.io/tags/Vue/"/>
    
  </entry>
  
  <entry>
    <title>Maven学习：使用maven-shade-plugin解决依赖包冲突</title>
    <link href="https://yangyichao-mango.github.io/2019/10/25/apache-maven:study-maven-shade-resolve-jar-conflicts/"/>
    <id>https://yangyichao-mango.github.io/2019/10/25/apache-maven:study-maven-shade-resolve-jar-conflicts/</id>
    <published>2019-10-25T06:23:20.000Z</published>
    <updated>2019-10-25T08:21:33.406Z</updated>
    
    <content type="html"><![CDATA[<p>java 依赖包冲突，使用maven-shade-plugin解决</p><span id="more"></span><h3 id="错误场景详情"><a href="#错误场景详情" class="headerlink" title="错误场景详情"></a><strong>错误场景详情</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoSuchMethodError: com.google.common.util.concurrent.MoreExecutors.directExecutor()Ljava/util/concurrent/Executor;</span><br></pre></td></tr></table></figure><h3 id="错误原因"><a href="#错误原因" class="headerlink" title="错误原因"></a><strong>错误原因</strong></h3><p>出现这样的错误详情一般是由于有下面这样的包依赖情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A - B - C(guava version 18)</span><br><span class="line">  \ D - C(guava version 23.6-jre)</span><br></pre></td></tr></table></figure><p><em>A</em>：代表我们所开发的当前项目<br><em>B和D</em>：代表当前项目所依赖的项目<br><em>C</em>：代表当前项目依赖的项目所依赖的项目</p><p>由于我们当前所开发的项目A依赖了B和D，B和D又依赖了项目C<br>我们打包运行项目时，maven只会将一个版本C(guava)打进包内（maven打包遇到相同依赖，最短路径优先，在路径相同时先在pom中声明优先）<br>比如此时打进包的版本是C(guava version 23.6-jre)，那么很有可能在运行B中的一个方法时，调用C的一个方法，这个方法是C(guava version 18)中的一个方法，在C(guava version 23.6-jre)中并不存在，这时候就会报出java.lang.NoSuchMethodError</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a><strong>解决方案</strong></h3><p>使用maven-shade-plugin将所有B项目依赖的包全部打进B.jar中，并且给guava包的路径重命名为我们的自定义路径\</p><h4 id="maven-shade-plugin基本功能"><a href="#maven-shade-plugin基本功能" class="headerlink" title="maven-shade-plugin基本功能"></a>maven-shade-plugin基本功能</h4><p>maven-shade-plugin提供了两大基本功能：</p><p>1、将依赖的jar包打包到当前jar包（常规打包是不会将所依赖jar包打进来的）<br>2、对依赖的jar包进行重命名（用于类的隔离，解决包冲突就是使用了这个功能）</p><h4 id="解决示例"><a href="#解决示例" class="headerlink" title="解决示例"></a>解决示例</h4><p>如下例，就可以在B项目中使用C(guava version 18)，只不过import路径变成了我们自定的路径</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span> <span class="attr">xmlns:xsi</span>=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>RpcModule<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>RpcModule<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">packaging</span>&gt;</span>jar<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>RpcModule<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">guava.version</span>&gt;</span>18<span class="tag">&lt;/<span class="name">guava.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>guava<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;guava.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">source</span>&gt;</span>8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">target</span>&gt;</span>8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">createDependencyReducedPom</span>&gt;</span>false<span class="tag">&lt;/<span class="name">createDependencyReducedPom</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="comment">&lt;!-- 给guava包的路径重命名为我们的自定义路径 --&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">relocations</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>shade.com.google.guava<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>org.joda<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>shade.com.google.joda<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>com.google.common<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>shade.com.google.common<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">relocations</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line">                                &lt;transformer</span><br><span class="line">                                        implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;/&gt;</span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><p>使用maven-shade-plugin之前</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> Entity.Request;</span><br><span class="line"><span class="keyword">import</span> Entity.Response;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.*;</span><br><span class="line"><span class="keyword">import</span> java.net.Socket;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.google.common.collect.ImmutableMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SocketClient</span> </span>&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用maven-shade-plugin之后编译后反编译结果</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> Entity.Request;</span><br><span class="line"><span class="keyword">import</span> Entity.Response;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.*;</span><br><span class="line"><span class="keyword">import</span> java.net.Socket;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> shade.com.google.common.collect.ImmutableMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SocketClient</span> </span>&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后将打好的包上传至我们的maven仓库，然后再在当前项目A中依赖，就没有依赖冲突了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A - B - C(guava version 18, shade.com.google.common.collect.ImmutableMap)</span><br><span class="line">  \ D - C(guava version 23.6-jre, com.google.common.collect.ImmutableMap)</span><br></pre></td></tr></table></figure><p>这样就可以做到将两个不同版本的包都引入使用，由于引入包路径不同，因此也没有冲突</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Maven" scheme="https://yangyichao-mango.github.io/categories/Apache-Maven/"/>
    
    
      <category term="Apache Maven" scheme="https://yangyichao-mango.github.io/tags/Apache-Maven/"/>
    
  </entry>
  
  <entry>
    <title>Apache Zookeeper 学习：Zookeeper实现统一配置管理中心</title>
    <link href="https://yangyichao-mango.github.io/2019/10/24/apache-zookeeper:study-zookeeper-implement-unified-configuration-management-center/"/>
    <id>https://yangyichao-mango.github.io/2019/10/24/apache-zookeeper:study-zookeeper-implement-unified-configuration-management-center/</id>
    <published>2019-10-24T10:40:24.000Z</published>
    <updated>2019-11-06T13:07:48.984Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Zookeeper 学习：模拟三台节点组成的Zookeeper集群实现的统一配置管理中心</p><span id="more"></span><h2 id="模拟Zookeeper集群"><a href="#模拟Zookeeper集群" class="headerlink" title="模拟Zookeeper集群"></a><strong>模拟Zookeeper集群</strong></h2><h3 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h3><p><img src="/blog-img/apache-zookeeper:study-zookeeper-implement-unified-configuration-management-center/zookeeper%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt="Zookeeper集群架构"></p><h3 id="创建zookeeper集群节点"><a href="#创建zookeeper集群节点" class="headerlink" title="创建zookeeper集群节点"></a>创建zookeeper集群节点</h3><p>模拟三台节点组成的zookeeper集群，需要在本机zookeeper目录下<br>创建三个zookeeper集群节点配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> conf/</span><br><span class="line">$ cp zoo_sample.cfg zoo1.cfg</span><br><span class="line">$ cp zoo_sample.cfg zoo2.cfg</span><br><span class="line">$ cp zoo_sample.cfg zoo3.cfg</span><br></pre></td></tr></table></figure><h3 id="创建所配置的各个文件夹"><a href="#创建所配置的各个文件夹" class="headerlink" title="创建所配置的各个文件夹"></a>创建所配置的各个文件夹</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir /tmp/zookeeper1</span><br><span class="line">$ mkdir /tmp/zookeeper1/data</span><br><span class="line">$ mkdir /tmp/zookeeper1/dataLog</span><br><span class="line">$ mkdir /tmp/zookeeper2</span><br><span class="line">$ mkdir /tmp/zookeeper2/data</span><br><span class="line">$ mkdir /tmp/zookeeper2/dataLog</span><br><span class="line">$ mkdir /tmp/zookeeper3</span><br><span class="line">$ mkdir /tmp/zookeeper3/data</span><br><span class="line">$ mkdir /tmp/zookeeper3/dataLog</span><br></pre></td></tr></table></figure><h3 id="tmp-zookeeperX-data文件夹下创建myid文件"><a href="#tmp-zookeeperX-data文件夹下创建myid文件" class="headerlink" title="/tmp/zookeeperX/data文件夹下创建myid文件"></a>/tmp/zookeeperX/data文件夹下创建myid文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> 1 &gt; /tmp/zookeeper1/data/myid</span><br><span class="line">$ <span class="built_in">echo</span> 2 &gt; /tmp/zookeeper2/data/myid</span><br><span class="line">$ <span class="built_in">echo</span> 3 &gt; /tmp/zookeeper3/data/myid</span><br></pre></td></tr></table></figure><h3 id="配置zookeeper节点信息"><a href="#配置zookeeper节点信息" class="headerlink" title="配置zookeeper节点信息"></a>配置zookeeper节点信息</h3><p><strong>zoo1.cfg</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The number of milliseconds of each tick</span></span><br><span class="line">tickTime=2000</span><br><span class="line"><span class="comment"># The number of ticks that the initial</span></span><br><span class="line"><span class="comment"># synchronization phase can take</span></span><br><span class="line">initLimit=10</span><br><span class="line"><span class="comment"># The number of ticks that can pass between</span></span><br><span class="line"><span class="comment"># sending a request and getting an acknowledgement</span></span><br><span class="line">syncLimit=5</span><br><span class="line"><span class="comment"># the directory where the snapshot is stored.</span></span><br><span class="line"><span class="comment"># do not use /tmp for storage, /tmp here is just</span></span><br><span class="line"><span class="comment"># example sakes.</span></span><br><span class="line">dataDir=/tmp/zookeeper1/data</span><br><span class="line">dataLogDir=/tmp/zookeeper1/dataLog</span><br><span class="line"><span class="comment"># the port at which the clients will connect</span></span><br><span class="line">clientPort=2181</span><br><span class="line"><span class="comment"># the maximum number of client connections.</span></span><br><span class="line"><span class="comment"># increase this if you need to handle more clients</span></span><br><span class="line"><span class="comment">#maxClientCnxns=60</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Be sure to read the maintenance section of the</span></span><br><span class="line"><span class="comment"># administrator guide before turning on autopurge.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The number of snapshots to retain in dataDir</span></span><br><span class="line"><span class="comment">#autopurge.snapRetainCount=3</span></span><br><span class="line"><span class="comment"># Purge task interval in hours</span></span><br><span class="line"><span class="comment"># Set to &quot;0&quot; to disable auto purge feature</span></span><br><span class="line"><span class="comment">#autopurge.purgeInterval=1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 格式:server.num=xxxx:port1:port2</span></span><br><span class="line"><span class="comment"># num对应myid中的内容，port1是zookeeper集群中各服务间的通信端口，port2是zookeeper集群选举leader的端口</span></span><br><span class="line">server.1=localhost:2888:3888</span><br><span class="line">server.2=localhost:2899:3899</span><br><span class="line">server.3=localhost:2877:3877</span><br></pre></td></tr></table></figure><p><strong>zoo2.cfg</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the directory where the snapshot is stored.</span></span><br><span class="line"><span class="comment"># do not use /tmp for storage, /tmp here is just</span></span><br><span class="line"><span class="comment"># example sakes.</span></span><br><span class="line">dataDir=/tmp/zookeeper2/data</span><br><span class="line">dataLogDir=/tmp/zookeeper2/dataLog</span><br><span class="line"><span class="comment"># the port at which the clients will connect</span></span><br><span class="line">clientPort=2182</span><br><span class="line">...</span><br><span class="line">server.1=localhost:2888:3888</span><br><span class="line">server.2=localhost:2899:3899</span><br><span class="line">server.3=localhost:2877:3877</span><br></pre></td></tr></table></figure><p><strong>zoo3.cfg</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the directory where the snapshot is stored.</span></span><br><span class="line"><span class="comment"># do not use /tmp for storage, /tmp here is just</span></span><br><span class="line"><span class="comment"># example sakes.</span></span><br><span class="line">dataDir=/tmp/zookeeper3/data</span><br><span class="line">dataLogDir=/tmp/zookeeper3/dataLog</span><br><span class="line"><span class="comment"># the port at which the clients will connect</span></span><br><span class="line">clientPort=2183</span><br><span class="line">...</span><br><span class="line">server.1=localhost:2888:3888</span><br><span class="line">server.2=localhost:2899:3899</span><br><span class="line">server.3=localhost:2877:3877</span><br></pre></td></tr></table></figure><p>配置文件中dataDir，dataLogDir，clientPort，三个zookeeper节点配置信息都不同<br>搭建zookeeper集群，需要在每个zookeeper安装目录下的data文件中创建名为myid的文件，修改zooX.cfg内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">server.1=xxx:2888:3888</span><br><span class="line">server.2=xxx:2899:3899</span><br><span class="line">server.3=xxx:2877:3877</span><br></pre></td></tr></table></figure><p>格式:server.num=xxxx:port1:port2<br>num对应myid中的内容，port1是zookeeper集群中各服务间的通信端口，port2是zookeeper集群选举leader的端口</p><h3 id="启动模拟集群节点"><a href="#启动模拟集群节点" class="headerlink" title="启动模拟集群节点"></a>启动模拟集群节点</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ ./zkServer.sh start ../conf/zoo1.cfg</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: ../conf/zoo1.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">$ ./zkServer.sh start ../conf/zoo2.cfg</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: ../conf/zoo2.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">$ ./zkServer.sh start ../conf/zoo3.cfg</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: ../conf/zoo3.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure><p>查看集群状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ ./zkServer.sh status ../conf/zoo1.cfg</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: ../conf/zoo1.cfg</span><br><span class="line">Mode: follower</span><br><span class="line">$ ./zkServer.sh status ../conf/zoo2.cfg</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: ../conf/zoo2.cfg</span><br><span class="line">Mode: leader</span><br><span class="line">$ ./zkServer.sh status ../conf/zoo3.cfg</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: ../conf/zoo3.cfg</span><br><span class="line">Mode: follower</span><br></pre></td></tr></table></figure><h2 id="创建zookeeper集群client"><a href="#创建zookeeper集群client" class="headerlink" title="创建zookeeper集群client"></a><strong>创建zookeeper集群client</strong></h2><h3 id="创建监听节点变化的server（zookeeper集群client）"><a href="#创建监听节点变化的server（zookeeper集群client）" class="headerlink" title="创建监听节点变化的server（zookeeper集群client）"></a>创建监听节点变化的server（zookeeper集群client）</h3><p>模拟监听节点变化server，启动两个BaseWatcher程序作为监听server（对zookeeper集群来说是client）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BaseWatcher</span> <span class="keyword">implements</span> <span class="title">Watcher</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> ZooKeeper zookeeper;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 超时时间</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> SESSION_TIME_OUT = <span class="number">2000</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> CountDownLatch defaultCountDownLatch = <span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> CountDownLatch childrenCountDownLatch = <span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> CountDownLatch dataCountDownLatch = <span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (event.getState() == KeeperState.SyncConnected) &#123;</span><br><span class="line">            LOGGER.info(<span class="string">&quot;Watch received event&quot;</span>);</span><br><span class="line">            defaultCountDownLatch.countDown();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (event.getType() == EventType.NodeCreated) &#123;</span><br><span class="line">            LOGGER.info(<span class="string">&quot;创建节点&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (event.getType() == EventType.NodeDataChanged) &#123;</span><br><span class="line">            LOGGER.info(<span class="string">&quot;节点改变&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (event.getType() == EventType.NodeChildrenChanged) &#123;</span><br><span class="line">            LOGGER.info(<span class="string">&quot;子节点节点改变&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (event.getType() == EventType.NodeDeleted) &#123;</span><br><span class="line">            LOGGER.info(<span class="string">&quot;节点删除&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 连接zookeeper</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> host</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception </span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">connectZookeeper</span><span class="params">(String host, Watcher defaultWatcher)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        zookeeper = <span class="keyword">new</span> ZooKeeper(host, SESSION_TIME_OUT, defaultWatcher);</span><br><span class="line">        defaultCountDownLatch.await();</span><br><span class="line">        LOGGER.info(<span class="string">&quot;zookeeper connection success&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取路径下所有子节点</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> path</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> KeeperException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;String&gt; <span class="title">getChildren</span><span class="params">(String path, Watcher childrenWatcher)</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> zookeeper.getChildren(path, childrenWatcher);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取节点上面的数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> path  路径</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> KeeperException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException </span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">getData</span><span class="params">(String path, Watcher dataWatcher)</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">byte</span>[] data = zookeeper.getData(path, dataWatcher, <span class="keyword">null</span>);</span><br><span class="line">        <span class="keyword">if</span> (data == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> String(data);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 关闭连接</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">closeConnection</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (zookeeper != <span class="keyword">null</span>) &#123;</span><br><span class="line">            zookeeper.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String HOST = <span class="string">&quot;localhost:2181,localhost:2182,localhost:2183&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = LoggerFactory.getLogger(BaseWatcher.class);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 运行程序之前需要启动zookeeper服务端，&#123;<span class="doctag">@link</span> BaseWatcher.HOST&#125; 根据zookeeper服务端具体配置去修改</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * 除了默认watcher外其他watcher一旦触发就会失效，需要充新注册，本示例中因为</span></span><br><span class="line"><span class="comment">         * 还未想到比较好的重新注册watcher方式(考虑到如果在Watcher中持有一个zk客户端的</span></span><br><span class="line"><span class="comment">         * 实例可能存在循环引用的问题)，因此暂不实现watcher失效后重新注册watcher的问题，</span></span><br><span class="line"><span class="comment">         * 后续可以查阅curator重新注册watcher的实现方法。</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        BaseWatcher defaultWatcher = <span class="keyword">new</span> BaseWatcher();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 连接zookeeper并设置一个默认的watcher监听zookeeper文件节点的变化</span></span><br><span class="line">        connectZookeeper(HOST, defaultWatcher);</span><br><span class="line">        TimeUnit.SECONDS.sleep(<span class="number">1000000</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="创建改变节点数据的server（zookeeper集群client）"><a href="#创建改变节点数据的server（zookeeper集群client）" class="headerlink" title="创建改变节点数据的server（zookeeper集群client）"></a>创建改变节点数据的server（zookeeper集群client）</h3><p>如果一个节点向被监听节点中写数据，其他节点就会接受到zookeeper的 <strong>NodeDataChanged</strong> event</p><p>模拟改变数据server，启动一个BaseWatcher程序作为改变数据server（对zookeeper集群来说是client）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 运行程序之前需要启动zookeeper服务端，&#123;<span class="doctag">@link</span> BaseWatcher.HOST&#125; 根据zookeeper服务端具体配置去修改</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 除了默认watcher外其他watcher一旦触发就会失效，需要充新注册，本示例中因为</span></span><br><span class="line"><span class="comment">     * 还未想到比较好的重新注册watcher方式(考虑到如果在Watcher中持有一个zk客户端的</span></span><br><span class="line"><span class="comment">     * 实例可能存在循环引用的问题)，因此暂不实现watcher失效后重新注册watcher的问题，</span></span><br><span class="line"><span class="comment">     * 后续可以查阅curator重新注册watcher的实现方法。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    BaseWatcher defaultWatcher = <span class="keyword">new</span> BaseWatcher();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 连接zookeeper并设置一个默认的watcher监听zookeeper文件节点的变化</span></span><br><span class="line">    connectZookeeper(HOST, defaultWatcher);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 向/GetChildren节点写数据之前需要先创建此文件夹</span></span><br><span class="line">    <span class="comment">// 向/GetChildren节点写数据，则监听程序就会收到zookeeper的 [NodeDataChanged] event</span></span><br><span class="line">    setData(<span class="string">&quot;/GetChildren&quot;</span>, <span class="string">&quot;8&quot;</span>);</span><br><span class="line">    TimeUnit.SECONDS.sleep(<span class="number">1000000</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h3><p>两个监听程序收到zookeeper的 <strong>NodeDataChanged</strong> event，log如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">11:24:27.295 [main-SendThread(localhost:2183)] DEBUG org.apache.zookeeper.ClientCnxn - Got notification sessionid:0x300001251b50003</span><br><span class="line">11:24:27.297 [main-SendThread(localhost:2183)] DEBUG org.apache.zookeeper.ClientCnxn - Got WatchedEvent state:SyncConnected <span class="built_in">type</span>:NodeDataChanged path:/GetChildren <span class="keyword">for</span> sessionid 0x300001251b50003</span><br><span class="line">11:24:27.297 [main-EventThread] INFO com.github.xxx.bigdata.demo.zookeeper.BaseWatcher - Watch received event</span><br><span class="line">11:24:27.297 [main-EventThread] INFO com.github.xxx.bigdata.demo.zookeeper.BaseWatcher - 节点改变</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Zookeeper" scheme="https://yangyichao-mango.github.io/categories/Apache-Zookeeper/"/>
    
    
      <category term="Apache Zookeeper" scheme="https://yangyichao-mango.github.io/tags/Apache-Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Apache Druid 学习：组件以及查询类型</title>
    <link href="https://yangyichao-mango.github.io/2019/10/22/apache-druid:study-components-and-query-types/"/>
    <id>https://yangyichao-mango.github.io/2019/10/22/apache-druid:study-components-and-query-types/</id>
    <published>2019-10-22T02:40:12.000Z</published>
    <updated>2019-10-22T13:18:53.844Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Druid 学习：组件以及查询类型</p><span id="more"></span><h2 id="OLAP"><a href="#OLAP" class="headerlink" title="OLAP"></a><strong>OLAP</strong></h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>维度(Dimension): 指的是观察数据的一个角度，是考虑问题的一类属性，这些属性的集合统称为一个维。<br>维的级别(Level): 对数据的观察还存在细节程度的不同，在druid中一般表示为时间的粒度(granularity)，比如一秒，一分钟，一小时，一天…… <br>度量(Measure): 度量是用来聚合分析计算的数字信息，在druid中称为”metrics”，它可以是存储在数据库中，也可以是通过策略计算得出的。比如一篇文章的点击数、或者是根据评论数、点击数、转发数计算出的热点值</p><h3 id="对于数据处理"><a href="#对于数据处理" class="headerlink" title="对于数据处理"></a>对于数据处理</h3><p>向下钻取(Drill-down)/上卷(Roll-up): <strong>改变维的层次和级别，变换分析的粒度</strong>。Roll-up在于提升维的级别（或者称粒度）或者减少维度来聚合数据，展现总览，Drill-down反之，降低维的级别(或者称粒度)或增加维度来查看细节<br>切片(slice)和切块(dice): 当维度为两个时，我们对获取数据(查询)的操作称之为切片，当维度的数量大于两个时，我们称之为切块<br>旋转(Pivoting): 变换维的方向，例如表格中的行列互换</p><h2 id="查询条件参数"><a href="#查询条件参数" class="headerlink" title="查询条件参数"></a><strong>查询条件参数</strong></h2><table><thead><tr><th align="center">字段名</th><th align="center">描述</th><th align="center">是否必须</th></tr></thead><tbody><tr><td align="center">queryType</td><td align="center">查询类型，对应聚合查询下的类型值：timeseries、topN、groupBy等</td><td align="center">是</td></tr><tr><td align="center">dataSource</td><td align="center">数据源，类似关系数据库中表的概念，对应数据导入时Json配置属性dataSource值</td><td align="center">是</td></tr><tr><td align="center">descending</td><td align="center">返回结果是否逆序，默认值为否（正序）</td><td align="center">否</td></tr><tr><td align="center">intervals</td><td align="center">查询时间区间</td><td align="center">是</td></tr><tr><td align="center">filter</td><td align="center">对Dimension进行过滤，可以根据情况对几个维度组合不同的filter类型(and、or、not、bound)，还可以根据需要定义javascript function进行过滤</td><td align="center">否</td></tr><tr><td align="center">aggregations</td><td align="center">指定度量在聚合时候的计算策略，例如相加、或者求平均值、又或者取最后一个值，在内置类型不满足的情况下可以使用javascript。比如某手游中我统计了我每一局击杀小怪数量，以及野怪的数量，通过聚合策略sum，我能知道我从开号以来击杀了多少小怪和野怪。</td><td align="center">否</td></tr><tr><td align="center">postAggregations</td><td align="center">后聚合策略，提供了多个度量组合生成新度量的能力，主要有利于聚合计算的抽象，避免对一些指标的重复计算。举个例子，假如我需要一个度量，是我击杀小怪和野怪的总和，那么，我只需要在后聚合阶段计算，只需要拿小怪和野怪的数量相加一次，大大地提高了计算效率。</td><td align="center">否</td></tr><tr><td align="center">granularity</td><td align="center">查询的时间粒度，最细粒度为秒，最大粒度为all，提供了时间维度级别的调整并对数据进行上卷和向下钻取的能力</td><td align="center">是</td></tr><tr><td align="center">dimensionSpec</td><td align="center">提供了维度在聚合前输出展示值定制的能力，比如在Dimension age一列中，拿到的是字符串类型的数字，我希望转成数字类型，又或者定制一个javascript function，统一以 ${age} year old的形式展现</td><td align="center">否</td></tr><tr><td align="center">limit</td><td align="center">返回结果数量限制</td><td align="center">否</td></tr><tr><td align="center">context</td><td align="center">表示对当前查询本身的一些配置，比如设置查询超时的时间，又比如是否使用缓存，在通用的配置基础上，每种查询类型还有特定的配置，详见文档</td><td align="center">否</td></tr></tbody></table><h2 id="基本组件"><a href="#基本组件" class="headerlink" title="基本组件"></a><strong>基本组件</strong></h2><h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><p>过滤器，在查询语句中是一个json对象，<strong>用来对维度进行筛选</strong>，表示满足filter的是我们需要的数据。类似于SQL中的where。</p><table><thead><tr><th align="center">类型</th><th align="center">功能</th></tr></thead><tbody><tr><td align="center">SelectorFilter</td><td align="center">功能类似于SQL中的where key=value</td></tr><tr><td align="center">AndFilter, OrFilter, NotFilter</td><td align="center">功能类似于SQL中and、or、not三种过滤器。支持递归嵌套，可以构造出丰富的逻辑表达式</td></tr><tr><td align="center">RegexFilter</td><td align="center">正则表达式，支持任意维度值的java正则</td></tr><tr><td align="center">SearchFilter</td><td align="center">通过字符串匹配维度，支持多种表达式</td></tr><tr><td align="center">InFilter</td><td align="center">功能类似于SQL中where key in (value1, value2)</td></tr><tr><td align="center">IntervalFilter</td><td align="center">针对于时间维度过滤</td></tr><tr><td align="center">BoundFilter</td><td align="center">功能类似于SQL中的大于、小于、等于三种算子</td></tr><tr><td align="center">JavaScriptFilter</td><td align="center">上述filter均不能满足可以自己写JavaScript来过滤维度</td></tr></tbody></table><h3 id="aggregator"><a href="#aggregator" class="headerlink" title="aggregator"></a>aggregator</h3><p>聚合可以在采集数据时规格部分的一种方式，汇总数据进入Druid之前提供。<br>聚合也可以被指定为在查询时多查询的部分，聚合类型如下：</p><table><thead><tr><th align="center">类型</th><th align="center">功能</th></tr></thead><tbody><tr><td align="center">CountAggregator</td><td align="center">SQL count(key)</td></tr><tr><td align="center">SumAggregator</td><td align="center">SQL sum(key)</td></tr><tr><td align="center">MaxAggregator, MinAggregator</td><td align="center">SQL max(key), min(key)</td></tr><tr><td align="center">DistinctCountAggregator</td><td align="center">SQL count(distinct key)</td></tr><tr><td align="center">JavaScriptAggregator</td><td align="center">上述aggregator均不能满足可以自己写JavaScript来定义计算</td></tr></tbody></table><h3 id="post-aggregator"><a href="#post-aggregator" class="headerlink" title="post-aggregator"></a>post-aggregator</h3><table><thead><tr><th align="center">类型</th><th align="center">功能</th></tr></thead><tbody><tr><td align="center">ArithmeticPostAggregator</td><td align="center">支持对聚合后<strong>指标</strong>进行”+ - * / quotient”计算</td></tr><tr><td align="center">FieldAccessPostAggregator</td><td align="center">直接获取聚合的字段（维度，指标）</td></tr><tr><td align="center">ConstantPostAggregator</td><td align="center">返回常数</td></tr><tr><td align="center">JavaScriptPostAggregator</td><td align="center">上述postAggregator均不能满足可以自己写JavaScript来定义计算</td></tr></tbody></table><h2 id="查询类型"><a href="#查询类型" class="headerlink" title="查询类型"></a><strong>查询类型</strong></h2><h3 id="聚合查询"><a href="#聚合查询" class="headerlink" title="聚合查询"></a>聚合查询</h3><h4 id="timeseries"><a href="#timeseries" class="headerlink" title="timeseries"></a>timeseries</h4><p>时序查询，实际上即是对数据基于时间点(timestamp)的一次上卷。适合用来看某几个度量在一个时间段内的趋势。排序可按时间降序或升序</p><table><thead><tr><th align="center">字段名</th><th align="center">描述</th><th align="center">是否必须</th></tr></thead><tbody><tr><td align="center">queryType</td><td align="center">查询类型，必须为 “timeseries”</td><td align="center">是</td></tr><tr><td align="center">dataSource</td><td align="center">数据源，比如 “wikipedia”</td><td align="center">是</td></tr><tr><td align="center">descending</td><td align="center">返回结果是否逆序，默认值为否（正序）</td><td align="center">否</td></tr><tr><td align="center">intervals</td><td align="center">查询时间区间</td><td align="center">是</td></tr><tr><td align="center">granularity</td><td align="center">聚合粒度，粒度决定如何在跨时间维度得到数据块</td><td align="center">是</td></tr><tr><td align="center">filter</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">aggregations</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">postAggregations</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">limit</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">context</td><td align="center"></td><td align="center">否</td></tr></tbody></table><p>context：</p><p>1.grandTotal</p><p>2.零填充<br>如果时间范围内没有值，则会填充0<br>时间序列查询通常用零填充空的内部时间段。例如，如果您对间隔2012-01-01 / 2012-01-04发出“天”粒度时间序列查询，而2012-01-02没有数据存在，您将收到：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">&quot;timestamp&quot;</span>: <span class="string">&quot;2012-01-01T00:00:00.000Z&quot;</span>,</span><br><span class="line">    <span class="string">&quot;result&quot;</span>: &#123; <span class="string">&quot;sample_name1&quot;</span>: &lt;some_value&gt; &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">   <span class="string">&quot;timestamp&quot;</span>: <span class="string">&quot;2012-01-02T00:00:00.000Z&quot;</span>,</span><br><span class="line">   <span class="string">&quot;result&quot;</span>: &#123; <span class="string">&quot;sample_name1&quot;</span>: 0 &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">&quot;timestamp&quot;</span>: <span class="string">&quot;2012-01-03T00:00:00.000Z&quot;</span>,</span><br><span class="line">    <span class="string">&quot;result&quot;</span>: &#123; <span class="string">&quot;sample_name1&quot;</span>: &lt;some_value&gt; &#125;</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h4 id="topN"><a href="#topN" class="headerlink" title="topN"></a>topN</h4><p>在时间点的基础上，又增加了一个维度(OLAP的概念算两个维度)，进而对源数据进行切片，切片之后分别上卷，最后返回一个聚合集，你可以指定某个指标作为排序的依据。官方文档称这对比单个druid dimension 的groupBy 更高效。适合看某个维度下的时间趋势，（比如美国和中国十年内GDP的增长趋势比对，在这里除了时间外国家就是另外一个维度）</p><table><thead><tr><th align="center">字段名</th><th align="center">描述</th><th align="center">是否必须</th></tr></thead><tbody><tr><td align="center">queryType</td><td align="center">查询类型，必须为 “topN”</td><td align="center">是</td></tr><tr><td align="center">dataSource</td><td align="center">数据源，比如 “wikipedia”</td><td align="center">是</td></tr><tr><td align="center">intervals</td><td align="center">查询时间区间</td><td align="center">是</td></tr><tr><td align="center">granularity</td><td align="center">聚合粒度，粒度决定如何在跨时间维度得到数据块</td><td align="center">是</td></tr><tr><td align="center">filter</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">aggregations</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">postAggregations</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">dimension</td><td align="center">除了时间之外聚合的维度，只能定义一个维度</td><td align="center">是</td></tr><tr><td align="center">threshold</td><td align="center">topN中的N，例如：希望查询到top2，则值为2</td><td align="center">是</td></tr><tr><td align="center">metric</td><td align="center">topN中用来排序的指标</td><td align="center">是</td></tr><tr><td align="center">context</td><td align="center"></td><td align="center">否</td></tr></tbody></table><h4 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h4><p>适用于两个维度以上的查询，druid会根据维度切块，并且分别上卷，最后返回聚合集。相对于topN而言，这是一个向下钻取的操作，每多一个维度意味着保留更多的细节。(比如增加一个行业的维度，就可以知道美国和中国十年内，每一年不同行业贡献GDP的占比)<br><strong>注意：如果要使用时间作为唯一分组进行聚合，或者在单个维度上使用有序groupBy进行聚合，请优先考虑使用Timeseries和TopN查询。在某些情况下，它们的性能可能会更好。有关更多详细信息，请参见下面的替代方法。</strong></p><table><thead><tr><th align="center">字段名</th><th align="center">描述</th><th align="center">是否必须</th></tr></thead><tbody><tr><td align="center">queryType</td><td align="center">查询类型，必须为 “groupBy”</td><td align="center">是</td></tr><tr><td align="center">dataSource</td><td align="center">数据源，比如 “wikipedia”</td><td align="center">是</td></tr><tr><td align="center">dimensions</td><td align="center">需要聚合的所有维度</td><td align="center">是</td></tr><tr><td align="center">limitSpec</td><td align="center">同关系型数据库中的limit</td><td align="center">否</td></tr><tr><td align="center">having</td><td align="center">同关系型数据库中的having</td><td align="center">否</td></tr><tr><td align="center">granularity</td><td align="center">聚合粒度，粒度决定如何在跨时间维度得到数据块</td><td align="center">是</td></tr><tr><td align="center">filter</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">aggregations</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">postAggregations</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">intervals</td><td align="center">查询时间区间</td><td align="center">是</td></tr><tr><td align="center">subtotalsSpec</td><td align="center">类似于的grouping sets</td><td align="center">否</td></tr><tr><td align="center">context</td><td align="center"></td><td align="center">否</td></tr></tbody></table><h3 id="普通查询"><a href="#普通查询" class="headerlink" title="普通查询"></a>普通查询</h3><h4 id="select"><a href="#select" class="headerlink" title="select"></a>select</h4><p>类似SQL中的select操作，select用来查看Druid中存储的数据，并支持按照指定过滤器和时间查看指定维度和指标。不支持aggregations和post aggregations</p><p><strong>注意：建议您尽可能使用scan查询类型而不是select。在涉及大量segment的情况下，select查询可能具有很高的内存和性能开销，但是scan查询没有此问题。<br>两者之间的主要区别是“扫描”查询不支持分页。但是，即使没有分页，scan查询类型也能够返回几乎无限数量的结果，使得分页在许多情况下是不必要的。</strong></p><table><thead><tr><th align="center">字段名</th><th align="center">描述</th><th align="center">是否必须</th></tr></thead><tbody><tr><td align="center">queryType</td><td align="center">查询类型，必须为 “select”</td><td align="center">是</td></tr><tr><td align="center">dataSource</td><td align="center">数据源，比如 “wikipedia”</td><td align="center">是</td></tr><tr><td align="center">intervals</td><td align="center">查询时间区间</td><td align="center">是</td></tr><tr><td align="center">descending</td><td align="center">返回结果是否逆序，默认值为否（正序）</td><td align="center">否</td></tr><tr><td align="center">filter</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">dimensions</td><td align="center">需要查询的维度列表</td><td align="center">否</td></tr><tr><td align="center">metrics</td><td align="center">需要查询的指标列表</td><td align="center">否</td></tr><tr><td align="center">granularity</td><td align="center">聚合粒度，粒度决定如何在跨时间维度得到数据块，默认是Granularity.ALL</td><td align="center">否</td></tr><tr><td align="center">pagingSpec</td><td align="center">分页</td><td align="center">是</td></tr><tr><td align="center">context</td><td align="center"></td><td align="center">否</td></tr></tbody></table><h4 id="scan"><a href="#scan" class="headerlink" title="scan"></a>scan</h4><p>扫描查询以流模式返回行，Select查询和Scan查询之间的最大区别是，Scan查询子返回给客户端数据之前不会将所有行数据保留在内存中<br>而select查询将把行保留在内存中，如果返回太多行，则会导致内存压力。扫描查询可以返回所有行，而无需发出另一个分页查询。</p><p>除了将scan查询发送给server的用法外，还可以直接向historical历史记录进程或streaming ingestion流式提取任务发出扫描查询。如果要并行检索大量数据，这将很有用。</p><table><thead><tr><th align="center">字段名</th><th align="center">描述</th><th align="center">是否必须</th></tr></thead><tbody><tr><td align="center">queryType</td><td align="center">查询类型，必须为 “scan”</td><td align="center">是</td></tr><tr><td align="center">dataSource</td><td align="center">数据源，比如 “wikipedia”</td><td align="center">是</td></tr><tr><td align="center">intervals</td><td align="center">查询时间区间</td><td align="center">是</td></tr><tr><td align="center">resultFormat</td><td align="center">返回结果类型：list，compactedList或valueVector。目前仅list和compactedList受支持。默认是list</td><td align="center">否</td></tr><tr><td align="center">filter</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">columns</td><td align="center">需要scan的维度和指标，默认为所有</td><td align="center">否</td></tr><tr><td align="center">batchSize</td><td align="center">返回数据之前默认缓存最多多少行</td><td align="center">否</td></tr><tr><td align="center">limit</td><td align="center">查询返回最大的数据条目，如果不指定，则返回所有的数据</td><td align="center">否</td></tr><tr><td align="center">order</td><td align="center">返回数据的order，基于timestamp，并且只有__time被包含在columns中才生效</td><td align="center">否</td></tr><tr><td align="center">legacy</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">context</td><td align="center"></td><td align="center">否</td></tr></tbody></table><h4 id="search"><a href="#search" class="headerlink" title="search"></a>search</h4><p>类似SQL中的Like操作，但是支持更多的匹配操作</p><table><thead><tr><th align="center">字段名</th><th align="center">描述</th><th align="center">是否必须</th></tr></thead><tbody><tr><td align="center">queryType</td><td align="center">查询类型，必须为 “search”</td><td align="center">是</td></tr><tr><td align="center">dataSource</td><td align="center">数据源，比如 “wikipedia”</td><td align="center">是</td></tr><tr><td align="center">granularity</td><td align="center">聚合粒度，粒度决定如何在跨时间维度得到数据块</td><td align="center">是</td></tr><tr><td align="center">filter</td><td align="center"></td><td align="center">否</td></tr><tr><td align="center">limit</td><td align="center">每个历史进程的最大查询返回数据条目（默认是1000）</td><td align="center">否</td></tr><tr><td align="center">intervals</td><td align="center">查询时间区间</td><td align="center">是</td></tr><tr><td align="center">searchDimensions</td><td align="center">需要search的维度（默认是所有维度），key like value中的key</td><td align="center">否</td></tr><tr><td align="center">query</td><td align="center">search维度需要匹配的value，key like value中的value</td><td align="center">是</td></tr><tr><td align="center">sort</td><td align="center">指定应如何对搜索结果进行排序，包括字典编排（默认排序），字母数字，strlen和数字排序</td><td align="center">否</td></tr><tr><td align="center">context</td><td align="center"></td><td align="center">否</td></tr></tbody></table><h3 id="元数据查询"><a href="#元数据查询" class="headerlink" title="元数据查询"></a>元数据查询</h3><h4 id="time-bounding"><a href="#time-bounding" class="headerlink" title="time bounding"></a>time bounding</h4><h4 id="segment-metadata"><a href="#segment-metadata" class="headerlink" title="segment metadata"></a>segment metadata</h4><h4 id="dataSource-metadata"><a href="#dataSource-metadata" class="headerlink" title="dataSource metadata"></a>dataSource metadata</h4>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/categories/Apache-Druid/"/>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/tags/Apache-Druid/"/>
    
  </entry>
  
  <entry>
    <title>Apache Druid 学习：kafka to druid</title>
    <link href="https://yangyichao-mango.github.io/2019/10/21/apache-druid:study-kafka-to-druid/"/>
    <id>https://yangyichao-mango.github.io/2019/10/21/apache-druid:study-kafka-to-druid/</id>
    <published>2019-10-21T07:08:45.000Z</published>
    <updated>2019-10-21T07:26:54.920Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Druid 学习：kafka to druid demo</p><span id="more"></span><h3 id="Druid-Web操作"><a href="#Druid-Web操作" class="headerlink" title="Druid Web操作"></a><strong>Druid Web操作</strong></h3><p><a href="http://druid.apache.org/docs/latest/tutorials/tutorial-kafka.html">官网教程</a></p><h3 id="Java-Demo"><a href="#Java-Demo" class="headerlink" title="Java Demo"></a><strong>Java Demo</strong></h3>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/categories/Apache-Druid/"/>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/tags/Apache-Druid/"/>
    
      <category term="Apache Kafka" scheme="https://yangyichao-mango.github.io/tags/Apache-Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Mac安装Apache Druid-0.16.0-incubating</title>
    <link href="https://yangyichao-mango.github.io/2019/10/21/apache-druid:0.16.0-incubating-mac-install/"/>
    <id>https://yangyichao-mango.github.io/2019/10/21/apache-druid:0.16.0-incubating-mac-install/</id>
    <published>2019-10-21T06:21:53.000Z</published>
    <updated>2019-10-22T02:39:59.582Z</updated>
    
    <content type="html"><![CDATA[<p>Mac安装Apache Druid-0.16.0-incubating教程</p><span id="more"></span><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a><strong>安装</strong></h3><p>参考<a href="http://druid.apache.org/docs/latest/tutorials/index.html">官网教程</a></p><h4 id="brew安装"><a href="#brew安装" class="headerlink" title="brew安装"></a>brew安装</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install druid</span><br></pre></td></tr></table></figure><p>quer</p><h4 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ curl https://www-us.apache.org/dist/incubator/druid/0.16.0-incubating/apache-druid-0.16.0-incubating-bin.tar.gz</span><br><span class="line">$ tar -xzf apache-druid-0.16.0-incubating-bin.tar.gz</span><br><span class="line">$ <span class="built_in">cd</span> apache-druid-0.16.0-incubating</span><br></pre></td></tr></table></figure><h3 id="配置启动"><a href="#配置启动" class="headerlink" title="配置启动"></a><strong>配置启动</strong></h3><p>启动druid服务之前需要先启动zookeeper，下面有两种方式启动和使用zookeeper</p><h4 id="使用集成zookeeper"><a href="#使用集成zookeeper" class="headerlink" title="使用集成zookeeper"></a>使用集成zookeeper</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> <span class="variable">$DRUID_HOME</span> // 需要在~/.bash_profile中进行配置</span><br><span class="line">$ curl https://archive.apache.org/dist/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz -o zookeeper-3.4.14.tar.gz</span><br><span class="line">$ tar -xzf zookeeper-3.4.14.tar.gz</span><br><span class="line">$ mv zookeeper-3.4.14 zk</span><br></pre></td></tr></table></figure><h4 id="使用外部zookeeper"><a href="#使用外部zookeeper" class="headerlink" title="使用外部zookeeper"></a>使用外部zookeeper</h4><h5 id="修改-DRUID-HOME-conf-supervise-single-server-micro-quickstart-conf-中的配置"><a href="#修改-DRUID-HOME-conf-supervise-single-server-micro-quickstart-conf-中的配置" class="headerlink" title="修改 $DRUID_HOME/conf/supervise/single-server/micro-quickstart.conf 中的配置"></a>修改 <strong>$DRUID_HOME/conf/supervise/single-server/micro-quickstart.conf</strong> 中的配置</h5><p>将 <strong>!p10 zk bin/run-zk conf</strong> 注释掉</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ vi <span class="variable">$DRUID_HOME</span>/conf/supervise/single-server/micro-quickstart.conf</span><br><span class="line"></span><br><span class="line">:verify bin/verify-java</span><br><span class="line">:verify bin/verify-default-ports</span><br><span class="line">:kill-timeout 10</span><br><span class="line"></span><br><span class="line"><span class="comment"># !p10 zk bin/run-zk conf // 这里是运行集成zookeeper的代码，所以要注释掉</span></span><br><span class="line">coordinator-overlord bin/run-druid coordinator-overlord conf/druid/single-server/micro-quickstart</span><br><span class="line">broker bin/run-druid broker conf/druid/single-server/micro-quickstart</span><br><span class="line">router bin/run-druid router conf/druid/single-server/micro-quickstart</span><br><span class="line">historical bin/run-druid historical conf/druid/single-server/micro-quickstart</span><br><span class="line">!p90 middleManager bin/run-druid middleManager conf/druid/single-server/micro-quickstart</span><br><span class="line"></span><br><span class="line"><span class="comment"># Uncomment to use Tranquility Server</span></span><br><span class="line"><span class="comment">#!p95 tranquility-server tranquility/bin/tranquility server -configFile conf/tranquility/wikipedia-server.json -Ddruid.extensions.loadList=[]</span></span><br></pre></td></tr></table></figure><h5 id="修改-DRUID-HOME-conf-druid-single-server-micro-quickstart-common-common-runtime-properties-中的配置"><a href="#修改-DRUID-HOME-conf-druid-single-server-micro-quickstart-common-common-runtime-properties-中的配置" class="headerlink" title="修改 $DRUID_HOME/conf/druid/single-server/micro-quickstart/_common/common.runtime.properties 中的配置"></a>修改 <strong>$DRUID_HOME/conf/druid/single-server/micro-quickstart/_common/common.runtime.properties</strong> 中的配置</h5><p>修改zookeeper的client配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ vim <span class="variable">$DRUID_HOME</span>/conf/druid/single-server/micro-quickstart/_common/common.runtime.properties</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置外部zookeeper的信息</span></span><br><span class="line"><span class="comment"># zookeeper，大概在46~55行中间，对zk进行配置</span></span><br><span class="line"><span class="comment"># zookeeper的server运行在2181端口上</span></span><br><span class="line">druid.zk.service.host=127.0.0.1:2181</span><br><span class="line">druid.zk.paths.base=/druid</span><br></pre></td></tr></table></figure><h5 id="修改-DRUID-HOME-bin-verify-default-ports-中的配置"><a href="#修改-DRUID-HOME-bin-verify-default-ports-中的配置" class="headerlink" title="修改 $DRUID_HOME/bin/verify-default-ports 中的配置"></a>修改 <strong>$DRUID_HOME/bin/verify-default-ports</strong> 中的配置</h5><p>因为使用了外部zookeeper，并且外部zookeeper的ip:port为127.0.0.1:2181<br>所以需要将zookeeper的2181端口删除，否则会校验本机2181端口是否被占用，因为本机zookeeper已经将其占用，则会报错，服务不能启动<br>如果使用的zookeeper的不在本机部署，则可以不注释2181</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ vi <span class="variable">$DRUID_HOME</span>/bin/verify-default-ports</span><br><span class="line"></span><br><span class="line"><span class="comment"># my @ports = (1527, 2181, 8081, 8082, 8083, 8090, 8091, 8200, 9095);</span></span><br><span class="line"></span><br><span class="line">my @ports = (1527, 8081, 8082, 8083, 8090, 8091, 8200, 9095);</span><br></pre></td></tr></table></figure><h4 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/start-micro-quickstart</span><br></pre></td></tr></table></figure><p>可以到<a href="http://localhost:8888">http://localhost:8888</a>查看是否启动成功</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/categories/Apache-Druid/"/>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/tags/Apache-Druid/"/>
    
      <category term="Apache Zookeeper" scheme="https://yangyichao-mango.github.io/tags/Apache-Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：hbase作为sink</title>
    <link href="https://yangyichao-mango.github.io/2019/10/20/apache-flink:study-hbase-sink/"/>
    <id>https://yangyichao-mango.github.io/2019/10/20/apache-flink:study-hbase-sink/</id>
    <published>2019-10-20T06:27:49.000Z</published>
    <updated>2019-10-25T03:40:04.204Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：hbase作为sink的demo</p><span id="more"></span><h3 id="依赖项"><a href="#依赖项" class="headerlink" title="依赖项"></a><strong>依赖项</strong></h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hbase.version</span>&gt;</span>2.0.5<span class="tag">&lt;/<span class="name">hbase.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hbase.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="demo代码"><a href="#demo代码" class="headerlink" title="demo代码"></a><strong>demo代码</strong></h3><p>使用了hbase作为source，hbase作为sink</p><p><strong>运行之前需要运行hadoop集群（zookeeper集群），hbase集群</strong><br><strong>flink根据部署的集群信息（比如zookeeper的ip:port为127.0.0.1:2181等的信息）去连接hbase</strong></p><h4 id="hbase-site-xml"><a href="#hbase-site-xml" class="headerlink" title="hbase-site.xml"></a>hbase-site.xml</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--Autogenerated by Cloudera Manager--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- zk configuration --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>zookeeper.session.timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>60000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>zookeeper.znode.parent<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>127.0.0.1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.clientPort<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="HBaseReader"><a href="#HBaseReader" class="headerlink" title="HBaseReader"></a>HBaseReader</h4><p>HBase作为source</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseReader</span> <span class="keyword">extends</span> <span class="title">RichSourceFunction</span>&lt;<span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = LoggerFactory.getLogger(HBaseReader.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> HBaseClient hBaseClient;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_HBASE_SOURCE_TABLE_NAME = <span class="string">&quot;student&quot;</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_HBASE_SOURCE_START_ROW = <span class="string">&quot;row1&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_HBASE_SOURCE_STOP_ROW = <span class="string">&quot;row1&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">                    hBaseClient = <span class="keyword">new</span> HBaseClient();</span><br><span class="line">                    hBaseClient.initialize();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;String&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        List&lt;<span class="keyword">byte</span>[]&gt; results = hBaseClient.scan(</span><br><span class="line">                DEFAULT_HBASE_SOURCE_TABLE_NAME</span><br><span class="line">                , DEFAULT_HBASE_SOURCE_START_ROW</span><br><span class="line">                , DEFAULT_HBASE_SOURCE_STOP_ROW);</span><br><span class="line">        results.forEach(result -&gt; ctx.collect(<span class="keyword">new</span> String(result)));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        hBaseClient.destroy();</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                        LOGGER.error(<span class="string">&quot;&quot;</span>, e);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="HBaseWriter"><a href="#HBaseWriter" class="headerlink" title="HBaseWriter"></a>HBaseWriter</h4><p>HBase作为sink</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseWriter</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = LoggerFactory.getLogger(HBaseWriter.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> HBaseClient hBaseClient;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">                    hBaseClient = <span class="keyword">new</span> HBaseClient();</span><br><span class="line">                    hBaseClient.initialize();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(Tuple2&lt;String, Integer&gt; value, Context context)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        String result = value.toString();</span><br><span class="line">        Put put = hBaseClient.createPut(<span class="string">&quot;row2&quot;</span>);</span><br><span class="line">        hBaseClient.addValueOnPut(put, <span class="string">&quot;description&quot;</span>, <span class="string">&quot;age&quot;</span>, <span class="string">&quot;19&quot;</span>);</span><br><span class="line">        hBaseClient.put(<span class="string">&quot;student&quot;</span>, put);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.close();</span><br><span class="line">        <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (Objects.isNull(hBaseClient)) &#123;</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        hBaseClient.destroy();</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                        LOGGER.error(<span class="string">&quot;&quot;</span>, e);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="定义dag"><a href="#定义dag" class="headerlink" title="定义dag"></a>定义dag</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> SourceFunction&lt;String&gt; source;</span><br><span class="line">    <span class="keyword">final</span> ParameterTool params = ParameterTool.fromArgs(args);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/******************************* hbase source *******************************/</span></span><br><span class="line">    source = <span class="keyword">new</span> HBaseReader();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/******************************* define dag *******************************/</span></span><br><span class="line">    <span class="comment">// create the environment to create streams and configure execution</span></span><br><span class="line">    <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    <span class="comment">// make parameters available in the web interface</span></span><br><span class="line">    env.getConfig().setGlobalJobParameters(params);</span><br><span class="line">    DataStream&lt;String&gt; sentenceStream = env.addSource(source);</span><br><span class="line">    DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; wordCountStream = sentenceStream</span><br><span class="line">            .flatMap(<span class="keyword">new</span> LineSplitter())</span><br><span class="line">            .keyBy(<span class="number">0</span>)</span><br><span class="line">            .sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/******************************* hbase sink *******************************/</span></span><br><span class="line">    wordCountStream.addSink(<span class="keyword">new</span> HBaseWriter());</span><br><span class="line">    wordCountStream.print();</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">&quot;Java hbase Word Count&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="查看hbase文件"><a href="#查看hbase文件" class="headerlink" title="查看hbase文件"></a><strong>查看hbase文件</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; scan <span class="string">&#x27;student&#x27;</span></span><br><span class="line">ROW                   COLUMN+CELL</span><br><span class="line"> row1                 column=description:age, timestamp=1571460125600, value=18</span><br><span class="line"> row1                 column=description:name, timestamp=1571460129987, value=li</span><br><span class="line">                      u</span><br><span class="line"> <span class="comment"># 记录以及被写入hbase</span></span><br><span class="line"> row2                 column=description:age, timestamp=1571576517072, value=19</span><br><span class="line">2 row(s) <span class="keyword">in</span> 0.2010 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):002:0&gt;</span><br></pre></td></tr></table></figure><p>发现columnFamilyName为<strong>description</strong>，columnName为<strong>age</strong>，rowkey为<strong>row2</strong>，value为<strong>19</strong>的记录已经被写入hbase</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
      <category term="Apache HBase" scheme="https://yangyichao-mango.github.io/tags/Apache-HBase/"/>
    
  </entry>
  
  <entry>
    <title>Mac安装Apache Zookeeper-3.4.12</title>
    <link href="https://yangyichao-mango.github.io/2019/10/20/apache-zookeeper:3.4.12-mac-install/"/>
    <id>https://yangyichao-mango.github.io/2019/10/20/apache-zookeeper:3.4.12-mac-install/</id>
    <published>2019-10-20T03:23:17.000Z</published>
    <updated>2019-10-25T03:41:16.346Z</updated>
    
    <content type="html"><![CDATA[<p>Mac安装Apache Zookeeper-3.4.12教程</p><span id="more"></span><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a><strong>安装</strong></h3><h4 id="安装方式1-brew安装"><a href="#安装方式1-brew安装" class="headerlink" title="安装方式1-brew安装"></a>安装方式1-brew安装</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew install zookeeper</span><br></pre></td></tr></table></figure><h4 id="安装方式2-下载压缩包"><a href="#安装方式2-下载压缩包" class="headerlink" title="安装方式2-下载压缩包"></a>安装方式2-下载压缩包</h4><p>从此地址下载<strong><a href="http://mirrors.hust.edu.cn/apache/zookeeper/stable/">http://mirrors.hust.edu.cn/apache/zookeeper/stable/</a></strong></p><p>解压配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ tar -zxvf zookeeper-3.4.12.tar.gz // 解压</span><br><span class="line">$ <span class="built_in">cd</span> zookeeper-3.4.12/conf // 切换到配置目录下</span><br><span class="line">$ mv zoo_sample.cfg zoo.cfg // 更改默认配置文件名称</span><br><span class="line">$ vi zoo.cfg // 编辑配置文件，自定义dataDir</span><br></pre></td></tr></table></figure><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a><strong>启动</strong></h3><h4 id="启动sever端"><a href="#启动sever端" class="headerlink" title="启动sever端"></a>启动sever端</h4><p>切换到bin目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">pwd</span></span><br><span class="line">/user/<span class="built_in">local</span>/Celler/zookeeper/3.4.12/bin</span><br><span class="line"></span><br><span class="line">$ ls</span><br><span class="line">README.txtzkCli.cmdzkEnv.cmdzkServer.cmdzookeeper.out</span><br><span class="line">zkCleanup.shzkCli.shzkEnv.shzkServer.sh</span><br><span class="line"></span><br><span class="line">$ ./zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /user/<span class="built_in">local</span>/Celler/zookeeper/3.4.12/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure><h4 id="启动client端"><a href="#启动client端" class="headerlink" title="启动client端"></a>启动client端</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">./zkCli.sh -server 127.0.0.1:2181</span><br><span class="line">Connecting to 127.0.0.1:2181</span><br><span class="line">2019-10-20 12:17:25,861 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.12-e5259e437540f349646870ea94dc2658c4e44b3b, built on 03/27/2018 03:55 GMT</span><br><span class="line">2019-10-20 12:17:25,864 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=localhost</span><br><span class="line">2019-10-20 12:17:25,864 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.8.0_191</span><br><span class="line">2019-10-20 12:17:25,866 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation</span><br><span class="line">2019-10-20 12:17:25,866 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_191.jdk/Contents/Home/jre</span><br><span class="line">2019-10-20 12:17:25,868 [myid:] - INFO  [main:ZooKeeper@441] - Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain<span class="variable">$MyWatcher</span>@799f7e29</span><br><span class="line">Welcome to ZooKeeper!</span><br><span class="line">2019-10-20 12:17:25,896 [myid:] - INFO  [main-SendThread(127.0.0.1:2181):ClientCnxn<span class="variable">$SendThread</span>@1028] - Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)</span><br><span class="line">JLine support is enabled</span><br><span class="line">2019-10-20 12:17:25,959 [myid:] - INFO  [main-SendThread(127.0.0.1:2181):ClientCnxn<span class="variable">$SendThread</span>@878] - Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session</span><br><span class="line">2019-10-20 12:17:25,966 [myid:] - INFO  [main-SendThread(127.0.0.1:2181):ClientCnxn<span class="variable">$SendThread</span>@1302] - Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x10000112e160008, negotiated timeout = 30000</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected <span class="built_in">type</span>:None path:null</span><br><span class="line">[zk: 127.0.0.1:2181(CONNECTED) 0] ls /</span><br><span class="line">[zookeeper, hbase]</span><br></pre></td></tr></table></figure><h4 id="停止server端"><a href="#停止server端" class="headerlink" title="停止server端"></a>停止server端</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; ./zkServer.sh stop //停止后，如果CLi没有关闭，将报错</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: zookeeper-3.4.12/bin/../conf/zoo.cfg</span><br><span class="line">Stopping zookeeper ... STOPPED</span><br></pre></td></tr></table></figure><h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a><strong>配置文件</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The number of milliseconds of each tick</span></span><br><span class="line">tickTime=2000</span><br><span class="line"><span class="comment"># The number of ticks that the initial</span></span><br><span class="line"><span class="comment"># synchronization phase can take</span></span><br><span class="line">initLimit=10</span><br><span class="line"><span class="comment"># The number of ticks that can pass between</span></span><br><span class="line"><span class="comment"># sending a request and getting an acknowledgement</span></span><br><span class="line">syncLimit=5</span><br><span class="line"><span class="comment"># the directory where the snapshot is stored.</span></span><br><span class="line"><span class="comment"># do not use /tmp for storage, /tmp here is just</span></span><br><span class="line"><span class="comment"># example sakes. </span></span><br><span class="line"><span class="comment"># 内存数据快照的保存目录；如果没有自定义Log也使用该目录</span></span><br><span class="line">dataDir=/tmp/zookeeper</span><br><span class="line"><span class="comment"># the port at which the clients will connect</span></span><br><span class="line"><span class="comment"># zookeeper服务端的端口，客户端启动时需要连接的端口</span></span><br><span class="line">clientPort=2181</span><br><span class="line"><span class="comment"># the maximum number of client connections.</span></span><br><span class="line"><span class="comment"># increase this if you need to handle more clients</span></span><br><span class="line"><span class="comment">#maxClientCnxns=60</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Be sure to read the maintenance section of the</span></span><br><span class="line"><span class="comment"># administrator guide before turning on autopurge.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The number of snapshots to retain in dataDir</span></span><br><span class="line"><span class="comment">#autopurge.snapRetainCount=3</span></span><br><span class="line"><span class="comment"># Purge task interval in hours</span></span><br><span class="line"><span class="comment"># Set to &quot;0&quot; to disable auto purge feature</span></span><br><span class="line"><span class="comment">#autopurge.purgeInterval=1</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Zookeeper" scheme="https://yangyichao-mango.github.io/categories/Apache-Zookeeper/"/>
    
    
      <category term="Apache Zookeeper" scheme="https://yangyichao-mango.github.io/tags/Apache-Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 学习：hdfs作为sink</title>
    <link href="https://yangyichao-mango.github.io/2019/10/19/apache-flink:study-hdfs-sink/"/>
    <id>https://yangyichao-mango.github.io/2019/10/19/apache-flink:study-hdfs-sink/</id>
    <published>2019-10-19T11:33:38.000Z</published>
    <updated>2019-10-25T03:40:04.196Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Flink 学习：hdfs作为source和sink的demo</p><span id="more"></span><h3 id="依赖项"><a href="#依赖项" class="headerlink" title="依赖项"></a><strong>依赖项</strong></h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">flink.version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">flink.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-filesystem_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span> </span><br></pre></td></tr></table></figure><h3 id="demo代码"><a href="#demo代码" class="headerlink" title="demo代码"></a><strong>demo代码</strong></h3><p>使用了kafka作为source，hdfs作为sink<br><strong>运行之前需要运行kafka集群，hadoop集群（zookeeper集群）</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/******************************* define dag *******************************/</span></span><br><span class="line"><span class="comment">// create the environment to create streams and configure execution</span></span><br><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// make parameters available in the web interface</span></span><br><span class="line">env.getConfig().setGlobalJobParameters(params);</span><br><span class="line">DataStream&lt;String&gt; sentenceStream = env.addSource(source);</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; wordCountStream = sentenceStream</span><br><span class="line">        .flatMap(<span class="keyword">new</span> LineSplitter())</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        .sum(<span class="number">1</span>);</span><br><span class="line">wordCountStream.print();</span><br><span class="line"></span><br><span class="line">DataStream&lt;String&gt; kafkaSinkStream = wordCountStream</span><br><span class="line">        .map(<span class="keyword">new</span> WordBuilder());</span><br><span class="line"></span><br><span class="line"><span class="comment">/******************************* hdfs sink *******************************/</span></span><br><span class="line"></span><br><span class="line">BucketingSink&lt;String&gt; bucketingSink = <span class="keyword">new</span> BucketingSink&lt;&gt;(<span class="string">&quot;/user/xxx/flink/from-kafka&quot;</span>); <span class="comment">//hdfs上的路径</span></span><br><span class="line">bucketingSink.setWriter(<span class="keyword">new</span> StringWriter&lt;&gt;())</span><br><span class="line">        .setBatchSize(<span class="number">1024</span> * <span class="number">1024L</span>)</span><br><span class="line">        .setBatchRolloverInterval(<span class="number">2000</span>)</span><br><span class="line">        .setInactiveBucketThreshold(<span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">kafkaSinkStream.addSink(bucketingSink);</span><br></pre></td></tr></table></figure><p>上面例子将创建一个 Sink，写入遵循下面格式的分桶文件中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/base/path/&#123;date-time&#125;/_part-&#123;parallel-task&#125;-&#123;count&#125;</span><br></pre></td></tr></table></figure><p><strong>date-time</strong>：<br>是从setBucketer()自定义的日期/时间格式的字符串，如果不进行设置，默认Bucketer是DateTimeBucketer，默认值是yyyy-MM-dd–HH（DateTimeBucketer.DEFAULT_FORMAT_STRING）</p><p><strong>_part-{parallel-task}-{count}：</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_VALID_PREFIX = <span class="string">&quot;_&quot;</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_PART_PREFIX = <span class="string">&quot;part&quot;</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_PENDING_SUFFIX = <span class="string">&quot;.pending&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">openNewPartFile</span><span class="params">(Path bucketPath, BucketState&lt;T&gt; bucketState)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Path partPath = assemblePartPath(bucketPath, subtaskIndex, bucketState.partCounter);</span><br><span class="line">    Path inProgressPath = getInProgressPathFor(partPath);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> Path <span class="title">assemblePartPath</span><span class="params">(Path bucket, <span class="keyword">int</span> subtaskIndex, <span class="keyword">int</span> partIndex)</span> </span>&#123;</span><br><span class="line">    String localPartSuffix = partSuffix != <span class="keyword">null</span> ? partSuffix : <span class="string">&quot;&quot;</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Path(bucket, String.format(<span class="string">&quot;%s-%s-%s%s&quot;</span>, partPrefix, subtaskIndex, partIndex, localPartSuffix));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> Path <span class="title">getInProgressPathFor</span><span class="params">(Path path)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Path(path.getParent(), inProgressPrefix + path.getName()).suffix(inProgressSuffix);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="查看hdfs文件"><a href="#查看hdfs文件" class="headerlink" title="查看hdfs文件"></a><strong>查看hdfs文件</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ ./hdfs dfs -ls /user/xxx/flink/from-kafka/2019-10-19--19</span><br><span class="line">2019-10-19 20:08:31,244 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">Found 15 items</span><br><span class="line">-rw-r--r--   1 xxx supergroup          4 2019-10-19 19:45 /user/xxx/flink/from-kafka/2019-10-19--19/_part-0-0.pending</span><br><span class="line">-rw-r--r--   1 xxx supergroup          2 2019-10-19 19:45 /user/xxx/flink/from-kafka/2019-10-19--19/_part-1-0.pending</span><br><span class="line">-rw-r--r--   1 xxx supergroup          5 2019-10-19 19:44 /user/xxx/flink/from-kafka/2019-10-19--19/_part-2-0.pending</span><br><span class="line">-rw-r--r--   1 xxx supergroup         11 2019-10-19 19:45 /user/xxx/flink/from-kafka/2019-10-19--19/_part-2-1.pending</span><br><span class="line">-rw-r--r--   1 xxx supergroup         10 2019-10-19 19:44 /user/xxx/flink/from-kafka/2019-10-19--19/_part-3-0.pending</span><br><span class="line">-rw-r--r--   1 xxx supergroup         13 2019-10-19 19:45 /user/xxx/flink/from-kafka/2019-10-19--19/_part-3-1.pending</span><br><span class="line">-rw-r--r--   1 xxx supergroup          9 2019-10-19 19:45 /user/xxx/flink/from-kafka/2019-10-19--19/_part-4-0.pending</span><br><span class="line">-rw-r--r--   1 xxx supergroup         10 2019-10-19 19:44 /user/xxx/flink/from-kafka/2019-10-19--19/_part-5-0.pending</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/tags/Apache-Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Mac安装Apache HBase-1.3.5</title>
    <link href="https://yangyichao-mango.github.io/2019/10/18/apache-hbase:1.3.5-mac-install/"/>
    <id>https://yangyichao-mango.github.io/2019/10/18/apache-hbase:1.3.5-mac-install/</id>
    <published>2019-10-18T15:14:59.000Z</published>
    <updated>2019-10-25T03:40:04.199Z</updated>
    
    <content type="html"><![CDATA[<p>Mac安装Apache HBase-1.3.5教程</p><span id="more"></span><h3 id="HBase安装"><a href="#HBase安装" class="headerlink" title="HBase安装"></a><strong>HBase安装</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew install hbase</span><br></pre></td></tr></table></figure><p>安装在<font color=red><strong>/usr/local/Cellar/hbase/1.3.5</strong></font></p><h3 id="HBase配置"><a href="#HBase配置" class="headerlink" title="HBase配置"></a><strong>HBase配置</strong></h3><h4 id="hbase-env-sh"><a href="#hbase-env-sh" class="headerlink" title="hbase-env.sh"></a>hbase-env.sh</h4><p>在<font color=red>conf/hbase-env.sh设置JAVA_HOME</font></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/Cellar/hbase/1.3.5/libexec/conf</span><br><span class="line">$ vim hbase-env.sh</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=<span class="string">&quot;<span class="subst">$(/usr/libexec/java_home --version 1.8)</span>&quot;</span></span><br></pre></td></tr></table></figure><p>Apache HBase-1.3.5中JAVA_HOME已经默认被配置好了<br>如果JAVA_HOME没有配置好，则需要设置JAVA_HOME，可以通过下面的命令查看JAVA_HOME</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/libexec/java_home</span><br><span class="line">/Library/Java/JavaVirtualMachines/jdk1.8.0_191.jdk/Contents/Home</span><br></pre></td></tr></table></figure><h4 id="hbase-site-xml"><a href="#hbase-site-xml" class="headerlink" title="hbase-site.xml"></a>hbase-site.xml</h4><p>在<font color=red>conf/hbase-site.xml</font>设置HBase的核心配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ vim hbase-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">    // 这里设置让HBase存储文件的地方</span><br><span class="line">    &lt;value&gt;file:///usr/<span class="built_in">local</span>/Cellar/hbase/tmp/hbase&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</span><br><span class="line">      // 这里设置让HBase存储内建zookeeper文件的地方</span><br><span class="line">      &lt;value&gt;/usr/<span class="built_in">local</span>/Cellar/hbase/tmp/zookeeper&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="启动HBase"><a href="#启动HBase" class="headerlink" title="启动HBase"></a>启动HBase</h4><p><font color=red>/usr/local/Cellar/hbase/1.3.5/bin/start-hbase.sh</font>提供HBase的启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ./start-hbase.sh</span><br><span class="line">starting master, logging to /usr/<span class="built_in">local</span>/var/<span class="built_in">log</span>/hbase/hbase-xxx-master-xxx.local.out</span><br></pre></td></tr></table></figure><h4 id="验证是否安装成功"><a href="#验证是否安装成功" class="headerlink" title="验证是否安装成功"></a>验证是否安装成功</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ jps</span><br><span class="line">722 Launcher</span><br><span class="line">1142 HMaster</span><br><span class="line">726 Launcher</span><br><span class="line">1256 Jps</span><br></pre></td></tr></table></figure><h4 id="启动HBase-Shell"><a href="#启动HBase-Shell" class="headerlink" title="启动HBase Shell"></a>启动HBase Shell</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ ./hbase shell</span><br><span class="line">2019-10-19 11:58:34,879 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">HBase Shell; enter <span class="string">&#x27;help&lt;RETURN&gt;&#x27;</span> <span class="keyword">for</span> list of supported commands.</span><br><span class="line">Type <span class="string">&quot;exit&lt;RETURN&gt;&quot;</span> to leave the HBase Shell</span><br><span class="line">Version 1.3.5, rb59afe7b1dc650ff3a86034477b563734e8799a9, Wed Jun  5 15:57:14 PDT 2019</span><br><span class="line"></span><br><span class="line">hbase(main):001:0&gt;</span><br></pre></td></tr></table></figure><h4 id="停止HBase运行"><a href="#停止HBase运行" class="headerlink" title="停止HBase运行"></a>停止HBase运行</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ./stop-hbase.sh</span><br><span class="line">stopping hbase...............</span><br></pre></td></tr></table></figure><h3 id="伪分布式模式"><a href="#伪分布式模式" class="headerlink" title="伪分布式模式"></a><strong>伪分布式模式</strong></h3><p>必须先关闭HBase</p><h4 id="修改hbase-env-sh"><a href="#修改hbase-env-sh" class="headerlink" title="修改hbase-env.sh"></a>修改hbase-env.sh</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HBASE_MANAGE_ZK = <span class="literal">true</span></span><br></pre></td></tr></table></figure><h4 id="修改hbase-site-xml"><a href="#修改hbase-site-xml" class="headerlink" title="修改hbase-site.xml"></a>修改hbase-site.xml</h4><p>设置HBase使用分布式模式运行</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    // Here you have to set the path where you want HBase to store its files.</span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><font color=red><strong>hbase.rootdir路径一定要跟hadoop中core-site.xml中fs.default.name相同</strong></font></p><p>change the hbase.rootdir from the local filesystem to the address of your HDFS instance —offical quick start</p><p>如何两处设置不同会引起ERROR: <font color=red>Can’t get master address from ZooKeeper; znode data == null错误错误</font></p><p>在启动HBase之前, 请先启动Hadoop, 使之运行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ ./start-hbase.sh</span><br><span class="line">localhost: starting zookeeper, logging to /usr/<span class="built_in">local</span>/var/<span class="built_in">log</span>/hbase/hbase-xxx-zookeeper-xxx.local.out</span><br><span class="line">starting master, logging to /usr/<span class="built_in">local</span>/var/<span class="built_in">log</span>/hbase/hbase-xxx-master-xxx.local.out</span><br><span class="line">starting regionserver, logging to /usr/<span class="built_in">local</span>/var/<span class="built_in">log</span>/hbase/hbase-xxx-1-regionserver-xxx.local.out</span><br><span class="line"></span><br><span class="line">$ jps  <span class="comment">#验证是否启动成功, 包含HMaster和HRegionServer说明启动成功</span></span><br><span class="line">5614 HRegionServer</span><br><span class="line">2222 NameNode</span><br><span class="line">722 Launcher</span><br><span class="line">2323 DataNode</span><br><span class="line">5461 HMaster</span><br><span class="line">726 Launcher</span><br><span class="line">2650 ResourceManager</span><br><span class="line">2747 NodeManager</span><br><span class="line">2459 SecondaryNameNode</span><br><span class="line">5405 HQuorumPeer</span><br><span class="line">285</span><br><span class="line">5726 Jps</span><br></pre></td></tr></table></figure><p>查看hdfs中文件夹</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ./hdfs dfs -ls /</span><br><span class="line">2019-10-19 12:39:03,895 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">Found 3 items</span><br><span class="line">drwxr-xr-x   - xxx supergroup          0 2019-10-19 12:38 /hbase</span><br><span class="line">drwxrwxr-x   - xxx supergroup          0 2019-10-17 14:41 /tmp</span><br><span class="line">drwxr-xr-x   - xxx supergroup          0 2019-10-17 11:44 /user</span><br></pre></td></tr></table></figure><h3 id="HBase-Shell"><a href="#HBase-Shell" class="headerlink" title="HBase Shell"></a><strong>HBase Shell</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$ hbase shell  <span class="comment">#启动HBase Shell</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建表</span></span><br><span class="line">&gt; create <span class="string">&#x27;student&#x27;</span>, <span class="string">&#x27;description&#x27;</span>, <span class="string">&#x27;course&#x27;</span>  <span class="comment">#创建表名为student的表, 指明两个列名, 分别为description和course</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 信息明细</span></span><br><span class="line">&gt; list <span class="string">&#x27;student&#x27;</span>  <span class="comment">#列出list表信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 插入数据</span></span><br><span class="line"><span class="comment"># 意思为在student表row1处插入description:age的数据为18</span></span><br><span class="line"><span class="comment"># rowKey为row1，columnFamilyName为description，columnName为age</span></span><br><span class="line">&gt; put <span class="string">&#x27;student&#x27;</span>, <span class="string">&#x27;row1&#x27;</span>, <span class="string">&#x27;description:age&#x27;</span>, <span class="string">&#x27;18&#x27;</span></span><br><span class="line">&gt; put <span class="string">&#x27;student&#x27;</span>, <span class="string">&#x27;row1&#x27;</span>, <span class="string">&#x27;description:name&#x27;</span>, <span class="string">&#x27;liu&#x27;</span></span><br><span class="line">&gt; put <span class="string">&#x27;student&#x27;</span>, <span class="string">&#x27;row1&#x27;</span>, <span class="string">&#x27;course:chinese&#x27;</span>, <span class="string">&#x27;100&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一次扫描所有数据</span></span><br><span class="line">&gt; scan <span class="string">&#x27;student&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使表失效 / 有效</span></span><br><span class="line">&gt; <span class="built_in">disable</span> <span class="string">&#x27;student&#x27;</span></span><br><span class="line">&gt; <span class="built_in">enable</span> <span class="string">&#x27;student&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除表(要先disable)</span></span><br><span class="line">&gt;  drop <span class="string">&#x27;student&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 退出shell</span></span><br><span class="line">&gt; quit</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache HBase" scheme="https://yangyichao-mango.github.io/categories/Apache-HBase/"/>
    
    
      <category term="Apache HBase" scheme="https://yangyichao-mango.github.io/tags/Apache-HBase/"/>
    
      <category term="Mac安装" scheme="https://yangyichao-mango.github.io/tags/Mac%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>IntelliJ IDEA 中如何查看一个类的所有继承关系</title>
    <link href="https://yangyichao-mango.github.io/2019/10/18/idea-mac:show-class-hierarchy/"/>
    <id>https://yangyichao-mango.github.io/2019/10/18/idea-mac:show-class-hierarchy/</id>
    <published>2019-10-18T06:09:17.000Z</published>
    <updated>2019-10-25T08:40:51.878Z</updated>
    
    <content type="html"><![CDATA[<p>IntelliJ IDEA 中如何查看一个类的所有继承关系，包括父类与子类</p><span id="more"></span><h3 id="查看方式"><a href="#查看方式" class="headerlink" title="查看方式"></a><strong>查看方式</strong></h3><p>IntelliJ IDEA 中最上端的Navigate，下拉选择Type Hierarchy，就会出现层级关系列表</p><h4 id="关于该类的父类和子类继承关系"><a href="#关于该类的父类和子类继承关系" class="headerlink" title="关于该类的父类和子类继承关系"></a>关于该类的父类和子类继承关系</h4><p><img src="/blog-img/idea-mac:show-class-hierarchy/%E7%88%B6%E7%B1%BB%E5%92%8C%E5%AD%90%E7%B1%BB%E7%BB%A7%E6%89%BF%E5%85%B3%E7%B3%BB.png" alt="父类和子类继承关系"></p><h4 id="关于该类的父类继承关系"><a href="#关于该类的父类继承关系" class="headerlink" title="关于该类的父类继承关系"></a>关于该类的父类继承关系</h4><p><img src="/blog-img/idea-mac:show-class-hierarchy/%E7%88%B6%E7%B1%BB%E7%BB%A7%E6%89%BF%E5%85%B3%E7%B3%BB.png" alt="父类继承关系"></p><h4 id="关于该类的子类继承关系"><a href="#关于该类的子类继承关系" class="headerlink" title="关于该类的子类继承关系"></a>关于该类的子类继承关系</h4><p><img src="/blog-img/idea-mac:show-class-hierarchy/%E5%AD%90%E7%B1%BB%E7%BB%A7%E6%89%BF%E5%85%B3%E7%B3%BB.png" alt="子类继承关系"></p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="IntelliJ IDEA" scheme="https://yangyichao-mango.github.io/categories/IntelliJ-IDEA/"/>
    
    
      <category term="IntelliJ IDEA" scheme="https://yangyichao-mango.github.io/tags/IntelliJ-IDEA/"/>
    
  </entry>
  
  <entry>
    <title>Apache Hive环境搭建错误：java.lang.IllegalArgumentException: java.net.URISyntaxException:...</title>
    <link href="https://yangyichao-mango.github.io/2019/10/17/apache-hive:error-URISyntaxException:Relative-path-in-absolute-URI:%7Bsystem:java.io.tmpdir%7D%7Bsystem-user-name%7D/"/>
    <id>https://yangyichao-mango.github.io/2019/10/17/apache-hive:error-URISyntaxException:Relative-path-in-absolute-URI:%7Bsystem:java.io.tmpdir%7D%7Bsystem-user-name%7D/</id>
    <published>2019-10-17T07:44:01.000Z</published>
    <updated>2019-10-25T03:40:04.210Z</updated>
    
    <content type="html"><![CDATA[<p>出现错误：Exception in thread “main” java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D</p><span id="more"></span><h3 id="错误场景详情"><a href="#错误场景详情" class="headerlink" title="错误场景详情"></a><strong>错误场景详情</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HIVE_HOME</span>/bin/hive</span><br><span class="line">Hive Session ID = 41e2ad09-81b3-4700-9b87-f42b25a29731</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration <span class="keyword">in</span> jar:file:/usr/<span class="built_in">local</span>/Cellar/hive/3.1.2/libexec/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: <span class="literal">true</span></span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">&quot;main&quot;</span> java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path <span class="keyword">in</span> absolute URI: <span class="variable">$&#123;system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D</span></span><br><span class="line"><span class="variable">at org.apache.hadoop.fs.Path.initialize(Path.java:263)</span></span><br><span class="line"><span class="variable">at org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:221)</span></span><br><span class="line"><span class="variable">at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:710)</span></span><br><span class="line"><span class="variable">at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:627)</span></span><br><span class="line"><span class="variable">at org.apache.hadoop.hive.ql.session.SessionState.beginStart(SessionState.java:591)</span></span><br><span class="line"><span class="variable">at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:747)</span></span><br><span class="line"><span class="variable">at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)</span></span><br><span class="line"><span class="variable">at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span></span><br><span class="line"><span class="variable">at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span></span><br><span class="line"><span class="variable">at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span></span><br><span class="line"><span class="variable">at java.lang.reflect.Method.invoke(Method.java:498)</span></span><br><span class="line"><span class="variable">at org.apache.hadoop.util.RunJar.run(RunJar.java:323)</span></span><br><span class="line"><span class="variable">at org.apache.hadoop.util.RunJar.main(RunJar.java:236)</span></span><br><span class="line"><span class="variable">Caused by: java.net.URISyntaxException: Relative path in absolute URI: <span class="variable">$&#123;system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D</span></span></span><br><span class="line"><span class="variable"><span class="variable">at java.net.URI.checkPath(URI.java:1823)</span></span></span><br><span class="line"><span class="variable"><span class="variable">at java.net.URI.&lt;init&gt;(URI.java:745)</span></span></span><br><span class="line"><span class="variable"><span class="variable">at org.apache.hadoop.fs.Path.initialize(Path.java:260)</span></span></span><br><span class="line"><span class="variable"><span class="variable">... 12 more</span></span></span><br></pre></td></tr></table></figure><h3 id="错误原因"><a href="#错误原因" class="headerlink" title="错误原因"></a><strong>错误原因</strong></h3><p>hive-site.xml里的临时目录没有设置好，一共有三个</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>Hive.exec.local.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;system:Java.io.tmpdir&#125;/$&#123;system:user.name&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Local scratch space for Hive jobs<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.downloaded.resources.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;system:java.io.tmpdir&#125;/$&#123;hive.session.id&#125;_resources<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Temporary local directory for added resources in the remote file system.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.logging.operation.log.location<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;system:Java.io.tmpdir&#125;/$&#123;system:user.name&#125;/operation_logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Top level directory where operation logs are stored if logging functionality is enabled<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a><strong>解决方案</strong></h3><p>将hive-site.xml文件中的${system:java.io.tmpdir}替换为hive的临时目录<br>例如我替换为<font color=red>/usr/local/Cellar/hive/tmp</font>，该目录如果不存在则要自己手工创建，并且赋予读写权限</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>Hive.exec.local.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/Cellar/hive/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Local scratch space for Hive jobs<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.downloaded.resources.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/Cellar/hive/tmp/$&#123;hive.session.id&#125;_resources<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Temporary local directory for added resources in the remote file system.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.logging.operation.log.location<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/Cellar/hive/tmp/root/operation_logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Top level directory where operation logs are stored if logging functionality is enabled<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/categories/Apache-Hive/"/>
    
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/tags/Apache-Hive/"/>
    
  </entry>
  
  <entry>
    <title>Mac安装Apache Hive-3.2.1</title>
    <link href="https://yangyichao-mango.github.io/2019/10/17/apache-hive:3.1.2-mac-install/"/>
    <id>https://yangyichao-mango.github.io/2019/10/17/apache-hive:3.1.2-mac-install/</id>
    <published>2019-10-17T06:58:34.000Z</published>
    <updated>2019-10-25T03:40:04.207Z</updated>
    
    <content type="html"><![CDATA[<p>Mac安装Apache Hive-3.2.1教程</p><span id="more"></span><h2 id="安装Hadoop"><a href="#安装Hadoop" class="headerlink" title="安装Hadoop"></a><strong>安装Hadoop</strong></h2><p>下载包进行安装，则hadoop需要独立安装</p><h2 id="安装Hive"><a href="#安装Hive" class="headerlink" title="安装Hive"></a><strong>安装Hive</strong></h2><h3 id="brew安装"><a href="#brew安装" class="headerlink" title="brew安装"></a>brew安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew install hive</span><br></pre></td></tr></table></figure><p>此命令会把hive依赖的hadoop安装，所以就不需要单独进行安装hadoop<br>该命令默认安装的版本较新，我的hive是3.1.2，hadoop是3.2.1，安装位置：<font color=red>/usr/local/Cellar/hive/</font></p><h3 id="环境变量修改"><a href="#环境变量修改" class="headerlink" title="环境变量修改"></a>环境变量修改</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ vim ~/.bash_profile</span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/usr/<span class="built_in">local</span>/Cellar/hive/3.1.2</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="string">&quot;<span class="variable">$HIVE_HOME</span>/bin:<span class="variable">$PATH</span>&quot;</span></span><br><span class="line"></span><br><span class="line">$ <span class="built_in">source</span> ~/.bash_profile</span><br></pre></td></tr></table></figure><h3 id="使用mysql作为hive元数据存储"><a href="#使用mysql作为hive元数据存储" class="headerlink" title="使用mysql作为hive元数据存储"></a>使用mysql作为hive元数据存储</h3><p>在mysql中为hive 创建用户，及初始化数据库<br>以下在mysql 中操作，注意：这里创建的用户名是 hadoop， 密码 mysql<br>第一行：创建数据库<br>第二、三行 创建用户，赋予权限<br>第四行 权限生效</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create database hive;</span><br><span class="line">CREATE USER  <span class="string">&#x27;hadoop&#x27;</span>@<span class="string">&#x27;%&#x27;</span>  IDENTIFIED BY <span class="string">&#x27;mysql&#x27;</span>;</span><br><span class="line">GRANT ALL PRIVILEGES ON  *.* TO <span class="string">&#x27;hadoop&#x27;</span>@<span class="string">&#x27;%&#x27;</span> WITH GRANT OPTION;</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure><p>查看权限是否已经存储</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM mysql.user;</span><br></pre></td></tr></table></figure><h3 id="修改配置文件hive-site-xml"><a href="#修改配置文件hive-site-xml" class="headerlink" title="修改配置文件hive-site.xml"></a>修改配置文件hive-site.xml</h3><p>修改hive配置文件，我的配置文件位置在 <font color=red>/usr/local/Cellar/hive/3.1.2/libexec/conf</font><br>如果不存在hive-site.xml文件，则使用下面这个命令创建一个默认的hive-site.xml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cp hive-default.xml.template hive-site.xml</span><br></pre></td></tr></table></figure><p>修改配置文件</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span>mysql</span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://localhost:3306/hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>javax.jdo.option.ConnectionUserName – 连接mysql的账号，hadoop<br>javax.jdo.option.ConnectionPassword – 连接mysql的密码，mysql<br>javax.jdo.option.ConnectionURL – 对应上一步创建的数据库，localhost:3306/hive</p><h3 id="hadoop中创建hive所需仓库"><a href="#hadoop中创建hive所需仓库" class="headerlink" title="hadoop中创建hive所需仓库"></a>hadoop中创建hive所需仓库</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -mkdir       /tmp</span><br><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -mkdir   -p  /user/hive/warehouse</span><br><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -chmod g+w   /tmp</span><br><span class="line">$ <span class="variable">$HADOOP_HOME</span>/bin/hadoop fs -chmod g+w   /user/hive/warehouse</span><br></pre></td></tr></table></figure><p>$HADOOP_HOME – 代表您的hadoop工作目录</p><h3 id="hive初始化mysql中的数据库hive"><a href="#hive初始化mysql中的数据库hive" class="headerlink" title="hive初始化mysql中的数据库hive"></a>hive初始化mysql中的数据库hive</h3><h4 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HIVE_HOME</span>/bin/schematool -dbType msyql -initSchema</span><br></pre></td></tr></table></figure><h4 id="可能出现错误1：java-lang-NoSuchMethodError-com-google-common…"><a href="#可能出现错误1：java-lang-NoSuchMethodError-com-google-common…" class="headerlink" title="可能出现错误1：java.lang.NoSuchMethodError: com.google.common…"></a>可能出现错误1：java.lang.NoSuchMethodError: com.google.common…</h4><p>解决方案：<a href="https://yangyichao-mango.github.io/2019/10/17/apache-hive-error-NoSuchMethodError:com.google.common.base.Preconditions.checkArgument/">java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument</a></p><h4 id="可能出现错误2：java-lang-ClassNotFoundException-com-mysql…"><a href="#可能出现错误2：java-lang-ClassNotFoundException-com-mysql…" class="headerlink" title="可能出现错误2：java.lang.ClassNotFoundException: com.mysql…"></a>可能出现错误2：java.lang.ClassNotFoundException: com.mysql…</h4><p>解决方案：<a href="https://yangyichao-mango.github.io/2019/10/17/apache-hive-error-ClassNotFoundException:com.mysql.jdbc.driver/">java.lang.ClassNotFoundException: com.mysql.jdbc.Driver</a></p><h3 id="启动Hive的Metastore-Server服务进程"><a href="#启动Hive的Metastore-Server服务进程" class="headerlink" title="启动Hive的Metastore Server服务进程"></a>启动Hive的Metastore Server服务进程</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HIVE_HOME</span>/bin/hive --service metastore &amp;</span><br></pre></td></tr></table></figure><h3 id="登录hive客户端"><a href="#登录hive客户端" class="headerlink" title="登录hive客户端"></a>登录hive客户端</h3><h4 id="命令-1"><a href="#命令-1" class="headerlink" title="命令"></a>命令</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HIVE_HOME</span>/bin/ hive </span><br></pre></td></tr></table></figure><h4 id="可能出现的错误1：java-lang-IllegalArgumentException-java-net-URISyntaxException-…"><a href="#可能出现的错误1：java-lang-IllegalArgumentException-java-net-URISyntaxException-…" class="headerlink" title="可能出现的错误1：java.lang.IllegalArgumentException: java.net.URISyntaxException:…"></a>可能出现的错误1：java.lang.IllegalArgumentException: java.net.URISyntaxException:…</h4><p>解决方案：<a href="https://yangyichao-mango.github.io/2019/10/17/apache-hive-error-URISyntaxException:Relative-path-in-absolute-URI:%7Bsystem:java.io.tmpdir%7D%7Bsystem-user-name%7D/">Exception in thread “main” java.lang.IllegalArgumentException: java.net.URISyntaxException:</a></p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/categories/Apache-Hive/"/>
    
    
      <category term="Mac安装" scheme="https://yangyichao-mango.github.io/tags/Mac%E5%AE%89%E8%A3%85/"/>
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/tags/Apache-Hive/"/>
    
  </entry>
  
  <entry>
    <title>Apache Hive环境搭建错误：com.mysql.jdbc.Driver was not found in the CLASSPATH</title>
    <link href="https://yangyichao-mango.github.io/2019/10/17/apache-hive:error-ClassNotFoundException:com.mysql.jdbc.driver/"/>
    <id>https://yangyichao-mango.github.io/2019/10/17/apache-hive:error-ClassNotFoundException:com.mysql.jdbc.driver/</id>
    <published>2019-10-17T06:35:11.000Z</published>
    <updated>2019-10-19T04:52:31.852Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Hive环境搭建错误：com.mysql.jdbc.Driver was not found in the CLASSPATH</p><span id="more"></span><h3 id="错误场景详情"><a href="#错误场景详情" class="headerlink" title="错误场景详情"></a><strong>错误场景详情</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HIVE_HOME</span>/bin/schematool -dbType mysql -initSchema</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/usr/<span class="built_in">local</span>/Cellar/hive/3.1.2/libexec/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/usr/<span class="built_in">local</span>/Cellar/hadoop/3.2.1/libexec/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html<span class="comment">#multiple_bindings for an explanation.</span></span><br><span class="line">SLF4J: Actual binding is of <span class="built_in">type</span> [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Metastore connection URL: jdbc:mysql://localhost:3306/hive?characterEncoding=UTF-8</span><br><span class="line">Metastore Connection Driver : com.mysql.jdbc.Driver</span><br><span class="line">Metastore connection User: hadoop</span><br><span class="line">org.apache.hadoop.hive.metastore.HiveMetaException: Failed to load driver</span><br><span class="line">Underlying cause: java.lang.ClassNotFoundException : com.mysql.jdbc.Driver</span><br><span class="line">Use --verbose <span class="keyword">for</span> detailed stacktrace.</span><br><span class="line">*** schemaTool failed ***</span><br></pre></td></tr></table></figure><h3 id="错误原因"><a href="#错误原因" class="headerlink" title="错误原因"></a><strong>错误原因</strong></h3><p>在配置hive-site.xml文件时配置了mysql驱动，而hive/lib目录下没有mysql驱动包。</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a><strong>解决方案</strong></h3><p>官网下载mysql驱动下载地址 (<a href="https://dev.mysql.com/downloads/connector/j/">https://dev.mysql.com/downloads/connector/j/</a>)<br>把下载好的压缩包（mysql-connector-java-8.0.18.zip）进行解压<br>unzip mysql-connector-java-8.0.18.zip<br>复制到hive/lib下<br>cp mysql-connector-java-8.0.18/mysql-connector-java-8.0.18.jar hive/lib</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/categories/Apache-Hive/"/>
    
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/tags/Apache-Hive/"/>
    
  </entry>
  
  <entry>
    <title>Apache Hive环境搭建错误：java.lang.NoSuchMethodError: com.google.common...</title>
    <link href="https://yangyichao-mango.github.io/2019/10/17/apache-hive:error-NoSuchMethodError:com.google.common.base.Preconditions.checkArgument/"/>
    <id>https://yangyichao-mango.github.io/2019/10/17/apache-hive:error-NoSuchMethodError:com.google.common.base.Preconditions.checkArgument/</id>
    <published>2019-10-17T06:14:27.000Z</published>
    <updated>2019-10-19T04:52:31.858Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Hive环境搭建错误：java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument</p><span id="more"></span><h3 id="错误场景详情"><a href="#错误场景详情" class="headerlink" title="错误场景详情"></a><strong>错误场景详情</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="variable">$HIVE_HOME</span>/bin/schematool -dbType mysql -initSchema</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/usr/<span class="built_in">local</span>/Cellar/hive/3.1.2/libexec/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/usr/<span class="built_in">local</span>/Cellar/hadoop/3.2.1/libexec/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html<span class="comment">#multiple_bindings for an explanation.</span></span><br><span class="line">SLF4J: Actual binding is of <span class="built_in">type</span> [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">&quot;main&quot;</span> java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V</span><br><span class="line">at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)</span><br><span class="line">at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)</span><br><span class="line">at org.apache.hadoop.mapred.JobConf.setJar(JobConf.java:536)</span><br><span class="line">at org.apache.hadoop.mapred.JobConf.setJarByClass(JobConf.java:554)</span><br><span class="line">at org.apache.hadoop.mapred.JobConf.&lt;init&gt;(JobConf.java:448)</span><br><span class="line">at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:5141)</span><br><span class="line">at org.apache.hadoop.hive.conf.HiveConf.&lt;init&gt;(HiveConf.java:5104)</span><br><span class="line">at org.apache.hive.beeline.HiveSchemaTool.&lt;init&gt;(HiveSchemaTool.java:96)</span><br><span class="line">at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:1473)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class="line">at org.apache.hadoop.util.RunJar.run(RunJar.java:323)</span><br><span class="line">at org.apache.hadoop.util.RunJar.main(RunJar.java:236)</span><br></pre></td></tr></table></figure><h3 id="错误原因"><a href="#错误原因" class="headerlink" title="错误原因"></a><strong>错误原因</strong></h3><p>这是因为hive内依赖的guava.jar和hadoop内的版本不一致造成的。</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a><strong>解决方案</strong></h3><p>查看hadoop安装目录下share/hadoop/common/lib内guava.jar版本<br>查看hive安装目录下lib内guava.jar的版本，如果两者不一致，删除版本低的，并拷贝高版本的，问题解决！</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/categories/Apache-Hive/"/>
    
    
      <category term="Apache Hive" scheme="https://yangyichao-mango.github.io/tags/Apache-Hive/"/>
    
  </entry>
  
  <entry>
    <title>Apache Hadoop错误：Unable to load native-hadoop library for your platform</title>
    <link href="https://yangyichao-mango.github.io/2019/10/17/apache-hadoop:error-unable-to-load-native-hadoop-library-from-you-platform/"/>
    <id>https://yangyichao-mango.github.io/2019/10/17/apache-hadoop:error-unable-to-load-native-hadoop-library-from-you-platform/</id>
    <published>2019-10-17T03:34:57.000Z</published>
    <updated>2019-10-25T03:40:04.184Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Hadoop错误：Unable to load native-hadoop library for your platform</p><span id="more"></span><h3 id="错误场景详情"><a href="#错误场景详情" class="headerlink" title="错误场景详情"></a><strong>错误场景详情</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -ls /</span><br><span class="line">WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br></pre></td></tr></table></figure><h3 id="错误原因"><a href="#错误原因" class="headerlink" title="错误原因"></a><strong>错误原因</strong></h3><p>Hadoop是使用Java语言开发的,但是有一些需求和操作并不适合使用java所以会引入了本地库（Native Libraries）的概念，通过本地库，Hadoop可以更加高效地执行某一些操作.<br>当我们在linux 输入 hdoop fs -ls / 去查看 hdfs 文件系统上的资源时会出现下面错误</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a><strong>解决方案</strong></h3><h4 id="解决方案1"><a href="#解决方案1" class="headerlink" title="解决方案1"></a>解决方案1</h4><p>在Hadoop的配置文件core-site.xml中可以设置是否使用本地库：（Hadoop默认的配置为启用本地库）</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.native.lib<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Should native hadoop libraries, if present, be used.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="解决方案2"><a href="#解决方案2" class="headerlink" title="解决方案2"></a>解决方案2</h4><p>有博客说可以直接下载编译好的位包，替换原来的native包<br>由于在我本地安装的Apache Hadoop 3.2.1版本中没有找到lib文件夹，所以在3.2.1版本中暂时不能使用此种方法</p><p>执行查看文件命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -ls /</span><br><span class="line">2019-10-17 11:33:09,369 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - xxx supergroup          0 2019-10-17 11:23 /tmp</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/categories/Apache-Hadoop/"/>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/tags/Apache-Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Mac安装Apache Hadoop-3.2.1</title>
    <link href="https://yangyichao-mango.github.io/2019/10/17/apache-hadoop:3.2.1-mac-install/"/>
    <id>https://yangyichao-mango.github.io/2019/10/17/apache-hadoop:3.2.1-mac-install/</id>
    <published>2019-10-17T02:12:31.000Z</published>
    <updated>2019-10-25T08:40:28.142Z</updated>
    
    <content type="html"><![CDATA[<p>Mac安装Apache hadoop-3.2.1教程</p><span id="more"></span><h2 id="Java环境配置"><a href="#Java环境配置" class="headerlink" title="Java环境配置"></a><strong>Java环境配置</strong></h2><h3 id="安装Java，查看Java版本以测试是否安装成功"><a href="#安装Java，查看Java版本以测试是否安装成功" class="headerlink" title="安装Java，查看Java版本以测试是否安装成功"></a>安装Java，查看Java版本以测试是否安装成功</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ java -version</span><br><span class="line">java version <span class="string">&quot;1.8.0_191&quot;</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_191-b12)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode)</span><br></pre></td></tr></table></figure><h3 id="查看Java安装位置信息，之后配置Hadoop运行环境需要使用"><a href="#查看Java安装位置信息，之后配置Hadoop运行环境需要使用" class="headerlink" title="查看Java安装位置信息，之后配置Hadoop运行环境需要使用"></a>查看Java安装位置信息，之后配置Hadoop运行环境需要使用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/libexec/java_home</span><br><span class="line">/Library/Java/JavaVirtualMachines/jdk1.8.0_191.jdk/Contents/Home</span><br></pre></td></tr></table></figure><h2 id="ssh配置"><a href="#ssh配置" class="headerlink" title="ssh配置"></a><strong>ssh配置</strong></h2><h3 id="配置ssh"><a href="#配置ssh" class="headerlink" title="配置ssh"></a>配置ssh</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line">$ chmod 0600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><h3 id="创建ssh公钥"><a href="#创建ssh公钥" class="headerlink" title="创建ssh公钥"></a>创建ssh公钥</h3><p>如果没有ssh公钥，执行以下命令创建</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa</span><br></pre></td></tr></table></figure><h3 id="开启远程登录"><a href="#开启远程登录" class="headerlink" title="开启远程登录"></a>开启远程登录</h3><p><img src="/blog-img/apache-hadoop:3.2.1-mac-install/remote-login.png" alt="系统偏好设置-&gt;共享"></p><h3 id="测试远程登录是否开启"><a href="#测试远程登录是否开启" class="headerlink" title="测试远程登录是否开启"></a>测试远程登录是否开启</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh localhost</span><br></pre></td></tr></table></figure><h2 id="安装hadoop"><a href="#安装hadoop" class="headerlink" title="安装hadoop"></a><strong>安装hadoop</strong></h2><h3 id="brew安装hadoop"><a href="#brew安装hadoop" class="headerlink" title="brew安装hadoop"></a>brew安装hadoop</h3><p>brew安装的一般都是最新的hadoop，我这里是hadoop 3.2.1 <br>如果需要安装其他版本的hadoop，通过<a href="https://www.jianshu.com/p/aadb54eac0a8">brew安装指定版本的软件</a>进行安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ brew install hadoop</span><br><span class="line">Updating Homebrew...</span><br><span class="line">==&gt; Downloading https://www.apache.org/dyn/closer.cgi?path=hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz</span><br><span class="line">==&gt; Downloading from http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz</span><br><span class="line"><span class="comment">######################################################################## 100.0%</span></span><br><span class="line">🍺  /usr/<span class="built_in">local</span>/Cellar/hadoop/3.2.1: 21,686 files, 774.1MB, built <span class="keyword">in</span> 10 minutes 1 second</span><br></pre></td></tr></table></figure><p>注意上面的下载信息中 <br>默认brew是会从apache官方的镜像中下载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">==&gt; Downloading https://www.apache.org/dyn/closer.cgi?path=hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz</span><br></pre></td></tr></table></figure><p>如果下载很慢，可以配置国内镜像进行下载(<a href="https://mirror.tuna.tsinghua.edu.cn/help/homebrew/">清华大学开源软件镜像站</a>)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">==&gt; Downloading from http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz</span><br></pre></td></tr></table></figure><p>安装完之后查看hadoop安装位置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ brew info hadoop</span><br><span class="line">hadoop: stable 3.2.1</span><br><span class="line">Framework <span class="keyword">for</span> distributed processing of large data sets</span><br><span class="line">https://hadoop.apache.org/</span><br><span class="line">Conflicts with:</span><br><span class="line">  yarn (because both install `yarn` binaries)</span><br><span class="line">/usr/<span class="built_in">local</span>/Cellar/hadoop/hdfs (20 files, 1MB)</span><br><span class="line">  Built from <span class="built_in">source</span></span><br><span class="line">/usr/<span class="built_in">local</span>/Cellar/hadoop/3.2.1 (22,408 files, 815.8MB)</span><br><span class="line">  Built from <span class="built_in">source</span> on 2019-10-17 at 09:46:37</span><br><span class="line">From: https://github.com/Homebrew/homebrew-core/blob/master/Formula/hadoop.rb</span><br><span class="line">==&gt; Requirements</span><br><span class="line">Required: java &gt;= 1.8 ✔</span><br><span class="line">==&gt; Analytics</span><br><span class="line">install: 4,572 (30 days), 10,774 (90 days), 44,762 (365 days)</span><br><span class="line">install_on_request: 3,822 (30 days), 9,128 (90 days), 38,206 (365 days)</span><br><span class="line">build_error: 0 (30 days)</span><br></pre></td></tr></table></figure><h3 id="配置hadoop"><a href="#配置hadoop" class="headerlink" title="配置hadoop"></a>配置hadoop</h3><p>需要修改的配置文件都在<font color=red>/usr/local/Cellar/hadoop/3.2.1/libexec/etc/hadoop</font>这个目录下</p><h4 id="hadoop-env-sh"><a href="#hadoop-env-sh" class="headerlink" title="hadoop-env.sh"></a>hadoop-env.sh</h4><p>配置 export JAVA_HOME</p><p>将/usr/libexec/java_home查到的 Java 路径配置进去，记得去掉注释 #。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_191.jdk/Contents/Home</span><br></pre></td></tr></table></figure><h4 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h4><p>修改core-site.xml 文件参数,配置NameNode的主机名和端口号</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/Cellar/hadoop/hdfs/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>A base for other temporary directories<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h4><p>变量dfs.replication指定了每个HDFS数据库的复制次数。 通常为3, 由于我们只有一台主机和一个伪分布式模式的DataNode，将此值修改为1</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="格式化"><a href="#格式化" class="headerlink" title="格式化"></a>格式化</h4><p>格式化hdfs操作只要第一次才使用，否则会造成数据全部丢失</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hdfs namenode -format</span><br></pre></td></tr></table></figure><h3 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h3><p>启动服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./start-all.sh</span><br></pre></td></tr></table></figure><p>启动成功后，可以在<br><a href="http://localhost:9870/">http://localhost:9870/</a><br><a href="http://localhost:8088/cluster">http://localhost:8088/cluster</a><br>进行查看</p><p>关闭服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./stop-all.sh</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/categories/Apache-Hadoop/"/>
    
    
      <category term="Apache Hadoop" scheme="https://yangyichao-mango.github.io/tags/Apache-Hadoop/"/>
    
      <category term="Mac安装" scheme="https://yangyichao-mango.github.io/tags/Mac%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>useful-api-for-java</title>
    <link href="https://yangyichao-mango.github.io/2019/10/16/java-api:useful-api/"/>
    <id>https://yangyichao-mango.github.io/2019/10/16/java-api:useful-api/</id>
    <published>2019-10-16T02:16:10.000Z</published>
    <updated>2019-10-19T04:52:48.324Z</updated>
    
    <content type="html"><![CDATA[<h2 id="API"><a href="#API" class="headerlink" title="API"></a><strong>API</strong></h2><ul><li><a href="https://www.jianshu.com/p/865e9ae667a0">Retrofit（http请求工具包）</a></li><li><a href="https://www.jianshu.com/p/4271ebc40be8">Resilience4j（接口重试，限流，熔断器工具）</a></li></ul>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Java Api" scheme="https://yangyichao-mango.github.io/categories/Java-Api/"/>
    
    
      <category term="Java Api" scheme="https://yangyichao-mango.github.io/tags/Java-Api/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 零基础入门（四）：DataStream API 编程 学习心得</title>
    <link href="https://yangyichao-mango.github.io/2019/10/15/apache-flink:study-4-datastream-api/"/>
    <id>https://yangyichao-mango.github.io/2019/10/15/apache-flink:study-4-datastream-api/</id>
    <published>2019-10-15T07:57:16.000Z</published>
    <updated>2019-11-06T13:07:48.987Z</updated>
    
    <content type="html"><![CDATA[<p>学习心得</p><span id="more"></span><h2 id="DataStream"><a href="#DataStream" class="headerlink" title="DataStream"></a><strong>DataStream</strong></h2><h3 id="RichParallelSourceFunction"><a href="#RichParallelSourceFunction" class="headerlink" title="RichParallelSourceFunction"></a>RichParallelSourceFunction</h3><p>用户通过实现SourceFunction自定义DataSource</p><p>如果设置了并行度，则会产生指定并行度个数的DataSource消费客户端去消费DataSource</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment.setParallelism(int)</span><br></pre></td></tr></table></figure><p>举例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">GroupedProcessingTimeWindow</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOGGER = LoggerFactory.getLogger(GroupedProcessingTimeWindow.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">DataSource</span> <span class="keyword">extends</span> <span class="title">RichParallelSourceFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isRunning = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Tuple2&lt;String, Integer&gt;&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            Random random = <span class="keyword">new</span> Random();</span><br><span class="line">            <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">                TimeUnit.MILLISECONDS.sleep((getRuntimeContext().getIndexOfThisSubtask() + <span class="number">1</span>) * <span class="number">1000</span> * <span class="number">5</span>);</span><br><span class="line">                String key = <span class="string">&quot;类别&quot;</span> + (<span class="keyword">char</span>) (<span class="string">&#x27;A&#x27;</span> + random.nextInt(<span class="number">3</span>));</span><br><span class="line">                <span class="keyword">int</span> value = random.nextInt(<span class="number">10</span>) + <span class="number">1</span>;</span><br><span class="line">                LOGGER.info(<span class="string">&quot;Thread: &#123;&#125;, key: &#123;&#125;, value: &#123;&#125;, dataSource object: &#123;&#125;)&quot;</span></span><br><span class="line">                        , Thread.currentThread().getName()</span><br><span class="line">                        , key</span><br><span class="line">                        , value</span><br><span class="line">                        , <span class="keyword">this</span>);</span><br><span class="line">                ctx.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(key, value));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            isRunning = <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        DataSource dataSource = <span class="keyword">new</span> DataSource();</span><br><span class="line">        LOGGER.info(<span class="string">&quot;dataSource object: &#123;&#125;&quot;</span>, dataSource);</span><br><span class="line"></span><br><span class="line">        DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; ds = env.addSource(dataSource);</span><br><span class="line">        KeyedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple&gt; keyedStream = ds.keyBy(<span class="number">0</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// KeyedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple&gt; keyedStream = ds.keyBy(&quot;f0&quot;); 通过指定字段名 f0</span></span><br><span class="line"></span><br><span class="line">        keyedStream</span><br><span class="line">            .sum(<span class="number">1</span>)</span><br><span class="line">            <span class="comment">// .sum(&quot;f1&quot;) 通过制定字段名 f1</span></span><br><span class="line">            .keyBy((KeySelector&lt;Tuple2&lt;String, Integer&gt;, Object&gt;) stringIntegerTuple2 -&gt; StringUtils.EMPTY)</span><br><span class="line">            .fold(<span class="keyword">new</span> HashMap&lt;String, Integer&gt;(),</span><br><span class="line">                    <span class="keyword">new</span> FoldFunction&lt;Tuple2&lt;String, Integer&gt;, HashMap&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> HashMap&lt;String, Integer&gt; <span class="title">fold</span><span class="params">(HashMap&lt;String, Integer&gt; accumulator,</span></span></span><br><span class="line"><span class="function"><span class="params">                                Tuple2&lt;String, Integer&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                            accumulator.put(value.f0, value.f1);</span><br><span class="line">                            <span class="keyword">return</span> accumulator;</span><br><span class="line">                        &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            .addSink(<span class="keyword">new</span> SinkFunction&lt;HashMap&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(HashMap&lt;String, Integer&gt; value, Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    <span class="comment">// 每个类型的商品成交量</span></span><br><span class="line">                    LOGGER.info(<span class="string">&quot;&#123;&#125;&quot;</span></span><br><span class="line">                            , value);</span><br><span class="line">                    <span class="comment">// 商品成交总量</span></span><br><span class="line">                    LOGGER.info(<span class="string">&quot;&#123;&#125;&quot;</span></span><br><span class="line">                            , value.values().stream().mapToInt(v -&gt; v).sum());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过查看dataSource object:的log就会发现上面这个例子中国产生了3个DataSource实例。</p><h3 id="Evictor"><a href="#Evictor" class="headerlink" title="Evictor"></a>Evictor</h3><p>CountEvictor：保持窗口内元素数量符合用户指定数量，如果多于用户指定的数量，从窗口缓冲区的开头丢弃剩余的元素。<br>DeltaEvictor：使用 DeltaFunction和 一个阈值，计算窗口缓冲区中的最后一个元素与其余每个元素之间的 delta 值，并删除 delta 值大于或等于阈值的元素。<br>TimeEvictor：以毫秒为单位的时间间隔作为参数，对于给定的窗口，找到元素中的最大的时间戳max_ts，并删除时间戳小于max_ts - interval的所有元素。</p><h2 id="keyedStream"><a href="#keyedStream" class="headerlink" title="keyedStream"></a><strong>keyedStream</strong></h2><h3 id="KeyedStream-fold-R-initialValue-FoldFunction-lt-T-R-gt-folder"><a href="#KeyedStream-fold-R-initialValue-FoldFunction-lt-T-R-gt-folder" class="headerlink" title="KeyedStream.fold(R initialValue, FoldFunction&lt;T, R&gt; folder)"></a>KeyedStream.fold(R initialValue, FoldFunction&lt;T, R&gt; folder)</h3><p>添加一个合并key分组的算子，FoldFunction会接收到同一key的value，只有key相同的值才会被分发到同一个folder。</p><h2 id="可能出现的问题"><a href="#可能出现的问题" class="headerlink" title="可能出现的问题"></a><strong>可能出现的问题</strong></h2><h3 id="Apache-Flink-Return-type-of-function-could-not-be-determined-automatically-due-to-type-erasure"><a href="#Apache-Flink-Return-type-of-function-could-not-be-determined-automatically-due-to-type-erasure" class="headerlink" title="Apache Flink: Return type of function could not be determined automatically due to type erasure"></a><font color=red>Apache Flink: Return type of function could not be determined automatically due to type erasure</font></h3><p>错误场景：<br>在用户定义DAG图算子的时候，可能会出现不支持lambda表达式的情况</p><p>原因：<br>为了执行程序，Flink需要知道要处理的值的类型，因为它需要序列化和反序列化数据。<br>Flink的类型系统基于描述数据类型的TypeInformation进行序列化和反序列化，会将Java中的基本类型以及Object类型与TypeInformation进行映射。<br>当您指定一个函数时，Flink会尝试推断该函数的返回类型。<br>但是某些Lambda函数由于类型擦除而丢失了此信息（可以自己编译后再对编译成的.class文件进行反编译，然后查看函数签名，发现函数签名具体类型被擦除），<br>因此Flink无法通过此自动推断类型。<br><a href="https://flink.sojb.cn/dev/java_lambdas.html">Flink Java Lambda表达式</a></p><p>因此，必须显式声明返回类型。</p><p>解决方案1：用户自己定义返回类型</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; wordDataStream = dataStream.flatMap(</span><br><span class="line">    (String sentence, Collector&lt;String&gt; out) -&gt; &#123;</span><br><span class="line">        <span class="keyword">for</span>(String word: sentence.split(<span class="string">&quot;\\W+&quot;</span>)) &#123;</span><br><span class="line">            out.collect(word); <span class="comment">// collect objects of type String</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">).returns(Types.STRING);</span><br></pre></td></tr></table></figure><p>解决方案2：显示声明返回类型</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; wordDataStream = dataStream.flatMap(</span><br><span class="line">    <span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String sentence, Collector&lt;String&gt; out)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// normalize and split the line</span></span><br><span class="line">            String[] words = sentence.toLowerCase().split(<span class="string">&quot;\\W+&quot;</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// emit the pairs</span></span><br><span class="line">            <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                <span class="keyword">if</span> (word.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                    out.collect(word);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://yangyichao-mango.github.io/2019/10/14/hello-world/"/>
    <id>https://yangyichao-mango.github.io/2019/10/14/hello-world/</id>
    <published>2019-10-14T12:54:25.000Z</published>
    <updated>2019-10-19T11:37:10.532Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><span id="more"></span><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>生产实践 | 基于 Flink 的视频直播核心指标监控</title>
    <link href="https://yangyichao-mango.github.io/2019/09/01/wechat-blog/apache-flink:learning-files/"/>
    <id>https://yangyichao-mango.github.io/2019/09/01/wechat-blog/apache-flink:learning-files/</id>
    <published>2019-09-01T06:21:53.000Z</published>
    <updated>2021-04-04T11:13:04.155Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Flink-学习资料"><a href="#Flink-学习资料" class="headerlink" title="Flink 学习资料"></a>Flink 学习资料</h1><blockquote><p>Flink 系列官网文档还是最准确的。直接上<strong>视频</strong>和<strong>资料</strong>，懒癌患者直接拿走即可~</p></blockquote><h2 id="视频"><a href="#视频" class="headerlink" title="视频"></a>视频</h2><p><a href="https://space.bilibili.com/33807709。" title="b站链接">b站链接</a>，很多大厂的实战，讲解很详细，很适合初学者。</p><h3 id="资料"><a href="#资料" class="headerlink" title="资料"></a><a href="https://mp.weixin.qq.com/s/0Wa5rcO6DU8t8Gzy7zIv2Q。" title="Flink菜鸟">资料</a></h3><ul><li>链接：<a href="https://pan.baidu.com/s/1GzVJYpUxkucLS9ZikZWuRw、提取码：64n4">https://pan.baidu.com/s/1GzVJYpUxkucLS9ZikZWuRw、提取码：64n4</a></li><li>其他视频链接可点击公众号 [call me] 联系博主私聊获取</li></ul><h2 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h2><p>1.<a href="https://flink.apache.org/。" title="Flink 官网文档">Flink 官网文档</a>、<a href="https://flink.apache.org/zh/。" title="Flink 官网中文文档">Flink 官网中文文档</a></p><p>2.《Stream Processing with Apache Flink》 这本由 Flink PMC 写的 Flink 书籍也是很适合初学者，并且中文版已经出版了 ：《基于 Apache Flink 的流处理》，由 Flink committer 崔星灿大佬翻译</p><p>3.<a href="https://ververica.cn。" title="ververica 中文网">ververica 中文网</a>和 <a href="https://ververica.cn/developers/flink-training-course-basics。" title="Flink 钉钉群直播教程">Flink 钉钉群直播教程</a></p><p>4.微信公众号(由Flink 中文社区维护)：<strong>Flink 中文社区</strong> 以及 <strong>Flink</strong></p><p>5.<a href="https://ververica.cn/developers/special-issue。" title="Flink 知识图谱以及社区专刊">Flink 知识图谱以及社区专刊</a></p><p>6.加入 Apache Flink China社区 钉钉群（群号：23138101），很多大佬活跃</p><p>7.订阅 Flink user、user-zh 邮件列表，可在官网查看订阅方式，邮件列表中有很多解决方案，也有大佬们活跃解答</p><p>8.<a href="https://github.com/flink-china/flink-training-course/blob/master/README.md。" title="Flink 视频系列 github地址">Flink 视频系列 github地址</a></p><h3 id="oreilly-流模型介绍"><a href="#oreilly-流模型介绍" class="headerlink" title="oreilly 流模型介绍"></a>oreilly 流模型介绍</h3><ul><li><p><strong><a href="https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101。" title="Streaming 101: The world beyond batch">Streaming 101: The world beyond batch</a></strong></p></li><li><p><strong><a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102。" title="Streaming 102: The world beyond batch">Streaming 102: The world beyond batch</a></strong></p></li></ul><h2 id="Flink-源码"><a href="#Flink-源码" class="headerlink" title="Flink 源码"></a>Flink 源码</h2><ul><li><strong><a href="https://github.com/apache/flink。" title="github">github</a></strong></li><li><strong><a href="https://gitee.com/mirrors/apache-flink?_from=gitee_search。" title="gitee">gitee</a></strong></li></ul><h2 id="书籍社刊"><a href="#书籍社刊" class="headerlink" title="书籍社刊"></a>书籍社刊</h2><h3 id="书籍"><a href="#书籍" class="headerlink" title="书籍"></a>书籍</h3><p>《Stream Processing with Apache Flink》</p><p>《基于 Apache Flink 的流处理》</p><h3 id="社刊"><a href="#社刊" class="headerlink" title="社刊"></a>社刊</h3><p>源自 ververica，<a href="https://ververica.cn/developers/special-issue/。" title="社刊链接">社刊链接</a>，总共分为 4 期，可以直接在 ververica 官网下载。</p><h3 id="系列博客"><a href="#系列博客" class="headerlink" title="系列博客"></a>系列博客</h3><p>1.<a href="http://wuchong.me/tags/Flink%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0/。" title="云邪原理系列">云邪原理系列</a></p><p>2.<a href="https://yq.aliyun.com/users/ohyfzrwxmb3me?spm=a2c4e.11153940.0.0.1b994ae7llGnsY。" title="金竹漫谈系列">金竹漫谈系列</a></p><p>3.<a href="https://github.com/bethunebtj/flink_tutorial/blob/master/%E8%BF%BD%E6%BA%90%E7%B4%A2%E9%AA%A5%EF%BC%9A%E9%80%8F%E8%BF%87%E6%BA%90%E7%A0%81%E7%9C%8B%E6%87%82Flink%E6%A0%B8%E5%BF%83%E6%A1%86%E6%9E%B6%E7%9A%84%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B.pdf。" title="杨华的源码">杨华的源码</a></p><p>4.<a href="https://www.infoq.cn/article/fRt1RF1pxu_ZtmeObOoJ。" title="Apache Flink 零基础入门系列">Apache Flink 零基础入门系列</a></p><p>5.<a href="http://www.54tianzhisheng.cn/tags/Flink/。" title="zhisheng">zhisheng</a></p><p>6.<a href="https://www.aboutyun.com/forum.php?mod=forumdisplay&fid=443&filter=typeid&typeid=1393。" title="彻底明白Flink系统学习系列">彻底明白Flink系统学习系列</a></p><p>7.<a href="https://www.jianshu.com/p/0242f456f02a。" title="kangqi">kangqi</a></p><p>8.<a href="https://www.cnblogs.com/Springmoon-venn/category/1380180.html。" title="flink菜鸟">flink菜鸟</a></p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>实时开发需求确认模板</title>
    <link href="https://yangyichao-mango.github.io/2019/09/01/wechat-blog/apache-flink:realtime-demand-template/"/>
    <id>https://yangyichao-mango.github.io/2019/09/01/wechat-blog/apache-flink:realtime-demand-template/</id>
    <published>2019-09-01T06:21:53.000Z</published>
    <updated>2021-04-04T11:13:04.160Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实时开发需求确认模板"><a href="#实时开发需求确认模板" class="headerlink" title="实时开发需求确认模板"></a>实时开发需求确认模板</h1><blockquote><p>实时指标整个链路开发过程中的一些经验。</p></blockquote><h2 id="需求评估"><a href="#需求评估" class="headerlink" title="需求评估"></a>需求评估</h2><p>分析实时指标是否符合该需求场景，以及在该场景中实时指标能够发挥的价值。</p><h3 id="1-实时指标所能提供的分析能力"><a href="#1-实时指标所能提供的分析能力" class="headerlink" title="1.实时指标所能提供的分析能力"></a>1.实时指标所能提供的分析能力</h3><p>实时计算的输出内容，以及提供的分析能力：OLAP 分析，key-value 实时数据服务，维度填充，数据打标等。</p><h3 id="2-产出维度，指标的合理性"><a href="#2-产出维度，指标的合理性" class="headerlink" title="2.产出维度，指标的合理性"></a>2.产出维度，指标的合理性</h3><p>从需求出发，评估实时指标产出的维度和指标的合理性。</p><h3 id="3-需求可配置变量"><a href="#3-需求可配置变量" class="headerlink" title="3.需求可配置变量"></a>3.需求可配置变量</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">评估维度</th>            <th style="width: 20%; text-align: center;">评估指标</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">监控数据范围</td>            <td>监控数据范围动态配置改变？</td>        </tr>        <tr>            <td style="text-align: center;">监控数据起止时间</td>            <td>监控数据的起止时间动态配置改变？</td>        </tr>        <tr>            <td style="text-align: center;">监控数据的某些配置变量</td>            <td>当变量发生变动时，可能会对产出的实时数据有什么影响，对计算链路有什么影响，会决定实时计算链路的实现方式。</td>        </tr>    </tbody></table><h3 id="4-面向用户范围"><a href="#4-面向用户范围" class="headerlink" title="4.面向用户范围"></a>4.面向用户范围</h3><p>评估 SLA 等服务质量等级保障，以及提供的实时数据服务的可用性等级，并且提前和业务方确认可用性和出现故障时的恢复时间等问题。</p><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">评估维度</th>            <th style="width: 20%; text-align: center;">评估指标</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">时间语义</td>            <td>事件时间、处理时间？事件时间：可以通过获取数据的时间戳，使用处理时间来真实反映和还原事件，但是可能会出现数据条数过小窗口不能触发，或者在一些有截止日期的活动结束的最后一个窗口不能触发的问题；处理时间：处理时间一般和事件时间差距很小，经验值一般 diff 小于 1%，只能反映流式框架处理数据时的时间戳，但是不会出现上述事件时间的两个问题。因此需要评估需求的逻辑精确度是否要求很高？</td>        </tr>        <tr>            <td style="text-align: center;">数据一致性</td>            <td>至少一次、精确一次？至少一次：受限于目前的上下游以及依赖中间件的能力，比如 010 版本及以下的 Kafka 不支持两阶段提交，所以只能达到至少一次的语义；精确一次：整条实时计算链路中的所有组件都需要支持精确一次的语义（从技术层面或者业务逻辑层面达到精确一次）。评估需求逻辑是否可接受任务发生失败时有重复数据产生？</td>        </tr>        <tr>            <td style="text-align: center;">SLA 要求</td>            <td>评估需求的 SLA 要求，整条实时计算链路的 SLA 要求，产出数据最多延迟多长时间？数据准确率几个9？提供的接口服务是否需要考虑跨集群、机房备份、双写；是否需要建立多条计算链路以供故障切换？一旦发生故障，下游消费方能容忍的最大故障时长？下游消费方在发生故障时的兜底策略？</td>        </tr>    </tbody></table><h2 id="可行性评估"><a href="#可行性评估" class="headerlink" title="可行性评估"></a>可行性评估</h2><h3 id="1-技术可行性"><a href="#1-技术可行性" class="headerlink" title="1.技术可行性"></a>1.技术可行性</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">评估维度</th>            <th style="width: 20%; text-align: center;">评估指标</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">QPS</td>            <td>确定 QPS 以评估实时上下游以及依赖组件的选型以及能力。</td>        </tr>        <tr>            <td style="text-align: center;">数据输入</td>            <td>首先确定数据输入是否能够计算实时指标，然后评估上游提供的数据在计算实时指标时整个实时计算链路的逻辑以及复杂度。比如：是否需要用到双流 join，需要评估双流 join 所存在的误差是否在可接受范围内，一般可通过离线误差对比或经验值给出结论。常见输入中间件：消息队列，接口等，常用中间件：Kafka，rpc，http。</td>        </tr>        <tr>            <td style="text-align: center;">数据依赖</td>            <td>调研目前可用的哪些中间件可以提供能力来支持当前指标计算？举例：key-value等，常用依赖中间件：Redis。</td>        </tr>        <tr>            <td style="text-align: center;">数据输出</td>            <td>确定输出下游消费方的消费需求以及能力，以评估实时产出的数据以及存储组件是否能够满足其需求？常见输出中间件：消息队列，OLAP，key-value，接口等，常用中间件：Kafka，Druid，Redis，rpc。产出维度，一般场景下，维度值不建议是大数量级别的数据，比如说使用 user_id，device_id。</td>        </tr>    </tbody></table><h3 id="2-成本可行性"><a href="#2-成本可行性" class="headerlink" title="2.成本可行性"></a>2.成本可行性</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">评估维度</th>            <th style="width: 20%; text-align: center;">评估指标</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">QPS</td>            <td>确定 QPS 以评估实时上下游以及依赖中间件的资源消耗。确定资源消耗是否在可接受范围之内？</td>        </tr>        <tr>            <td style="text-align: center;">数据输入</td>            <td>确定整个实时计算链路的逻辑以及复杂度，来评估可能的资源消耗。</td>        </tr>        <tr>            <td style="text-align: center;">数据依赖</td>            <td>确定整个实时计算链路的逻辑以及复杂度，来评估可能的资源消耗。</td>        </tr>        <tr>            <td style="text-align: center;">数据输出</td>            <td>由输出内容以及存储组件来评估下游存储中间件的资源消耗。举例：维度值不建议是大数量级别的数据，比如说使用 user_id，device_id 作为维度或者产出明细数据，虽然使用 OLAP 在维度聚合场景下很灵活，但是这些场景下使用 OLAP 可能会造成很大的资源消耗。</td>        </tr>    </tbody></table><p>综合以上技术和成本可行性以及需求收益等指标，以评估实时指标的 ROI。</p><h3 id="3-数据输入"><a href="#3-数据输入" class="headerlink" title="3.数据输入"></a>3.数据输入</h3><h4 id="消息队列日志"><a href="#消息队列日志" class="headerlink" title="消息队列日志"></a>消息队列日志</h4><p>最常见的实时数据源就是消息队列日志，首先我们需要确定日志类型，不同的日志类型决定了指标或者维度字段是否可以产出以及其准确性，一般情况下有以下三种类型日志：</p><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">日志类型</th>            <th style="width: 20%; text-align: center;">备注</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">埋点日志</td>            <td>维度最全，数据准确</td>        </tr>        <tr>            <td style="text-align: center;">web server log</td>            <td>维度次全，数据准确度一般</td>        </tr>        <tr>            <td style="text-align: center;">binlog</td>            <td>数据库真实数据，反映的是真实数据，数据最准确，维度信息一般很少</td>        </tr>    </tbody></table><h4 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h4><p>这里的接口一般是用来根据需求圈定一批需要进行监控的数据内容。接口的提供方式可以是http，配置中心配置，rpc接口等。</p><h3 id="4-数据依赖"><a href="#4-数据依赖" class="headerlink" title="4.数据依赖"></a>4.数据依赖</h3><p>实时一般情况下都会或多或少依赖到外部组件，最常见的就是 key-value 存储。<br>场景：很常见的一类需求就是对数据源中的数据进行打标然后产出，这里的标签数据就会存储在 key-value 中间件中。<br>需要评估访问外存的 QPS，以及外存提供的能力。</p><h3 id="5-数据输出"><a href="#5-数据输出" class="headerlink" title="5.数据输出"></a>5.数据输出</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">输出组件</th>            <th style="width: 20%; text-align: center;">备注</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">消息队列</td>            <td>常见中间件：kafka等</td>        </tr>        <tr>            <td style="text-align: center;">key-value存储</td>            <td>常见中间件：Redis，HBase，服务化接口等</td>        </tr>        <tr>            <td style="text-align: center;">OLAP</td>            <td>常见中间件：Druid，ClickHouse等</td>        </tr>    </tbody></table><h2 id="技术方案评估"><a href="#技术方案评估" class="headerlink" title="技术方案评估"></a>技术方案评估</h2>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://yangyichao-mango.github.io/2019/09/01/wechat-blog/broadcast/"/>
    <id>https://yangyichao-mango.github.io/2019/09/01/wechat-blog/broadcast/</id>
    <published>2019-09-01T06:21:53.000Z</published>
    <updated>2021-04-04T11:13:17.180Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实时新增类指标标准化处理方案"><a href="#实时新增类指标标准化处理方案" class="headerlink" title="实时新增类指标标准化处理方案"></a>实时新增类指标标准化处理方案</h1><blockquote><p>实时指标整个链路开发过程中的一些经验。</p></blockquote><h2 id="实时新增类指标"><a href="#实时新增类指标" class="headerlink" title="实时新增类指标"></a>实时新增类指标</h2><p>大体上可以将实时新增类指标以以下两种维度进行分类。</p><h3 id="identity-id-类型维度"><a href="#identity-id-类型维度" class="headerlink" title="identity id 类型维度"></a>identity id 类型维度</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">identity id 类型</th>            <th style="width: 20%; text-align: center;">备注</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">number(long) 类型 identity id</td>            <td>数值类型 identity id 的好处在于可以使用 Bitmap 类组件做到精确去重。</td>        </tr>        <tr>            <td style="text-align: center;">字符类型 identity id</td>            <td>字符类型 identity id 去重相对复杂，有两种方式，在误差允许范围之内使用 BloomFilter 进行去重，或者使用 key-value 组件进行精确去重。</td>        </tr>    </tbody></table><h3 id="产出数据类型维度"><a href="#产出数据类型维度" class="headerlink" title="产出数据类型维度"></a>产出数据类型维度</h3><table>    <thead>        <tr>            <th style="width: 10%; text-align: center;">产出数据类型</th>            <th style="width: 20%; text-align: center;">备注</th>        </tr>    </thead>    <tbody>        <tr>            <td style="text-align: center;">明细类数据</td>            <td>此类数据一般是要求将新增的数据明细产出，uv 的含义是做过滤，产出的明细数据中的 identity id 不会有重复。输出明细数据的好处在于，我们可以在下游使用 OLAP 引擎对明细数据进行各种维度的聚合计算，从而很方便的产出不同维度下的 uv 数据。</td>        </tr>        <tr>            <td style="text-align: center;">聚合类数据</td>            <td>将一个时间窗口内的 uv 进行聚合，并且可以计算出分维度的 uv，其产出数据一般都是[维度 + uv_count]，但是这里的维度一般情况下是都是固定维度。如果需要拓展则需要改动源码。</td>        </tr>    </tbody></table><h2 id="计算链路"><a href="#计算链路" class="headerlink" title="计算链路"></a>计算链路</h2><p>因此新增产出的链路多数就是以上两种维度因子的相互组合。</p><h3 id="number-long-类型-identity-id"><a href="#number-long-类型-identity-id" class="headerlink" title="number(long) 类型 identity id"></a>number(long) 类型 identity id</h3><p><img src="new_uid_roaringbitmap.png" alt="使用 RoaringBitmap 的 uv 计算链路"></p><p>代码示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">RoaringBitmapDuplicateable</span>&lt;<span class="title">Model</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> DEFAULT_DUPLICATE_MILLS = <span class="number">24</span> * <span class="number">3600</span> * <span class="number">1000L</span>;</span><br><span class="line"></span><br><span class="line">    BiPredicate&lt;Long, Long&gt; ROARING_BIT_MAP_CLEAR_BI_PREDICATE =</span><br><span class="line">            (start, end) -&gt; end - start &gt;= DEFAULT_DUPLICATE_MILLS;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化</span></span><br><span class="line">    <span class="keyword">default</span> ValueState&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt; getBitMapValueState(String name) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.getRuntimeContext().getState(</span><br><span class="line">                <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(name, TypeInformation.of(</span><br><span class="line">                        <span class="keyword">new</span> TypeHint&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt;() &#123; &#125;))</span><br><span class="line">        );</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">RuntimeContext <span class="title">getRuntimeContext</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">getLongId</span><span class="params">(Model model)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">Optional&lt;Logger&gt; <span class="title">getLogger</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> BiPredicate&lt;Long, Long&gt; <span class="title">roaringBitMapClearBiPredicate</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> ROARING_BIT_MAP_CLEAR_BI_PREDICATE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> List&lt;Model&gt; <span class="title">duplicateAndGet</span><span class="params">(List&lt;Model&gt; models, <span class="keyword">long</span> windowStartTimestamp</span></span></span><br><span class="line"><span class="function"><span class="params">            , ValueState&lt;Tuple2&lt;Date, Roaring64NavigableMap&gt;&gt; bitMapValueState)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Tuple2&lt;Date, Roaring64NavigableMap&gt; bitMap = checkAndGetState(windowStartTimestamp, bitMapValueState);</span><br><span class="line"></span><br><span class="line">        Map&lt;Long, Model&gt; idModelsMap = models</span><br><span class="line">                .stream()</span><br><span class="line">                .collect(Collectors.toMap(<span class="keyword">this</span>::getLongId, Function.identity(), (oldOne, newOne) -&gt; oldOne));</span><br><span class="line"></span><br><span class="line">        Set&lt;Long&gt; ids = idModelsMap.keySet();</span><br><span class="line"></span><br><span class="line">        List&lt;Model&gt; newModels = Lists.newArrayList();</span><br><span class="line">        <span class="keyword">for</span> (Long id : ids) &#123;</span><br><span class="line">            <span class="keyword">if</span> (!bitMap.f1.contains(id)) &#123;</span><br><span class="line">                <span class="keyword">if</span> (idModelsMap.containsKey(id)) &#123;</span><br><span class="line">                    newModels.add(idModelsMap.get(id));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        newModels.stream()</span><br><span class="line">                .map(<span class="keyword">this</span>::getLongId)</span><br><span class="line">                .forEach(bitMap.f1::add);</span><br><span class="line">        bitMapValueState.update(bitMap);</span><br><span class="line">        <span class="keyword">return</span> newModels;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> <span class="keyword">long</span> <span class="title">duplicateAndCount</span><span class="params">(List&lt;Model&gt; models, <span class="keyword">long</span> windowStartTimestamp</span></span></span><br><span class="line"><span class="function"><span class="params">            , ValueState&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt; bitMapValueState)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Tuple2&lt;Long, Roaring64NavigableMap&gt; bitMap = checkAndGetState(windowStartTimestamp, bitMapValueState);</span><br><span class="line"></span><br><span class="line">        Set&lt;Long&gt; ids = models</span><br><span class="line">                .stream()</span><br><span class="line">                .map(<span class="keyword">this</span>::getLongId)</span><br><span class="line">                .collect(Collectors.toSet());</span><br><span class="line"></span><br><span class="line">        List&lt;Long&gt; newIds = Lists.newArrayList();</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (Long id : ids) &#123;</span><br><span class="line">            <span class="keyword">if</span> (!bitMap.f1.contains(id)) &#123;</span><br><span class="line">                newIds.add(id);</span><br><span class="line">                count++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        newIds.forEach(bitMap.f1::add);</span><br><span class="line">        bitMapValueState.update(bitMap);</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> Tuple2&lt;Long, Roaring64NavigableMap&gt; <span class="title">checkAndGetState</span><span class="params">(<span class="keyword">long</span> windowStartTimestamp</span></span></span><br><span class="line"><span class="function"><span class="params">            , ValueState&lt;Tuple2&lt;Long, Roaring64NavigableMap&gt;&gt; bitMapValueState)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Tuple2&lt;Long, Roaring64NavigableMap&gt; bitmap = bitMapValueState.value();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> == bitmap) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">this</span>.getLogger().ifPresent(logger -&gt;</span><br><span class="line">                    logger.info(<span class="string">&quot;New RoaringBitMapValueState Timestamp=&#123;&#125;&quot;</span>, windowStartTimestamp));</span><br><span class="line">            Tuple2&lt;Long, Roaring64NavigableMap&gt; newBitMap = Tuple2.of(windowStartTimestamp, <span class="keyword">new</span> Roaring64NavigableMap());</span><br><span class="line">            bitMapValueState.update(newBitMap);</span><br><span class="line">            <span class="keyword">return</span> newBitMap;</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="keyword">this</span>.roaringBitMapClearBiPredicate().test(bitmap.f0, windowStartTimestamp)) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">this</span>.getLogger().ifPresent(logger -&gt;</span><br><span class="line">                    logger.info(<span class="string">&quot;Clear RoaringBitMapValueState, from start=&#123;&#125; to end=&#123;&#125;&quot;</span>, bitmap.f0, windowStartTimestamp));</span><br><span class="line"></span><br><span class="line">            bitMapValueState.clear();</span><br><span class="line">            bitmap.f1.clear();</span><br><span class="line">            Tuple2&lt;Long, Roaring64NavigableMap&gt; newBitMap = Tuple2.of(windowStartTimestamp, <span class="keyword">new</span> Roaring64NavigableMap());</span><br><span class="line">            bitMapValueState.update(newBitMap);</span><br><span class="line">            <span class="keyword">return</span> newBitMap;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> bitmap;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="字符类型-identity-id"><a href="#字符类型-identity-id" class="headerlink" title="字符类型 identity id"></a>字符类型 identity id</h3><h4 id="使用-Flink-state"><a href="#使用-Flink-state" class="headerlink" title="使用 Flink state"></a>使用 Flink state</h4><p><img src="new_did_flink_state.png" alt="使用 flink state 的 uv 计算链路"></p><h4 id="使用-key-value-外存"><a href="#使用-key-value-外存" class="headerlink" title="使用 key-value 外存"></a>使用 key-value 外存</h4><p><img src="new_did_key_value.png" alt="使用 key-value 的 uv 计算链路"></p><p>如果选用的是 Redis 作为 key-value 过滤，那么这里会有一个巧用 Redis bit 特性的优化。举一个一般场景下的方案与使用 Redis bit 特性的方案做对比：</p><p>场景：假如需要同一天有几十场活动，并且都希望计算出这几十场活动的 uv，那么我们就可以按照下图设计 Redis bit 结构。</p><p>通常方案：</p><p><img src="new_did_redis.png" alt="使用 Redis 的 多 uv 指标计算链路"></p><p>这种场景下，如果有 1 亿用户，需要同时计算 50 个活动或者 50 个不同维度下的 uv。那么理论上最大 key 数量为 1 亿 * 50 = 50 亿个 key。</p><p>Redis bit 方案：</p><p><img src="new_did_redis_bit.png" alt="使用 Redis bit 特性的 多 uv 指标计算链路"></p><p>这样做的一个优点，就是这几十场活动的 uv 计算都使用了相同的 Redis key 来计算，可以大幅度减少 Redis 的容量占用。使用此方案的话，以上述相同的用户和活动场数，理论上最大<br>key 数量仅仅为 1 亿，只是 value 数量会多占几十个 bit。</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/categories/Apache-Druid/"/>
    
    
      <category term="Apache Druid" scheme="https://yangyichao-mango.github.io/tags/Apache-Druid/"/>
    
      <category term="Apache Zookeeper" scheme="https://yangyichao-mango.github.io/tags/Apache-Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>快手 Java 内推 - 实习场</title>
    <link href="https://yangyichao-mango.github.io/2019/09/01/wechat-blog/02_java/04_inc_java/"/>
    <id>https://yangyichao-mango.github.io/2019/09/01/wechat-blog/02_java/04_inc_java/</id>
    <published>2019-09-01T06:21:53.000Z</published>
    <updated>2021-07-11T05:52:19.421Z</updated>
    
    <content type="html"><![CDATA[<h1 id="快手-Java-内推-实习场"><a href="#快手-Java-内推-实习场" class="headerlink" title="快手 Java 内推 - 实习场"></a>快手 Java 内推 - 实习场</h1><blockquote><p>不仅限于以下 JD，hc 满满，有意可以直接加博主微信私聊。</p></blockquote><h1 id="Java开发实习生-【音视频技术部】"><a href="#Java开发实习生-【音视频技术部】" class="headerlink" title="Java开发实习生-【音视频技术部】"></a>Java开发实习生-【音视频技术部】</h1><h2 id="工作地点：北京"><a href="#工作地点：北京" class="headerlink" title="工作地点：北京"></a>工作地点：北京</h2><h2 id="职位描述"><a href="#职位描述" class="headerlink" title="职位描述"></a>职位描述</h2><ol><li>将快手音视频核心能力进行产品化封装，针对场景开发针对性解决方案，为各类应用提供一站式. 全链路覆盖的音视频能力，包括并不限于短视频. 直播. 音视频会议等；</li><li>接受高并发. 海量数据的挑战，分析和发现系统的优化点，负责推动系统的性能和可用性的提升；</li><li>跟进业内最新技术进展，优化业务音视频体验，构建一流的音视频云平台。<h2 id="任职要求"><a href="#任职要求" class="headerlink" title="任职要求"></a>任职要求</h2></li><li>计算机. 通信等相关专业本科以上学历，有NOI. ACM. TopCoder等相关竞赛经验者优先；</li><li>java基础扎实，理解io. 多线程. 集合等基础框架，对JVM原理有一定了解，熟悉面向对象设计开发；掌握Linux操作系统和大型数据库；熟悉容器化运维，熟悉Linux下的DevOps；</li><li>对用过的开源框架能了解它的原理和机制，如服务框架. RPC. service mesh. 服务注册中心. 定时任务. 动态配置. 服务治理. 应用容器等；熟悉微服务和领域设计；熟悉前后端分离的系统结构；</li><li>熟悉分布式系统的设计和应用，熟悉分布式. 缓存. 消息等机制，能对分布式常用技术进行合理应用和解决问题；</li><li>热爱技术，对代码质量和开发规范有近乎苛刻的要求，善于沟通与团队协作；了解业内技术的发展方向；</li><li>有音视频技术背景，有视频云相关开发经验者优先。</li></ol><h1 id="Java开发实习生-【用户画像】"><a href="#Java开发实习生-【用户画像】" class="headerlink" title="Java开发实习生-【用户画像】"></a>Java开发实习生-【用户画像】</h1><h2 id="工作地点：北京-1"><a href="#工作地点：北京-1" class="headerlink" title="工作地点：北京"></a>工作地点：北京</h2><h2 id="职位描述-1"><a href="#职位描述-1" class="headerlink" title="职位描述"></a>职位描述</h2><ol><li>负责快手画像平台及标签系统研发工作；通过敏捷开发支持产品需求快速迭代，不断优化系统架构，支撑业务规模增长，保障服务稳定；</li><li>对现有系统的不足进行分析，找到目前系统的瓶颈，改进提高系统性能；</li><li>参与解决大数据分布式处理. 高效查询. 数据一致性. 准确性等方面带来的各种技术难题和挑战。<h2 id="任职要求-1"><a href="#任职要求-1" class="headerlink" title="任职要求"></a>任职要求</h2></li><li>本科及以上学历，计算机. 软件相关专业优先；</li><li>熟悉Java，有扎实的计算机基础，对数据结构. 算法基础有深入理解；</li><li>熟悉面向对象的设计思想，了解软件开发流程；</li><li>有实际项目经验或互联网公司实习经历者优先，有OI. ICPC等竞赛经验者优先；</li><li>热爱互联网，对互联网产品和技术有浓厚的兴趣，热衷于追求技术极致与创新；</li><li>具有良好的沟通能力和团队合作精神. 优秀的分析问题和解决问题的能力。</li></ol><h1 id="Java开发实习生-【电商生态】"><a href="#Java开发实习生-【电商生态】" class="headerlink" title="Java开发实习生-【电商生态】"></a>Java开发实习生-【电商生态】</h1><h2 id="工作地点：北京-2"><a href="#工作地点：北京-2" class="headerlink" title="工作地点：北京"></a>工作地点：北京</h2><h2 id="职位描述-2"><a href="#职位描述-2" class="headerlink" title="职位描述"></a>职位描述</h2><ol><li>参与快手电商c端需求开发，深入理解业务需求，撰写技术方案和系统设计，并完成相关代码的开发工作；</li><li>愿意接受多种差异化资源投放业务逻辑复杂性. 海量数据挑战. 服务高可用的挑战，推动系统可用性和可扩展性的提升；</li><li>为团队引入创新的技术. 创新的解决方案，用创新的思路解决问题。<h2 id="任职要求-2"><a href="#任职要求-2" class="headerlink" title="任职要求"></a>任职要求</h2></li><li>计算机科学或其他相关专业，本科及以上学历，每周至少4天，连续实习3个月以上； </li><li>精通多线程编程，熟悉JVM，熟悉常见的开源分布式中间件.缓存.消息队列等，有分布式系统设计相关经验，熟悉MySQL；</li><li>精通Spring MVC.Spring boot编程；</li><li>熟悉面向对象设计，有一定的系统架构设计能力。</li></ol><h1 id="JAVA开发实习生-【国内用户增长】"><a href="#JAVA开发实习生-【国内用户增长】" class="headerlink" title="JAVA开发实习生-【国内用户增长】"></a>JAVA开发实习生-【国内用户增长】</h1><h2 id="工作地点：北京-3"><a href="#工作地点：北京-3" class="headerlink" title="工作地点：北京"></a>工作地点：北京</h2><h2 id="职位描述-3"><a href="#职位描述-3" class="headerlink" title="职位描述"></a>职位描述</h2><ol><li>负责快手各内外部产品后端系统. 平台系统的研发工作，通过敏捷开发支持产品需求快速迭代，不断优化系统架构，支撑业务规模增长，保障服务稳定；</li><li>对现有系统的不足进行分析，找到目前系统的瓶颈，改进提高系统性能；</li><li>参与解决海量数据分布式处理. 高效查询. 数据一致性. 准确性等方面带来的各种技术难题和挑战。<h2 id="任职要求-3"><a href="#任职要求-3" class="headerlink" title="任职要求"></a>任职要求</h2></li><li>本科及以上学历，计算机. 软件相关专业优先；</li><li>熟悉Java，有扎实的计算机基础，对数据结构. 算法基础有深入理解；</li><li>熟悉面向对象的设计思想，了解软件开发流程；</li><li>有实际项目经验或互联网公司实习经历者优先，有OI. ICPC等竞赛经验者优先；</li><li>热爱互联网，对互联网产品和技术有浓厚的兴趣，热衷于追求技术极致与创新；</li><li>具有良好的沟通能力和团队合作精神. 优秀的分析问题和解决问题的能力。</li></ol><h1 id="Java开发实习生-【服务端效能】"><a href="#Java开发实习生-【服务端效能】" class="headerlink" title="Java开发实习生-【服务端效能】"></a>Java开发实习生-【服务端效能】</h1><h2 id="工作地点：北京-4"><a href="#工作地点：北京-4" class="headerlink" title="工作地点：北京"></a>工作地点：北京</h2><h2 id="职位描述-4"><a href="#职位描述-4" class="headerlink" title="职位描述"></a>职位描述</h2><ol><li>负责快手研发效能相关平台及工具的设计研发工作；</li><li>负责服务高可扩展性. 高可用性方向的提升和优化；</li><li>参与平台框架研究. 设计和实现，关键技术路径可行性研究及技术选型。<br>【此岗位需立刻入职，如暑期才能入职请投递暑期实习生的相关岗位】<h2 id="任职要求-4"><a href="#任职要求-4" class="headerlink" title="任职要求"></a>任职要求</h2></li><li>2022届本科及以上学历，计算机科学与技术.软件工程或相关专业方向；</li><li>熟悉Java，有扎实的计算机基础，对数据结构.算法基础有深入理解，有ACM等竞赛经验者优先；</li><li>熟悉面对对象的设计思想，了解软件开发流程，有实际项目经验或互联网公司实习经历者优先；</li><li>对技术充满热情，有较强的责任心和抗压能力，愿意接受千万级并发的大型分布式系统开发过程中面临的各种挑战；</li><li>有较好的沟通能力，能快速融入团队，有较强的学习能力，能快速掌握最前沿的技术。<br>【此岗位需立刻入职，如暑期才能入职请投递暑期实习生的相关岗位】</li></ol><h1 id="Java开发实习生-【商业化研发】"><a href="#Java开发实习生-【商业化研发】" class="headerlink" title="Java开发实习生- 【商业化研发】"></a>Java开发实习生- 【商业化研发】</h1><h2 id="工作地点：北京-5"><a href="#工作地点：北京-5" class="headerlink" title="工作地点：北京"></a>工作地点：北京</h2><h2 id="职位描述-5"><a href="#职位描述-5" class="headerlink" title="职位描述"></a>职位描述</h2><ol><li>参与聚星（原快接单）内容营销撮合交易平台产品后台服务的设计. 开发. 优化等研发工作，保证产品的质量和开发进度；</li><li>和产品等团队合作，确保前后端模块的协同工作；</li><li>参与研究新兴技术，对产品进行持续优化。<h2 id="任职要求-5"><a href="#任职要求-5" class="headerlink" title="任职要求"></a>任职要求</h2></li><li>岗位面向2022届毕业生：本科及以上学历，计算机相关专业，每周实习3天及以上. 可实习4个月以上者优先；</li><li>熟悉面向对象编程，掌握Java/C++/Python/PHP中的至少一门语言；</li><li>学习能力强. 有独立解决问题的能力；</li><li>有良好的沟通能力和业务理解能力。</li></ol><h1 id="【暑期实习】Java开发实习生-海外业务"><a href="#【暑期实习】Java开发实习生-海外业务" class="headerlink" title="【暑期实习】Java开发实习生-海外业务"></a>【暑期实习】Java开发实习生-海外业务</h1><h2 id="工作地点：北京-6"><a href="#工作地点：北京-6" class="headerlink" title="工作地点：北京"></a>工作地点：北京</h2><ol><li>负责海外快速增长的产品需求，深入发掘和分析业务需求，撰写技术方案和系统设计，以及相关的代码开发；</li><li>迎接高并发. 海量数据的挑战，分析和发现系统的优化点，负责推动系统的性能和可用性的提升； </li><li>积极尝试新技术. 新方案，拥抱技术革新，用创新的思路解决问题。<h2 id="任职要求-6"><a href="#任职要求-6" class="headerlink" title="任职要求"></a>任职要求</h2></li><li>2022届计算机. 通信等相关专业本科及以上学历；</li><li>熟悉Java，有扎实的计算机基础，对数据结构. 算法基础有深入理解；</li><li>熟悉面向对象的设计思想，了解软件开发流程；</li><li>有实际项目经验或互联网公司实习经历者优先，有OI. ICPC等竞赛经验者优先；</li><li>热爱互联网，对互联网产品和技术有浓厚的兴趣，热衷于追求技术极致与创新；</li><li>具有良好的沟通能力和团队合作精神. 优秀的分析问题和解决问题的能力。</li></ol><h1 id="【暑期实习】Java开发实习生（iHR）-【效率工程】"><a href="#【暑期实习】Java开发实习生（iHR）-【效率工程】" class="headerlink" title="【暑期实习】Java开发实习生（iHR）-【效率工程】"></a>【暑期实习】Java开发实习生（iHR）-【效率工程】</h1><h2 id="工作地点：北京-7"><a href="#工作地点：北京-7" class="headerlink" title="工作地点：北京"></a>工作地点：北京</h2><p>负责快手相关产品服务器端的研发工作，通过敏捷开发支持产品需求快速迭代，不断优化系统架构，支撑业务规模增长，保障服务稳定。</p><h2 id="任职要求-7"><a href="#任职要求-7" class="headerlink" title="任职要求"></a>任职要求</h2><ol><li>2021届. 2022届本科及以上学历，计算机科学与技术.软件工程或相关专业方向；</li><li>熟悉Java，有扎实的计算机基础，对数据结构.算法基础有深入理解，有ACM等竞赛经验者优先；</li><li>熟悉面对对象的设计思想，了解软件开发流程，有实际项目经验或互联网公司实习经历者优先；</li><li>对技术充满热情，有较强的责任心和抗压能力，愿意接受千万级并发的大型分布式系统开发过程中面临的各种挑战；</li><li>有较好的沟通能力，能快速融入团队，有较强的学习能力，能快速掌握最前沿的技术。</li></ol>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>快手大数据内推</title>
    <link href="https://yangyichao-mango.github.io/2019/09/01/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/05_%E6%95%B0%E6%8D%AE%E5%9B%A2%E9%98%9F%E5%BB%BA%E8%AE%BE/01_%E6%8B%9B%E8%81%98/01_inc_de/"/>
    <id>https://yangyichao-mango.github.io/2019/09/01/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/05_%E6%95%B0%E6%8D%AE%E5%9B%A2%E9%98%9F%E5%BB%BA%E8%AE%BE/01_%E6%8B%9B%E8%81%98/01_inc_de/</id>
    <published>2019-09-01T06:21:53.000Z</published>
    <updated>2021-07-08T15:00:28.239Z</updated>
    
    <content type="html"><![CDATA[<h1 id="快手大数据-DE-内推"><a href="#快手大数据-DE-内推" class="headerlink" title="快手大数据 DE 内推"></a>快手大数据 DE 内推</h1><blockquote><p>不仅限于以下 JD，hc 满满，有意可以直接加博主微信私聊。</p></blockquote><h1 id="（大数据专场）数据研发工程师-专家-【数据资产】"><a href="#（大数据专场）数据研发工程师-专家-【数据资产】" class="headerlink" title="（大数据专场）数据研发工程师/专家-【数据资产】"></a>（大数据专场）数据研发工程师/专家-【数据资产】</h1><h2 id="工作地点：北京"><a href="#工作地点：北京" class="headerlink" title="工作地点：北京"></a>工作地点：北京</h2><h2 id="职位描述"><a href="#职位描述" class="headerlink" title="职位描述"></a>职位描述</h2><ol><li>建设全站的基础数据能力，提供丰富、稳定的短视频社区公共基础数据，探索更多数据能力的增量价值；</li><li>各类数据专题体系（如社交、内容生产/消费、直播等）的建设，通过数据+算法+产品，赋能业务，提供全链路、可分析、可复用的数据能力，提供更直观、更具分析指导性的产品化能力；</li><li>建设公司层面的核心数据资产，与业务场景深度结合，为社区服务提供数据服务化、数据业务化的数据&amp;产品解决方案；</li><li>建设全站数据治理和管理体系，结合业务+元数据+技术，保障公司各个业务服务的数据质量和产出稳定。<h2 id="任职要求"><a href="#任职要求" class="headerlink" title="任职要求"></a>任职要求</h2></li><li>较为丰富的数据仓库及数据平台架构经验，期望通过对业务的深入理解，进行数据仓库、数据体系和数据价值的建设和优化；</li><li>有从事分布式数据存储与计算平台应用开发经验，熟悉Hive，Kafka，Spark，Storm，Hbase，Flink 等相关技术并有相关开发经验；</li><li>有系统化的思维和工程化的能力，掌握JAVA和前端技术，有工程化落地的经验尤佳；</li><li>有较丰富的应用算法开发经验，对机器学习和AI有一定的了解；</li><li>3-5年及更长的经验均有需求。</li></ol><h1 id="（大数据专场）数据研发工程师-专家-【商业化】"><a href="#（大数据专场）数据研发工程师-专家-【商业化】" class="headerlink" title="（大数据专场）数据研发工程师/专家-【商业化】"></a>（大数据专场）数据研发工程师/专家-【商业化】</h1><h2 id="工作地点：北京-1"><a href="#工作地点：北京-1" class="headerlink" title="工作地点：北京"></a>工作地点：北京</h2><h2 id="职位描述-1"><a href="#职位描述-1" class="headerlink" title="职位描述"></a>职位描述</h2><ol><li>负责数据中台-商业化各个业务线数据仓库建设，构建商业化垂直数据集市；</li><li>定义并开发业务核心指标数据，负责垂直业务数据建模，如用户画像；</li><li>根据具体问题，设计并实现合适的可视化展示，构建数据持续观测平台；</li><li>参与数据平台的搭建，优化数据处理流程具体工作；</li><li>数据收集，反作弊数据仓库，用户数据仓库，UGC数据仓库，审核数据仓库的研发；</li><li>A/B测试实时ETL研发，转化漏斗分析平台研发。<h2 id="任职要求-1"><a href="#任职要求-1" class="headerlink" title="任职要求"></a>任职要求</h2></li><li>5-10年工作经验优先，3年以上也可看；</li><li>有Hive，Kafka，Spark，Storm，Hbase，Flink等两种以上两年以上使用经验；</li><li>熟悉数据仓库建设方法和ETL相关技术，对于数据的设计有自己的思考，具备优秀的数学思维和建模思维；</li><li>熟练使用SQL，对类SQL有过优化经验，对数据倾斜有深度的理解。了解特征工程常用方法；</li><li>具备扎实的编程功底，很强的学习、分析和解决问题能力，良好的团队意识和协作精神，有较强的内外沟通能力。</li></ol><h1 id="（大数据专场）数据研发工程师-专家-【国际化】"><a href="#（大数据专场）数据研发工程师-专家-【国际化】" class="headerlink" title="（大数据专场）数据研发工程师/专家-【国际化】"></a>（大数据专场）数据研发工程师/专家-【国际化】</h1><h2 id="工作地点：北京-2"><a href="#工作地点：北京-2" class="headerlink" title="工作地点：北京"></a>工作地点：北京</h2><h2 id="职位描述-2"><a href="#职位描述-2" class="headerlink" title="职位描述"></a>职位描述</h2><ol><li>建设全站的基础数据能力，提供丰富、稳定的短视频社区公共基础数据，探索更多数据能力的增量价值；</li><li>各类数据专题体系（如社交、内容生产/消费、直播等）的建设，通过数据+算法+产品，赋能业务，提供全链路、可分析、可复用的数据能力，提供更直观、更具分析指导性的产品化能力；</li><li>建设公司层面的核心数据资产，与业务场景深度结合，为社区服务提供数据服务化、数据业务化的数据&amp;产品解决方案；</li><li>建设全站数据治理和管理体系，结合业务+元数据+技术，保障公司各个业务服务的数据质量和产出稳定。<h2 id="任职要求-2"><a href="#任职要求-2" class="headerlink" title="任职要求"></a>任职要求</h2></li><li>较为丰富的数据仓库及数据支持业务经验，期望通过对业务的深入理解，进行数据仓库、数据体系和数据价值的建设和优化；</li><li>有从事分布式数据存储与计算平台应用开发经验，熟悉Hive，Kafka，Spark，Storm，Hbase，Flink 等相关技术并有相关开发经验；</li><li>熟练掌握数据质量的保障规范和执行步骤，擅长系统性的解决问题；</li><li>3-5年及更长的经验均有不同岗位。</li></ol><h1 id="（大数据专场）数据研发工程师-专家-【杭州】"><a href="#（大数据专场）数据研发工程师-专家-【杭州】" class="headerlink" title="（大数据专场）数据研发工程师/专家-【杭州】"></a>（大数据专场）数据研发工程师/专家-【杭州】</h1><h2 id="工作地点：杭州"><a href="#工作地点：杭州" class="headerlink" title="工作地点：杭州"></a>工作地点：杭州</h2><h2 id="职位描述-3"><a href="#职位描述-3" class="headerlink" title="职位描述"></a>职位描述</h2><ol><li>建设全站的基础数据能力，提供丰富、稳定的短视频社区公共基础数据，探索更多数据能力的增量价值；</li><li>各类数据专题体系（如社交、内容生产/消费、直播等）的建设，通过数据+算法+产品，赋能业务，提供全链路、可分析、可复用的数据能力，提供更直观、更具分析指导性的产品化能力；</li><li>建设公司层面的核心数据资产，与业务场景深度结合，为社区服务提供数据服务化、数据业务化的数据&amp;产品解决方案；</li><li>建设全站数据治理和管理体系，结合业务+元数据+技术，保障公司各个业务服务的数据质量和产出稳定。<h2 id="任职要求-3"><a href="#任职要求-3" class="headerlink" title="任职要求"></a>任职要求</h2></li><li>较为丰富的数据仓库及数据平台架构经验，期望通过对业务的深入理解，进行数据仓库、数据体系和数据价值的建设和优化；</li><li>有从事分布式数据存储与计算平台应用开发经验，熟悉Hive，Kafka，Spark，Storm，Hbase，Flink 等相关技术并有相关开发经验；</li><li>有系统化的思维和工程化的能力，掌握JAVA和前端技术，有工程化落地的经验尤佳；</li><li>有较丰富的应用算法开发经验，对机器学习和AI有一定的了解；</li><li>3-5年及更长的经验均有需求。</li></ol>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>快手大数据内推</title>
    <link href="https://yangyichao-mango.github.io/2019/09/01/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/05_%E6%95%B0%E6%8D%AE%E5%9B%A2%E9%98%9F%E5%BB%BA%E8%AE%BE/01_%E6%8B%9B%E8%81%98/02_inc_de_shixi/"/>
    <id>https://yangyichao-mango.github.io/2019/09/01/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/05_%E6%95%B0%E6%8D%AE%E5%9B%A2%E9%98%9F%E5%BB%BA%E8%AE%BE/01_%E6%8B%9B%E8%81%98/02_inc_de_shixi/</id>
    <published>2019-09-01T06:21:53.000Z</published>
    <updated>2021-07-08T15:01:18.183Z</updated>
    
    <content type="html"><![CDATA[<h1 id="快手大数据实习内推"><a href="#快手大数据实习内推" class="headerlink" title="快手大数据实习内推"></a>快手大数据实习内推</h1><blockquote><p>不仅限于以下 JD，hc 满满，有意可以直接加博主微信私聊。</p></blockquote><h1 id="（大数据专场）【暑期实习】数据研发实习生-数据平台"><a href="#（大数据专场）【暑期实习】数据研发实习生-数据平台" class="headerlink" title="（大数据专场）【暑期实习】数据研发实习生-数据平台"></a>（大数据专场）【暑期实习】数据研发实习生-数据平台</h1><h2 id="工作地点：北京"><a href="#工作地点：北京" class="headerlink" title="工作地点：北京"></a>工作地点：北京</h2><h2 id="职位描述"><a href="#职位描述" class="headerlink" title="职位描述"></a>职位描述</h2><ol><li>协助建设全站的基础数据能力，提供丰富、稳定的短视频社区公共基础数据，探索更多数据能力的增量价值；</li><li>各类数据专题体系（如社交、内容生产/消费、直播、增长等）的建设，通过数据+算法+产品，赋能业务，提供全链路、可分析、可复用的数据能力，提供更直观、更具分析指导性的产品化能力；</li><li>建设公司层面的核心数据资产，与业务场景深度结合，为社区服务提供数据服务化、数据业务化的数据&amp;产品解决方案。<h2 id="任职要求"><a href="#任职要求" class="headerlink" title="任职要求"></a>任职要求</h2></li><li>22届及以后毕业的本科或研究生，能够连续实习3个月及以上，每周能保证全勤者优先；</li><li>计算机相关专业，有一定Java代码功底；</li><li>熟悉Hadoop数据体系，擅长实时或离线计算的实施优化；</li><li>有数据仓库或者PB级数据处理经验的优先。</li></ol><h1 id="（大数据专场）数据研发实习生-【电商】"><a href="#（大数据专场）数据研发实习生-【电商】" class="headerlink" title="（大数据专场）数据研发实习生-【电商】"></a>（大数据专场）数据研发实习生-【电商】</h1><h2 id="工作地点：北京-1"><a href="#工作地点：北京-1" class="headerlink" title="工作地点：北京"></a>工作地点：北京</h2><h2 id="职位描述-1"><a href="#职位描述-1" class="headerlink" title="职位描述"></a>职位描述</h2><ol><li>紧密协助业务分析师或其他部门，用快速而有效的方式满足他们对数据的需求；</li><li>利用Hive，Spark等组件处理千亿级以上数据，支持电商业务的数据需求；</li><li>基于onedata的建模思路进行电商数仓的建模实践；</li><li>日常报表数据的开发和维护，保证自动报表的准时和有效； </li><li>承担其他电商大数据相关的开发和维护工作； </li><li>使用Flink建设秒级别响应实时处理链路，支持电商大促、营销等线上业务。<h2 id="任职要求-1"><a href="#任职要求-1" class="headerlink" title="任职要求"></a>任职要求</h2></li><li>计算机以及相关专业毕业；</li><li>熟练使用SQL，有相关项目经历；</li><li>了解 Hadoop 生态系统，掌握 Hive／Spark / Flink/HBase等一种数据开发技术优先； </li><li>了解大型数据仓库架构、模型设计、ETL等。</li></ol>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>快手大数据内推</title>
    <link href="https://yangyichao-mango.github.io/2019/09/01/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/05_%E6%95%B0%E6%8D%AE%E5%9B%A2%E9%98%9F%E5%BB%BA%E8%AE%BE/01_%E6%8B%9B%E8%81%98/03_inc_dpm/"/>
    <id>https://yangyichao-mango.github.io/2019/09/01/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/05_%E6%95%B0%E6%8D%AE%E5%9B%A2%E9%98%9F%E5%BB%BA%E8%AE%BE/01_%E6%8B%9B%E8%81%98/03_inc_dpm/</id>
    <published>2019-09-01T06:21:53.000Z</published>
    <updated>2021-07-08T15:01:58.230Z</updated>
    
    <content type="html"><![CDATA[<h1 id="快手大数据-DPM-内推"><a href="#快手大数据-DPM-内推" class="headerlink" title="快手大数据 DPM 内推"></a>快手大数据 DPM 内推</h1><blockquote><p>不仅限于以下 JD，hc 满满，有意可以直接加博主微信私聊。</p></blockquote><h1 id="（大数据专场）数据产品经理-【数据内容】"><a href="#（大数据专场）数据产品经理-【数据内容】" class="headerlink" title="（大数据专场）数据产品经理-【数据内容】"></a>（大数据专场）数据产品经理-【数据内容】</h1><h2 id="工作地点：北京"><a href="#工作地点：北京" class="headerlink" title="工作地点：北京"></a>工作地点：北京</h2><h2 id="职位描述"><a href="#职位描述" class="headerlink" title="职位描述"></a>职位描述</h2><ol><li>主动深入业务，理解业务运作逻辑，提炼或者对接业务数据需求，协调业务方和数据研发，通过工具、流程让数据供应做到及时、准确；</li><li>通过业务数据需求，抽象库表需求，辅助数据开发进行数据仓库模型设计，库表验收并及时把库表同步给业务方，数据字典建设与维护；</li><li>对接业务埋点需求，跟进埋点全流程，交付结果，推进埋点质量相关建设，辅助优化数据采集和埋点方案；</li><li>负责收集和挖掘各业务线的数据需求，协调数据分析师和数据开发团队完成需求，根据业务需要协助制定数据运营策略，规划和完善数据平台；</li><li>抽象数据分析需求，形成数据产品，通过数据描绘出整个业务流程、用户行为和重点环节的转化，帮助业务方更深刻的理解、使用和运营数据。<h2 id="任职要求"><a href="#任职要求" class="headerlink" title="任职要求"></a>任职要求</h2></li><li>本科及以上学历，3年以上产品、分析或咨询工作经验，统计、数学、计算机相关专业优先；</li><li>良好的数据敏感度，了解大数据相关基础知识，熟悉常见的数据分析和处理方法；</li><li>熟练使用SQL，熟悉数据仓库，能够从业务需求抽象库表，熟悉业内埋点方案的使用场景，参与制定过产品埋点方案及规范设计；</li><li>能深刻理解业务，对数据分析、用户增长、数据化运营有一定了解，与业务部门数据需对接设计过业务型数据产品相关工作优先；</li><li>自我驱动，有Owner意识，交付能力强，面对复杂情况能够独立工作，结果导向，跨团队与部门的沟通能力强，有较强的团队协作意识和能力。</li></ol><h1 id="（大数据专场）数据产品经理-【数据质量监控】"><a href="#（大数据专场）数据产品经理-【数据质量监控】" class="headerlink" title="（大数据专场）数据产品经理-【数据质量监控】"></a>（大数据专场）数据产品经理-【数据质量监控】</h1><h2 id="工作地点：北京-1"><a href="#工作地点：北京-1" class="headerlink" title="工作地点：北京"></a>工作地点：北京</h2><h2 id="职位描述-1"><a href="#职位描述-1" class="headerlink" title="职位描述"></a>职位描述</h2><ol><li>设计并落地数据全链路监控平台，实现端到端监控报警，通过工具协助数据开发快速发现及定位数据问题；</li><li>负责监控体系搭建，与关键用户合作，构建质量指标体系，提升组织质量管理效率；</li><li>负责产品需求文档撰写、产品交互设计，协助研发团队理解和实现用户需求，对产品的用户体验负责；</li><li>跟踪和分析国内外同行的发展动态，保证产品竞争力；</li><li>把控产品研发项目进度和风险，推动协调资源和外部合作。<h2 id="任职要求-1"><a href="#任职要求-1" class="headerlink" title="任职要求"></a>任职要求</h2></li><li>5年以上互联网数据产品经验，有数据质量监控产品的相关经验（必备条件）；</li><li>对于Hadoop生态相关的技术有一定的了解，了解离线数据开发以及流式数据开发；</li><li>具备良好的用户思维，能够从用户角度出发思考问题；</li><li>积极主动，具备良好的沟通能力、抗压能力和团队合作能力；</li><li>有数据开发经验者优先。</li></ol>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
  <entry>
    <title>实时数开吊打面试官系列</title>
    <link href="https://yangyichao-mango.github.io/2019/09/01/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/05_%E6%95%B0%E6%8D%AE%E5%9B%A2%E9%98%9F%E5%BB%BA%E8%AE%BE/02_%E9%9D%A2%E8%AF%95/01_interview_de/"/>
    <id>https://yangyichao-mango.github.io/2019/09/01/wechat-blog/01_%E5%A4%A7%E6%95%B0%E6%8D%AE/01_%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/01_%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/05_%E6%95%B0%E6%8D%AE%E5%9B%A2%E9%98%9F%E5%BB%BA%E8%AE%BE/02_%E9%9D%A2%E8%AF%95/01_interview_de/</id>
    <published>2019-09-01T06:21:53.000Z</published>
    <updated>2021-07-11T14:40:56.543Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实时数开吊打面试官系列"><a href="#实时数开吊打面试官系列" class="headerlink" title="实时数开吊打面试官系列"></a>实时数开吊打面试官系列</h1><h1 id="1-面试官会想考察候选人的什么能力？"><a href="#1-面试官会想考察候选人的什么能力？" class="headerlink" title="1.面试官会想考察候选人的什么能力？"></a>1.面试官会想考察候选人的什么能力？</h1><h1 id="2-面试官会围绕哪些方面考察候选人？"><a href="#2-面试官会围绕哪些方面考察候选人？" class="headerlink" title="2.面试官会围绕哪些方面考察候选人？"></a>2.面试官会围绕哪些方面考察候选人？</h1><p>做数据的就是建、管、用数三个方向考察</p>]]></content>
    
    <summary type="html">
    
      本站&amp;作者
    
    </summary>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/categories/Apache-Flink/"/>
    
    
      <category term="Apache Flink" scheme="https://yangyichao-mango.github.io/tags/Apache-Flink/"/>
    
  </entry>
  
</feed>
